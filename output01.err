Importing data files...
['mm1d_1.csv', 'mm1d_4.csv', 'mm1d_0.csv', 'mm1d_2.csv', 'mm1d_3.csv', 'mm1d_5.csv']
Splitting data into training and test sets with a ratio of: 0.2
Total number of training samples is 38400
Total number of test samples is 9600
Length of an output spectrum is 300
Generating torch datasets
training using gradient ascent
Starting training process
Doing Evaluation on the model now
This is Epoch 0, training loss 7.05597, validation loss 4.10093
Resetting learning rate to 0.01000
Doing Evaluation on the model now
This is Epoch 10, training loss 0.20307, validation loss 0.16680
Epoch    28: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 20, training loss 0.17017, validation loss 0.22099
Doing Evaluation on the model now
This is Epoch 30, training loss 0.07458, validation loss 0.06058
Epoch    62: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 40, training loss 0.11889, validation loss 0.07858
Epoch    94: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 50, training loss 0.06270, validation loss 0.05740
Doing Evaluation on the model now
This is Epoch 60, training loss 0.04677, validation loss 0.04484
Epoch   124: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 62
Mean train loss for ascent epoch 63: -0.00015424270532093942
Mean eval for ascent epoch 63: 0.05479724332690239
Epoch   138: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 70, training loss 0.73380, validation loss 0.80367
Epoch   149: reducing learning rate of group 0 to 2.5000e-03.
Epoch   160: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 80, training loss 0.21336, validation loss 0.09545
Epoch   171: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 90, training loss 0.09712, validation loss 0.07997
Epoch   182: reducing learning rate of group 0 to 3.1250e-04.
Epoch   193: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 100, training loss 0.05440, validation loss 0.03957
Epoch   204: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 102
Mean train loss for ascent epoch 103: -0.00016520422650501132
Mean eval for ascent epoch 103: 0.03473877161741257
Epoch   217: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 110, training loss 0.30946, validation loss 0.14959
Epoch   228: reducing learning rate of group 0 to 2.5000e-03.
Epoch   239: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 120, training loss 0.25134, validation loss 0.21578
Epoch   250: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 130, training loss 0.06873, validation loss 0.04614
Epoch   261: reducing learning rate of group 0 to 3.1250e-04.
Epoch   272: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 140, training loss 0.02771, validation loss 0.02269
Epoch   283: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 142
Mean train loss for ascent epoch 143: -0.00015807466115802526
Mean eval for ascent epoch 143: 0.026278799399733543
Epoch   294: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 150, training loss 0.19397, validation loss 0.55508
Epoch   305: reducing learning rate of group 0 to 2.5000e-03.
Epoch   316: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 160, training loss 0.16307, validation loss 0.27446
Epoch   327: reducing learning rate of group 0 to 6.2500e-04.
Epoch   338: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 170, training loss 0.06181, validation loss 0.03361
Epoch   349: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 180, training loss 0.02057, validation loss 0.02364
Epoch   360: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 181
Mean train loss for ascent epoch 182: -0.00014221909805200994
Mean eval for ascent epoch 182: 0.022698741406202316
Epoch   371: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 190, training loss 0.28172, validation loss 0.62257
Epoch   382: reducing learning rate of group 0 to 2.5000e-03.
Epoch   393: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 200, training loss 0.06563, validation loss 0.10818
Resetting learning rate to 0.01000
Epoch   404: reducing learning rate of group 0 to 5.0000e-04.
Epoch   415: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 210, training loss 0.03003, validation loss 0.02416
Epoch   426: reducing learning rate of group 0 to 1.2500e-04.
Epoch   437: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 220
Doing Evaluation on the model now
This is Epoch 220, training loss 0.01845, validation loss 0.01724
Mean train loss for ascent epoch 221: -0.0002006704016821459
Mean eval for ascent epoch 221: 0.021228309720754623
Epoch   450: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 230, training loss 0.47634, validation loss 0.73724
Epoch   461: reducing learning rate of group 0 to 2.5000e-03.
Epoch   472: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 240, training loss 0.04512, validation loss 0.03565
Epoch   483: reducing learning rate of group 0 to 6.2500e-04.
Epoch   494: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 250, training loss 0.02348, validation loss 0.01997
Epoch   505: reducing learning rate of group 0 to 1.5625e-04.
Epoch   516: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 260
Doing Evaluation on the model now
This is Epoch 260, training loss 0.01822, validation loss 0.02017
Mean train loss for ascent epoch 261: -0.00013796887651551515
Mean eval for ascent epoch 261: 0.018496906384825706
Epoch   527: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 270, training loss 0.30013, validation loss 0.18682
Epoch   538: reducing learning rate of group 0 to 2.5000e-03.
Epoch   549: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 280, training loss 0.08521, validation loss 0.10511
Epoch   560: reducing learning rate of group 0 to 6.2500e-04.
Epoch   571: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 290, training loss 0.01845, validation loss 0.01640
Epoch   582: reducing learning rate of group 0 to 1.5625e-04.
Epoch   593: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 299
Mean train loss for ascent epoch 300: -0.0001512582239229232
Mean eval for ascent epoch 300: 0.01955808699131012
Doing Evaluation on the model now
This is Epoch 300, training loss 0.01956, validation loss 0.01480
Epoch   604: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 310, training loss 0.15248, validation loss 0.23854
Epoch   615: reducing learning rate of group 0 to 2.5000e-03.
Epoch   626: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 320, training loss 0.05420, validation loss 0.06853
Epoch   637: reducing learning rate of group 0 to 6.2500e-04.
Epoch   648: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 330, training loss 0.03524, validation loss 0.02334
Epoch   659: reducing learning rate of group 0 to 1.5625e-04.
Epoch   670: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 338
Mean train loss for ascent epoch 339: -0.00012884067837148905
Mean eval for ascent epoch 339: 0.020430533215403557
Doing Evaluation on the model now
This is Epoch 340, training loss 0.36571, validation loss 0.48215
Epoch   681: reducing learning rate of group 0 to 5.0000e-03.
Epoch   692: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 350, training loss 0.12905, validation loss 0.32317
Epoch   703: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 360, training loss 0.10524, validation loss 0.05592
Epoch   714: reducing learning rate of group 0 to 6.2500e-04.
Epoch   725: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 370, training loss 0.02455, validation loss 0.02846
Epoch   736: reducing learning rate of group 0 to 1.5625e-04.
Epoch   747: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 377
Mean train loss for ascent epoch 378: -9.617901378078386e-05
Mean eval for ascent epoch 378: 0.02010400779545307
Doing Evaluation on the model now
This is Epoch 380, training loss 0.71350, validation loss 0.43925
Epoch   758: reducing learning rate of group 0 to 5.0000e-03.
Epoch   769: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 390, training loss 0.10087, validation loss 0.07864
Epoch   780: reducing learning rate of group 0 to 1.2500e-03.
Epoch   791: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 400, training loss 0.04721, validation loss 0.04466
Resetting learning rate to 0.01000
Epoch   802: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 410, training loss 0.02391, validation loss 0.01684
Epoch   813: reducing learning rate of group 0 to 2.5000e-04.
Epoch   824: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 420, training loss 0.01767, validation loss 0.01724
Epoch   835: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 422
Mean train loss for ascent epoch 423: -0.00012056880223099142
Mean eval for ascent epoch 423: 0.017947250977158546
Epoch   846: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 430, training loss 0.21037, validation loss 0.44433
Epoch   857: reducing learning rate of group 0 to 2.5000e-03.
Epoch   868: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 440, training loss 0.08496, validation loss 0.18729
Epoch   879: reducing learning rate of group 0 to 6.2500e-04.
Epoch   890: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 450, training loss 0.02048, validation loss 0.01448
Epoch   901: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 460, training loss 0.01491, validation loss 0.01667
Epoch   912: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 461
Mean train loss for ascent epoch 462: -0.00012332803453318775
Mean eval for ascent epoch 462: 0.015436409041285515
Epoch   923: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 470, training loss 0.22787, validation loss 0.50012
Epoch   934: reducing learning rate of group 0 to 2.5000e-03.
Epoch   945: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 480, training loss 0.03071, validation loss 0.03392
Epoch   956: reducing learning rate of group 0 to 6.2500e-04.
Epoch   967: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 490, training loss 0.02934, validation loss 0.02240
Epoch   978: reducing learning rate of group 0 to 1.5625e-04.
Epoch   989: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 500
Doing Evaluation on the model now
This is Epoch 500, training loss 0.01321, validation loss 0.01200
Mean train loss for ascent epoch 501: -0.00010019339970313013
Mean eval for ascent epoch 501: 0.0135337570682168
Epoch  1000: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 510, training loss 0.17139, validation loss 0.09583
Epoch  1011: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1022: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 520, training loss 0.06109, validation loss 0.16343
Epoch  1033: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1044: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 530, training loss 0.02041, validation loss 0.02101
Epoch  1055: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1066: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 539
Mean train loss for ascent epoch 540: -7.596972136525437e-05
Mean eval for ascent epoch 540: 0.014751501381397247
Doing Evaluation on the model now
This is Epoch 540, training loss 0.01475, validation loss 0.01417
Epoch  1077: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1088: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 550, training loss 0.30679, validation loss 0.11213
Epoch  1099: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 560, training loss 0.03859, validation loss 0.05206
Epoch  1110: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1121: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 570, training loss 0.01718, validation loss 0.01793
Epoch  1132: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1143: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 578
Mean train loss for ascent epoch 579: -0.00012329961464274675
Mean eval for ascent epoch 579: 0.013841730542480946
Doing Evaluation on the model now
This is Epoch 580, training loss 0.34063, validation loss 0.56103
Epoch  1154: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1165: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 590, training loss 0.07327, validation loss 0.08751
Epoch  1176: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1187: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 600, training loss 0.03162, validation loss 0.01904
Resetting learning rate to 0.01000
Epoch  1198: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 610, training loss 0.02424, validation loss 0.02797
Epoch  1209: reducing learning rate of group 0 to 2.5000e-04.
Epoch  1220: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 620, training loss 0.01405, validation loss 0.01533
Epoch  1231: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 622
Mean train loss for ascent epoch 623: -0.00010299938730895519
Mean eval for ascent epoch 623: 0.015227687545120716
Epoch  1242: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 630, training loss 0.12370, validation loss 0.22720
Epoch  1253: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1264: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 640, training loss 0.04843, validation loss 0.03536
Epoch  1275: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1286: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 650, training loss 0.01825, validation loss 0.01808
Epoch  1297: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 660, training loss 0.01366, validation loss 0.01664
Epoch  1308: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 661
Mean train loss for ascent epoch 662: -0.00010655329242581502
Mean eval for ascent epoch 662: 0.02229590155184269
Epoch  1319: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 670, training loss 0.39877, validation loss 0.48033
Epoch  1330: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1341: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 680, training loss 0.03387, validation loss 0.02307
Epoch  1352: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1363: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 690, training loss 0.02191, validation loss 0.01590
Epoch  1374: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1385: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 700
Doing Evaluation on the model now
This is Epoch 700, training loss 0.01323, validation loss 0.01388
Mean train loss for ascent epoch 701: -7.668293983442709e-05
Mean eval for ascent epoch 701: 0.013386568054556847
Epoch  1396: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 710, training loss 0.07514, validation loss 0.06679
Epoch  1407: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1418: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 720, training loss 0.04668, validation loss 0.06176
Epoch  1429: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1440: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 730, training loss 0.01457, validation loss 0.01491
Epoch  1451: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1462: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 739
Mean train loss for ascent epoch 740: -7.822665065759793e-05
Mean eval for ascent epoch 740: 0.013707236386835575
Doing Evaluation on the model now
This is Epoch 740, training loss 0.01371, validation loss 0.01342
Epoch  1473: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 750, training loss 0.13442, validation loss 0.06607
Epoch  1484: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1495: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 760, training loss 0.07840, validation loss 0.19927
Epoch  1506: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1517: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 770, training loss 0.01842, validation loss 0.01823
Epoch  1528: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1539: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 778
Mean train loss for ascent epoch 779: -0.00011541314597707242
Mean eval for ascent epoch 779: 0.016304735094308853
Doing Evaluation on the model now
This is Epoch 780, training loss 0.18963, validation loss 0.61743
Epoch  1550: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1561: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 790, training loss 0.09814, validation loss 0.16391
Epoch  1572: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 800, training loss 0.09275, validation loss 0.13027
Epoch  1583: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  1594: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 810, training loss 0.01842, validation loss 0.02098
Epoch  1605: reducing learning rate of group 0 to 2.5000e-04.
Epoch  1616: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 820, training loss 0.01341, validation loss 0.01175
Epoch  1627: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 823
Mean train loss for ascent epoch 824: -0.00011152670049341395
Mean eval for ascent epoch 824: 0.013596457429230213
Epoch  1638: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 830, training loss 0.20324, validation loss 0.27788
Epoch  1649: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1660: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 840, training loss 0.05129, validation loss 0.08670
Epoch  1671: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 850, training loss 0.01752, validation loss 0.01576
Epoch  1682: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1693: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 860, training loss 0.01324, validation loss 0.01225
Epoch  1704: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 862
Mean train loss for ascent epoch 863: -8.477544179186225e-05
Mean eval for ascent epoch 863: 0.012375486083328724
Epoch  1715: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 870, training loss 0.10710, validation loss 0.05979
Epoch  1726: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1737: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 880, training loss 0.02761, validation loss 0.01606
Epoch  1748: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1759: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 890, training loss 0.01685, validation loss 0.01589
Epoch  1770: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 900, training loss 0.01167, validation loss 0.01514
Epoch  1781: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 901
Mean train loss for ascent epoch 902: -8.789405546849594e-05
Mean eval for ascent epoch 902: 0.012432646937668324
Epoch  1792: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 910, training loss 0.33407, validation loss 0.17752
Epoch  1803: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1814: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 920, training loss 0.03341, validation loss 0.03758
Epoch  1825: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1836: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 930, training loss 0.01841, validation loss 0.01259
Epoch  1847: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1858: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 940
Doing Evaluation on the model now
This is Epoch 940, training loss 0.01347, validation loss 0.01298
Mean train loss for ascent epoch 941: -0.00010666281741578132
Mean eval for ascent epoch 941: 0.011469287797808647
Epoch  1869: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 950, training loss 0.11043, validation loss 0.20828
Epoch  1880: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1891: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 960, training loss 0.08407, validation loss 0.05485
Epoch  1902: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1913: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 970, training loss 0.01571, validation loss 0.01333
Epoch  1924: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1935: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 979
Mean train loss for ascent epoch 980: -6.351779302349314e-05
Mean eval for ascent epoch 980: 0.011172592639923096
Doing Evaluation on the model now
This is Epoch 980, training loss 0.01117, validation loss 0.01093
Epoch  1946: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1957: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 990, training loss 0.11746, validation loss 0.12416
Epoch  1968: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1000, training loss 0.06173, validation loss 0.03000
Resetting learning rate to 0.01000
Epoch  1979: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1990: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1010, training loss 0.01715, validation loss 0.01926
Epoch  2001: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2012: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1018
Mean train loss for ascent epoch 1019: -8.06733951321803e-05
Mean eval for ascent epoch 1019: 0.013351449742913246
Doing Evaluation on the model now
This is Epoch 1020, training loss 0.24235, validation loss 0.10997
Epoch  2023: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2034: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1030, training loss 0.15316, validation loss 0.06390
Epoch  2045: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2056: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1040, training loss 0.03383, validation loss 0.03971
Epoch  2067: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1050, training loss 0.01294, validation loss 0.01491
Epoch  2078: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2089: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1057
Mean train loss for ascent epoch 1058: -6.55154581181705e-05
Mean eval for ascent epoch 1058: 0.01044636219739914
Doing Evaluation on the model now
This is Epoch 1060, training loss 0.49725, validation loss 1.17462
Epoch  2100: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2111: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1070, training loss 0.03453, validation loss 0.03812
Epoch  2122: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2133: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1080, training loss 0.03353, validation loss 0.02986
Epoch  2144: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2155: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1090, training loss 0.01222, validation loss 0.01501
Epoch  2166: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1096
Mean train loss for ascent epoch 1097: -7.410458783851936e-05
Mean eval for ascent epoch 1097: 0.01136596966534853
Doing Evaluation on the model now
This is Epoch 1100, training loss 0.66278, validation loss 0.13673
Epoch  2177: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2188: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1110, training loss 0.07583, validation loss 0.07032
Epoch  2199: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2210: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1120, training loss 0.01607, validation loss 0.01682
Epoch  2221: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2232: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1130, training loss 0.01493, validation loss 0.01812
Epoch  2243: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1135
Mean train loss for ascent epoch 1136: -5.7166253100149333e-05
Mean eval for ascent epoch 1136: 0.011927071958780289
Doing Evaluation on the model now
This is Epoch 1140, training loss 0.41738, validation loss 0.89010
Epoch  2254: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2265: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1150, training loss 0.05627, validation loss 0.03635
Epoch  2276: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2287: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1160, training loss 0.01462, validation loss 0.01531
Epoch  2298: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2309: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1170, training loss 0.01166, validation loss 0.01248
Epoch  2320: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1174
Mean train loss for ascent epoch 1175: -9.368330211145803e-05
Mean eval for ascent epoch 1175: 0.012821372598409653
Epoch  2331: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1180, training loss 0.24080, validation loss 0.14776
Epoch  2342: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1190, training loss 0.04343, validation loss 0.09297
Epoch  2353: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2364: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1200, training loss 0.02827, validation loss 0.04169
Resetting learning rate to 0.01000
Epoch  2375: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2386: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1210, training loss 0.01052, validation loss 0.00991
Epoch  2397: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2408: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1218
Mean train loss for ascent epoch 1219: -6.49114153929986e-05
Mean eval for ascent epoch 1219: 0.010755720548331738
Doing Evaluation on the model now
This is Epoch 1220, training loss 0.15116, validation loss 0.23169
Epoch  2419: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2430: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1230, training loss 0.16109, validation loss 0.03771
Epoch  2441: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1240, training loss 0.05127, validation loss 0.03776
Epoch  2452: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2463: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1250, training loss 0.01371, validation loss 0.01679
Epoch  2474: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2485: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1257
Mean train loss for ascent epoch 1258: -9.428404155187309e-05
Mean eval for ascent epoch 1258: 0.011436077766120434
Doing Evaluation on the model now
This is Epoch 1260, training loss 0.25091, validation loss 0.28484
Epoch  2496: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2507: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1270, training loss 0.04615, validation loss 0.06311
Epoch  2518: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2529: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1280, training loss 0.01880, validation loss 0.01661
Epoch  2540: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1290, training loss 0.01248, validation loss 0.01064
Epoch  2551: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2562: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1296
Mean train loss for ascent epoch 1297: -8.001653623068705e-05
Mean eval for ascent epoch 1297: 0.010267399251461029
Doing Evaluation on the model now
This is Epoch 1300, training loss 0.96072, validation loss 0.52184
Epoch  2573: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2584: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1310, training loss 0.05142, validation loss 0.06604
Epoch  2595: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2606: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1320, training loss 0.03110, validation loss 0.04705
Epoch  2617: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2628: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1330, training loss 0.01688, validation loss 0.01510
Epoch  2639: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1335
Mean train loss for ascent epoch 1336: -6.394539377652109e-05
Mean eval for ascent epoch 1336: 0.014525266364216805
Doing Evaluation on the model now
This is Epoch 1340, training loss 0.45715, validation loss 0.28453
Epoch  2650: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2661: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1350, training loss 0.11830, validation loss 0.24475
Epoch  2672: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2683: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1360, training loss 0.02012, validation loss 0.01666
Epoch  2694: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2705: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1370, training loss 0.01407, validation loss 0.01418
Epoch  2716: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1374
Mean train loss for ascent epoch 1375: -7.832339179003611e-05
Mean eval for ascent epoch 1375: 0.013981635682284832
Epoch  2727: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1380, training loss 0.13281, validation loss 0.19091
Epoch  2738: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1390, training loss 0.04275, validation loss 0.05996
Epoch  2749: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2760: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1400, training loss 0.01297, validation loss 0.01222
Resetting learning rate to 0.01000
Epoch  2771: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2782: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1410, training loss 0.01201, validation loss 0.01082
Epoch  2793: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2804: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1419
Mean train loss for ascent epoch 1420: -7.087141420925036e-05
Mean eval for ascent epoch 1420: 0.01186115201562643
Doing Evaluation on the model now
This is Epoch 1420, training loss 0.01186, validation loss 0.01007
Epoch  2815: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2826: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1430, training loss 0.07478, validation loss 0.06308
Epoch  2837: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1440, training loss 0.02456, validation loss 0.01962
Epoch  2848: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2859: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1450, training loss 0.01240, validation loss 0.01173
Epoch  2870: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2881: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1458
Mean train loss for ascent epoch 1459: -7.695880049141124e-05
Mean eval for ascent epoch 1459: 0.011925994418561459
Doing Evaluation on the model now
This is Epoch 1460, training loss 0.12782, validation loss 0.44363
Epoch  2892: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2903: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1470, training loss 0.04298, validation loss 0.02062
Epoch  2914: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2925: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1480, training loss 0.01913, validation loss 0.01735
Epoch  2936: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1490, training loss 0.01247, validation loss 0.01361
Epoch  2947: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2958: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1497
Mean train loss for ascent epoch 1498: -6.31920775049366e-05
Mean eval for ascent epoch 1498: 0.01260372158139944
Doing Evaluation on the model now
This is Epoch 1500, training loss 0.18910, validation loss 0.17932
Epoch  2969: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2980: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1510, training loss 0.05510, validation loss 0.03054
Epoch  2991: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3002: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1520, training loss 0.02194, validation loss 0.02343
Epoch  3013: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3024: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1530, training loss 0.01170, validation loss 0.01084
Epoch  3035: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1536
Mean train loss for ascent epoch 1537: -4.9954920541495085e-05
Mean eval for ascent epoch 1537: 0.009766774252057076
Doing Evaluation on the model now
This is Epoch 1540, training loss 0.66824, validation loss 0.75007
Epoch  3046: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3057: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1550, training loss 0.08566, validation loss 0.05463
Epoch  3068: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3079: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1560, training loss 0.01542, validation loss 0.01499
Epoch  3090: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3101: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1570, training loss 0.01160, validation loss 0.01132
Epoch  3112: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1575
Mean train loss for ascent epoch 1576: -5.9354028053348884e-05
Mean eval for ascent epoch 1576: 0.011387302540242672
Doing Evaluation on the model now
This is Epoch 1580, training loss 0.49132, validation loss 0.22041
Epoch  3123: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3134: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1590, training loss 0.02885, validation loss 0.03611
Epoch  3145: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3156: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1600, training loss 0.01627, validation loss 0.02741
Resetting learning rate to 0.01000
Epoch  3167: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3178: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1610, training loss 0.01011, validation loss 0.00868
Epoch  3189: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3200: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1619
Mean train loss for ascent epoch 1620: -5.2779618272325024e-05
Mean eval for ascent epoch 1620: 0.009749310091137886
Doing Evaluation on the model now
This is Epoch 1620, training loss 0.00975, validation loss 0.00869
Epoch  3211: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1630, training loss 0.07403, validation loss 0.05984
Epoch  3222: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3233: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1640, training loss 0.04071, validation loss 0.02315
Epoch  3244: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3255: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1650, training loss 0.01163, validation loss 0.01626
Epoch  3266: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3277: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1658
Mean train loss for ascent epoch 1659: -3.772474155994132e-05
Mean eval for ascent epoch 1659: 0.009761719964444637
Doing Evaluation on the model now
This is Epoch 1660, training loss 0.13686, validation loss 0.11527
Epoch  3288: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3299: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1670, training loss 0.04268, validation loss 0.02715
Epoch  3310: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1680, training loss 0.04489, validation loss 0.05444
Epoch  3321: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3332: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1690, training loss 0.01031, validation loss 0.00936
Epoch  3343: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3354: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1697
Mean train loss for ascent epoch 1698: -6.811868661316112e-05
Mean eval for ascent epoch 1698: 0.010929122567176819
Doing Evaluation on the model now
This is Epoch 1700, training loss 0.21115, validation loss 0.30889
Epoch  3365: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3376: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1710, training loss 0.07034, validation loss 0.01964
Epoch  3387: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3398: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1720, training loss 0.01387, validation loss 0.01170
Epoch  3409: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1730, training loss 0.01206, validation loss 0.01028
Epoch  3420: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3431: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1736
Mean train loss for ascent epoch 1737: -8.540708950022236e-05
Mean eval for ascent epoch 1737: 0.012791958637535572
Doing Evaluation on the model now
This is Epoch 1740, training loss 0.24142, validation loss 0.10725
Epoch  3442: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3453: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1750, training loss 0.04116, validation loss 0.10529
Epoch  3464: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3475: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1760, training loss 0.01668, validation loss 0.03291
Epoch  3486: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3497: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1770, training loss 0.01023, validation loss 0.00915
Epoch  3508: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1775
Mean train loss for ascent epoch 1776: -5.416635030996986e-05
Mean eval for ascent epoch 1776: 0.010238807648420334
Doing Evaluation on the model now
This is Epoch 1780, training loss 0.23866, validation loss 0.08171
Epoch  3519: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3530: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1790, training loss 0.02860, validation loss 0.06119
Epoch  3541: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3552: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1800, training loss 0.01275, validation loss 0.01392
Resetting learning rate to 0.01000
Epoch  3563: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3574: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1810, training loss 0.00872, validation loss 0.00866
Epoch  3585: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3596: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1820
Doing Evaluation on the model now
This is Epoch 1820, training loss 0.00866, validation loss 0.00819
Mean train loss for ascent epoch 1821: -5.223376501817256e-05
Mean eval for ascent epoch 1821: 0.00894387811422348
Epoch  3607: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1830, training loss 0.05342, validation loss 0.02821
Epoch  3618: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3629: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1840, training loss 0.02338, validation loss 0.03399
Epoch  3640: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3651: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1850, training loss 0.01317, validation loss 0.01183
Epoch  3662: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3673: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1859
Mean train loss for ascent epoch 1860: -5.4055948567111045e-05
Mean eval for ascent epoch 1860: 0.009914851747453213
Doing Evaluation on the model now
This is Epoch 1860, training loss 0.00991, validation loss 0.00808
Epoch  3684: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3695: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1870, training loss 0.07017, validation loss 0.13978
Epoch  3706: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1880, training loss 0.03420, validation loss 0.02262
Epoch  3717: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3728: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1890, training loss 0.00937, validation loss 0.01132
Epoch  3739: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3750: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1898
Mean train loss for ascent epoch 1899: -7.424060459015891e-05
Mean eval for ascent epoch 1899: 0.009415695443749428
Doing Evaluation on the model now
This is Epoch 1900, training loss 0.10675, validation loss 0.10086
Epoch  3761: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3772: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1910, training loss 0.03607, validation loss 0.02488
Epoch  3783: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3794: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1920, training loss 0.03618, validation loss 0.01437
Epoch  3805: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1930, training loss 0.01084, validation loss 0.00900
Epoch  3816: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3827: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1937
Mean train loss for ascent epoch 1938: -4.869694748776965e-05
Mean eval for ascent epoch 1938: 0.009063231758773327
Doing Evaluation on the model now
This is Epoch 1940, training loss 0.18560, validation loss 0.22425
Epoch  3838: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3849: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1950, training loss 0.09491, validation loss 0.12711
Epoch  3860: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3871: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1960, training loss 0.01700, validation loss 0.01175
Epoch  3882: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3893: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1970, training loss 0.01185, validation loss 0.00984
Epoch  3904: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1976
Mean train loss for ascent epoch 1977: -6.126939842943102e-05
Mean eval for ascent epoch 1977: 0.011056169867515564
Doing Evaluation on the model now
This is Epoch 1980, training loss 0.16153, validation loss 0.04937
Epoch  3915: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3926: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1990, training loss 0.03605, validation loss 0.01873
Epoch  3937: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3948: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2000, training loss 0.01554, validation loss 0.01756
Resetting learning rate to 0.01000
Epoch  3959: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3970: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2010, training loss 0.01276, validation loss 0.01471
Epoch  3981: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3992: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2020
Doing Evaluation on the model now
This is Epoch 2020, training loss 0.00882, validation loss 0.00853
Mean train loss for ascent epoch 2021: -4.937333142152056e-05
Mean eval for ascent epoch 2021: 0.008882214315235615
Epoch  4003: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2030, training loss 0.09853, validation loss 0.04495
Epoch  4014: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4025: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2040, training loss 0.02086, validation loss 0.01153
Epoch  4036: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4047: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2050, training loss 0.01055, validation loss 0.01084
Epoch  4058: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4069: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2059
Mean train loss for ascent epoch 2060: -5.9243058785796165e-05
Mean eval for ascent epoch 2060: 0.008530845865607262
Doing Evaluation on the model now
This is Epoch 2060, training loss 0.00853, validation loss 0.00773
Epoch  4080: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2070, training loss 0.05750, validation loss 0.01867
Epoch  4091: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4102: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2080, training loss 0.01695, validation loss 0.01241
Epoch  4113: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4124: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2090, training loss 0.00895, validation loss 0.00878
Epoch  4135: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4146: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2098
Mean train loss for ascent epoch 2099: -5.7958171964855865e-05
Mean eval for ascent epoch 2099: 0.008729097433388233
Doing Evaluation on the model now
This is Epoch 2100, training loss 0.17204, validation loss 0.60505
Epoch  4157: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4168: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2110, training loss 0.03885, validation loss 0.01940
Epoch  4179: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2120, training loss 0.03032, validation loss 0.05623
Epoch  4190: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4201: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2130, training loss 0.00858, validation loss 0.01053
Epoch  4212: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4223: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2137
Mean train loss for ascent epoch 2138: -4.8387355491286144e-05
Mean eval for ascent epoch 2138: 0.0092520946636796
Doing Evaluation on the model now
This is Epoch 2140, training loss 0.19328, validation loss 0.28067
Epoch  4234: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4245: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2150, training loss 0.01897, validation loss 0.02005
Epoch  4256: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4267: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2160, training loss 0.01566, validation loss 0.01905
Epoch  4278: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2170, training loss 0.00909, validation loss 0.00834
Epoch  4289: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4300: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2176
Mean train loss for ascent epoch 2177: -4.685582825914025e-05
Mean eval for ascent epoch 2177: 0.008279494941234589
Doing Evaluation on the model now
This is Epoch 2180, training loss 0.40999, validation loss 0.21915
Epoch  4311: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4322: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2190, training loss 0.02533, validation loss 0.01742
Epoch  4333: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4344: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2200, training loss 0.00943, validation loss 0.01054
Resetting learning rate to 0.01000
Epoch  4355: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4366: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2210, training loss 0.00791, validation loss 0.00796
Epoch  4377: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2220, training loss 0.00758, validation loss 0.00821
Epoch  4388: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2221
Mean train loss for ascent epoch 2222: -4.513719250098802e-05
Mean eval for ascent epoch 2222: 0.007266560569405556
Epoch  4399: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2230, training loss 0.03185, validation loss 0.02801
Epoch  4410: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4421: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2240, training loss 0.01170, validation loss 0.01246
Epoch  4432: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4443: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2250, training loss 0.00678, validation loss 0.00689
Epoch  4454: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4465: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2260
Doing Evaluation on the model now
This is Epoch 2260, training loss 0.00690, validation loss 0.00599
Mean train loss for ascent epoch 2261: -4.048323899041861e-05
Mean eval for ascent epoch 2261: 0.006800658535212278
Epoch  4476: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2270, training loss 0.06364, validation loss 0.13881
Epoch  4487: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4498: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2280, training loss 0.02003, validation loss 0.01303
Epoch  4509: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4520: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2290, training loss 0.00705, validation loss 0.00687
Epoch  4531: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4542: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2299
Mean train loss for ascent epoch 2300: -4.655430529965088e-05
Mean eval for ascent epoch 2300: 0.006563456263393164
Doing Evaluation on the model now
This is Epoch 2300, training loss 0.00656, validation loss 0.00655
Epoch  4553: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4564: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2310, training loss 0.09036, validation loss 0.03111
Epoch  4575: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2320, training loss 0.01692, validation loss 0.01970
Epoch  4586: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4597: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2330, training loss 0.00685, validation loss 0.00788
Epoch  4608: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4619: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2338
Mean train loss for ascent epoch 2339: -3.829800334642641e-05
Mean eval for ascent epoch 2339: 0.007314739283174276
Doing Evaluation on the model now
This is Epoch 2340, training loss 0.11016, validation loss 0.05303
Epoch  4630: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4641: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2350, training loss 0.03588, validation loss 0.05367
Epoch  4652: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4663: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2360, training loss 0.01829, validation loss 0.01889
Epoch  4674: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2370, training loss 0.00745, validation loss 0.00777
Epoch  4685: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4696: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2377
Mean train loss for ascent epoch 2378: -5.1825503760483116e-05
Mean eval for ascent epoch 2378: 0.0073967427015304565
Doing Evaluation on the model now
This is Epoch 2380, training loss 0.24990, validation loss 0.38516
Epoch  4707: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4718: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2390, training loss 0.05977, validation loss 0.14639
Epoch  4729: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4740: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2400, training loss 0.01163, validation loss 0.00796
Resetting learning rate to 0.01000
Epoch  4751: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4762: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2410, training loss 0.01160, validation loss 0.01825
Epoch  4773: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2420, training loss 0.00686, validation loss 0.00735
Epoch  4784: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2421
Mean train loss for ascent epoch 2422: -4.16875809605699e-05
Mean eval for ascent epoch 2422: 0.006832648068666458
Epoch  4795: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2430, training loss 0.06285, validation loss 0.04634
Epoch  4806: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4817: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2440, training loss 0.01873, validation loss 0.03198
Epoch  4828: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4839: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2450, training loss 0.00928, validation loss 0.00945
Epoch  4850: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4861: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2460
Doing Evaluation on the model now
This is Epoch 2460, training loss 0.00629, validation loss 0.00663
Mean train loss for ascent epoch 2461: -3.8593949284404516e-05
Mean eval for ascent epoch 2461: 0.0068569425493478775
Epoch  4872: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2470, training loss 0.07616, validation loss 0.05546
Epoch  4883: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4894: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2480, training loss 0.03656, validation loss 0.01333
Epoch  4905: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4916: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2490, training loss 0.00806, validation loss 0.00814
Epoch  4927: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4938: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2499
Mean train loss for ascent epoch 2500: -3.66429885616526e-05
Mean eval for ascent epoch 2500: 0.007252762094140053
Doing Evaluation on the model now
This is Epoch 2500, training loss 0.00725, validation loss 0.00742
Epoch  4949: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2510, training loss 0.05696, validation loss 0.01485
Epoch  4960: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4971: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2520, training loss 0.01480, validation loss 0.00947
Epoch  4982: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4993: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2530, training loss 0.00737, validation loss 0.00657
Epoch  5004: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5015: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2538
Mean train loss for ascent epoch 2539: -3.3711410651449114e-05
Mean eval for ascent epoch 2539: 0.006766077596694231
Doing Evaluation on the model now
This is Epoch 2540, training loss 0.18482, validation loss 0.05229
Epoch  5026: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5037: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2550, training loss 0.05029, validation loss 0.01170
Epoch  5048: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2560, training loss 0.01251, validation loss 0.01152
Epoch  5059: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5070: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2570, training loss 0.00697, validation loss 0.00680
Epoch  5081: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5092: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2577
Mean train loss for ascent epoch 2578: -4.0468661609338596e-05
Mean eval for ascent epoch 2578: 0.006672512739896774
Doing Evaluation on the model now
This is Epoch 2580, training loss 0.25728, validation loss 0.10927
Epoch  5103: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5114: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2590, training loss 0.04144, validation loss 0.10058
Epoch  5125: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5136: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2600, training loss 0.00960, validation loss 0.00714
Resetting learning rate to 0.01000
Epoch  5147: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2610, training loss 0.00930, validation loss 0.01507
Epoch  5158: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5169: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2620, training loss 0.00736, validation loss 0.00713
Epoch  5180: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2622
Mean train loss for ascent epoch 2623: -3.106174335698597e-05
Mean eval for ascent epoch 2623: 0.006356800440698862
Epoch  5191: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2630, training loss 0.09257, validation loss 0.10404
Epoch  5202: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5213: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2640, training loss 0.01507, validation loss 0.01169
Epoch  5224: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5235: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2650, training loss 0.00853, validation loss 0.00724
Epoch  5246: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2660, training loss 0.00646, validation loss 0.00811
Epoch  5257: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2661
Mean train loss for ascent epoch 2662: -2.4019775082706474e-05
Mean eval for ascent epoch 2662: 0.00647981371730566
Epoch  5268: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2670, training loss 0.09011, validation loss 0.12994
Epoch  5279: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5290: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2680, training loss 0.02333, validation loss 0.00854
Epoch  5301: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5312: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2690, training loss 0.00622, validation loss 0.00622
Epoch  5323: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5334: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2700
Doing Evaluation on the model now
This is Epoch 2700, training loss 0.00608, validation loss 0.00567
Mean train loss for ascent epoch 2701: -4.61003728560172e-05
Mean eval for ascent epoch 2701: 0.005771858152002096
Epoch  5345: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2710, training loss 0.06596, validation loss 0.03089
Epoch  5356: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5367: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2720, training loss 0.01252, validation loss 0.03422
Epoch  5378: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5389: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2730, training loss 0.00625, validation loss 0.00591
Epoch  5400: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5411: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2739
Mean train loss for ascent epoch 2740: -4.864571747020818e-05
Mean eval for ascent epoch 2740: 0.006935989949852228
Doing Evaluation on the model now
This is Epoch 2740, training loss 0.00694, validation loss 0.00731
Epoch  5422: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5433: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2750, training loss 0.06956, validation loss 0.07269
Epoch  5444: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2760, training loss 0.01358, validation loss 0.01961
Epoch  5455: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5466: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2770, training loss 0.00619, validation loss 0.00586
Epoch  5477: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5488: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2778
Mean train loss for ascent epoch 2779: -2.6327214072807692e-05
Mean eval for ascent epoch 2779: 0.006586266215890646
Doing Evaluation on the model now
This is Epoch 2780, training loss 0.10544, validation loss 0.07632
Epoch  5499: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5510: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2790, training loss 0.03092, validation loss 0.06338
Epoch  5521: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5532: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2800, training loss 0.01335, validation loss 0.01056
Resetting learning rate to 0.01000
Epoch  5543: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2810, training loss 0.00660, validation loss 0.00597
Epoch  5554: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5565: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2820, training loss 0.00558, validation loss 0.00580
Epoch  5576: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2822
Mean train loss for ascent epoch 2823: -2.74724679911742e-05
Mean eval for ascent epoch 2823: 0.006515379063785076
Epoch  5587: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2830, training loss 0.18152, validation loss 0.04789
Epoch  5598: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5609: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2840, training loss 0.02652, validation loss 0.02731
Epoch  5620: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5631: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2850, training loss 0.00674, validation loss 0.00816
Epoch  5642: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2860, training loss 0.00585, validation loss 0.00568
Epoch  5653: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2861
Mean train loss for ascent epoch 2862: -3.7388846976682544e-05
Mean eval for ascent epoch 2862: 0.006573511753231287
Epoch  5664: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2870, training loss 0.09111, validation loss 0.02212
Epoch  5675: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5686: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2880, training loss 0.02456, validation loss 0.01921
Epoch  5697: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5708: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2890, training loss 0.00883, validation loss 0.00755
Epoch  5719: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5730: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2900
Doing Evaluation on the model now
This is Epoch 2900, training loss 0.00716, validation loss 0.01094
Mean train loss for ascent epoch 2901: -2.5344284949824214e-05
Mean eval for ascent epoch 2901: 0.007464093621820211
Epoch  5741: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2910, training loss 0.02819, validation loss 0.03309
Epoch  5752: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5763: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2920, training loss 0.01634, validation loss 0.01942
Epoch  5774: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5785: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2930, training loss 0.00685, validation loss 0.00563
Epoch  5796: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5807: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2939
Mean train loss for ascent epoch 2940: -3.8194801163626835e-05
Mean eval for ascent epoch 2940: 0.006154533475637436
Doing Evaluation on the model now
This is Epoch 2940, training loss 0.00615, validation loss 0.00603
Epoch  5818: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2950, training loss 0.03609, validation loss 0.02332
Epoch  5829: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5840: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2960, training loss 0.01592, validation loss 0.01377
Epoch  5851: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5862: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2970, training loss 0.00590, validation loss 0.00571
Epoch  5873: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5884: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2978
Mean train loss for ascent epoch 2979: -2.6297933800378814e-05
Mean eval for ascent epoch 2979: 0.0053287590853869915
Doing Evaluation on the model now
This is Epoch 2980, training loss 0.07835, validation loss 0.22743
Epoch  5895: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5906: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2990, training loss 0.05678, validation loss 0.02056
Epoch  5917: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3000, training loss 0.01632, validation loss 0.02826
Epoch  5928: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  5939: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 3010, training loss 0.01197, validation loss 0.00647
Epoch  5950: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5961: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3020, training loss 0.00522, validation loss 0.00480
Epoch  5972: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3023
Mean train loss for ascent epoch 3024: -3.2601634302409366e-05
Mean eval for ascent epoch 3024: 0.005403489340096712
Epoch  5983: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3030, training loss 0.05627, validation loss 0.02663
Epoch  5994: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6005: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3040, training loss 0.02825, validation loss 0.01961
Epoch  6016: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3050, training loss 0.00886, validation loss 0.00721
Epoch  6027: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6038: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3060, training loss 0.00550, validation loss 0.00648
Epoch  6049: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3062
Mean train loss for ascent epoch 3063: -3.695148916449398e-05
Mean eval for ascent epoch 3063: 0.005934579763561487
Epoch  6060: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3070, training loss 0.05013, validation loss 0.03385
Epoch  6071: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6082: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3080, training loss 0.01841, validation loss 0.01484
Epoch  6093: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6104: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3090, training loss 0.00883, validation loss 0.00766
Epoch  6115: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3100, training loss 0.00564, validation loss 0.00496
Epoch  6126: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3101
Mean train loss for ascent epoch 3102: -2.2747384718968533e-05
Mean eval for ascent epoch 3102: 0.004539851099252701
Epoch  6137: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3110, training loss 0.05581, validation loss 0.11736
Epoch  6148: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6159: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3120, training loss 0.01771, validation loss 0.01468
Epoch  6170: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6181: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3130, training loss 0.00596, validation loss 0.00498
Epoch  6192: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6203: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3140
Doing Evaluation on the model now
This is Epoch 3140, training loss 0.00510, validation loss 0.00585
Mean train loss for ascent epoch 3141: -2.100356505252421e-05
Mean eval for ascent epoch 3141: 0.0049842712469398975
Epoch  6214: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3150, training loss 0.04859, validation loss 0.08916
Epoch  6225: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6236: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3160, training loss 0.01716, validation loss 0.01391
Epoch  6247: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6258: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3170, training loss 0.00661, validation loss 0.00699
Epoch  6269: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6280: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3179
Mean train loss for ascent epoch 3180: -2.8971402571187355e-05
Mean eval for ascent epoch 3180: 0.005461418069899082
Doing Evaluation on the model now
This is Epoch 3180, training loss 0.00546, validation loss 0.00571
Epoch  6291: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6302: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3190, training loss 0.08470, validation loss 0.06381
Epoch  6313: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3200, training loss 0.02898, validation loss 0.02929
Resetting learning rate to 0.01000
Epoch  6324: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6335: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3210, training loss 0.00639, validation loss 0.00631
Epoch  6346: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6357: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3218
Mean train loss for ascent epoch 3219: -2.9105862267897464e-05
Mean eval for ascent epoch 3219: 0.005066802725195885
Doing Evaluation on the model now
This is Epoch 3220, training loss 0.12522, validation loss 0.11464
Epoch  6368: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6379: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3230, training loss 0.04547, validation loss 0.06417
Epoch  6390: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6401: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3240, training loss 0.02569, validation loss 0.02018
Epoch  6412: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3250, training loss 0.00648, validation loss 0.00869
Epoch  6423: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6434: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3257
Mean train loss for ascent epoch 3258: -4.020016058348119e-05
Mean eval for ascent epoch 3258: 0.005572699476033449
Doing Evaluation on the model now
This is Epoch 3260, training loss 0.78519, validation loss 0.64525
Epoch  6445: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6456: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3270, training loss 0.02575, validation loss 0.01611
Epoch  6467: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6478: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3280, training loss 0.01524, validation loss 0.00795
Epoch  6489: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6500: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3290, training loss 0.00813, validation loss 0.00903
Epoch  6511: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3296
Mean train loss for ascent epoch 3297: -2.7714131647371687e-05
Mean eval for ascent epoch 3297: 0.004920385777950287
Doing Evaluation on the model now
This is Epoch 3300, training loss 1.22462, validation loss 1.55558
Epoch  6522: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6533: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3310, training loss 0.05424, validation loss 0.04548
Epoch  6544: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6555: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3320, training loss 0.01460, validation loss 0.01323
Epoch  6566: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6577: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3330, training loss 0.00614, validation loss 0.00643
Epoch  6588: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3335
Mean train loss for ascent epoch 3336: -3.5591761843534186e-05
Mean eval for ascent epoch 3336: 0.005693208426237106
Doing Evaluation on the model now
This is Epoch 3340, training loss 0.92693, validation loss 0.73914
Epoch  6599: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6610: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3350, training loss 0.05968, validation loss 0.08751
Epoch  6621: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6632: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3360, training loss 0.01222, validation loss 0.00876
Epoch  6643: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6654: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3370, training loss 0.00535, validation loss 0.00621
Epoch  6665: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3374
Mean train loss for ascent epoch 3375: -2.8921946068294346e-05
Mean eval for ascent epoch 3375: 0.005764777306467295
Epoch  6676: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3380, training loss 0.36542, validation loss 0.04463
Epoch  6687: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3390, training loss 0.11474, validation loss 0.14441
Epoch  6698: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6709: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3400, training loss 0.01576, validation loss 0.01877
Resetting learning rate to 0.01000
Epoch  6720: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6731: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3410, training loss 0.00793, validation loss 0.00887
Epoch  6742: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6753: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3418
Mean train loss for ascent epoch 3419: -2.6884887120104395e-05
Mean eval for ascent epoch 3419: 0.005649923346936703
Doing Evaluation on the model now
This is Epoch 3420, training loss 0.26132, validation loss 0.24735
Epoch  6764: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6775: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3430, training loss 0.06296, validation loss 0.04335
Epoch  6786: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3440, training loss 0.01921, validation loss 0.01006
Epoch  6797: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6808: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3450, training loss 0.00754, validation loss 0.00894
Epoch  6819: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6830: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3457
Mean train loss for ascent epoch 3458: -3.5024528187932447e-05
Mean eval for ascent epoch 3458: 0.005755518563091755
Doing Evaluation on the model now
This is Epoch 3460, training loss 0.32389, validation loss 0.44138
Epoch  6841: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6852: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3470, training loss 0.03271, validation loss 0.02072
Epoch  6863: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6874: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3480, training loss 0.01463, validation loss 0.01467
Epoch  6885: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3490, training loss 0.00641, validation loss 0.00649
Epoch  6896: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6907: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3496
Mean train loss for ascent epoch 3497: -3.1741266866447404e-05
Mean eval for ascent epoch 3497: 0.00555685767903924
Doing Evaluation on the model now
This is Epoch 3500, training loss 1.06022, validation loss 0.67666
Epoch  6918: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6929: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3510, training loss 0.03184, validation loss 0.03485
Epoch  6940: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6951: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3520, training loss 0.01024, validation loss 0.01150
Epoch  6962: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6973: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3530, training loss 0.00577, validation loss 0.00503
Epoch  6984: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3535
Mean train loss for ascent epoch 3536: -2.5343419110868126e-05
Mean eval for ascent epoch 3536: 0.006003170739859343
Doing Evaluation on the model now
This is Epoch 3540, training loss 1.55456, validation loss 0.72233
Epoch  6995: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7006: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3550, training loss 0.05237, validation loss 0.07356
Epoch  7017: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7028: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3560, training loss 0.01034, validation loss 0.01461
Epoch  7039: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7050: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3570, training loss 0.00536, validation loss 0.00497
Epoch  7061: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3574
Mean train loss for ascent epoch 3575: -2.4277835109387524e-05
Mean eval for ascent epoch 3575: 0.0055212113074958324
Epoch  7072: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3580, training loss 0.39887, validation loss 0.34727
Epoch  7083: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3590, training loss 0.07306, validation loss 0.02822
Epoch  7094: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7105: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3600, training loss 0.01195, validation loss 0.01170
Resetting learning rate to 0.01000
Epoch  7116: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7127: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3610, training loss 0.00612, validation loss 0.00510
Epoch  7138: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7149: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3619
Mean train loss for ascent epoch 3620: -3.116103107458912e-05
Mean eval for ascent epoch 3620: 0.0049885353073477745
Doing Evaluation on the model now
This is Epoch 3620, training loss 0.00499, validation loss 0.00492
Epoch  7160: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7171: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3630, training loss 0.08354, validation loss 0.09094
Epoch  7182: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3640, training loss 0.01784, validation loss 0.02584
Epoch  7193: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7204: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3650, training loss 0.00776, validation loss 0.00976
Epoch  7215: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7226: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3658
Mean train loss for ascent epoch 3659: -2.4923827368183993e-05
Mean eval for ascent epoch 3659: 0.004472543951123953
Doing Evaluation on the model now
This is Epoch 3660, training loss 0.23435, validation loss 0.89284
Epoch  7237: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7248: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3670, training loss 0.07987, validation loss 0.04432
Epoch  7259: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7270: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3680, training loss 0.02653, validation loss 0.03226
Epoch  7281: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3690, training loss 0.00690, validation loss 0.00948
Epoch  7292: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7303: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3697
Mean train loss for ascent epoch 3698: -2.8437334549380466e-05
Mean eval for ascent epoch 3698: 0.006350102834403515
Doing Evaluation on the model now
This is Epoch 3700, training loss 0.23611, validation loss 0.23562
Epoch  7314: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7325: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3710, training loss 0.05610, validation loss 0.07257
Epoch  7336: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7347: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3720, training loss 0.01739, validation loss 0.01171
Epoch  7358: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7369: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3730, training loss 0.00684, validation loss 0.00535
Epoch  7380: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3736
Mean train loss for ascent epoch 3737: -2.2513622752740048e-05
Mean eval for ascent epoch 3737: 0.004900109488517046
Doing Evaluation on the model now
This is Epoch 3740, training loss 0.13273, validation loss 0.05399
Epoch  7391: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7402: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3750, training loss 0.06871, validation loss 0.07856
Epoch  7413: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7424: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3760, training loss 0.01763, validation loss 0.01431
Epoch  7435: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7446: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3770, training loss 0.00600, validation loss 0.00511
Epoch  7457: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3775
Mean train loss for ascent epoch 3776: -2.30083769565681e-05
Mean eval for ascent epoch 3776: 0.004482494667172432
Doing Evaluation on the model now
This is Epoch 3780, training loss 0.63171, validation loss 0.12191
Epoch  7468: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7479: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3790, training loss 0.09274, validation loss 0.03328
Epoch  7490: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7501: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3800, training loss 0.01145, validation loss 0.01167
Resetting learning rate to 0.01000
Epoch  7512: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7523: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3810, training loss 0.00643, validation loss 0.00679
Epoch  7534: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7545: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3819
Mean train loss for ascent epoch 3820: -2.8274542273720726e-05
Mean eval for ascent epoch 3820: 0.00547226844355464
Doing Evaluation on the model now
This is Epoch 3820, training loss 0.00547, validation loss 0.00606
Epoch  7556: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3830, training loss 0.26796, validation loss 0.27806
Epoch  7567: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7578: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3840, training loss 0.02897, validation loss 0.01413
Epoch  7589: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7600: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3850, training loss 0.00607, validation loss 0.00817
Epoch  7611: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7622: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3858
Mean train loss for ascent epoch 3859: -2.7772764951805584e-05
Mean eval for ascent epoch 3859: 0.004646291024982929
Doing Evaluation on the model now
This is Epoch 3860, training loss 0.33371, validation loss 0.21399
Epoch  7633: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7644: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3870, training loss 0.07664, validation loss 0.04076
Epoch  7655: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3880, training loss 0.02800, validation loss 0.03898
Epoch  7666: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7677: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3890, training loss 0.00854, validation loss 0.00514
Epoch  7688: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7699: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3897
Mean train loss for ascent epoch 3898: -2.9839771741535515e-05
Mean eval for ascent epoch 3898: 0.0054265535436570644
Doing Evaluation on the model now
This is Epoch 3900, training loss 0.33644, validation loss 1.42041
Epoch  7710: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7721: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3910, training loss 0.07577, validation loss 0.05343
Epoch  7732: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7743: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3920, training loss 0.02049, validation loss 0.01068
Epoch  7754: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3930, training loss 0.00729, validation loss 0.00633
Epoch  7765: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7776: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3936
Mean train loss for ascent epoch 3937: -3.026482227141969e-05
Mean eval for ascent epoch 3937: 0.005113933235406876
Doing Evaluation on the model now
This is Epoch 3940, training loss 0.16039, validation loss 0.04439
Epoch  7787: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7798: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3950, training loss 0.06960, validation loss 0.04378
Epoch  7809: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7820: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3960, training loss 0.00948, validation loss 0.00836
Epoch  7831: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7842: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3970, training loss 0.00568, validation loss 0.00408
Epoch  7853: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3975
Mean train loss for ascent epoch 3976: -2.497671084711328e-05
Mean eval for ascent epoch 3976: 0.0059349252842366695
Doing Evaluation on the model now
This is Epoch 3980, training loss 0.76665, validation loss 0.57770
Epoch  7864: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7875: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3990, training loss 0.10329, validation loss 0.05858
Epoch  7886: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7897: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4000, training loss 0.01574, validation loss 0.03137
Resetting learning rate to 0.01000
Epoch  7908: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7919: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4010, training loss 0.00891, validation loss 0.00660
Epoch  7930: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7941: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4020
Doing Evaluation on the model now
This is Epoch 4020, training loss 0.00503, validation loss 0.00551
Mean train loss for ascent epoch 4021: -2.5352097509312443e-05
Mean eval for ascent epoch 4021: 0.005472075194120407
Epoch  7952: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4030, training loss 0.10558, validation loss 0.05946
Epoch  7963: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7974: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4040, training loss 0.02177, validation loss 0.02992
Epoch  7985: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7996: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4050, training loss 0.00706, validation loss 0.00609
Epoch  8007: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8018: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4059
Mean train loss for ascent epoch 4060: -2.6727855583885685e-05
Mean eval for ascent epoch 4060: 0.004888803232461214
Doing Evaluation on the model now
This is Epoch 4060, training loss 0.00489, validation loss 0.00483
Epoch  8029: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8040: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4070, training loss 0.09727, validation loss 0.18273
Epoch  8051: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4080, training loss 0.02601, validation loss 0.02821
Epoch  8062: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8073: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4090, training loss 0.00700, validation loss 0.00729
Epoch  8084: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8095: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4098
Mean train loss for ascent epoch 4099: -2.0881141608697362e-05
Mean eval for ascent epoch 4099: 0.005613724701106548
Doing Evaluation on the model now
This is Epoch 4100, training loss 0.23375, validation loss 0.24639
Epoch  8106: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8117: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4110, training loss 0.06068, validation loss 0.04450
Epoch  8128: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8139: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4120, training loss 0.02538, validation loss 0.02321
Epoch  8150: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4130, training loss 0.00577, validation loss 0.00702
Epoch  8161: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8172: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4137
Mean train loss for ascent epoch 4138: -1.7200969523401e-05
Mean eval for ascent epoch 4138: 0.004261319059878588
Doing Evaluation on the model now
This is Epoch 4140, training loss 0.58170, validation loss 0.53137
Epoch  8183: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8194: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4150, training loss 0.04543, validation loss 0.03097
Epoch  8205: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8216: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4160, training loss 0.01784, validation loss 0.01548
Epoch  8227: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8238: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4170, training loss 0.00631, validation loss 0.00809
Epoch  8249: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4176
Mean train loss for ascent epoch 4177: -2.6266194254276343e-05
Mean eval for ascent epoch 4177: 0.0042380946688354015
Doing Evaluation on the model now
This is Epoch 4180, training loss 1.14085, validation loss 2.11299
Epoch  8260: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8271: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4190, training loss 0.05476, validation loss 0.06925
Epoch  8282: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8293: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4200, training loss 0.01232, validation loss 0.01351
Resetting learning rate to 0.01000
Epoch  8304: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8315: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4210, training loss 0.00638, validation loss 0.00806
Epoch  8326: reducing learning rate of group 0 to 1.2500e-04.
Epoch  8337: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4220
Doing Evaluation on the model now
This is Epoch 4220, training loss 0.00494, validation loss 0.00388
Mean train loss for ascent epoch 4221: -1.905231511045713e-05
Mean eval for ascent epoch 4221: 0.003929751459509134
Epoch  8348: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4230, training loss 0.11460, validation loss 0.10669
Epoch  8359: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8370: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4240, training loss 0.01957, validation loss 0.01548
Epoch  8381: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8392: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4250, training loss 0.00473, validation loss 0.00511
Epoch  8403: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8414: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4259
Mean train loss for ascent epoch 4260: -3.054494663956575e-05
Mean eval for ascent epoch 4260: 0.0041435398161411285
Doing Evaluation on the model now
This is Epoch 4260, training loss 0.00414, validation loss 0.00414
Epoch  8425: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4270, training loss 0.09025, validation loss 0.07023
Epoch  8436: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8447: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4280, training loss 0.02954, validation loss 0.01885
Epoch  8458: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8469: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4290, training loss 0.00553, validation loss 0.00633
Epoch  8480: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8491: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4298
Mean train loss for ascent epoch 4299: -2.2252057533478364e-05
Mean eval for ascent epoch 4299: 0.004376114811748266
Doing Evaluation on the model now
This is Epoch 4300, training loss 0.18110, validation loss 0.17659
Epoch  8502: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8513: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4310, training loss 0.05438, validation loss 0.05231
Epoch  8524: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4320, training loss 0.01775, validation loss 0.02619
Epoch  8535: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8546: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4330, training loss 0.00663, validation loss 0.00612
Epoch  8557: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8568: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4337
Mean train loss for ascent epoch 4338: -2.185539051424712e-05
Mean eval for ascent epoch 4338: 0.00496333185583353
Doing Evaluation on the model now
This is Epoch 4340, training loss 2.18357, validation loss 2.28088
Epoch  8579: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8590: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4350, training loss 0.06815, validation loss 0.11794
Epoch  8601: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8612: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4360, training loss 0.01573, validation loss 0.01296
Epoch  8623: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4370, training loss 0.00558, validation loss 0.00468
Epoch  8634: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8645: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4376
Mean train loss for ascent epoch 4377: -1.9381759557290934e-05
Mean eval for ascent epoch 4377: 0.005491530988365412
Doing Evaluation on the model now
This is Epoch 4380, training loss 0.72492, validation loss 0.41266
Epoch  8656: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8667: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4390, training loss 0.05821, validation loss 0.02436
Epoch  8678: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8689: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4400, training loss 0.01562, validation loss 0.00920
Resetting learning rate to 0.01000
Epoch  8700: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8711: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4410, training loss 0.00757, validation loss 0.00441
Epoch  8722: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4420, training loss 0.00408, validation loss 0.00466
Epoch  8733: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4421
Mean train loss for ascent epoch 4422: -2.4355866116820835e-05
Mean eval for ascent epoch 4422: 0.003758700331673026
Epoch  8744: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4430, training loss 0.16959, validation loss 0.04357
Epoch  8755: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8766: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4440, training loss 0.03369, validation loss 0.02629
Epoch  8777: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8788: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4450, training loss 0.00515, validation loss 0.00410
Epoch  8799: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8810: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4460
Doing Evaluation on the model now
This is Epoch 4460, training loss 0.00525, validation loss 0.00459
Mean train loss for ascent epoch 4461: -1.949189208971802e-05
Mean eval for ascent epoch 4461: 0.004079421050846577
Epoch  8821: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4470, training loss 0.19115, validation loss 0.20058
Epoch  8832: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8843: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4480, training loss 0.02640, validation loss 0.01636
Epoch  8854: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8865: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4490, training loss 0.00564, validation loss 0.00649
Epoch  8876: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8887: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4499
Mean train loss for ascent epoch 4500: -3.288688458269462e-05
Mean eval for ascent epoch 4500: 0.006162160541862249
Doing Evaluation on the model now
This is Epoch 4500, training loss 0.00616, validation loss 0.00511
Epoch  8898: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8909: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4510, training loss 0.05609, validation loss 0.02095
Epoch  8920: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4520, training loss 0.02665, validation loss 0.02375
Epoch  8931: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8942: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4530, training loss 0.00470, validation loss 0.00498
Epoch  8953: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8964: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4538
Mean train loss for ascent epoch 4539: -2.423718433419708e-05
Mean eval for ascent epoch 4539: 0.004217451438307762
Doing Evaluation on the model now
This is Epoch 4540, training loss 0.21694, validation loss 0.39138
Epoch  8975: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8986: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4550, training loss 0.09340, validation loss 0.07311
Epoch  8997: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9008: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4560, training loss 0.01765, validation loss 0.01531
Epoch  9019: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4570, training loss 0.00553, validation loss 0.00534
Epoch  9030: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9041: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4577
Mean train loss for ascent epoch 4578: -2.444447818561457e-05
Mean eval for ascent epoch 4578: 0.0048110648058354855
Doing Evaluation on the model now
This is Epoch 4580, training loss 0.90555, validation loss 1.32960
Epoch  9052: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9063: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4590, training loss 0.03098, validation loss 0.02287
Epoch  9074: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9085: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4600, training loss 0.02532, validation loss 0.02112
Resetting learning rate to 0.01000
Epoch  9096: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9107: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4610, training loss 0.01115, validation loss 0.00534
Epoch  9118: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4620, training loss 0.00414, validation loss 0.00418
Epoch  9129: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4621
Mean train loss for ascent epoch 4622: -2.7327627321938053e-05
Mean eval for ascent epoch 4622: 0.004190158098936081
Epoch  9140: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4630, training loss 0.12984, validation loss 0.09722
Epoch  9151: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9162: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4640, training loss 0.03653, validation loss 0.03913
Epoch  9173: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9184: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4650, training loss 0.01000, validation loss 0.00867
Epoch  9195: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9206: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4660
Doing Evaluation on the model now
This is Epoch 4660, training loss 0.00480, validation loss 0.00577
Mean train loss for ascent epoch 4661: -2.1285470211296342e-05
Mean eval for ascent epoch 4661: 0.0045438106171786785
Epoch  9217: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4670, training loss 0.16554, validation loss 0.11082
Epoch  9228: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9239: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4680, training loss 0.03550, validation loss 0.03451
Epoch  9250: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9261: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4690, training loss 0.00655, validation loss 0.00833
Epoch  9272: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9283: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4699
Mean train loss for ascent epoch 4700: -2.6999367037205957e-05
Mean eval for ascent epoch 4700: 0.004907389637082815
Doing Evaluation on the model now
This is Epoch 4700, training loss 0.00491, validation loss 0.00385
Epoch  9294: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4710, training loss 0.17117, validation loss 0.09591
Epoch  9305: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9316: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4720, training loss 0.04027, validation loss 0.02728
Epoch  9327: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9338: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4730, training loss 0.00623, validation loss 0.00587
Epoch  9349: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9360: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4738
Mean train loss for ascent epoch 4739: -2.2450722099165432e-05
Mean eval for ascent epoch 4739: 0.004203526768833399
Doing Evaluation on the model now
This is Epoch 4740, training loss 0.19426, validation loss 0.09384
Epoch  9371: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9382: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4750, training loss 0.07148, validation loss 0.05668
Epoch  9393: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4760, training loss 0.02155, validation loss 0.01539
Epoch  9404: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9415: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4770, training loss 0.00639, validation loss 0.00638
Epoch  9426: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9437: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4777
Mean train loss for ascent epoch 4778: -2.1375737560447305e-05
Mean eval for ascent epoch 4778: 0.0041799480095505714
Doing Evaluation on the model now
This is Epoch 4780, training loss 1.39671, validation loss 0.89420
Epoch  9448: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9459: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4790, training loss 0.03469, validation loss 0.02288
Epoch  9470: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9481: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4800, training loss 0.01275, validation loss 0.00692
Resetting learning rate to 0.01000
Epoch  9492: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4810, training loss 0.00809, validation loss 0.01560
Epoch  9503: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9514: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4820, training loss 0.00517, validation loss 0.00456
Epoch  9525: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4822
Mean train loss for ascent epoch 4823: -2.5735291274031624e-05
Mean eval for ascent epoch 4823: 0.00424921652302146
Epoch  9536: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4830, training loss 0.10052, validation loss 0.04407
Epoch  9547: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9558: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4840, training loss 0.01306, validation loss 0.00778
Epoch  9569: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9580: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4850, training loss 0.00949, validation loss 0.00598
Epoch  9591: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4860, training loss 0.00540, validation loss 0.00478
Epoch  9602: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4861
Mean train loss for ascent epoch 4862: -2.539169872761704e-05
Mean eval for ascent epoch 4862: 0.004609985277056694
Epoch  9613: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4870, training loss 0.15031, validation loss 0.14913
Epoch  9624: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9635: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4880, training loss 0.02262, validation loss 0.01677
Epoch  9646: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9657: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4890, training loss 0.00654, validation loss 0.00419
Epoch  9668: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9679: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4900
Doing Evaluation on the model now
This is Epoch 4900, training loss 0.00444, validation loss 0.00351
Mean train loss for ascent epoch 4901: -1.8731236195890233e-05
Mean eval for ascent epoch 4901: 0.0037382743321359158
Epoch  9690: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4910, training loss 0.14700, validation loss 0.03986
Epoch  9701: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9712: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4920, training loss 0.02557, validation loss 0.01209
Epoch  9723: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9734: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4930, training loss 0.00629, validation loss 0.00749
Epoch  9745: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9756: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4939
Mean train loss for ascent epoch 4940: -1.9005659851245582e-05
Mean eval for ascent epoch 4940: 0.0037826241459697485
Doing Evaluation on the model now
This is Epoch 4940, training loss 0.00378, validation loss 0.00456
Epoch  9767: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9778: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4950, training loss 0.12919, validation loss 0.17140
Epoch  9789: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4960, training loss 0.02755, validation loss 0.05837
Epoch  9800: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9811: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4970, training loss 0.00517, validation loss 0.00564
Epoch  9822: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9833: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4978
Mean train loss for ascent epoch 4979: -1.8202086721430533e-05
Mean eval for ascent epoch 4979: 0.0037637599743902683
Doing Evaluation on the model now
This is Epoch 4980, training loss 0.19720, validation loss 0.08950
Epoch  9844: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9855: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4990, training loss 0.05012, validation loss 0.02715
Epoch  9866: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9877: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5000, training loss 0.03934, validation loss 0.02793
Resetting learning rate to 0.01000
Epoch  9888: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5010, training loss 0.01135, validation loss 0.01314
Epoch  9899: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9910: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5020, training loss 0.00444, validation loss 0.00420
Epoch  9921: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5022
Mean train loss for ascent epoch 5023: -2.275154292874504e-05
Mean eval for ascent epoch 5023: 0.003684836672618985
Epoch  9932: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5030, training loss 0.09449, validation loss 0.14200
Epoch  9943: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9954: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5040, training loss 0.02577, validation loss 0.03492
Epoch  9965: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9976: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5050, training loss 0.01519, validation loss 0.01466
Epoch  9987: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5060, training loss 0.00369, validation loss 0.00325
Epoch  9998: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5061
Mean train loss for ascent epoch 5062: -1.6412681361543946e-05
Mean eval for ascent epoch 5062: 0.004492099862545729
Epoch 10009: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5070, training loss 0.12218, validation loss 0.06502
Epoch 10020: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10031: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5080, training loss 0.02307, validation loss 0.01239
Epoch 10042: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10053: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5090, training loss 0.00634, validation loss 0.00380
Epoch 10064: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10075: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5100
Doing Evaluation on the model now
This is Epoch 5100, training loss 0.00447, validation loss 0.00342
Mean train loss for ascent epoch 5101: -1.9682905985973775e-05
Mean eval for ascent epoch 5101: 0.004117198754101992
Epoch 10086: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5110, training loss 0.11575, validation loss 0.12284
Epoch 10097: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10108: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5120, training loss 0.02797, validation loss 0.04903
Epoch 10119: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10130: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5130, training loss 0.00489, validation loss 0.00551
Epoch 10141: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10152: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5139
Mean train loss for ascent epoch 5140: -2.045242945314385e-05
Mean eval for ascent epoch 5140: 0.0046437275595963
Doing Evaluation on the model now
This is Epoch 5140, training loss 0.00464, validation loss 0.00461
Epoch 10163: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5150, training loss 0.13477, validation loss 0.16082
Epoch 10174: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10185: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5160, training loss 0.02693, validation loss 0.02458
Epoch 10196: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10207: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5170, training loss 0.00540, validation loss 0.00569
Epoch 10218: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10229: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5178
Mean train loss for ascent epoch 5179: -2.0477362340898253e-05
Mean eval for ascent epoch 5179: 0.004566743969917297
Doing Evaluation on the model now
This is Epoch 5180, training loss 0.20819, validation loss 0.42113
Epoch 10240: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10251: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5190, training loss 0.09358, validation loss 0.08492
Epoch 10262: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5200, training loss 0.03127, validation loss 0.02714
Epoch 10273: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 10284: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5210, training loss 0.00747, validation loss 0.00904
Epoch 10295: reducing learning rate of group 0 to 2.5000e-04.
Epoch 10306: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5220, training loss 0.00465, validation loss 0.00429
Epoch 10317: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5223
Mean train loss for ascent epoch 5224: -2.1665007807314396e-05
Mean eval for ascent epoch 5224: 0.004172333050519228
Epoch 10328: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5230, training loss 0.09207, validation loss 0.16150
Epoch 10339: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10350: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5240, training loss 0.02757, validation loss 0.01514
Epoch 10361: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5250, training loss 0.01149, validation loss 0.01542
Epoch 10372: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10383: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5260, training loss 0.00526, validation loss 0.00478
Epoch 10394: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5262
Mean train loss for ascent epoch 5263: -3.026106242032256e-05
Mean eval for ascent epoch 5263: 0.004237747751176357
Epoch 10405: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5270, training loss 0.18116, validation loss 0.12686
Epoch 10416: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10427: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5280, training loss 0.01953, validation loss 0.01583
Epoch 10438: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10449: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5290, training loss 0.00671, validation loss 0.00597
Epoch 10460: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5300, training loss 0.00403, validation loss 0.00415
Epoch 10471: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5301
Mean train loss for ascent epoch 5302: -2.8054484573658556e-05
Mean eval for ascent epoch 5302: 0.004023503512144089
Epoch 10482: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5310, training loss 0.13566, validation loss 0.09103
Epoch 10493: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10504: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5320, training loss 0.01939, validation loss 0.02043
Epoch 10515: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10526: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5330, training loss 0.00471, validation loss 0.00443
Epoch 10537: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10548: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5340
Doing Evaluation on the model now
This is Epoch 5340, training loss 0.00411, validation loss 0.00413
Mean train loss for ascent epoch 5341: -2.589161704236176e-05
Mean eval for ascent epoch 5341: 0.003505381755530834
Epoch 10559: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5350, training loss 0.10015, validation loss 0.19445
Epoch 10570: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10581: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5360, training loss 0.03016, validation loss 0.05103
Epoch 10592: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10603: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5370, training loss 0.00650, validation loss 0.00643
Epoch 10614: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10625: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5379
Mean train loss for ascent epoch 5380: -2.2251735572353937e-05
Mean eval for ascent epoch 5380: 0.004635485354810953
Doing Evaluation on the model now
This is Epoch 5380, training loss 0.00464, validation loss 0.00435
Epoch 10636: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10647: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5390, training loss 0.06439, validation loss 0.07823
Epoch 10658: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5400, training loss 0.01612, validation loss 0.03662
Resetting learning rate to 0.01000
Epoch 10669: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10680: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5410, training loss 0.00543, validation loss 0.00446
Epoch 10691: reducing learning rate of group 0 to 1.2500e-04.
Epoch 10702: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5418
Mean train loss for ascent epoch 5419: -2.128923370037228e-05
Mean eval for ascent epoch 5419: 0.00419235322624445
Doing Evaluation on the model now
This is Epoch 5420, training loss 0.39211, validation loss 0.49012
Epoch 10713: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10724: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5430, training loss 0.07444, validation loss 0.08551
Epoch 10735: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10746: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5440, training loss 0.02175, validation loss 0.02530
Epoch 10757: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5450, training loss 0.00542, validation loss 0.00495
Epoch 10768: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10779: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5457
Mean train loss for ascent epoch 5458: -1.929440077219624e-05
Mean eval for ascent epoch 5458: 0.00466455053538084
Doing Evaluation on the model now
This is Epoch 5460, training loss 0.53102, validation loss 0.43640
Epoch 10790: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10801: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5470, training loss 0.05173, validation loss 0.02984
Epoch 10812: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10823: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5480, training loss 0.01008, validation loss 0.01221
Epoch 10834: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10845: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5490, training loss 0.00518, validation loss 0.00490
Epoch 10856: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5496
Mean train loss for ascent epoch 5497: -2.2427930161939003e-05
Mean eval for ascent epoch 5497: 0.004380073864012957
Doing Evaluation on the model now
This is Epoch 5500, training loss 4.39250, validation loss 2.70709
Epoch 10867: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10878: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5510, training loss 0.02735, validation loss 0.03959
Epoch 10889: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10900: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5520, training loss 0.00853, validation loss 0.00744
Epoch 10911: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10922: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5530, training loss 0.00455, validation loss 0.00442
Epoch 10933: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5535
Mean train loss for ascent epoch 5536: -2.4966690034489147e-05
Mean eval for ascent epoch 5536: 0.00534281600266695
Doing Evaluation on the model now
This is Epoch 5540, training loss 1.02099, validation loss 0.71899
Epoch 10944: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10955: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5550, training loss 0.03277, validation loss 0.03964
Epoch 10966: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10977: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5560, training loss 0.01195, validation loss 0.00921
Epoch 10988: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10999: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5570, training loss 0.00484, validation loss 0.00412
Epoch 11010: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5574
Mean train loss for ascent epoch 5575: -2.2648770027444698e-05
Mean eval for ascent epoch 5575: 0.0045370557345449924
Epoch 11021: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5580, training loss 0.13592, validation loss 0.02816
Epoch 11032: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5590, training loss 0.17802, validation loss 0.24545
Epoch 11043: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11054: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5600, training loss 0.00831, validation loss 0.01174
Resetting learning rate to 0.01000
Epoch 11065: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11076: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5610, training loss 0.00454, validation loss 0.00451
Epoch 11087: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11098: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5618
Mean train loss for ascent epoch 5619: -2.1968015062157065e-05
Mean eval for ascent epoch 5619: 0.005543958395719528
Doing Evaluation on the model now
This is Epoch 5620, training loss 0.23096, validation loss 0.36291
Epoch 11109: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11120: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5630, training loss 0.08609, validation loss 0.08727
Epoch 11131: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5640, training loss 0.02915, validation loss 0.02111
Epoch 11142: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11153: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5650, training loss 0.00538, validation loss 0.00551
Epoch 11164: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11175: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5657
Mean train loss for ascent epoch 5658: -2.411811328784097e-05
Mean eval for ascent epoch 5658: 0.0047527183778584
Doing Evaluation on the model now
This is Epoch 5660, training loss 0.52817, validation loss 0.60790
Epoch 11186: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11197: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5670, training loss 0.04664, validation loss 0.06388
Epoch 11208: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11219: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5680, training loss 0.01107, validation loss 0.01022
Epoch 11230: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5690, training loss 0.00581, validation loss 0.00545
Epoch 11241: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11252: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5696
Mean train loss for ascent epoch 5697: -2.2096810425864533e-05
Mean eval for ascent epoch 5697: 0.0053599863313138485
Doing Evaluation on the model now
This is Epoch 5700, training loss 2.02110, validation loss 0.93418
Epoch 11263: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11274: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5710, training loss 0.07639, validation loss 0.08805
Epoch 11285: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11296: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5720, training loss 0.00792, validation loss 0.00628
Epoch 11307: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11318: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5730, training loss 0.00429, validation loss 0.00445
Epoch 11329: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5735
Mean train loss for ascent epoch 5736: -2.317654616490472e-05
Mean eval for ascent epoch 5736: 0.004173466935753822
Doing Evaluation on the model now
This is Epoch 5740, training loss 0.51145, validation loss 0.10893
Epoch 11340: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11351: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5750, training loss 0.04362, validation loss 0.04659
Epoch 11362: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11373: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5760, training loss 0.01033, validation loss 0.00770
Epoch 11384: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11395: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5770, training loss 0.00393, validation loss 0.00399
Epoch 11406: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5774
Mean train loss for ascent epoch 5775: -2.149703686882276e-05
Mean eval for ascent epoch 5775: 0.004680108278989792
Epoch 11417: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5780, training loss 0.10446, validation loss 0.10483
Epoch 11428: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5790, training loss 0.02670, validation loss 0.02742
Epoch 11439: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11450: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5800, training loss 0.00731, validation loss 0.00748
Resetting learning rate to 0.01000
Epoch 11461: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11472: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5810, training loss 0.00463, validation loss 0.00653
Epoch 11483: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11494: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5819
Mean train loss for ascent epoch 5820: -3.633846790762618e-05
Mean eval for ascent epoch 5820: 0.003962439019232988
Doing Evaluation on the model now
This is Epoch 5820, training loss 0.00396, validation loss 0.00524
Epoch 11505: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11516: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5830, training loss 0.08703, validation loss 0.09012
Epoch 11527: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5840, training loss 0.01222, validation loss 0.01035
Epoch 11538: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11549: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5850, training loss 0.00459, validation loss 0.00386
Epoch 11560: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11571: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5858
Mean train loss for ascent epoch 5859: -2.401382698735688e-05
Mean eval for ascent epoch 5859: 0.0029214026872068644
Doing Evaluation on the model now
This is Epoch 5860, training loss 0.21200, validation loss 0.38665
Epoch 11582: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11593: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5870, training loss 0.06054, validation loss 0.05586
Epoch 11604: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11615: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5880, training loss 0.01235, validation loss 0.01819
Epoch 11626: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5890, training loss 0.00576, validation loss 0.00421
Epoch 11637: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11648: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5897
Mean train loss for ascent epoch 5898: -2.1591757104033604e-05
Mean eval for ascent epoch 5898: 0.0034058995079249144
Doing Evaluation on the model now
This is Epoch 5900, training loss 0.11940, validation loss 0.06861
Epoch 11659: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11670: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5910, training loss 0.01381, validation loss 0.02304
Epoch 11681: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11692: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5920, training loss 0.01178, validation loss 0.00535
Epoch 11703: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11714: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5930, training loss 0.00537, validation loss 0.00470
Epoch 11725: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5936
Mean train loss for ascent epoch 5937: -1.4449057744059246e-05
Mean eval for ascent epoch 5937: 0.0033139879815280437
Doing Evaluation on the model now
This is Epoch 5940, training loss 0.24009, validation loss 0.43792
Epoch 11736: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11747: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5950, training loss 0.02344, validation loss 0.03832
Epoch 11758: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11769: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5960, training loss 0.00647, validation loss 0.00833
Epoch 11780: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11791: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5970, training loss 0.00289, validation loss 0.00358
Epoch 11802: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5975
Mean train loss for ascent epoch 5976: -1.7136339010903612e-05
Mean eval for ascent epoch 5976: 0.002529470482841134
Doing Evaluation on the model now
This is Epoch 5980, training loss 0.44004, validation loss 0.52442
Epoch 11813: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11824: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5990, training loss 0.03316, validation loss 0.02326
Epoch 11835: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11846: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6000, training loss 0.00812, validation loss 0.00439
Resetting learning rate to 0.01000
Epoch 11857: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11868: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6010, training loss 0.00354, validation loss 0.00301
Epoch 11879: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11890: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6019
Mean train loss for ascent epoch 6020: -1.5787016309332103e-05
Mean eval for ascent epoch 6020: 0.0027169997338205576
Doing Evaluation on the model now
This is Epoch 6020, training loss 0.00272, validation loss 0.00279
Epoch 11901: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6030, training loss 0.15697, validation loss 0.06744
Epoch 11912: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11923: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6040, training loss 0.00804, validation loss 0.00884
Epoch 11934: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11945: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6050, training loss 0.00525, validation loss 0.00373
Epoch 11956: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11967: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6058
Mean train loss for ascent epoch 6059: -1.794878153305035e-05
Mean eval for ascent epoch 6059: 0.0037249138113111258
Doing Evaluation on the model now
This is Epoch 6060, training loss 0.16491, validation loss 0.22137
Epoch 11978: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11989: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6070, training loss 0.08185, validation loss 0.05203
Epoch 12000: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6080, training loss 0.01442, validation loss 0.01230
Epoch 12011: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12022: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6090, training loss 0.00415, validation loss 0.00406
Epoch 12033: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12044: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6097
Mean train loss for ascent epoch 6098: -1.722031447570771e-05
Mean eval for ascent epoch 6098: 0.003089784411713481
Doing Evaluation on the model now
This is Epoch 6100, training loss 2.20564, validation loss 3.90082
Epoch 12055: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12066: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6110, training loss 0.05715, validation loss 0.06773
Epoch 12077: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12088: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6120, training loss 0.01150, validation loss 0.01154
Epoch 12099: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6130, training loss 0.00682, validation loss 0.00376
Epoch 12110: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12121: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6136
Mean train loss for ascent epoch 6137: -1.801810685719829e-05
Mean eval for ascent epoch 6137: 0.0036547372583299875
Doing Evaluation on the model now
This is Epoch 6140, training loss 0.81126, validation loss 0.58360
Epoch 12132: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12143: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6150, training loss 0.04972, validation loss 0.03241
Epoch 12154: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12165: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6160, training loss 0.00771, validation loss 0.00746
Epoch 12176: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12187: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6170, training loss 0.00434, validation loss 0.00345
Epoch 12198: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6175
Mean train loss for ascent epoch 6176: -1.7951691916096024e-05
Mean eval for ascent epoch 6176: 0.004233395215123892
Doing Evaluation on the model now
This is Epoch 6180, training loss 0.55185, validation loss 0.13290
Epoch 12209: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12220: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6190, training loss 0.06681, validation loss 0.05685
Epoch 12231: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12242: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6200, training loss 0.00898, validation loss 0.00641
Resetting learning rate to 0.01000
Epoch 12253: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12264: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6210, training loss 0.00350, validation loss 0.00382
Epoch 12275: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12286: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6220
Doing Evaluation on the model now
This is Epoch 6220, training loss 0.00269, validation loss 0.00268
Mean train loss for ascent epoch 6221: -1.590241663507186e-05
Mean eval for ascent epoch 6221: 0.0029950328171253204
Epoch 12297: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6230, training loss 0.11611, validation loss 0.13146
Epoch 12308: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12319: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6240, training loss 0.02099, validation loss 0.01181
Epoch 12330: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12341: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6250, training loss 0.00465, validation loss 0.00366
Epoch 12352: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12363: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6259
Mean train loss for ascent epoch 6260: -1.3521057553589344e-05
Mean eval for ascent epoch 6260: 0.0027997407596558332
Doing Evaluation on the model now
This is Epoch 6260, training loss 0.00280, validation loss 0.00282
Epoch 12374: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12385: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6270, training loss 0.15162, validation loss 0.09793
Epoch 12396: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6280, training loss 0.01431, validation loss 0.02405
Epoch 12407: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12418: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6290, training loss 0.00414, validation loss 0.00571
Epoch 12429: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12440: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6298
Mean train loss for ascent epoch 6299: -1.87316290976014e-05
Mean eval for ascent epoch 6299: 0.0032414363231509924
Doing Evaluation on the model now
This is Epoch 6300, training loss 0.21520, validation loss 0.48121
Epoch 12451: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12462: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6310, training loss 0.03955, validation loss 0.01661
Epoch 12473: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12484: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6320, training loss 0.01812, validation loss 0.02489
Epoch 12495: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6330, training loss 0.00495, validation loss 0.00475
Epoch 12506: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12517: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6337
Mean train loss for ascent epoch 6338: -2.318041879334487e-05
Mean eval for ascent epoch 6338: 0.002916281344369054
Doing Evaluation on the model now
This is Epoch 6340, training loss 0.13786, validation loss 0.06513
Epoch 12528: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12539: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6350, training loss 0.04979, validation loss 0.06820
Epoch 12550: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12561: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6360, training loss 0.01301, validation loss 0.00947
Epoch 12572: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12583: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6370, training loss 0.00438, validation loss 0.00450
Epoch 12594: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6376
Mean train loss for ascent epoch 6377: -1.3188644516048953e-05
Mean eval for ascent epoch 6377: 0.0032615577802062035
Doing Evaluation on the model now
This is Epoch 6380, training loss 1.16762, validation loss 0.75753
Epoch 12605: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12616: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6390, training loss 0.02009, validation loss 0.03362
Epoch 12627: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12638: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6400, training loss 0.00681, validation loss 0.00935
Resetting learning rate to 0.01000
Epoch 12649: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12660: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6410, training loss 0.00414, validation loss 0.00333
Epoch 12671: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12682: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6420
Doing Evaluation on the model now
This is Epoch 6420, training loss 0.00360, validation loss 0.00419
Mean train loss for ascent epoch 6421: -1.8334547348786145e-05
Mean eval for ascent epoch 6421: 0.00284652435220778
Epoch 12693: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6430, training loss 0.17351, validation loss 0.15438
Epoch 12704: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12715: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6440, training loss 0.01547, validation loss 0.01771
Epoch 12726: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12737: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6450, training loss 0.00431, validation loss 0.00439
Epoch 12748: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12759: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6459
Mean train loss for ascent epoch 6460: -1.2160592632426415e-05
Mean eval for ascent epoch 6460: 0.00348236714489758
Doing Evaluation on the model now
This is Epoch 6460, training loss 0.00348, validation loss 0.00291
Epoch 12770: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6470, training loss 0.05835, validation loss 0.03525
Epoch 12781: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12792: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6480, training loss 0.01504, validation loss 0.00906
Epoch 12803: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12814: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6490, training loss 0.00428, validation loss 0.00350
Epoch 12825: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12836: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6498
Mean train loss for ascent epoch 6499: -1.3255238627607469e-05
Mean eval for ascent epoch 6499: 0.003387998091056943
Doing Evaluation on the model now
This is Epoch 6500, training loss 0.19093, validation loss 0.23881
Epoch 12847: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12858: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6510, training loss 0.04974, validation loss 0.05030
Epoch 12869: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6520, training loss 0.01654, validation loss 0.01340
Epoch 12880: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12891: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6530, training loss 0.00428, validation loss 0.00335
Epoch 12902: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12913: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6537
Mean train loss for ascent epoch 6538: -2.7056657927460037e-05
Mean eval for ascent epoch 6538: 0.003247329033911228
Doing Evaluation on the model now
This is Epoch 6540, training loss 0.44938, validation loss 0.68759
Epoch 12924: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12935: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6550, training loss 0.05938, validation loss 0.08272
Epoch 12946: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12957: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6560, training loss 0.01027, validation loss 0.00460
Epoch 12968: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6570, training loss 0.00434, validation loss 0.00402
Epoch 12979: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12990: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6576
Mean train loss for ascent epoch 6577: -1.9604032786446624e-05
Mean eval for ascent epoch 6577: 0.0039811814203858376
Doing Evaluation on the model now
This is Epoch 6580, training loss 0.55732, validation loss 0.33032
Epoch 13001: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13012: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6590, training loss 0.03638, validation loss 0.03390
Epoch 13023: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13034: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6600, training loss 0.00833, validation loss 0.00616
Resetting learning rate to 0.01000
Epoch 13045: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13056: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6610, training loss 0.00369, validation loss 0.00420
Epoch 13067: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6620, training loss 0.00282, validation loss 0.00338
Epoch 13078: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6621
Mean train loss for ascent epoch 6622: -2.2715010345564224e-05
Mean eval for ascent epoch 6622: 0.003052492393180728
Epoch 13089: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6630, training loss 0.15793, validation loss 0.14276
Epoch 13100: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13111: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6640, training loss 0.01504, validation loss 0.01283
Epoch 13122: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13133: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6650, training loss 0.00466, validation loss 0.00427
Epoch 13144: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13155: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6660
Doing Evaluation on the model now
This is Epoch 6660, training loss 0.00279, validation loss 0.00296
Mean train loss for ascent epoch 6661: -1.6502506696269847e-05
Mean eval for ascent epoch 6661: 0.0028603787068277597
Epoch 13166: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6670, training loss 0.10304, validation loss 0.06138
Epoch 13177: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13188: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6680, training loss 0.01435, validation loss 0.01869
Epoch 13199: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13210: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6690, training loss 0.00363, validation loss 0.00300
Epoch 13221: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13232: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6699
Mean train loss for ascent epoch 6700: -2.1258238120935857e-05
Mean eval for ascent epoch 6700: 0.0032734526321291924
Doing Evaluation on the model now
This is Epoch 6700, training loss 0.00327, validation loss 0.00264
Epoch 13243: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13254: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6710, training loss 0.04766, validation loss 0.06665
Epoch 13265: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6720, training loss 0.01411, validation loss 0.01243
Epoch 13276: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13287: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6730, training loss 0.00543, validation loss 0.00413
Epoch 13298: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13309: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6738
Mean train loss for ascent epoch 6739: -1.4300547263701446e-05
Mean eval for ascent epoch 6739: 0.0024643121287226677
Doing Evaluation on the model now
This is Epoch 6740, training loss 0.14995, validation loss 0.07141
Epoch 13320: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13331: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6750, training loss 0.05746, validation loss 0.07133
Epoch 13342: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13353: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6760, training loss 0.01139, validation loss 0.00803
Epoch 13364: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6770, training loss 0.00476, validation loss 0.00456
Epoch 13375: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13386: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6777
Mean train loss for ascent epoch 6778: -2.1907524569542147e-05
Mean eval for ascent epoch 6778: 0.002665901090949774
Doing Evaluation on the model now
This is Epoch 6780, training loss 0.11281, validation loss 0.15786
Epoch 13397: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13408: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6790, training loss 0.03478, validation loss 0.01549
Epoch 13419: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13430: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6800, training loss 0.00798, validation loss 0.00705
Resetting learning rate to 0.01000
Epoch 13441: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13452: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6810, training loss 0.00733, validation loss 0.00846
Epoch 13463: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6820, training loss 0.00300, validation loss 0.00315
Epoch 13474: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6821
Mean train loss for ascent epoch 6822: -1.808086562959943e-05
Mean eval for ascent epoch 6822: 0.002524860203266144
Epoch 13485: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6830, training loss 0.35896, validation loss 0.30402
Epoch 13496: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13507: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6840, training loss 0.01313, validation loss 0.00771
Epoch 13518: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13529: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6850, training loss 0.00549, validation loss 0.00452
Epoch 13540: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13551: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6860
Doing Evaluation on the model now
This is Epoch 6860, training loss 0.00367, validation loss 0.00347
Mean train loss for ascent epoch 6861: -1.926283584907651e-05
Mean eval for ascent epoch 6861: 0.0031424646731466055
Epoch 13562: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6870, training loss 0.09389, validation loss 0.07884
Epoch 13573: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13584: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6880, training loss 0.00763, validation loss 0.00861
Epoch 13595: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13606: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6890, training loss 0.00517, validation loss 0.00320
Epoch 13617: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13628: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6899
Mean train loss for ascent epoch 6900: -1.6273625078611076e-05
Mean eval for ascent epoch 6900: 0.003436972387135029
Doing Evaluation on the model now
This is Epoch 6900, training loss 0.00344, validation loss 0.00256
Epoch 13639: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6910, training loss 0.09857, validation loss 0.07205
Epoch 13650: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13661: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6920, training loss 0.01694, validation loss 0.01230
Epoch 13672: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13683: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6930, training loss 0.00480, validation loss 0.00386
Epoch 13694: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13705: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6938
Mean train loss for ascent epoch 6939: -1.5213906408462208e-05
Mean eval for ascent epoch 6939: 0.0030808004084974527
Doing Evaluation on the model now
This is Epoch 6940, training loss 0.26875, validation loss 0.91795
Epoch 13716: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13727: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6950, training loss 0.05017, validation loss 0.03353
Epoch 13738: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6960, training loss 0.01803, validation loss 0.03216
Epoch 13749: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13760: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6970, training loss 0.00430, validation loss 0.00421
Epoch 13771: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13782: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6977
Mean train loss for ascent epoch 6978: -1.3671914530277718e-05
Mean eval for ascent epoch 6978: 0.003082549199461937
Doing Evaluation on the model now
This is Epoch 6980, training loss 0.90246, validation loss 1.53229
Epoch 13793: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13804: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6990, training loss 0.04416, validation loss 0.01595
Epoch 13815: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13826: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7000, training loss 0.00893, validation loss 0.00766
Resetting learning rate to 0.01000
Epoch 13837: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7010, training loss 0.00693, validation loss 0.00649
Epoch 13848: reducing learning rate of group 0 to 2.5000e-04.
Epoch 13859: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7020, training loss 0.00301, validation loss 0.00315
Epoch 13870: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7022
Mean train loss for ascent epoch 7023: -1.4581851246475708e-05
Mean eval for ascent epoch 7023: 0.002835766179487109
Epoch 13881: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7030, training loss 0.13004, validation loss 0.19628
Epoch 13892: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13903: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7040, training loss 0.00960, validation loss 0.01001
Epoch 13914: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13925: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7050, training loss 0.00568, validation loss 0.00334
Epoch 13936: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7060, training loss 0.00310, validation loss 0.00304
Epoch 13947: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7061
Mean train loss for ascent epoch 7062: -1.3655553630087525e-05
Mean eval for ascent epoch 7062: 0.00308005278930068
Epoch 13958: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7070, training loss 0.06908, validation loss 0.07777
Epoch 13969: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13980: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7080, training loss 0.01888, validation loss 0.01700
Epoch 13991: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14002: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7090, training loss 0.00570, validation loss 0.00343
Epoch 14013: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14024: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7100
Doing Evaluation on the model now
This is Epoch 7100, training loss 0.00374, validation loss 0.00293
Mean train loss for ascent epoch 7101: -1.5809759133844636e-05
Mean eval for ascent epoch 7101: 0.0028210156597197056
Epoch 14035: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7110, training loss 0.09459, validation loss 0.06343
Epoch 14046: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14057: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7120, training loss 0.02024, validation loss 0.02272
Epoch 14068: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14079: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7130, training loss 0.00509, validation loss 0.00438
Epoch 14090: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14101: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7139
Mean train loss for ascent epoch 7140: -1.7313748685410246e-05
Mean eval for ascent epoch 7140: 0.0029691243544220924
Doing Evaluation on the model now
This is Epoch 7140, training loss 0.00297, validation loss 0.00305
Epoch 14112: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14123: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7150, training loss 0.18866, validation loss 0.08823
Epoch 14134: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7160, training loss 0.01514, validation loss 0.01175
Epoch 14145: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14156: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7170, training loss 0.00450, validation loss 0.00444
Epoch 14167: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14178: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7178
Mean train loss for ascent epoch 7179: -1.6112602679640986e-05
Mean eval for ascent epoch 7179: 0.0036264085210859776
Doing Evaluation on the model now
This is Epoch 7180, training loss 0.50198, validation loss 0.73268
Epoch 14189: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14200: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7190, training loss 0.05876, validation loss 0.08222
Epoch 14211: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14222: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7200, training loss 0.01392, validation loss 0.02286
Resetting learning rate to 0.01000
Epoch 14233: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7210, training loss 0.00820, validation loss 0.00640
Epoch 14244: reducing learning rate of group 0 to 2.5000e-04.
Epoch 14255: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7220, training loss 0.00303, validation loss 0.00307
Epoch 14266: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7222
Mean train loss for ascent epoch 7223: -1.570998392708134e-05
Mean eval for ascent epoch 7223: 0.0033040407579392195
Epoch 14277: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7230, training loss 0.15517, validation loss 0.05260
Epoch 14288: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14299: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7240, training loss 0.03285, validation loss 0.02312
Epoch 14310: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14321: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7250, training loss 0.00694, validation loss 0.00793
Epoch 14332: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7260, training loss 0.00302, validation loss 0.00275
Epoch 14343: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7261
Mean train loss for ascent epoch 7262: -1.6319803762598895e-05
Mean eval for ascent epoch 7262: 0.0031357980333268642
Epoch 14354: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7270, training loss 0.06017, validation loss 0.05660
Epoch 14365: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14376: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7280, training loss 0.01523, validation loss 0.00616
Epoch 14387: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14398: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7290, training loss 0.00405, validation loss 0.00330
Epoch 14409: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14420: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7300
Doing Evaluation on the model now
This is Epoch 7300, training loss 0.00318, validation loss 0.00366
Mean train loss for ascent epoch 7301: -1.97888002730906e-05
Mean eval for ascent epoch 7301: 0.003287083003669977
Epoch 14431: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7310, training loss 0.07831, validation loss 0.14619
Epoch 14442: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14453: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7320, training loss 0.01035, validation loss 0.00650
Epoch 14464: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14475: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7330, training loss 0.00448, validation loss 0.00328
Epoch 14486: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14497: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7339
Mean train loss for ascent epoch 7340: -1.9094966773991473e-05
Mean eval for ascent epoch 7340: 0.002911090385168791
Doing Evaluation on the model now
This is Epoch 7340, training loss 0.00291, validation loss 0.00323
Epoch 14508: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7350, training loss 0.10429, validation loss 0.05011
Epoch 14519: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14530: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7360, training loss 0.01136, validation loss 0.00951
Epoch 14541: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14552: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7370, training loss 0.00446, validation loss 0.00348
Epoch 14563: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14574: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7378
Mean train loss for ascent epoch 7379: -2.141515869880095e-05
Mean eval for ascent epoch 7379: 0.0029224175959825516
Doing Evaluation on the model now
This is Epoch 7380, training loss 0.14968, validation loss 0.09144
Epoch 14585: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14596: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7390, training loss 0.05479, validation loss 0.03194
Epoch 14607: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7400, training loss 0.01652, validation loss 0.02835
Epoch 14618: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 14629: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7410, training loss 0.00621, validation loss 0.00379
Epoch 14640: reducing learning rate of group 0 to 2.5000e-04.
Epoch 14651: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7420, training loss 0.00304, validation loss 0.00269
Epoch 14662: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7423
Mean train loss for ascent epoch 7424: -2.5760145945241675e-05
Mean eval for ascent epoch 7424: 0.002746505895629525
Epoch 14673: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7430, training loss 0.14319, validation loss 0.15227
Epoch 14684: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14695: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7440, training loss 0.01254, validation loss 0.00618
Epoch 14706: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7450, training loss 0.00532, validation loss 0.00432
Epoch 14717: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14728: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7460, training loss 0.00270, validation loss 0.00318
Epoch 14739: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7462
Mean train loss for ascent epoch 7463: -1.6390700693591498e-05
Mean eval for ascent epoch 7463: 0.002762517426162958
Epoch 14750: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7470, training loss 0.06003, validation loss 0.09236
Epoch 14761: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14772: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7480, training loss 0.01846, validation loss 0.00965
Epoch 14783: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14794: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7490, training loss 0.00444, validation loss 0.00359
Epoch 14805: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7500, training loss 0.00320, validation loss 0.00434
Epoch 14816: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7501
Mean train loss for ascent epoch 7502: -1.5822879504412413e-05
Mean eval for ascent epoch 7502: 0.0026872847229242325
Epoch 14827: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7510, training loss 0.12865, validation loss 0.21360
Epoch 14838: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14849: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7520, training loss 0.01257, validation loss 0.00662
Epoch 14860: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14871: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7530, training loss 0.00337, validation loss 0.00304
Epoch 14882: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14893: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7540
Doing Evaluation on the model now
This is Epoch 7540, training loss 0.00281, validation loss 0.00259
Mean train loss for ascent epoch 7541: -1.4594802451028954e-05
Mean eval for ascent epoch 7541: 0.00335652194917202
Epoch 14904: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7550, training loss 0.07353, validation loss 0.05537
Epoch 14915: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14926: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7560, training loss 0.01457, validation loss 0.00998
Epoch 14937: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14948: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7570, training loss 0.00368, validation loss 0.00291
Epoch 14959: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14970: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7579
Mean train loss for ascent epoch 7580: -2.570952165115159e-05
Mean eval for ascent epoch 7580: 0.0030934754759073257
Doing Evaluation on the model now
This is Epoch 7580, training loss 0.00309, validation loss 0.00405
Epoch 14981: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14992: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7590, training loss 0.04940, validation loss 0.05788
Epoch 15003: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7600, training loss 0.01654, validation loss 0.01089
Resetting learning rate to 0.01000
Epoch 15014: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15025: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7610, training loss 0.00327, validation loss 0.00270
Epoch 15036: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15047: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7618
Mean train loss for ascent epoch 7619: -1.5977753719198518e-05
Mean eval for ascent epoch 7619: 0.00266876514069736
Doing Evaluation on the model now
This is Epoch 7620, training loss 0.34438, validation loss 0.93090
Epoch 15058: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15069: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7630, training loss 0.06371, validation loss 0.02975
Epoch 15080: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15091: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7640, training loss 0.01545, validation loss 0.00718
Epoch 15102: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7650, training loss 0.00419, validation loss 0.00398
Epoch 15113: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15124: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7657
Mean train loss for ascent epoch 7658: -1.6533136658836156e-05
Mean eval for ascent epoch 7658: 0.003078267676755786
Doing Evaluation on the model now
This is Epoch 7660, training loss 0.38187, validation loss 0.59316
Epoch 15135: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15146: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7670, training loss 0.02761, validation loss 0.05495
Epoch 15157: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15168: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7680, training loss 0.00778, validation loss 0.00781
Epoch 15179: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15190: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7690, training loss 0.00372, validation loss 0.00431
Epoch 15201: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7696
Mean train loss for ascent epoch 7697: -1.6860120013006963e-05
Mean eval for ascent epoch 7697: 0.0031767068430781364
Doing Evaluation on the model now
This is Epoch 7700, training loss 1.04075, validation loss 1.96936
Epoch 15212: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15223: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7710, training loss 0.03256, validation loss 0.02124
Epoch 15234: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15245: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7720, training loss 0.00718, validation loss 0.00661
Epoch 15256: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15267: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7730, training loss 0.00371, validation loss 0.00359
Epoch 15278: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7735
Mean train loss for ascent epoch 7736: -1.5663812519051135e-05
Mean eval for ascent epoch 7736: 0.0035012494772672653
Doing Evaluation on the model now
This is Epoch 7740, training loss 0.53839, validation loss 0.49714
Epoch 15289: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15300: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7750, training loss 0.02933, validation loss 0.04253
Epoch 15311: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15322: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7760, training loss 0.00612, validation loss 0.00889
Epoch 15333: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15344: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7770, training loss 0.00330, validation loss 0.00328
Epoch 15355: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7774
Mean train loss for ascent epoch 7775: -1.689935743343085e-05
Mean eval for ascent epoch 7775: 0.0032841793727129698
Epoch 15366: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7780, training loss 0.31359, validation loss 0.02741
Epoch 15377: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7790, training loss 0.05001, validation loss 0.03684
Epoch 15388: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15399: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7800, training loss 0.00603, validation loss 0.00767
Resetting learning rate to 0.01000
Epoch 15410: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15421: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7810, training loss 0.00382, validation loss 0.00304
Epoch 15432: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15443: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7818
Mean train loss for ascent epoch 7819: -1.5819427062524483e-05
Mean eval for ascent epoch 7819: 0.0033717993646860123
Doing Evaluation on the model now
This is Epoch 7820, training loss 0.12380, validation loss 0.35500
Epoch 15454: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15465: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7830, training loss 0.04747, validation loss 0.05538
Epoch 15476: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7840, training loss 0.01357, validation loss 0.02882
Epoch 15487: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15498: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7850, training loss 0.00477, validation loss 0.00422
Epoch 15509: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15520: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7857
Mean train loss for ascent epoch 7858: -2.0524790670606308e-05
Mean eval for ascent epoch 7858: 0.0032524149864912033
Doing Evaluation on the model now
This is Epoch 7860, training loss 0.14455, validation loss 0.03059
Epoch 15531: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15542: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7870, training loss 0.03971, validation loss 0.01672
Epoch 15553: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15564: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7880, training loss 0.00890, validation loss 0.00609
Epoch 15575: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7890, training loss 0.00357, validation loss 0.00435
Epoch 15586: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15597: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7896
Mean train loss for ascent epoch 7897: -1.8253356756758876e-05
Mean eval for ascent epoch 7897: 0.003095016349107027
Doing Evaluation on the model now
This is Epoch 7900, training loss 0.21486, validation loss 0.10567
Epoch 15608: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15619: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7910, training loss 0.02916, validation loss 0.02585
Epoch 15630: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15641: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7920, training loss 0.00497, validation loss 0.00570
Epoch 15652: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15663: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7930, training loss 0.00396, validation loss 0.00292
Epoch 15674: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7935
Mean train loss for ascent epoch 7936: -1.6854586647241376e-05
Mean eval for ascent epoch 7936: 0.0028088660910725594
Doing Evaluation on the model now
This is Epoch 7940, training loss 0.35930, validation loss 0.09534
Epoch 15685: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15696: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7950, training loss 0.02308, validation loss 0.02263
Epoch 15707: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15718: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7960, training loss 0.00590, validation loss 0.00487
Epoch 15729: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15740: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7970, training loss 0.00305, validation loss 0.00313
Epoch 15751: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7974
Mean train loss for ascent epoch 7975: -1.7081436453736387e-05
Mean eval for ascent epoch 7975: 0.0026023248210549355
Epoch 15762: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7980, training loss 0.51269, validation loss 0.25062
Epoch 15773: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7990, training loss 0.03380, validation loss 0.03074
Epoch 15784: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15795: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8000, training loss 0.00644, validation loss 0.00673
Resetting learning rate to 0.01000
Epoch 15806: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15817: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8010, training loss 0.00292, validation loss 0.00344
Epoch 15828: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15839: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8019
Mean train loss for ascent epoch 8020: -1.0924024536507204e-05
Mean eval for ascent epoch 8020: 0.002405799925327301
Doing Evaluation on the model now
This is Epoch 8020, training loss 0.00241, validation loss 0.00247
Epoch 15850: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15861: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8030, training loss 0.04137, validation loss 0.04778
Epoch 15872: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8040, training loss 0.02243, validation loss 0.01870
Epoch 15883: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15894: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8050, training loss 0.00305, validation loss 0.00356
Epoch 15905: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15916: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8058
Mean train loss for ascent epoch 8059: -1.3581581697508227e-05
Mean eval for ascent epoch 8059: 0.0023631195072084665
Doing Evaluation on the model now
This is Epoch 8060, training loss 0.14674, validation loss 0.34600
Epoch 15927: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15938: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8070, training loss 0.06751, validation loss 0.06412
Epoch 15949: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15960: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8080, training loss 0.01893, validation loss 0.01022
Epoch 15971: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8090, training loss 0.00386, validation loss 0.00314
Epoch 15982: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15993: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8097
Mean train loss for ascent epoch 8098: -1.0898640539380722e-05
Mean eval for ascent epoch 8098: 0.002504033036530018
Doing Evaluation on the model now
This is Epoch 8100, training loss 0.56641, validation loss 1.07106
Epoch 16004: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16015: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8110, training loss 0.03006, validation loss 0.01414
Epoch 16026: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16037: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8120, training loss 0.00707, validation loss 0.00477
Epoch 16048: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16059: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8130, training loss 0.00436, validation loss 0.00453
Epoch 16070: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8136
Mean train loss for ascent epoch 8137: -1.280983087781351e-05
Mean eval for ascent epoch 8137: 0.0025799355935305357
Doing Evaluation on the model now
This is Epoch 8140, training loss 0.30481, validation loss 0.25179
Epoch 16081: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16092: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8150, training loss 0.02894, validation loss 0.03583
Epoch 16103: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16114: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8160, training loss 0.00508, validation loss 0.00472
Epoch 16125: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16136: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8170, training loss 0.00307, validation loss 0.00280
Epoch 16147: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8175
Mean train loss for ascent epoch 8176: -2.0909757949993946e-05
Mean eval for ascent epoch 8176: 0.00337226246483624
Doing Evaluation on the model now
This is Epoch 8180, training loss 0.58488, validation loss 0.27792
Epoch 16158: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16169: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8190, training loss 0.01652, validation loss 0.02230
Epoch 16180: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16191: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8200, training loss 0.01177, validation loss 0.00704
Resetting learning rate to 0.01000
Epoch 16202: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16213: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8210, training loss 0.00373, validation loss 0.00496
Epoch 16224: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16235: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8219
Mean train loss for ascent epoch 8220: -2.219718044216279e-05
Mean eval for ascent epoch 8220: 0.0029243065509945154
Doing Evaluation on the model now
This is Epoch 8220, training loss 0.00292, validation loss 0.00267
Epoch 16246: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8230, training loss 0.10373, validation loss 0.05045
Epoch 16257: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16268: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8240, training loss 0.01084, validation loss 0.02568
Epoch 16279: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16290: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8250, training loss 0.00328, validation loss 0.00376
Epoch 16301: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16312: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8258
Mean train loss for ascent epoch 8259: -1.499930840509478e-05
Mean eval for ascent epoch 8259: 0.0029469896107912064
Doing Evaluation on the model now
This is Epoch 8260, training loss 0.17470, validation loss 0.25515
Epoch 16323: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16334: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8270, training loss 0.04972, validation loss 0.01880
Epoch 16345: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8280, training loss 0.01466, validation loss 0.02175
Epoch 16356: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16367: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8290, training loss 0.00351, validation loss 0.00441
Epoch 16378: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16389: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8297
Mean train loss for ascent epoch 8298: -1.298601728194626e-05
Mean eval for ascent epoch 8298: 0.002913782140240073
Doing Evaluation on the model now
This is Epoch 8300, training loss 2.61970, validation loss 0.96308
Epoch 16400: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16411: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8310, training loss 0.03234, validation loss 0.03650
Epoch 16422: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16433: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8320, training loss 0.00875, validation loss 0.00330
Epoch 16444: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8330, training loss 0.00487, validation loss 0.00552
Epoch 16455: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16466: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8336
Mean train loss for ascent epoch 8337: -1.736511694616638e-05
Mean eval for ascent epoch 8337: 0.002472552005201578
Doing Evaluation on the model now
This is Epoch 8340, training loss 1.21186, validation loss 0.83965
Epoch 16477: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16488: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8350, training loss 0.04041, validation loss 0.02865
Epoch 16499: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16510: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8360, training loss 0.01281, validation loss 0.01210
Epoch 16521: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16532: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8370, training loss 0.00368, validation loss 0.00524
Epoch 16543: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8375
Mean train loss for ascent epoch 8376: -1.4748916328244377e-05
Mean eval for ascent epoch 8376: 0.002846537623554468
Doing Evaluation on the model now
This is Epoch 8380, training loss 1.74903, validation loss 2.14714
Epoch 16554: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16565: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8390, training loss 0.02058, validation loss 0.04104
Epoch 16576: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16587: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8400, training loss 0.00569, validation loss 0.00596
Resetting learning rate to 0.01000
Epoch 16598: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16609: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8410, training loss 0.00278, validation loss 0.00294
Epoch 16620: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16631: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8420
Doing Evaluation on the model now
This is Epoch 8420, training loss 0.00303, validation loss 0.00298
Mean train loss for ascent epoch 8421: -1.4736718185304198e-05
Mean eval for ascent epoch 8421: 0.0027142756152898073
Epoch 16642: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8430, training loss 0.06955, validation loss 0.03688
Epoch 16653: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16664: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8440, training loss 0.01789, validation loss 0.01543
Epoch 16675: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16686: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8450, training loss 0.00354, validation loss 0.00312
Epoch 16697: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16708: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8459
Mean train loss for ascent epoch 8460: -1.8576058209873736e-05
Mean eval for ascent epoch 8460: 0.0026346337981522083
Doing Evaluation on the model now
This is Epoch 8460, training loss 0.00263, validation loss 0.00267
Epoch 16719: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16730: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8470, training loss 0.08167, validation loss 0.04768
Epoch 16741: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8480, training loss 0.01143, validation loss 0.00544
Epoch 16752: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16763: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8490, training loss 0.00408, validation loss 0.00327
Epoch 16774: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16785: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8498
Mean train loss for ascent epoch 8499: -1.533356225991156e-05
Mean eval for ascent epoch 8499: 0.002940640551969409
Doing Evaluation on the model now
This is Epoch 8500, training loss 0.13218, validation loss 0.15173
Epoch 16796: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16807: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8510, training loss 0.03626, validation loss 0.04086
Epoch 16818: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16829: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8520, training loss 0.01116, validation loss 0.00602
Epoch 16840: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8530, training loss 0.00464, validation loss 0.00573
Epoch 16851: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16862: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8537
Mean train loss for ascent epoch 8538: -1.4975930753280409e-05
Mean eval for ascent epoch 8538: 0.0029329361859709024
Doing Evaluation on the model now
This is Epoch 8540, training loss 1.23900, validation loss 1.36872
Epoch 16873: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16884: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8550, training loss 0.03453, validation loss 0.08043
Epoch 16895: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16906: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8560, training loss 0.00588, validation loss 0.00322
Epoch 16917: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16928: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8570, training loss 0.00477, validation loss 0.00328
Epoch 16939: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8576
Mean train loss for ascent epoch 8577: -2.046698318736162e-05
Mean eval for ascent epoch 8577: 0.00297924829646945
Doing Evaluation on the model now
This is Epoch 8580, training loss 2.90046, validation loss 1.14677
Epoch 16950: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16961: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8590, training loss 0.05232, validation loss 0.05311
Epoch 16972: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16983: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8600, training loss 0.00485, validation loss 0.00375
Resetting learning rate to 0.01000
Epoch 16994: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17005: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8610, training loss 0.00448, validation loss 0.00614
Epoch 17016: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17027: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8620
Doing Evaluation on the model now
This is Epoch 8620, training loss 0.00402, validation loss 0.00328
Mean train loss for ascent epoch 8621: -1.1919786629732698e-05
Mean eval for ascent epoch 8621: 0.003169039497151971
Epoch 17038: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8630, training loss 0.11311, validation loss 0.06852
Epoch 17049: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17060: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8640, training loss 0.02047, validation loss 0.00518
Epoch 17071: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17082: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8650, training loss 0.00436, validation loss 0.00303
Epoch 17093: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17104: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8659
Mean train loss for ascent epoch 8660: -1.564115700602997e-05
Mean eval for ascent epoch 8660: 0.0025230906903743744
Doing Evaluation on the model now
This is Epoch 8660, training loss 0.00252, validation loss 0.00282
Epoch 17115: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8670, training loss 0.04938, validation loss 0.08447
Epoch 17126: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17137: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8680, training loss 0.01164, validation loss 0.01250
Epoch 17148: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17159: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8690, training loss 0.00317, validation loss 0.00311
Epoch 17170: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17181: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8698
Mean train loss for ascent epoch 8699: -1.2804790458176285e-05
Mean eval for ascent epoch 8699: 0.0027420329861342907
Doing Evaluation on the model now
This is Epoch 8700, training loss 0.12027, validation loss 0.16022
Epoch 17192: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17203: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8710, training loss 0.03714, validation loss 0.02290
Epoch 17214: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8720, training loss 0.01659, validation loss 0.00698
Epoch 17225: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17236: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8730, training loss 0.00301, validation loss 0.00331
Epoch 17247: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17258: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8737
Mean train loss for ascent epoch 8738: -1.1164916031702887e-05
Mean eval for ascent epoch 8738: 0.0026804974768310785
Doing Evaluation on the model now
This is Epoch 8740, training loss 0.16996, validation loss 0.18029
Epoch 17269: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17280: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8750, training loss 0.03144, validation loss 0.05462
Epoch 17291: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17302: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8760, training loss 0.00835, validation loss 0.00913
Epoch 17313: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8770, training loss 0.00313, validation loss 0.00349
Epoch 17324: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17335: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8776
Mean train loss for ascent epoch 8777: -1.4673410078103188e-05
Mean eval for ascent epoch 8777: 0.0030855548102408648
Doing Evaluation on the model now
This is Epoch 8780, training loss 0.40769, validation loss 0.25934
Epoch 17346: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17357: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8790, training loss 0.02724, validation loss 0.01894
Epoch 17368: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17379: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8800, training loss 0.00748, validation loss 0.00593
Resetting learning rate to 0.01000
Epoch 17390: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17401: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8810, training loss 0.00359, validation loss 0.00368
Epoch 17412: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8820, training loss 0.00260, validation loss 0.00247
Epoch 17423: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8821
Mean train loss for ascent epoch 8822: -1.4203520549926907e-05
Mean eval for ascent epoch 8822: 0.00268614967353642
Epoch 17434: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8830, training loss 0.17332, validation loss 0.16057
Epoch 17445: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17456: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8840, training loss 0.01117, validation loss 0.01004
Epoch 17467: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17478: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8850, training loss 0.00389, validation loss 0.00526
Epoch 17489: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17500: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8860
Doing Evaluation on the model now
This is Epoch 8860, training loss 0.00311, validation loss 0.00382
Mean train loss for ascent epoch 8861: -1.5339222954935394e-05
Mean eval for ascent epoch 8861: 0.0030752771999686956
Epoch 17511: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8870, training loss 0.03574, validation loss 0.05187
Epoch 17522: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17533: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8880, training loss 0.01229, validation loss 0.01451
Epoch 17544: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17555: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8890, training loss 0.00369, validation loss 0.00361
Epoch 17566: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17577: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8899
Mean train loss for ascent epoch 8900: -1.6629701349302195e-05
Mean eval for ascent epoch 8900: 0.0027533189859241247
Doing Evaluation on the model now
This is Epoch 8900, training loss 0.00275, validation loss 0.00240
Epoch 17588: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17599: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8910, training loss 0.09193, validation loss 0.10671
Epoch 17610: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8920, training loss 0.00849, validation loss 0.00769
Epoch 17621: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17632: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8930, training loss 0.00364, validation loss 0.00299
Epoch 17643: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17654: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8938
Mean train loss for ascent epoch 8939: -1.096024425351061e-05
Mean eval for ascent epoch 8939: 0.0024282485246658325
Doing Evaluation on the model now
This is Epoch 8940, training loss 0.39647, validation loss 0.99483
Epoch 17665: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17676: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8950, training loss 0.03649, validation loss 0.02696
Epoch 17687: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17698: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8960, training loss 0.00782, validation loss 0.01130
Epoch 17709: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8970, training loss 0.00527, validation loss 0.00300
Epoch 17720: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17731: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8977
Mean train loss for ascent epoch 8978: -1.335906381427776e-05
Mean eval for ascent epoch 8978: 0.0026000409852713346
Doing Evaluation on the model now
This is Epoch 8980, training loss 0.64900, validation loss 1.24441
Epoch 17742: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17753: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8990, training loss 0.10988, validation loss 0.09558
Epoch 17764: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17775: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9000, training loss 0.00524, validation loss 0.00571
Resetting learning rate to 0.01000
Epoch 17786: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17797: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9010, training loss 0.00599, validation loss 0.00332
Epoch 17808: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9020, training loss 0.00316, validation loss 0.00322
Epoch 17819: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9021
Mean train loss for ascent epoch 9022: -1.4538666619046126e-05
Mean eval for ascent epoch 9022: 0.0028315589297562838
Epoch 17830: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9030, training loss 0.06831, validation loss 0.02527
Epoch 17841: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17852: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9040, training loss 0.01124, validation loss 0.00709
Epoch 17863: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17874: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9050, training loss 0.00510, validation loss 0.00432
Epoch 17885: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17896: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9060
Doing Evaluation on the model now
This is Epoch 9060, training loss 0.00324, validation loss 0.00301
Mean train loss for ascent epoch 9061: -1.79570652107941e-05
Mean eval for ascent epoch 9061: 0.0029144759755581617
Epoch 17907: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9070, training loss 0.08846, validation loss 0.03973
Epoch 17918: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17929: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9080, training loss 0.00951, validation loss 0.00718
Epoch 17940: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17951: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9090, training loss 0.00342, validation loss 0.00280
Epoch 17962: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17973: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9099
Mean train loss for ascent epoch 9100: -1.598276139702648e-05
Mean eval for ascent epoch 9100: 0.0027370646130293608
Doing Evaluation on the model now
This is Epoch 9100, training loss 0.00274, validation loss 0.00245
Epoch 17984: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9110, training loss 0.10836, validation loss 0.06147
Epoch 17995: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18006: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9120, training loss 0.01586, validation loss 0.01390
Epoch 18017: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18028: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9130, training loss 0.00446, validation loss 0.00253
Epoch 18039: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18050: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9138
Mean train loss for ascent epoch 9139: -1.6351132217096165e-05
Mean eval for ascent epoch 9139: 0.0032672923989593983
Doing Evaluation on the model now
This is Epoch 9140, training loss 0.13337, validation loss 0.26575
Epoch 18061: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18072: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9150, training loss 0.07314, validation loss 0.03763
Epoch 18083: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9160, training loss 0.01865, validation loss 0.01505
Epoch 18094: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18105: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9170, training loss 0.00435, validation loss 0.00391
Epoch 18116: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18127: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9177
Mean train loss for ascent epoch 9178: -1.2608501492650248e-05
Mean eval for ascent epoch 9178: 0.0033727632835507393
Doing Evaluation on the model now
This is Epoch 9180, training loss 0.19938, validation loss 0.05542
Epoch 18138: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18149: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9190, training loss 0.02647, validation loss 0.04203
Epoch 18160: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18171: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9200, training loss 0.00782, validation loss 0.00373
Resetting learning rate to 0.01000
Epoch 18182: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9210, training loss 0.00424, validation loss 0.00481
Epoch 18193: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18204: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9220, training loss 0.00310, validation loss 0.00357
Epoch 18215: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9222
Mean train loss for ascent epoch 9223: -1.686529503786005e-05
Mean eval for ascent epoch 9223: 0.00310384389013052
Epoch 18226: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9230, training loss 0.09113, validation loss 0.09770
Epoch 18237: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18248: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9240, training loss 0.01386, validation loss 0.01091
Epoch 18259: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18270: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9250, training loss 0.00497, validation loss 0.00599
Epoch 18281: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9260, training loss 0.00336, validation loss 0.00326
Epoch 18292: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9261
Mean train loss for ascent epoch 9262: -1.2026255717501044e-05
Mean eval for ascent epoch 9262: 0.00259879301302135
Epoch 18303: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9270, training loss 0.15856, validation loss 0.10163
Epoch 18314: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18325: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9280, training loss 0.01403, validation loss 0.01215
Epoch 18336: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18347: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9290, training loss 0.00368, validation loss 0.00351
Epoch 18358: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18369: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9300
Doing Evaluation on the model now
This is Epoch 9300, training loss 0.00317, validation loss 0.00283
Mean train loss for ascent epoch 9301: -1.986363531614188e-05
Mean eval for ascent epoch 9301: 0.0031542605720460415
Epoch 18380: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9310, training loss 0.10027, validation loss 0.12931
Epoch 18391: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18402: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9320, training loss 0.01420, validation loss 0.00811
Epoch 18413: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18424: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9330, training loss 0.00351, validation loss 0.00401
Epoch 18435: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18446: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9339
Mean train loss for ascent epoch 9340: -1.5597754099871963e-05
Mean eval for ascent epoch 9340: 0.00247228448279202
Doing Evaluation on the model now
This is Epoch 9340, training loss 0.00247, validation loss 0.00223
Epoch 18457: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18468: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9350, training loss 0.08276, validation loss 0.06223
Epoch 18479: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9360, training loss 0.01005, validation loss 0.00928
Epoch 18490: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18501: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9370, training loss 0.00449, validation loss 0.00580
Epoch 18512: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18523: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9378
Mean train loss for ascent epoch 9379: -1.4713996279169805e-05
Mean eval for ascent epoch 9379: 0.0025316986721009016
Doing Evaluation on the model now
This is Epoch 9380, training loss 0.12863, validation loss 0.18283
Epoch 18534: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18545: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9390, training loss 0.04798, validation loss 0.03460
Epoch 18556: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18567: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9400, training loss 0.01960, validation loss 0.00924
Resetting learning rate to 0.01000
Epoch 18578: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9410, training loss 0.00464, validation loss 0.00641
Epoch 18589: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18600: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9420, training loss 0.00344, validation loss 0.00432
Epoch 18611: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9422
Mean train loss for ascent epoch 9423: -1.874649024102837e-05
Mean eval for ascent epoch 9423: 0.0036003582645207644
Epoch 18622: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9430, training loss 0.21969, validation loss 0.18389
Epoch 18633: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18644: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9440, training loss 0.02019, validation loss 0.01799
Epoch 18655: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18666: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9450, training loss 0.00904, validation loss 0.01698
Epoch 18677: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9460, training loss 0.00347, validation loss 0.00313
Epoch 18688: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9461
Mean train loss for ascent epoch 9462: -1.622721720195841e-05
Mean eval for ascent epoch 9462: 0.003442893736064434
Epoch 18699: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9470, training loss 0.14774, validation loss 0.22520
Epoch 18710: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18721: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9480, training loss 0.01413, validation loss 0.01343
Epoch 18732: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18743: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9490, training loss 0.00412, validation loss 0.00340
Epoch 18754: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18765: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9500
Doing Evaluation on the model now
This is Epoch 9500, training loss 0.00332, validation loss 0.00349
Mean train loss for ascent epoch 9501: -1.876232636277564e-05
Mean eval for ascent epoch 9501: 0.0032262986060231924
Epoch 18776: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9510, training loss 0.02517, validation loss 0.01236
Epoch 18787: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18798: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9520, training loss 0.01328, validation loss 0.00887
Epoch 18809: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18820: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9530, training loss 0.00392, validation loss 0.00302
Epoch 18831: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18842: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9539
Mean train loss for ascent epoch 9540: -1.917036934173666e-05
Mean eval for ascent epoch 9540: 0.0026026361156255007
Doing Evaluation on the model now
This is Epoch 9540, training loss 0.00260, validation loss 0.00292
Epoch 18853: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9550, training loss 0.11041, validation loss 0.07587
Epoch 18864: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18875: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9560, training loss 0.01828, validation loss 0.01025
Epoch 18886: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18897: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9570, training loss 0.00443, validation loss 0.00406
Epoch 18908: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18919: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9578
Mean train loss for ascent epoch 9579: -1.6575750123593025e-05
Mean eval for ascent epoch 9579: 0.0033155065029859543
Doing Evaluation on the model now
This is Epoch 9580, training loss 0.17463, validation loss 0.60380
Epoch 18930: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18941: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9590, training loss 0.04662, validation loss 0.01818
Epoch 18952: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9600, training loss 0.01116, validation loss 0.01009
Epoch 18963: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 18974: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9610, training loss 0.00517, validation loss 0.00742
Epoch 18985: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18996: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9620, training loss 0.00284, validation loss 0.00266
Epoch 19007: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9623
Mean train loss for ascent epoch 9624: -1.5727078789495863e-05
Mean eval for ascent epoch 9624: 0.0031600594520568848
Epoch 19018: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9630, training loss 0.17186, validation loss 0.40981
Epoch 19029: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19040: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9640, training loss 0.02374, validation loss 0.01922
Epoch 19051: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9650, training loss 0.00823, validation loss 0.00921
Epoch 19062: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19073: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9660, training loss 0.00396, validation loss 0.00362
Epoch 19084: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9662
Mean train loss for ascent epoch 9663: -1.6093918020487763e-05
Mean eval for ascent epoch 9663: 0.0033681043423712254
Epoch 19095: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9670, training loss 0.09213, validation loss 0.14293
Epoch 19106: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19117: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9680, training loss 0.01873, validation loss 0.01096
Epoch 19128: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19139: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9690, training loss 0.00610, validation loss 0.00474
Epoch 19150: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9700, training loss 0.00376, validation loss 0.00321
Epoch 19161: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9701
Mean train loss for ascent epoch 9702: -1.4920522517058998e-05
Mean eval for ascent epoch 9702: 0.003447470022365451
Epoch 19172: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9710, training loss 0.05577, validation loss 0.06051
Epoch 19183: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19194: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9720, training loss 0.01813, validation loss 0.00822
Epoch 19205: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19216: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9730, training loss 0.00509, validation loss 0.00422
Epoch 19227: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19238: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9740
Doing Evaluation on the model now
This is Epoch 9740, training loss 0.00378, validation loss 0.00311
Mean train loss for ascent epoch 9741: -1.8073065803037025e-05
Mean eval for ascent epoch 9741: 0.0034358755219727755
Epoch 19249: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9750, training loss 0.06981, validation loss 0.05951
Epoch 19260: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19271: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9760, training loss 0.01723, validation loss 0.02441
Epoch 19282: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19293: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9770, training loss 0.00468, validation loss 0.00605
Epoch 19304: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19315: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9779
Mean train loss for ascent epoch 9780: -1.6491758287884295e-05
Mean eval for ascent epoch 9780: 0.0034813685342669487
Doing Evaluation on the model now
This is Epoch 9780, training loss 0.00348, validation loss 0.00343
Epoch 19326: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19337: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9790, training loss 0.03969, validation loss 0.03672
Epoch 19348: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9800, training loss 0.01731, validation loss 0.01553
Resetting learning rate to 0.01000
Epoch 19359: reducing learning rate of group 0 to 5.0000e-04.
Epoch 19370: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9810, training loss 0.00442, validation loss 0.00351
Epoch 19381: reducing learning rate of group 0 to 1.2500e-04.
Epoch 19392: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9818
Mean train loss for ascent epoch 9819: -1.631013583391905e-05
Mean eval for ascent epoch 9819: 0.0033533258829265833
Doing Evaluation on the model now
This is Epoch 9820, training loss 0.15881, validation loss 0.27147
Epoch 19403: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19414: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9830, training loss 0.03722, validation loss 0.04459
Epoch 19425: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19436: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9840, training loss 0.01361, validation loss 0.01354
Epoch 19447: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9850, training loss 0.00403, validation loss 0.00351
Epoch 19458: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19469: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9857
Mean train loss for ascent epoch 9858: -1.0857957931875717e-05
Mean eval for ascent epoch 9858: 0.002909755101427436
Doing Evaluation on the model now
This is Epoch 9860, training loss 1.05523, validation loss 0.76694
Epoch 19480: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19491: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9870, training loss 0.02478, validation loss 0.03371
Epoch 19502: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19513: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9880, training loss 0.00647, validation loss 0.00609
Epoch 19524: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19535: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9890, training loss 0.00403, validation loss 0.00338
Epoch 19546: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9896
Mean train loss for ascent epoch 9897: -1.3732763363805134e-05
Mean eval for ascent epoch 9897: 0.0031555627938359976
Doing Evaluation on the model now
This is Epoch 9900, training loss 1.78259, validation loss 0.86235
Epoch 19557: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19568: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9910, training loss 0.02659, validation loss 0.01309
Epoch 19579: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19590: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9920, training loss 0.00874, validation loss 0.00789
Epoch 19601: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19612: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9930, training loss 0.00367, validation loss 0.00279
Epoch 19623: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9935
Mean train loss for ascent epoch 9936: -1.5335785064962693e-05
Mean eval for ascent epoch 9936: 0.003537495620548725
Doing Evaluation on the model now
This is Epoch 9940, training loss 0.34492, validation loss 0.31233
Epoch 19634: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19645: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9950, training loss 0.02499, validation loss 0.01702
Epoch 19656: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19667: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9960, training loss 0.00624, validation loss 0.00831
Epoch 19678: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19689: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9970, training loss 0.00332, validation loss 0.00340
Epoch 19700: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9974
Mean train loss for ascent epoch 9975: -1.3743278941547032e-05
Mean eval for ascent epoch 9975: 0.0027366727590560913
Epoch 19711: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9980, training loss 0.38173, validation loss 0.08279
Epoch 19722: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9990, training loss 0.04366, validation loss 0.05277
Epoch 19733: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19744: reducing learning rate of group 0 to 6.2500e-04.
POS: 
[5.743432, 2.1195652, 0.9666811, 0.7287132, 0.5828511, 0.4491954, 0.4433942, 0.3648406, 0.2759034, 0.29690063, 0.31583506, 0.3257489, 0.45579028, 0.3897891, 0.36188588, 0.2381138, 0.27028552, 0.2558764, 0.3879454, 0.23367666, 0.3604444, 0.36763617, 0.22807202, 0.20259431, 0.24122524, 0.19766243, 0.20868854, 0.20247616, 0.22405753, 0.23073266, 0.19929183, 0.17001383, 0.20578477, 0.19630226, 0.33819708, 0.20924748, 0.1652324, 0.22164033, 0.18711625, 0.21520025, 0.23886879, 0.15568773, 0.17293935, 0.18606672, 0.17656337, 0.18510672, 0.21332197, 0.17802697, 0.16746864, 0.20149718, 0.22794698, 0.1477932, 0.16203368, 0.1646665, 0.19277562, 0.15239222, 0.12842445, 0.1603165, 0.16232154, 0.20173402, 0.1736732, 0.19173317, 0.16189174, 3.1755543, 1.9330344, 1.1657908, 1.6512554, 1.0612111, 0.7911102, 1.0016121, 0.51880676, 0.682555, 0.51227206, 0.42974532, 0.45013303, 0.24590984, 0.34093627, 0.23978414, 0.3417354, 0.3230881, 0.32050774, 0.42888054, 0.24618618, 0.23926695, 0.24521649, 0.13596417, 0.3180048, 0.23638579, 0.30738038, 0.28489107, 0.20788641, 0.17857037, 0.20258103, 0.14517364, 0.12049671, 0.12590298, 0.17678176, 0.19179958, 0.22073394, 0.20596182, 0.21169168, 0.1464252, 0.6421733, 0.724569, 1.0735863, 0.97644806, 1.4625549, 1.074481, 0.33478126, 0.3594551, 0.3318424, 0.4164181, 0.45507726, 0.2903491, 0.36372006, 0.61615676, 0.38103196, 0.21305415, 0.4113584, 0.29761964, 0.26866478, 0.25803652, 0.42167407, 0.27437422, 0.2741621, 0.19946246, 0.19235937, 0.18086262, 0.2045468, 0.18647176, 0.14462341, 0.16096167, 0.1676928, 0.26323003, 0.18703139, 0.16692217, 0.1275658, 0.2011867, 0.12632206, 0.15100627, 0.11688374, 0.34283498, 0.71478564, 1.4502838, 1.0596616, 0.80263406, 0.41526484, 0.30801687, 0.48591584, 0.49386537, 0.35508984, 0.21433647, 0.14513606, 0.26794243, 0.24429758, 0.3526593, 0.25758982, 0.29613185, 0.23549025, 0.24275117, 0.13810933, 0.19219844, 0.18835795, 0.27633056, 0.22786404, 0.19214693, 0.18587458, 0.19438556, 0.11644842, 0.09994762, 0.14835219, 0.11071225, 0.1802835, 0.15617836, 0.17671396, 0.1217786, 0.11825043, 0.11510229, 0.10268186, 0.99030524, 0.93009746, 0.8914113, 0.60517186, 0.86944467, 0.59413505, 0.26211852, 0.40397903, 0.28166547, 0.28454882, 0.35247454, 0.48131165, 0.4651313, 0.27856, 0.15881658, 0.31133634, 0.25394115, 0.21215633, 0.14168568, 0.17237777, 0.27333724, 0.1919719, 0.1755862, 0.18824382, 0.14721757, 0.29024088, 0.19420786, 0.12628257, 0.25765997, 0.11565183, 0.100119896, 0.115784466, 0.13274519, 0.092529684, 0.14256613, 0.08364065, 0.18174568, 0.07983182, 0.5962865, 0.6909336, 0.4568124, 0.44434014, 0.40930247, 0.6788907, 0.44954115, 0.31431508, 0.5863089, 0.6640433, 0.4481957, 0.3087904, 0.5188245, 0.24916036, 0.3383292, 0.40477633, 0.4477049, 0.23360968, 0.24081336, 0.23692387, 0.24509297, 0.12721413, 0.15396245, 0.15374495, 0.13896197, 0.18709242, 0.21450908, 0.20332165, 0.12098428, 0.13991638, 0.1500465, 0.12892303, 0.07827117, 0.10811605, 0.15393253, 0.16359542, 0.15896155, 0.1206266, 0.10070216, 0.38539276, 0.78292674, 0.6884898, 0.7587791, 0.47527826, 0.62493604, 0.3667989, 0.36690354, 0.33644605, 0.20713954, 0.3207874, 0.2414681, 0.20200147, 0.18650188, 0.22555284, 0.17887747, 0.19712822, 0.2132297, 0.14688544, 0.22842862, 0.26184434, 0.14802298, 0.12234299, 0.14047861, 0.12169431, 0.14826636, 0.17914782, 0.112137295, 0.11733405, 0.14408365, 0.15616068, 0.09808266, 0.115510166, 0.11044759, 0.11014311, 0.10417782, 0.12877782, 0.12378524, 0.61478186, 1.1649041, 1.67285, 0.70490915, 0.92141944, 0.38979137, 0.4244718, 0.3395663, 0.7337674, 0.23801902, 0.34652478, 0.26376647, 0.4260282, 0.2409409, 0.20957491, 0.24996795, 0.20804144, 0.24294166, 0.14219466, 0.23274572, 0.19538473, 0.14860114, 0.1761801, 0.13330022, 0.17140423, 0.1418743, 0.22247113, 0.1866519, 0.18810858, 0.14058273, 0.1655628, 0.1245415, 0.107924275, 0.13557014, 0.12869234, 0.14672452, 0.15411071, 0.10879218, 0.43240958, 0.8359115, 0.8339511, 0.8013792, 0.7994113, 0.47703147, 0.24177004, 0.7141068, 0.479865, 0.23331958, 0.20089367, 0.1954993, 0.3698859, 0.22051887, 0.40212885, 0.32463518, 0.14058694, 0.1269371, 0.20004053, 0.19005299, 0.2504973, 0.1533663, 0.2176681, 0.17197351, 0.17437732, 0.15089673, 0.19114552, 0.15172954, 0.15082857, 0.13086998, 0.10734233, 0.13637194, 0.16579446, 0.16907974, 0.1663446, 0.12809582, 0.16355415, 0.1378697, 0.3748087, 0.7818821, 0.420843, 0.39615437, 0.984789, 0.3662862, 0.252704, 0.1871694, 0.49709642, 0.47477612, 0.18596222, 0.28711635, 0.3010408, 0.17798294, 0.22362936, 0.3718698, 0.29931572, 0.15007694, 0.15194945, 0.26168877, 0.19681247, 0.18034397, 0.19391501, 0.20829794, 0.2808962, 0.1859849, 0.0765266, 0.12734234, 0.110655606, 0.15462232, 0.1625082, 0.13270357, 0.08832278, 0.07798859, 0.08203731, 0.19682975, 0.10278777, 0.12482229, 0.13910608, 0.1367835, 0.13017642, 0.09985305, 0.21490717, 0.106658235, 0.5253928, 0.6592527, 0.6187292, 0.9688263, 0.5057248, 0.32040644, 0.20629607, 0.29076755, 0.40372428, 0.510411, 0.47934112, 0.297229, 0.2297455, 0.27029592, 0.20528458, 0.15582097, 0.18345256, 0.20949656, 0.19860263, 0.18319641, 0.32021946, 0.20308839, 0.13097067, 0.08795214, 0.11429429, 0.14382502, 0.117636316, 0.11917879, 0.09861167, 0.099777356, 0.1630526, 0.108193696, 0.13458538, 0.10958846, 0.16183308, 0.110424906, 0.1232695, 0.11778512, 0.37369102, 0.5940226, 0.54163504, 1.3012356, 0.36295688, 0.56571656, 0.36603445, 0.34456107, 0.2828004, 0.31226406, 0.19931805, 0.14738876, 0.2136024, 0.15482838, 0.18047734, 0.13836879, 0.119824775, 0.12990321, 0.12335885, 0.1478315, 0.24011831, 0.15238973, 0.105870776, 0.11222639, 0.14260995, 0.16301604, 0.19511198, 0.12355984, 0.10180382, 0.13633834, 0.12728316, 0.15648898, 0.10461714, 0.11124947, 0.07166635, 0.05934697, 0.10641698, 0.08668098, 0.35425168, 0.48000312, 0.33187607, 0.27846184, 0.41400817, 0.35840398, 0.26707107, 0.3436456, 0.20406051, 0.27101937, 0.43409213, 0.3460956, 0.28557467, 0.25682044, 0.33559465, 0.25183845, 0.22597097, 0.1798258, 0.15352534, 0.18454045, 0.124282084, 0.18862316, 0.16179502, 0.15703382, 0.11828124, 0.11194633, 0.12893176, 0.08694329, 0.14312263, 0.13422938, 0.113612115, 0.13851154, 0.12652256, 0.11546236, 0.095329784, 0.12086905, 0.16973478, 0.08765734, 0.5130109, 0.3080091, 0.39110556, 0.24311934, 0.27065602, 0.45826676, 0.30254012, 0.20144333, 0.3543942, 0.45516485, 0.20594315, 0.15095134, 0.18162191, 0.16495228, 0.26442185, 0.16845556, 0.16802505, 0.12957837, 0.15756354, 0.19213328, 0.10681334, 0.19785826, 0.13960233, 0.11213998, 0.14918928, 0.11928536, 0.097351395, 0.060755465, 0.10835502, 0.09392374, 0.115709044, 0.099546716, 0.18908373, 0.08423898, 0.10832761, 0.07588415, 0.08681501, 0.12387731, 0.43117598, 0.45127517, 0.4558396, 0.44780475, 0.37236652, 0.45030057, 0.2887247, 0.30202162, 0.36839393, 0.31775278, 0.15947312, 0.22339992, 0.23164149, 0.15671165, 0.1368863, 0.16227065, 0.22411391, 0.12809281, 0.11130808, 0.12524743, 0.11681597, 0.1552407, 0.22034928, 0.12505631, 0.1407239, 0.19157523, 0.13137639, 0.10203158, 0.15967421, 0.08717373, 0.12723412, 0.1454523, 0.0964697, 0.13224196, 0.10587952, 0.05830495, 0.104832284, 0.10696175, 0.13818897, 0.0843615, 0.12712508, 0.10305889, 0.09091438, 0.7136382, 0.800694, 0.6397487, 0.38707432, 0.43640608, 0.32646346, 0.20779036, 0.19912921, 0.30184516, 0.23644514, 0.2851139, 0.21046615, 0.17389002, 0.2007794, 0.14216618, 0.26173884, 0.13368249, 0.16306408, 0.16706757, 0.15374936, 0.14229052, 0.13605088, 0.113712646, 0.094394006, 0.10031939, 0.08795779, 0.099554956, 0.17605928, 0.14827897, 0.09116075, 0.15224709, 0.099943966, 0.09796581, 0.08198029, 0.12423649, 0.09347701, 0.107802704, 0.111528344, 0.5088142, 0.29151985, 0.41594163, 0.24173139, 0.33016863, 0.46256125, 0.21388887, 0.5365089, 0.39550996, 0.5312094, 0.21052256, 0.2558081, 0.17910314, 0.17517538, 0.15228362, 0.11356821, 0.14484783, 0.16068073, 0.16328533, 0.11416272, 0.16049604, 0.098405264, 0.15723729, 0.19161189, 0.10487556, 0.15430288, 0.14110768, 0.13484955, 0.084912166, 0.09357124, 0.07637823, 0.14673276, 0.10414854, 0.15520917, 0.062400196, 0.09249232, 0.08577597, 0.06511473, 0.26412827, 0.8542696, 0.35096854, 0.4826207, 0.32769218, 0.3865621, 0.30813435, 0.27041265, 0.17542227, 0.31783882, 0.2349339, 0.16338961, 0.15100773, 0.118471295, 0.148336, 0.23698752, 0.1569182, 0.14729805, 0.112920776, 0.17691433, 0.12295879, 0.1932273, 0.106408484, 0.11826284, 0.07631089, 0.10810181, 0.13864173, 0.09410534, 0.09419711, 0.14970554, 0.08344421, 0.08370145, 0.058180258, 0.08223019, 0.08113664, 0.08760682, 0.099083446, 0.11655401, 0.3363215, 0.42581478, 0.52447534, 0.7424318, 0.22888146, 0.263882, 0.2350926, 0.32784712, 0.2496236, 0.2197102, 0.17678228, 0.25244433, 0.17714226, 0.2560595, 0.23724802, 0.2105725, 0.21040201, 0.19437866, 0.09568875, 0.19438587, 0.12936674, 0.098018445, 0.1308266, 0.1826103, 0.11692637, 0.10866618, 0.14172606, 0.08252913, 0.10582504, 0.113083854, 0.11405699, 0.106558435, 0.096604966, 0.06582501, 0.110426724, 0.09711186, 0.057457842, 0.11184485, 0.1744655, 0.3896164, 0.2526765, 0.44918287, 0.51684344, 0.43178603, 0.24818267, 0.34153852, 0.25366268, 0.2476233, 0.13739485, 0.15507016, 0.21757959, 0.15352441, 0.14672792, 0.33092204, 0.12156209, 0.1289708, 0.13176668, 0.18625416, 0.1979914, 0.103918515, 0.16323669, 0.12350711, 0.1755541, 0.14065793, 0.09363318, 0.16415134, 0.099004395, 0.10406933, 0.11554284, 0.11580938, 0.10418025, 0.10437885, 0.11317873, 0.12718403, 0.123761155, 0.063367665, 0.0795823, 0.11399234, 0.110640705, 0.10644784, 0.069845155, 0.11570544, 0.32171854, 0.5287061, 0.32044196, 0.24214554, 0.1640045, 0.22325456, 0.3607572, 0.39195225, 0.15621331, 0.16841422, 0.16300066, 0.19095285, 0.16878498, 0.15068243, 0.20255014, 0.16186592, 0.16926296, 0.12892784, 0.120680206, 0.14768453, 0.123226345, 0.11228462, 0.07301342, 0.13886559, 0.07859546, 0.09998751, 0.10396399, 0.105829276, 0.10510709, 0.08806444, 0.06808729, 0.07894639, 0.10056392, 0.08168966, 0.099443935, 0.082334995, 0.090779096, 0.09349879, 0.361098, 0.21271838, 0.22921424, 0.3565875, 0.2664478, 0.31507492, 0.19405816, 0.14404824, 0.27016285, 0.21778238, 0.11991526, 0.19409, 0.11421698, 0.13546571, 0.11579211, 0.11998849, 0.08596439, 0.113827154, 0.115001254, 0.1201144, 0.1069682, 0.08935406, 0.11654839, 0.12667724, 0.105629385, 0.07448735, 0.088264555, 0.117761284, 0.118892506, 0.07408744, 0.088150874, 0.11171899, 0.06482287, 0.10898036, 0.08250824, 0.07535011, 0.087843634, 0.1091704, 0.25018412, 0.6024594, 0.32228944, 0.39104497, 0.3745102, 0.45897508, 0.24727987, 0.51489943, 0.17893997, 0.10325512, 0.08477202, 0.101950824, 0.13706927, 0.22812396, 0.14012738, 0.15577006, 0.08589624, 0.14108405, 0.11335013, 0.1106022, 0.10333542, 0.089487255, 0.08871725, 0.100755155, 0.08746303, 0.0932137, 0.086896256, 0.09348967, 0.080464944, 0.07688456, 0.071199924, 0.0804732, 0.118090995, 0.087058686, 0.07185993, 0.103569545, 0.064668335, 0.087670185, 0.385837, 0.30965558, 0.36979604, 0.4876662, 0.33513322, 0.24353175, 0.1432782, 0.15088311, 0.15337765, 0.16095126, 0.13067779, 0.1050942, 0.099656515, 0.14260764, 0.2646606, 0.13530602, 0.12275956, 0.1122329, 0.15670797, 0.1014811, 0.11819726, 0.085403174, 0.1112805, 0.08885938, 0.10259359, 0.11151747, 0.08782189, 0.07770393, 0.086089395, 0.058213737, 0.070668325, 0.07156327, 0.09495866, 0.11447514, 0.08957508, 0.061294984, 0.06675603, 0.06965994, 0.22377351, 0.6249778, 0.304155, 0.26570687, 0.16423357, 0.2744029, 0.32641107, 0.26563373, 0.17852874, 0.19967309, 0.14476185, 0.10534352, 0.11048067, 0.20739974, 0.11975936, 0.09555449, 0.12603286, 0.116295286, 0.13976415, 0.14111507, 0.13447824, 0.18162246, 0.12561797, 0.06085313, 0.058859, 0.09679405, 0.08145424, 0.07067269, 0.090990186, 0.103459775, 0.16554993, 0.08621621, 0.07503587, 0.07757896, 0.064238936, 0.06509013, 0.05964057, 0.078999504, 0.2690589, 0.24055356, 0.33238247, 0.31276578, 0.21885014, 0.19965827, 0.16202524, 0.21668382, 0.4504536, 0.43364283, 0.28369096, 0.13483368, 0.10498276, 0.15266599, 0.13682725, 0.11130959, 0.100607924, 0.09161522, 0.0776551, 0.08874133, 0.13440828, 0.102906264, 0.06691137, 0.0808656, 0.11545836, 0.063053146, 0.08990195, 0.04656423, 0.06522513, 0.10098546, 0.09504061, 0.08896061, 0.06833717, 0.06954044, 0.04972162, 0.07077147, 0.072753504, 0.06922731, 0.27912328, 0.39320034, 0.33346897, 0.23910716, 0.21248496, 0.33138072, 0.16708347, 0.12876509, 0.22743411, 0.20401226, 0.12859118, 0.10136545, 0.12580349, 0.10145151, 0.085305974, 0.12443771, 0.12451808, 0.12672117, 0.119537316, 0.11500361, 0.0879878, 0.11907009, 0.095525034, 0.09293938, 0.076600105, 0.09792874, 0.08783988, 0.069767796, 0.09175641, 0.06317852, 0.075810984, 0.095567174, 0.07222407, 0.08767225, 0.08591199, 0.07271878, 0.09173081, 0.06783023, 0.5118254, 0.5575805, 0.4579244, 0.31280556, 0.18559822, 0.28509355, 0.4459496, 0.23860121, 0.26532176, 0.20704265, 0.13805433, 0.12920186, 0.18651025, 0.15422143, 0.14194545, 0.12812524, 0.08684905, 0.09033262, 0.09272312, 0.095029615, 0.11443837, 0.09426008, 0.06654727, 0.09839249, 0.09029266, 0.095860414, 0.093623854, 0.09604496, 0.09826567, 0.074882746, 0.075708106, 0.08048796, 0.068180464, 0.08345136, 0.06311396, 0.07810911, 0.0860569, 0.09690329, 0.23158737, 0.22533883, 0.2628601, 0.52929544, 0.5083794, 0.18162371, 0.22862153, 0.16429524, 0.17453921, 0.19593933, 0.10179228, 0.13500364, 0.07888136, 0.12938616, 0.12631154, 0.09453649, 0.089198634, 0.10604394, 0.10958344, 0.09142213, 0.10769279, 0.11158815, 0.0919621, 0.081701554, 0.0951692, 0.12120311, 0.08616082, 0.085397065, 0.06353934, 0.065894835, 0.09017319, 0.08891994, 0.11095264, 0.07713332, 0.09321313, 0.072276495, 0.094386026, 0.08364624, 0.17471187, 0.16600643, 0.37498814, 0.31192297, 0.27676782, 0.2681867, 0.16369124, 0.23419972, 0.22274421, 0.20428622, 0.15150668, 0.08602059, 0.09705221, 0.10292802, 0.107801214, 0.10062506, 0.078578174, 0.07782028, 0.09132391, 0.0894352, 0.08151134, 0.09670075, 0.13517329, 0.07862079, 0.10596543, 0.10504173, 0.067370676, 0.09369432, 0.08294187, 0.06639751, 0.092818044, 0.065635145, 0.063353784, 0.077110894, 0.06537161, 0.06568184, 0.11256385, 0.06389091, 0.05003463, 0.06552179, 0.08375227, 0.110044695, 0.08243065, 0.2015147, 0.20552193, 0.33258447, 0.2852424, 0.19402888, 0.18650942, 0.25675687, 0.122777976, 0.30103433, 0.38116974, 0.26252684, 0.15162338, 0.11889435, 0.14377062, 0.12202722, 0.10878583, 0.087813996, 0.09005625, 0.1181009, 0.14377773, 0.14905255, 0.08484191, 0.060994312, 0.13131613, 0.087104924, 0.0908162, 0.09300726, 0.061333347, 0.100277856, 0.06676311, 0.062106866, 0.08542272, 0.072562516, 0.05980909, 0.077922516, 0.045943797, 0.077569835, 0.056108505, 0.15392566, 0.19534081, 0.28411853, 0.22559507, 0.16522498, 0.20741725, 0.18571241, 0.11345906, 0.16225812, 0.11362483, 0.2722239, 0.093299225, 0.08920163, 0.115303375, 0.09309953, 0.11569934, 0.093000025, 0.08073036, 0.09508088, 0.16215242, 0.104221925, 0.12582925, 0.07242492, 0.06555887, 0.06582233, 0.06752447, 0.05742109, 0.07434386, 0.07965275, 0.055876143, 0.08350966, 0.089923486, 0.07162996, 0.05942654, 0.067233965, 0.09049262, 0.061045215, 0.08100616, 0.21752083, 0.47416934, 0.5945465, 0.43214902, 0.29443446, 0.19922103, 0.1522181, 0.15779391, 0.20295301, 0.3699615, 0.18858738, 0.14507067, 0.13927248, 0.13051195, 0.16578054, 0.08363968, 0.0640244, 0.14511313, 0.097541995, 0.116399236, 0.12392472, 0.14795657, 0.11387928, 0.09008014, 0.06858776, 0.065268256, 0.1022844, 0.09392558, 0.13496463, 0.07348384, 0.0988614, 0.08712213, 0.11209182, 0.057654984, 0.07926049, 0.09009553, 0.092221335, 0.09120782, 0.34368026, 0.26040325, 0.32407102, 0.3528817, 0.26172698, 0.17874506, 0.1557071, 0.11225852, 0.16088597, 0.27138284, 0.2269265, 0.10862057, 0.1140504, 0.1984402, 0.22251327, 0.11284526, 0.07139561, 0.07476761, 0.11581111, 0.15337764, 0.16150132, 0.08976278, 0.12463981, 0.12355977, 0.110076845, 0.10434716, 0.08075945, 0.056128815, 0.09351508, 0.07006116, 0.10605005, 0.117450215, 0.08840548, 0.072568096, 0.07249475, 0.09705167, 0.051192865, 0.09161031, 0.22917622, 0.20860422, 0.18324633, 0.24698907, 0.15141396, 0.16917019, 0.27330703, 0.21229355, 0.20137657, 0.1289597, 0.13913529, 0.16685691, 0.1152259, 0.09895768, 0.090765186, 0.119741775, 0.1469152, 0.08259953, 0.11914563, 0.09231013, 0.10724467, 0.08245096, 0.09021367, 0.0574569, 0.079890996, 0.089497685, 0.07693639, 0.085457616, 0.075206235, 0.089424945, 0.07770958, 0.052572098, 0.05960901, 0.06408745, 0.06752079, 0.0850338, 0.06997293, 0.07421465, 0.061510712, 0.058088064, 0.059075326, 0.07503164, 0.06421321, 0.06019266, 0.23953068, 0.31880584, 0.5362111, 0.16284233, 0.1363798, 0.19380212, 0.19821568, 0.23894234, 0.14539285, 0.16284144, 0.10145265, 0.10552456, 0.077853374, 0.08424901, 0.12466102, 0.13896626, 0.07823333, 0.094543494, 0.0764334, 0.0954789, 0.11767798, 0.07558274, 0.07110853, 0.082328096, 0.06893241, 0.07110648, 0.052691303, 0.07814037, 0.061401155, 0.053529352, 0.065653265, 0.08612528, 0.05875629, 0.09469404, 0.049824454, 0.077098124, 0.06067288, 0.05566916, 0.14159751, 0.30659074, 0.3366703, 0.32596603, 0.20345502, 0.19541153, 0.31106004, 0.3099471, 0.21108447, 0.15788221, 0.087856114, 0.09873321, 0.24361937, 0.1086941, 0.06634338, 0.09761085, 0.0780657, 0.11926343, 0.091803335, 0.098164506, 0.07171868, 0.07893132, 0.10075253, 0.10903599, 0.088180095, 0.074318595, 0.06667696, 0.06982205, 0.06896044, 0.1026735, 0.07239931, 0.08399774, 0.08616902, 0.08642091, 0.062276375, 0.0638313, 0.06455288, 0.07297513, 0.14127307, 0.16877577, 0.1823224, 0.2886447, 0.15534182, 0.18284795, 0.18695433, 0.13132893, 0.17227606, 0.09649156, 0.07779588, 0.12144756, 0.06374483, 0.114932835, 0.14479233, 0.106479466, 0.08300182, 0.09189749, 0.070790984, 0.08858222, 0.06670105, 0.092399575, 0.121786855, 0.06274127, 0.07698902, 0.112702586, 0.063933305, 0.0485032, 0.07921154, 0.04565845, 0.05353114, 0.07623733, 0.07765802, 0.06976109, 0.06895842, 0.07149408, 0.05921217, 0.054664135, 0.23954356, 0.3964113, 0.42944908, 0.29407802, 0.26655522, 0.17876717, 0.18625054, 0.18760419, 0.25407705, 0.138244, 0.08672267, 0.092609294, 0.1683003, 0.1501965, 0.116933085, 0.0934106, 0.102674104, 0.092339136, 0.07920812, 0.09539202, 0.11305875, 0.07383817, 0.086957596, 0.07066305, 0.05544687, 0.09370836, 0.077948, 0.043690223, 0.061117053, 0.080782674, 0.074082434, 0.0789035, 0.06648298, 0.07789176, 0.074871995, 0.06707834, 0.084339626, 0.06399117, 0.1665115, 0.2345646, 0.5189584, 0.28913602, 0.2053673, 0.14611465, 0.22612458, 0.18378548, 0.09108121, 0.18956904, 0.08910976, 0.1283453, 0.09202757, 0.09851632, 0.104012266, 0.08298835, 0.0627442, 0.08558346, 0.08641459, 0.06940627, 0.098274745, 0.08032639, 0.07107826, 0.071663685, 0.06903817, 0.084398754, 0.06947465, 0.07493486, 0.06984946, 0.08581542, 0.08466579, 0.07918154, 0.056771837, 0.05628353, 0.06726639, 0.066665865, 0.061761912, 0.05050442, 0.06743014, 0.047184657, 0.08762519, 0.067248076, 0.067317106, 0.2502149, 0.19211218, 0.231439, 0.5594986, 0.3673741, 0.13447179, 0.072923385, 0.14137805, 0.24488607, 0.11138868, 0.09336385, 0.18167503, 0.18948424, 0.09414762, 0.092256926, 0.08041895, 0.14229026, 0.063858725, 0.071491845, 0.121545136, 0.08965901, 0.08159752, 0.10330319, 0.0799112, 0.0992863, 0.057527028, 0.10308788, 0.08562547, 0.062285136, 0.08437769, 0.074343435, 0.046486035, 0.055183124, 0.06652864, 0.06356112, 0.05888043, 0.07467898, 0.05347484, 0.21071298, 0.24211271, 0.38651374, 0.2526175, 0.3380468, 0.15094812, 0.12726706, 0.12720406, 0.20565148, 0.17936766, 0.087067634, 0.07808574, 0.06041333, 0.087532006, 0.098118864, 0.071479015, 0.07381403, 0.07726073, 0.09994919, 0.0791261, 0.11846742, 0.10523638, 0.055735357, 0.104553156, 0.06789065, 0.076786526, 0.07198487, 0.0716614, 0.059471037, 0.082500786, 0.04723995, 0.06506379, 0.054259885, 0.098145, 0.07749499, 0.052582353, 0.07135244, 0.070936754, 0.25048122, 0.19272593, 0.19423203, 0.32912937, 0.28933564, 0.1469026, 0.13741921, 0.1467086, 0.14148049, 0.10177675, 0.11868629, 0.15314034, 0.14026661, 0.20452327, 0.10302414, 0.10906769, 0.07081302, 0.07458606, 0.101217724, 0.08665384, 0.09052855, 0.057360243, 0.07985168, 0.05310364, 0.07901078, 0.095695876, 0.10480343, 0.09075054, 0.07970563, 0.057829116, 0.051391426, 0.06784529, 0.060388006, 0.04978703, 0.050657548, 0.06748162, 0.07233316, 0.052464172, 0.29077384, 0.3098672, 0.26149592, 0.22924893, 0.25025216, 0.33461994, 0.31173953, 0.19787197, 0.20043306, 0.1594063, 0.10832996, 0.067813344, 0.07764602, 0.09297595, 0.07224317, 0.06513293, 0.09670424, 0.08389836, 0.08462932, 0.07059461, 0.071117304, 0.07077983, 0.096413955, 0.06412102, 0.0734144, 0.09620333, 0.076376155, 0.053892016, 0.05998792, 0.046472456, 0.036019523, 0.08344969, 0.05208239, 0.061251804, 0.04495234, 0.051490087, 0.058467977, 0.051779967, 0.1135549, 0.21773434, 0.23930469, 0.3368914, 0.25485647, 0.12679766, 0.19673185, 0.15546119, 0.11073362, 0.09471955, 0.10363304, 0.13814206, 0.12659629, 0.09998619, 0.13658361, 0.121495046, 0.06301663, 0.08225872, 0.084970765, 0.11948096, 0.06206476, 0.06685218, 0.06766364, 0.07481315, 0.07107358, 0.08561916, 0.090320036, 0.07997439, 0.056394827, 0.09233964, 0.069372155, 0.07880224, 0.05069364, 0.04132363, 0.05118824, 0.047131866, 0.0792907, 0.0458548, 0.06678624, 0.07347407, 0.058065485, 0.06263322, 0.05306108, 0.06374533, 0.32281303, 0.2941568, 0.21259575, 0.27844086, 0.14549275, 0.16054259, 0.16068344, 0.11672482, 0.11916414, 0.13773362, 0.11219155, 0.097923264, 0.10463226, 0.102528684, 0.07358065, 0.092812814, 0.100675784, 0.06837779, 0.08351774, 0.09254668, 0.08002243, 0.053512067, 0.066950984, 0.059618235, 0.09953335, 0.056282878, 0.07795105, 0.04563628, 0.14106293, 0.065684445, 0.07404711, 0.07813555, 0.048107304, 0.074593656, 0.045645922, 0.046815246, 0.045646902, 0.05658577, 0.38202, 0.43479052, 0.20245619, 0.10220946, 0.23638645, 0.09277059, 0.13706335, 0.22152118, 0.16951697, 0.12219281, 0.10003471, 0.121923976, 0.11513675, 0.10474104, 0.098292686, 0.06354726, 0.05639125, 0.06458196, 0.063956104, 0.117065266, 0.071753815, 0.08516147, 0.058115292, 0.07894856, 0.053337455, 0.060764343, 0.052059837, 0.0689367, 0.05530247, 0.072144486, 0.0636878, 0.05386523, 0.058358118, 0.053111624, 0.0722898, 0.06862021, 0.050780497, 0.050074708, 0.2329524, 0.30980158, 0.20531563, 0.2607092, 0.16783713, 0.09518076, 0.13033386, 0.12308308, 0.08647004, 0.12382931, 0.12759998, 0.099710986, 0.09910148, 0.104792394, 0.10032116, 0.111841, 0.064654484, 0.0617596, 0.11060617, 0.11810949, 0.08268793, 0.052383207, 0.09256373, 0.07469914, 0.07553977, 0.051885094, 0.053086236, 0.052098617, 0.06638737, 0.06204885, 0.04712056, 0.042804383, 0.050337184, 0.052643783, 0.0578602, 0.040912796, 0.050217412, 0.064055756, 0.1913958, 0.12452929, 0.2106036, 0.4128273, 0.4452098, 0.1586163, 0.11112771, 0.11798398, 0.12278701, 0.1186031, 0.21048571, 0.1633501, 0.11691895, 0.092768855, 0.101698376, 0.06252106, 0.08216512, 0.09858829, 0.08731996, 0.08596636, 0.057151996, 0.06215818, 0.045536414, 0.076717034, 0.05983173, 0.08983674, 0.08285733, 0.06649082, 0.04704537, 0.053438853, 0.06304529, 0.057164118, 0.052392166, 0.059234243, 0.04759258, 0.06372188, 0.050004195, 0.07852601, 0.10951249, 0.19422998, 0.15136725, 0.28257656, 0.19957113, 0.09703222, 0.14446194, 0.074093364, 0.1025806, 0.23837489, 0.1473625, 0.108353704, 0.06768832, 0.08523729, 0.092333905, 0.09556681, 0.08039049, 0.053304736, 0.10313716, 0.087370664, 0.06176602, 0.06579492, 0.06674846, 0.10624117, 0.110421255, 0.055347774, 0.070679, 0.088962674, 0.06959629, 0.066999964, 0.07051171, 0.08427222, 0.084535345, 0.049801923, 0.06330644, 0.04147498, 0.06428456, 0.06079308, 0.06198026, 0.04302186, 0.06535851, 0.05715739, 0.05136681, 0.2594578, 0.2460921, 0.16742422, 0.20046254, 0.14831112, 0.13405026, 0.11406976, 0.24362032, 0.13147771, 0.25911036, 0.13548596, 0.07163459, 0.06473272, 0.085410796, 0.09842587, 0.07141784, 0.074624754, 0.10685282, 0.05950616, 0.07871399, 0.064138, 0.05263405, 0.05345881, 0.058784787, 0.05668366, 0.07781563, 0.09000691, 0.058302786, 0.05892721, 0.06456147, 0.0480525, 0.047964703, 0.053072196, 0.041947056, 0.043020234, 0.048156492, 0.06502699, 0.04414955, 0.14043875, 0.2256096, 0.124759674, 0.2041711, 0.13625787, 0.12515879, 0.14034551, 0.14162062, 0.13240835, 0.11746498, 0.10382746, 0.12134059, 0.06583248, 0.093933105, 0.121342994, 0.050221056, 0.061225127, 0.08084728, 0.083346434, 0.06593058, 0.058865886, 0.05603218, 0.087387964, 0.074587144, 0.044909626, 0.058232497, 0.060721423, 0.0617407, 0.060512315, 0.051505342, 0.05003715, 0.04400316, 0.04330492, 0.05314549, 0.03779695, 0.04995626, 0.049076237, 0.060118895, 0.17649516, 0.19948006, 0.20560396, 0.4784967, 0.20036379, 0.116598554, 0.10410282, 0.09534444, 0.15916494, 0.23497152, 0.09887941, 0.07182827, 0.08850453, 0.087001234, 0.05935537, 0.04618169, 0.055770386, 0.06797071, 0.06704398, 0.07373542, 0.07580154, 0.04810282, 0.05571135, 0.062174477, 0.086256646, 0.10461988, 0.031519655, 0.05330572, 0.057057526, 0.053817373, 0.03957045, 0.073048435, 0.037629083, 0.04191606, 0.05195047, 0.049927726, 0.04913956, 0.05443388, 0.18036637, 0.22041449, 0.2023356, 0.22591616, 0.29187742, 0.11480115, 0.11932355, 0.1514505, 0.08785238, 0.10058786, 0.089938514, 0.07380541, 0.06343163, 0.08242902, 0.10750054, 0.06579413, 0.082320504, 0.056670897, 0.03979954, 0.0619379, 0.11806156, 0.067357235, 0.06845075, 0.051094733, 0.040422007, 0.071186505, 0.08065086, 0.052417383, 0.053768847, 0.06529727, 0.074022114, 0.05832486, 0.0469275, 0.05852096, 0.055461217, 0.03894698, 0.043460015, 0.038550336, 0.15027383, 0.3118558, 0.24632888, 0.15932822, 0.15586272, 0.09313062, 0.1902792, 0.093222305, 0.09019344, 0.08304145, 0.07688479, 0.058478814, 0.07826486, 0.07797027, 0.08614683, 0.08147645, 0.051647313, 0.088295154, 0.06275911, 0.06854022, 0.06928716, 0.06708252, 0.058988683, 0.048359483, 0.058541328, 0.061898783, 0.053364635, 0.036997944, 0.033173725, 0.05089187, 0.05252393, 0.04789692, 0.057744496, 0.040170252, 0.04212064, 0.026509129, 0.076893575, 0.039835416, 0.06455183, 0.04599702, 0.041788783, 0.039999098, 0.040136173, 0.043495223, 0.17987348, 0.17958464, 0.25538966, 0.14909598, 0.065119766, 0.11486726, 0.114931345, 0.049578182, 0.07430235, 0.083253965, 0.09709235, 0.059029635, 0.05634911, 0.08586519, 0.15423207, 0.073020555, 0.049070705, 0.046460442, 0.043510288, 0.057841256, 0.06608728, 0.04555441, 0.030753301, 0.055609062, 0.04368471, 0.03860228, 0.051222526, 0.03956015, 0.05033127, 0.04859365, 0.03930977, 0.050841495, 0.037235796, 0.047071945, 0.050743766, 0.049038183, 0.038552254, 0.042674426, 0.31902203, 0.32462302, 0.23350562, 0.16934156, 0.20828938, 0.07393877, 0.14049502, 0.09806285, 0.08674737, 0.121039726, 0.058951, 0.058824457, 0.05588283, 0.068878114, 0.049998358, 0.053197592, 0.05648135, 0.042381477, 0.04961558, 0.05528934, 0.07794936, 0.054744087, 0.043165646, 0.06520352, 0.04402281, 0.04622136, 0.055166136, 0.036642667, 0.04219774, 0.0521877, 0.07062782, 0.054000188, 0.043058127, 0.038936064, 0.041645695, 0.03285941, 0.046018697, 0.045360725, 0.13076474, 0.19234733, 0.28101647, 0.107260615, 0.11461036, 0.123733886, 0.083279945, 0.08330744, 0.102984786, 0.15129365, 0.046058256, 0.076052584, 0.07900052, 0.06364104, 0.063424505, 0.06961268, 0.054165043, 0.05037114, 0.053663444, 0.052653626, 0.05599594, 0.058452576, 0.033384945, 0.049371377, 0.054449786, 0.04749482, 0.050208423, 0.03434116, 0.046317965, 0.036372077, 0.032044005, 0.050527163, 0.04566983, 0.041010648, 0.038846813, 0.046114337, 0.045121793, 0.062993586, 0.13611583, 0.11544567, 0.12338851, 0.14043665, 0.077626206, 0.122177266, 0.100522876, 0.09701822, 0.14212836, 0.15481678, 0.120631695, 0.07775118, 0.126916, 0.06445963, 0.06639451, 0.04770793, 0.05933638, 0.05063867, 0.072154224, 0.06715241, 0.060508877, 0.07328771, 0.04670252, 0.038194858, 0.03867796, 0.0431776, 0.052615486, 0.046550184, 0.040230382, 0.057255417, 0.04804668, 0.037983336, 0.037385583, 0.052132007, 0.056307584, 0.038488697, 0.034641232, 0.03721464, 0.15967754, 0.22627479, 0.19440953, 0.16084695, 0.08595803, 0.09900453, 0.1581197, 0.22903794, 0.1776767, 0.12767512, 0.06450221, 0.12693886, 0.14284147, 0.114697956, 0.069083184, 0.049843322, 0.07345298, 0.04776473, 0.05526335, 0.052249547, 0.045810442, 0.054556772, 0.04623264, 0.04579098, 0.065688774, 0.0466931, 0.04835898, 0.056880474, 0.051732007, 0.03408156, 0.047249146, 0.05824003, 0.048662774, 0.054403994, 0.044984892, 0.039517544, 0.046824355, 0.049656674, 0.03570844, 0.04957139, 0.045025382, 0.04194271, 0.037051868, 0.18905175, 0.16242819, 0.09742316, 0.17071731, 0.20874964, 0.18809414, 0.113926664, 0.11323562, 0.12132902, 0.14905997, 0.13898453, 0.057749122, 0.08203185, 0.04787823, 0.043098338, 0.06733685, 0.057627823, 0.046715807, 0.044952407, 0.049993657, 0.07052454, 0.047347136, 0.05409087, 0.066017225, 0.059924, 0.049401518, 0.04893256, 0.057396267, 0.047062717, 0.04014199, 0.025020916, 0.05105725, 0.034442727, 0.053691857, 0.03410123, 0.036551423, 0.041696895, 0.0364633, 0.07338745, 0.13896458, 0.108257785, 0.40704888, 0.19075243, 0.07437981, 0.089504495, 0.11685298, 0.10375229, 0.10633454, 0.08393822, 0.09269515, 0.13625227, 0.07499197, 0.06759203, 0.08118815, 0.05690495, 0.08417529, 0.0814168, 0.04209215, 0.06128527, 0.051893286, 0.062364314, 0.04270218, 0.04876831, 0.054158267, 0.041826252, 0.041892618, 0.042608153, 0.07071805, 0.038461614, 0.03775685, 0.046437673, 0.05039917, 0.04635605, 0.026598664, 0.039400037, 0.035324227, 0.09698586, 0.12928693, 0.103531286, 0.14439484, 0.20317459, 0.08441478, 0.11341302, 0.07821201, 0.056160092, 0.073488206, 0.08875647, 0.052672543, 0.06797055, 0.067130506, 0.070086546, 0.06473944, 0.05512469, 0.05682096, 0.05136541, 0.053277995, 0.061972648, 0.045525678, 0.03992026, 0.033704057, 0.05284068, 0.049880277, 0.041262392, 0.0412292, 0.029724773, 0.056360822, 0.032729104, 0.04023256, 0.03734495, 0.038052093, 0.03621471, 0.043116618, 0.045781042, 0.0592638, 0.23647015, 0.11020353, 0.16018935, 0.1572672, 0.22194958, 0.09805532, 0.060002655, 0.102031425, 0.10425349, 0.08505059, 0.091242775, 0.08725244, 0.101032145, 0.11106034, 0.10395306, 0.11706046, 0.046595063, 0.050134286, 0.05581975, 0.039738484, 0.044158667, 0.041297846, 0.041767865, 0.04730789, 0.03053192, 0.06105885, 0.04613152, 0.04885053, 0.051760536, 0.033830237, 0.027524406, 0.033248898, 0.04565484, 0.042407703, 0.040230006, 0.043633908, 0.030429473, 0.04385677, 0.16111536, 0.19230308, 0.091917835, 0.18391462, 0.2265417, 0.16620122, 0.07212236, 0.045117643, 0.05049186, 0.06952575, 0.050114255, 0.08703895, 0.07433484, 0.048268426, 0.048953447, 0.05066085, 0.052201767, 0.05550645, 0.060767356, 0.06662148, 0.052586164, 0.041609097, 0.045843326, 0.0430651, 0.04863118, 0.046223603, 0.055202376, 0.03774586, 0.048986092, 0.037520714, 0.054011777, 0.046554007, 0.04798221, 0.041924432, 0.03579683, 0.029259788, 0.030394182, 0.048162114, 0.030942569, 0.052668527, 0.042529806, 0.04729522, 0.03647332, 0.031528562, 0.1308629, 0.2293228, 0.20003027, 0.11908698, 0.06586882, 0.07342937, 0.100333, 0.10141939, 0.11075511, 0.052093614, 0.04148677, 0.057851303, 0.047387194, 0.09515984, 0.045424573, 0.06134148, 0.05919219, 0.045313697, 0.04949822, 0.031453975, 0.047487903, 0.042221915, 0.04680516, 0.03157439, 0.05880837, 0.051350746, 0.041106593, 0.041940514, 0.05406962, 0.03361705, 0.04240128, 0.05093189, 0.037670214, 0.036096137, 0.045221355, 0.02671451, 0.040246855, 0.030427713, 0.22644036, 0.05369364, 0.094260715, 0.117197, 0.13880663, 0.17034836, 0.072053775, 0.12407244, 0.10454315, 0.07858024, 0.039014462, 0.073944695, 0.04856046, 0.094933845, 0.08858939, 0.050414156, 0.08618386, 0.07012357, 0.057582904, 0.031167444, 0.052591812, 0.029589904, 0.03449935, 0.05731335, 0.04208016, 0.057551183, 0.030739253, 0.025782524, 0.04444173, 0.038304, 0.033306804, 0.04734726, 0.028892728, 0.042266153, 0.041508343, 0.024381706, 0.034681536, 0.034702476, 0.1317522, 0.09277836, 0.0847104, 0.13339694, 0.1310577, 0.08279412, 0.08721854, 0.0419376, 0.06665089, 0.10807642, 0.06305689, 0.11344609, 0.052395575, 0.04644732, 0.057363246, 0.05627885, 0.07573572, 0.055749748, 0.05450226, 0.06966665, 0.045438103, 0.04094475, 0.03907551, 0.034174778, 0.024101427, 0.034661263, 0.040038474, 0.036633406, 0.03318432, 0.04558492, 0.023410596, 0.03213682, 0.04585004, 0.026762977, 0.034748476, 0.031914923, 0.029883811, 0.050010487, 0.1446883, 0.16812108, 0.1269474, 0.16377991, 0.15133245, 0.11729293, 0.12568328, 0.07242223, 0.10421865, 0.09746696, 0.051075324, 0.049986128, 0.05059958, 0.052929133, 0.03541948, 0.064975955, 0.03509243, 0.044891547, 0.037710357, 0.038176097, 0.041980386, 0.039867453, 0.04361585, 0.03638159, 0.047834113, 0.04677037, 0.03996949, 0.029967641, 0.0363449, 0.040765844, 0.034884144, 0.035058253, 0.03255906, 0.030197537, 0.03629922, 0.033672266, 0.03277145, 0.039205827, 0.11544158, 0.1560871, 0.2354496, 0.1720196, 0.07710485, 0.12060056, 0.07398715, 0.076028556, 0.085734054, 0.11800537, 0.05291374, 0.053896453, 0.049088687, 0.0482089, 0.054053474, 0.024496889, 0.06410446, 0.032038897, 0.032060057, 0.05911164, 0.03627207, 0.028865768, 0.045913316, 0.067481264, 0.045424808, 0.0512469, 0.038272683, 0.047741678, 0.037376434, 0.042437684, 0.029495114, 0.038310908, 0.036446676, 0.048272554, 0.030592822, 0.032369584, 0.037638873, 0.037598617, 0.03678777, 0.033316564, 0.024123745, 0.040194895, 0.034467638, 0.17303865, 0.16921714, 0.13274051, 0.12677898, 0.14833198, 0.26557302, 0.13919047, 0.14502789, 0.10792444, 0.10493247, 0.04493208, 0.035387717, 0.06160106, 0.0688394, 0.09297549, 0.08316542, 0.07636993, 0.035755467, 0.04617335, 0.033365525, 0.046149563, 0.03618713, 0.034635376, 0.032020908, 0.0362813, 0.027823055, 0.033061314, 0.026368486, 0.056560233, 0.034065254, 0.03816395, 0.04362484, 0.040481985, 0.035632957, 0.032993883, 0.029157365, 0.03952322, 0.022552306, 0.08091354, 0.09927611, 0.122450575, 0.21308132, 0.16099676, 0.166525, 0.10662383, 0.13551968, 0.09583683, 0.09344234, 0.030801933, 0.092933916, 0.059732087, 0.046923514, 0.05252371, 0.05531436, 0.102342814, 0.052935977, 0.04738082, 0.04884192, 0.035038933, 0.04333215, 0.046969865, 0.0519503, 0.042110443, 0.035725005, 0.047041547, 0.031170754, 0.03326717, 0.049395375, 0.026809234, 0.029825835, 0.025424642, 0.034348004, 0.04569406, 0.039344955, 0.03461556, 0.036285743, 0.09169285, 0.10924052, 0.11859685, 0.14245123, 0.39876914, 0.1671135, 0.08580422, 0.07621018, 0.05603793, 0.046676453, 0.046512935, 0.06349305, 0.058424566, 0.07373605, 0.10814643, 0.14403953, 0.05770158, 0.040248636, 0.043650996, 0.04078729, 0.039995506, 0.04487751, 0.03850143, 0.052890446, 0.045219235, 0.035167545, 0.025983322, 0.026974518, 0.035480734, 0.032737046, 0.036973335, 0.03321642, 0.029483287, 0.033491123, 0.030661348, 0.033123467, 0.03758103, 0.034428347, 0.14433205, 0.14526337, 0.09756579, 0.20210755, 0.22920935, 0.16312438, 0.113562204, 0.08557201, 0.08912331, 0.057273857, 0.05041219, 0.085878216, 0.066561095, 0.078571476, 0.054484293, 0.04275766, 0.04272173, 0.046025313, 0.05276381, 0.03654926, 0.041549113, 0.043245863, 0.050579082, 0.033070643, 0.03513643, 0.03318948, 0.032525003, 0.045261625, 0.030516079, 0.023395773, 0.021514513, 0.035898373, 0.028895548, 0.030261727, 0.038746685, 0.026879136, 0.041870426, 0.023480298, 0.07072972, 0.067492545, 0.07346873, 0.23557457, 0.09555163, 0.090823315, 0.056987826, 0.06370916, 0.06510488, 0.10479055, 0.07925775, 0.06641753, 0.05494612, 0.045847338, 0.043925095, 0.06252255, 0.055557057, 0.04528492, 0.03291716, 0.054138266, 0.03334402, 0.035334364, 0.04084823, 0.041632947, 0.05378299, 0.05496783, 0.03533575, 0.05623966, 0.04247891, 0.040752895, 0.043777943, 0.030871598, 0.029840574, 0.045707572, 0.03820808, 0.034664087, 0.03680386, 0.030670382, 0.02979423, 0.03336342, 0.028833652, 0.019907529, 0.034282498, 0.036853243, 0.11184011, 0.20222315, 0.12967907, 0.13417917, 0.10215852, 0.06753465, 0.08068731, 0.054928128, 0.043419335, 0.094308786, 0.10252901, 0.09654373, 0.059560005, 0.041518345, 0.09619233, 0.05671026, 0.04400908, 0.045694366, 0.031236246, 0.051352106, 0.04153189, 0.051203262, 0.048922364, 0.029846609, 0.027710473, 0.022469891, 0.035470612, 0.02669326, 0.028967544, 0.04405348, 0.04008747, 0.030425467, 0.02970944, 0.029584724, 0.028972136, 0.026239783, 0.029303646, 0.02842872, 0.08245517, 0.339842, 0.16998361, 0.11761339, 0.08708029, 0.0944543, 0.06351469, 0.05041537, 0.11098957, 0.06551448, 0.058102664, 0.03658436, 0.031093864, 0.040503725, 0.04429044, 0.06429229, 0.04730642, 0.040821385, 0.038703654, 0.033632793, 0.05344996, 0.05104101, 0.050272003, 0.042002153, 0.032860976, 0.050632242, 0.029072339, 0.025783794, 0.030359363, 0.02870607, 0.03130568, 0.031741284, 0.030410552, 0.029776441, 0.025270805, 0.031184122, 0.025197279, 0.019667441, 0.12428285, 0.15514076, 0.2888293, 0.28002098, 0.10294691, 0.109374136, 0.093225576, 0.066936515, 0.0724161, 0.10081086, 0.04402569, 0.046004955, 0.089497544, 0.056890734, 0.03805321, 0.035419434, 0.042406008, 0.05073497, 0.04830767, 0.041198254, 0.04890078, 0.033610627, 0.03539022, 0.03699126, 0.030196372, 0.030150304, 0.029235367, 0.029734947, 0.041372553, 0.029514408, 0.02481975, 0.028213028, 0.036866177, 0.029626027, 0.029026056, 0.033825863, 0.030308586, 0.022598708, 0.07252544, 0.106277406, 0.18171215, 0.13296641, 0.14176652, 0.10370916, 0.0563955, 0.0754556, 0.06360469, 0.05328262, 0.04519188, 0.08265471, 0.05503494, 0.051081013, 0.033653338, 0.054855853, 0.035277437, 0.03241432, 0.039718937, 0.048618097, 0.032180853, 0.029548889, 0.033323213, 0.027446596, 0.035516534, 0.053359378, 0.023210226, 0.039475884, 0.033847813, 0.032144435, 0.03238238, 0.046405718, 0.035953593, 0.035821762, 0.025172241, 0.03728806, 0.02960376, 0.022252472, 0.07392881, 0.16511889, 0.20760466, 0.19192629, 0.051419806, 0.045326598, 0.06708825, 0.09793613, 0.10156077, 0.06395861, 0.07179307, 0.0339555, 0.04725628, 0.033760197, 0.060744964, 0.05438238, 0.0364397, 0.088013634, 0.053658392, 0.059053604, 0.041282684, 0.02910415, 0.040657386, 0.030591244, 0.031274863, 0.025085986, 0.030220702, 0.033874445, 0.033761352, 0.030060774, 0.034839284, 0.032718416, 0.02577595, 0.024625592, 0.029547049, 0.03507657, 0.026006198, 0.024628334, 0.12379283, 0.11410792, 0.15551348, 0.06117224, 0.086092494, 0.08888374, 0.09405555, 0.15658209, 0.029140322, 0.10076291, 0.052602023, 0.05191447, 0.065781176, 0.08840998, 0.047427665, 0.0293219, 0.049421303, 0.028136803, 0.053907394, 0.06420247, 0.052113496, 0.03362409, 0.043374054, 0.038182333, 0.032258186, 0.039455727, 0.040851813, 0.035857696, 0.035876874, 0.02834155, 0.035920586, 0.029266326, 0.02797985, 0.030609572, 0.030635985, 0.030726973, 0.030634578, 0.033689894, 0.10993101, 0.12811217, 0.25859827, 0.12939838, 0.067357324, 0.08598926, 0.089119256, 0.13042358, 0.13605382, 0.1509443, 0.07630003, 0.057670053, 0.063231334, 0.03126851, 0.040103268, 0.04395062, 0.03565047, 0.03117583, 0.029067384, 0.036921415, 0.041271508, 0.027936364, 0.036439937, 0.02656542, 0.02957873, 0.03510804, 0.04170579, 0.03713861, 0.03779456, 0.029573303, 0.0493126, 0.037148282, 0.043271028, 0.031193865, 0.03547204, 0.027708305, 0.021741938, 0.030004516, 0.14194816, 0.1260686, 0.2025824, 0.24158315, 0.08425303, 0.06579891, 0.05578585, 0.074950024, 0.048796598, 0.0607154, 0.049980804, 0.07605743, 0.08786777, 0.043306194, 0.039870113, 0.048458528, 0.03026987, 0.034973968, 0.043442734, 0.040873118, 0.040363114, 0.043525934, 0.033586167, 0.031428628, 0.03156201, 0.039966144, 0.026585218, 0.03169927, 0.035466567, 0.039293576, 0.040045604, 0.040451813, 0.034947407, 0.023864474, 0.031024722, 0.025859341, 0.033733997, 0.029410815, 0.09203567, 0.094060495, 0.17122461, 0.14150193, 0.08259833, 0.12721947, 0.091446556, 0.08096265, 0.14241466, 0.12284447, 0.047210734, 0.101115465, 0.061947, 0.04722796, 0.08435824, 0.04681562, 0.06742863, 0.034793828, 0.043106448, 0.031432644, 0.044423718, 0.04104728, 0.035780407, 0.029071199, 0.03021275, 0.027487507, 0.028884608, 0.029779667, 0.030766651, 0.027754156, 0.0331058, 0.036118556, 0.026146995, 0.031495422, 0.04095272, 0.0279731, 0.028956229, 0.041528523, 0.1391855, 0.114409246, 0.25424194, 0.22737157, 0.10422836, 0.082901746, 0.09507269, 0.097058736, 0.059699178, 0.10439871, 0.0617567, 0.03227644, 0.040962264, 0.032455232, 0.043228313, 0.05892945, 0.04021209, 0.039726276, 0.026887266, 0.028454827, 0.037538152, 0.04756305, 0.029930973, 0.030252274, 0.034099527, 0.03232337, 0.026903605, 0.03047962, 0.040189866, 0.04821266, 0.03712166, 0.031056179, 0.03474596, 0.03131884, 0.030900024, 0.03621633, 0.029181233, 0.032831483, 0.030444916, 0.029016417, 0.030645236, 0.028095769, 0.0229342, 0.08580239, 0.2703174, 0.24543406, 0.25255677, 0.27558425, 0.15051465, 0.06728395, 0.070476234, 0.08754232, 0.109500624, 0.11264942, 0.038155943, 0.040910948, 0.032495342, 0.062242754, 0.059604157, 0.055238925, 0.045803696, 0.05052461, 0.041508343, 0.039650533, 0.053261977, 0.042038534, 0.030087784, 0.03048478, 0.04043512, 0.03529776, 0.02691495, 0.033130206, 0.03962566, 0.03589047, 0.026750112, 0.027372962, 0.034615733, 0.027850842, 0.02263843, 0.0433665, 0.022936886, 0.050117, 0.04601725, 0.19032145, 0.112843804, 0.09346451, 0.058473643, 0.07217348, 0.053063262, 0.055720102, 0.07722236, 0.040776208, 0.062393654, 0.054774754, 0.072431274, 0.046444785, 0.06571397, 0.03999029, 0.040259734, 0.037817445, 0.039571844, 0.041621614, 0.040557537, 0.042843804, 0.029992595, 0.0312788, 0.026880002, 0.030053584, 0.022436643, 0.0199276, 0.028449723, 0.032866262, 0.03432734, 0.03454997, 0.030790403, 0.026861593, 0.026135689, 0.030264584, 0.031201469, 0.04981251, 0.11164159, 0.13641791, 0.112151004, 0.14851964, 0.1348934, 0.1009687, 0.04397542, 0.070196085, 0.115862906, 0.08177259, 0.05083699, 0.053511318, 0.061339423, 0.038590617, 0.05004999, 0.056115445, 0.048713423, 0.037202634, 0.06412011, 0.044000328, 0.027782494, 0.038883526, 0.04946439, 0.026912658, 0.039135426, 0.032421276, 0.03695988, 0.028714854, 0.039408993, 0.030528918, 0.037398785, 0.03943653, 0.03092852, 0.034572884, 0.023443347, 0.03152374, 0.029158339, 0.08883261, 0.14030612, 0.21039523, 0.09060017, 0.08468737, 0.056008365, 0.06331234, 0.06681577, 0.0684737, 0.08394196, 0.049082194, 0.043740172, 0.054775614, 0.047583416, 0.06468656, 0.041567616, 0.053001873, 0.03713861, 0.048192687, 0.04479306, 0.052445892, 0.05109448, 0.03172296, 0.034020666, 0.033673067, 0.038337287, 0.05287409, 0.0422718, 0.038365252, 0.030495599, 0.02931665, 0.031793237, 0.02832509, 0.032308847, 0.0349168, 0.048961643, 0.036759507, 0.021831179, 0.07209184, 0.14025508, 0.06567041, 0.16575155, 0.17903174, 0.08543858, 0.09173925, 0.09313103, 0.07728893, 0.048560575, 0.047240723, 0.04527956, 0.08514029, 0.054345664, 0.08004433, 0.08827384, 0.053244922, 0.031142473, 0.032345574, 0.02332865, 0.064825796, 0.037061464, 0.041583005, 0.044288743, 0.023748234, 0.040349554, 0.038323294, 0.03197961, 0.025560228, 0.025677115, 0.03578604, 0.022805031, 0.031099105, 0.025913522, 0.026989736, 0.037595417, 0.03920167, 0.022904497, 0.036543224, 0.033323962, 0.028525457, 0.018645719, 0.022354104, 0.019613145, 0.116318166, 0.13757236, 0.12824364, 0.07149356, 0.050136115, 0.04019554, 0.06843942, 0.06783205, 0.064595945, 0.04715253, 0.037660282, 0.041827943, 0.028386319, 0.046882942, 0.0654579, 0.044703428, 0.048752848, 0.03346671, 0.04155419, 0.03268839, 0.049307324, 0.037239768, 0.04211677, 0.030898172, 0.040467322, 0.025411095, 0.03335907, 0.024582023, 0.02511481, 0.037587926, 0.029482506, 0.03122946, 0.024054581, 0.035804328, 0.027693855, 0.026487144, 0.020478865, 0.02784921, 0.11721666, 0.362628, 0.25488576, 0.088771395, 0.06394253, 0.060201555, 0.12021086, 0.14180821, 0.06067387, 0.10287846, 0.063194714, 0.057985216, 0.09753543, 0.1111583, 0.03394689, 0.03813245, 0.046843402, 0.04129988, 0.035013504, 0.03428337, 0.048251983, 0.03629918, 0.034267597, 0.02582205, 0.03615691, 0.03723682, 0.026434127, 0.025025466, 0.030474111, 0.024680492, 0.03507166, 0.027102891, 0.035825886, 0.025090093, 0.037921418, 0.042799313, 0.031819597, 0.021130387, 0.059234813, 0.103718475, 0.152926, 0.1535029, 0.2215648, 0.06589731, 0.08982557, 0.046289854, 0.072698876, 0.08605513, 0.085433, 0.050364368, 0.050296437, 0.054688804, 0.058773708, 0.037734732, 0.028155332, 0.042241447, 0.027671278, 0.02232271, 0.040003426, 0.026934084, 0.03725335, 0.03822357, 0.03713251, 0.04108653, 0.024755614, 0.030685315, 0.025939777, 0.026484389, 0.021268517, 0.031065527, 0.026393024, 0.025659876, 0.023429092, 0.022813777, 0.02711718, 0.027857864, 0.12293436, 0.08774092, 0.11476766, 0.09828053, 0.081770405, 0.04672207, 0.14199415, 0.09722174, 0.07299384, 0.05291514, 0.03905825, 0.03480452, 0.05214196, 0.053587068, 0.06732154, 0.047678348, 0.034936678, 0.03390534, 0.047751773, 0.052041993, 0.02210551, 0.031736445, 0.04578131, 0.03409173, 0.035794806, 0.033425175, 0.04299282, 0.03448769, 0.018629108, 0.030173713, 0.028420085, 0.029864393, 0.032447763, 0.039069124, 0.037667204, 0.02487947, 0.022265742, 0.023959247, 0.05512463, 0.17168917, 0.2118342, 0.0716963, 0.06325032, 0.06766973, 0.03412462, 0.067798026, 0.10496963, 0.06473357, 0.07154883, 0.041693747, 0.035280176, 0.047778517, 0.040475663, 0.043724682, 0.0326131, 0.044856668, 0.026381785, 0.02534919, 0.03963164, 0.034786228, 0.027951764, 0.022649154, 0.020311935, 0.037696708, 0.026856784, 0.033989314, 0.029726818, 0.029265707, 0.02764022, 0.023734692, 0.03511251, 0.02193504, 0.02235858, 0.04445973, 0.044596717, 0.02286288, 0.0398759, 0.027900072, 0.02594368, 0.032346655, 0.03528236, 0.076309465, 0.0948548, 0.09361947, 0.066529185, 0.11441548, 0.093961746, 0.06262277, 0.082163796, 0.15800126, 0.10305292, 0.048782617, 0.038591415, 0.049994644, 0.030633826, 0.03208749, 0.0642145, 0.041202832, 0.029307513, 0.029956726, 0.027025089, 0.042083908, 0.031547498, 0.028959537, 0.01997544, 0.019514993, 0.03092837, 0.021445973, 0.029976051, 0.026209144, 0.030523594, 0.032700717, 0.030871088, 0.019752052, 0.026879003, 0.017240584, 0.029960835, 0.019080892, 0.022194674, 0.0660275, 0.111799605, 0.12573361, 0.14465225, 0.11626201, 0.17838292, 0.11431674, 0.040855024, 0.056433953, 0.13323979, 0.07552266, 0.038342476, 0.047007293, 0.037066624, 0.059585698, 0.058706798, 0.058897525, 0.043633826, 0.049923304, 0.035802104, 0.040450785, 0.02756019, 0.021123402, 0.028983964, 0.0307864, 0.028180392, 0.032250736, 0.04353335, 0.024090283, 0.03611754, 0.03677664, 0.024031218, 0.028359586, 0.030208254, 0.039632116, 0.030493435, 0.03194999, 0.025465868, 0.06060175, 0.11239024, 0.13594115, 0.4253491, 0.33211812, 0.069706984, 0.09476664, 0.07259554, 0.07005324, 0.050266903, 0.060014267, 0.038812477, 0.053630408, 0.04314749, 0.040780187, 0.03213949, 0.0383251, 0.030679395, 0.03190859, 0.03884694, 0.029645506, 0.03656116, 0.040311493, 0.035928898, 0.022815645, 0.022867188, 0.031380564, 0.02455231, 0.02797605, 0.034296192, 0.026516886, 0.028079024, 0.023300476, 0.03561468, 0.025946379, 0.0291574, 0.023293553, 0.024918597, 0.05098293, 0.055338815, 0.075015284, 0.061526388, 0.16893382, 0.037423957, 0.06605081, 0.06928077, 0.055568147, 0.057738487, 0.074760355, 0.044387322, 0.049761996, 0.044952508, 0.028230432, 0.042364314, 0.030046677, 0.037000142, 0.04269023, 0.029051047, 0.031943914, 0.024690378, 0.032080885, 0.039998144, 0.028978975, 0.042736735, 0.04240962, 0.023293631, 0.04180053, 0.024716407, 0.025583237, 0.018858874, 0.020899976, 0.02920978, 0.033978987, 0.021247761, 0.023359807, 0.034369815, 0.11155389, 0.0967527, 0.090872765, 0.18016364, 0.13302325, 0.07592372, 0.04138425, 0.0609306, 0.035099454, 0.122001134, 0.045757562, 0.057019655, 0.046626125, 0.058878906, 0.05864723, 0.0537416, 0.030556647, 0.029564422, 0.0386747, 0.042342138, 0.041697472, 0.029212425, 0.029069727, 0.033457946, 0.03265636, 0.045651134, 0.033179317, 0.03147856, 0.026579285, 0.025857555, 0.033814758, 0.022504576, 0.030577105, 0.030807178, 0.026438639, 0.02444516, 0.029352224, 0.02533847, 0.02207315, 0.024066502, 0.02533687, 0.03020452, 0.021067165, 0.027153855, 0.051686995, 0.14377701, 0.17984651, 0.10208747, 0.075808905, 0.060915142, 0.10705509, 0.069621705, 0.076658435, 0.046189725, 0.049694058, 0.041065224, 0.048467312, 0.044875495, 0.042777643, 0.04004815, 0.038346276, 0.04923144, 0.02774366, 0.043215882, 0.060436983, 0.04002402, 0.028944965, 0.027315842, 0.028523605, 0.041127063, 0.029360496, 0.03042663, 0.02163263, 0.022000298, 0.033563945, 0.022023775, 0.03304952, 0.030973012, 0.030023515, 0.02516448, 0.027970277, 0.02833612, 0.077828534, 0.080073446, 0.068614066, 0.1474006, 0.1274305, 0.064654, 0.0817982, 0.079459794, 0.08148337, 0.07685242, 0.045092274, 0.067032136, 0.1115843, 0.061229914, 0.02354247, 0.030842647, 0.038828976, 0.046402592, 0.030898845, 0.037461795, 0.030342707, 0.02571985, 0.033715915, 0.033991452, 0.033037096, 0.04107102, 0.023502614, 0.025508687, 0.04033023, 0.028169582, 0.044799596, 0.02334401, 0.032519985, 0.027423432, 0.023661433, 0.023284031, 0.03105087, 0.030152345, 0.084757864, 0.18304664, 0.12038436, 0.11250136, 0.08081741, 0.08354774, 0.068787366, 0.04461542, 0.078357324, 0.039271418, 0.033389136, 0.025978014, 0.026837474, 0.04792691, 0.05620908, 0.034343477, 0.033940595, 0.044254173, 0.03994441, 0.03601599, 0.0271275, 0.031927694, 0.032395232, 0.027841637, 0.020935966, 0.02995795, 0.034073, 0.027117409, 0.034052275, 0.024713853, 0.027470332, 0.022826146, 0.026224777, 0.022553666, 0.029010868, 0.018687343, 0.019564461, 0.034667607, 0.13174325, 0.12776425, 0.121825494, 0.17090318, 0.11781921, 0.07391292, 0.057339486, 0.095857695, 0.056421675, 0.13177446, 0.057202432, 0.033291627, 0.038224712, 0.055167995, 0.044409, 0.034822922, 0.024819339, 0.026495062, 0.049607415, 0.04275246, 0.039327316, 0.033078317, 0.032960087, 0.03176116, 0.035302248, 0.031343352, 0.032287315, 0.02325412, 0.021029312, 0.028039066, 0.033836972, 0.035154823, 0.030970184, 0.019969905, 0.01857435, 0.02287618, 0.026623541, 0.025556369, 0.086929314, 0.044211593, 0.11665551, 0.15913619, 0.051993597, 0.03734975, 0.04679125, 0.08885903, 0.12618816, 0.07250216, 0.03920214, 0.08849872, 0.05690749, 0.057699397, 0.03652991, 0.028135246, 0.036956787, 0.037331212, 0.034813687, 0.03485434, 0.038790498, 0.037106942, 0.026398994, 0.032630194, 0.021786446, 0.02515259, 0.038083617, 0.025709774, 0.025328219, 0.022989113, 0.039217915, 0.038537517, 0.02688467, 0.024719305, 0.02527315, 0.027588455, 0.021439342, 0.030505635, 0.031907108, 0.017427953, 0.021886664, 0.021768406, 0.032756045, 0.11959066, 0.11969612, 0.083882175, 0.19015425, 0.14330061, 0.0830077, 0.12902066, 0.1280169, 0.090994515, 0.08932049, 0.105711795, 0.057275265, 0.04480481, 0.040947445, 0.04444367, 0.041939083, 0.03884817, 0.027923299, 0.030621056, 0.02579903, 0.029741572, 0.030794377, 0.032984234, 0.043592602, 0.025342992, 0.04016558, 0.025452659, 0.025743593, 0.019651277, 0.023265751, 0.023573924, 0.023688266, 0.027619857, 0.037072416, 0.032101575, 0.022750368, 0.025715217, 0.017769318, 0.0590867, 0.0616435, 0.09988958, 0.11357852, 0.11339197, 0.073285855, 0.06754961, 0.08310388, 0.11600655, 0.118690416, 0.053357217, 0.04673332, 0.06975969, 0.043840837, 0.04832135, 0.05386394, 0.038449135, 0.04568144, 0.035550024, 0.050129447, 0.0394613, 0.03047311, 0.040974207, 0.03363429, 0.033106413, 0.025769368, 0.03141883, 0.030585615, 0.019036805, 0.020286059, 0.026979342, 0.025265414, 0.029715698, 0.023824308, 0.040450368, 0.022366578, 0.029047126, 0.019218173, 0.07610973, 0.082880914, 0.11169307, 0.115963124, 0.1441303, 0.057634298, 0.0944427, 0.04991405, 0.050712626, 0.061502494, 0.051748935, 0.044837214, 0.036018845, 0.05284824, 0.03951566, 0.059100453, 0.059057217, 0.02927118, 0.04837653, 0.027150217, 0.023844907, 0.028301502, 0.024508536, 0.026512329, 0.02574493, 0.041702617, 0.03548352, 0.028227555, 0.023885345, 0.024875382, 0.028748399, 0.024479948, 0.029069502, 0.022898635, 0.025943022, 0.029602038, 0.025483703, 0.030879313, 0.1849829, 0.16279197, 0.09733052, 0.20626411, 0.12914047, 0.10902293, 0.04465017, 0.047418207, 0.08770528, 0.056200515, 0.038443554, 0.03538092, 0.032508023, 0.04712754, 0.029635612, 0.06505853, 0.06549303, 0.05031002, 0.0502384, 0.029743863, 0.04125875, 0.026910296, 0.03313189, 0.021861391, 0.03165024, 0.027513966, 0.026254967, 0.023826228, 0.04639986, 0.029052025, 0.024701742, 0.022245685, 0.029660529, 0.03398848, 0.021883942, 0.025316965, 0.035840485, 0.029283082, 0.07552812, 0.10490869, 0.09832057, 0.062550426, 0.18865253, 0.09589214, 0.061916746, 0.13456912, 0.08275774, 0.056935996, 0.030077344, 0.032374848, 0.041804608, 0.03810612, 0.026959341, 0.03748227, 0.026446665, 0.045462936, 0.0298064, 0.036651053, 0.027616397, 0.028522786, 0.034472525, 0.03271655, 0.031444777, 0.026932174, 0.027548438, 0.032348346, 0.03330043, 0.026723765, 0.020182813, 0.020829612, 0.02119125, 0.022797959, 0.021825235, 0.02452628, 0.020604353, 0.033930156, 0.027385795, 0.020267047, 0.027549662, 0.021310523, 0.018559644, 0.019592257, 0.07575652, 0.07914725, 0.11761381, 0.07976351, 0.07052601, 0.038304217, 0.051626217, 0.100379646, 0.07286748, 0.060987435, 0.06081351, 0.057476427, 0.059349574, 0.06348407, 0.05532093, 0.03254048, 0.039566703, 0.033000704, 0.03300348, 0.030746955, 0.032209497, 0.03460456, 0.026150616, 0.041479986, 0.021465564, 0.03013502, 0.027560482, 0.022437373, 0.024761481, 0.02898225, 0.021927096, 0.0352413, 0.021370843, 0.027317863, 0.031697, 0.02281559, 0.0258593, 0.023853505, 0.08708209, 0.15635477, 0.22866745, 0.12217553, 0.091421396, 0.06342704, 0.04047782, 0.059479266, 0.05455083, 0.03928989, 0.04049888, 0.043863535, 0.047759753, 0.069445394, 0.04213428, 0.028687363, 0.029824799, 0.04527494, 0.03815767, 0.031230688, 0.04272537, 0.030833853, 0.03058556, 0.030780379, 0.02399833, 0.031389248, 0.029011035, 0.022273893, 0.03156051, 0.029289098, 0.024576047, 0.03737823, 0.024847751, 0.013505727, 0.021661371, 0.020251438, 0.023698343, 0.026638756, 0.1322147, 0.11939208, 0.08996062, 0.13114353, 0.09507678, 0.07230569, 0.041974045, 0.070557415, 0.09534277, 0.045604732, 0.06759641, 0.070935465, 0.047933303, 0.05798412, 0.03292047, 0.04402048, 0.045739498, 0.03991354, 0.034777068, 0.027619077, 0.03770967, 0.03395521, 0.024230357, 0.034833215, 0.022949705, 0.022924896, 0.034812335, 0.03007598, 0.020442573, 0.025288558, 0.026176894, 0.023027405, 0.033015605, 0.024849148, 0.029724002, 0.02524354, 0.017981743, 0.030140745, 0.09850339, 0.066920064, 0.14580366, 0.15926047, 0.06852696, 0.082015455, 0.13079059, 0.088345826, 0.07480548, 0.05992331, 0.071102835, 0.045437388, 0.03102768, 0.03862188, 0.037128046, 0.045012087, 0.052516337, 0.044302363, 0.08663494, 0.027930861, 0.03295456, 0.036667235, 0.028462209, 0.025027324, 0.03258943, 0.021819647, 0.023658669, 0.028788539, 0.027950393, 0.019747714, 0.025604812, 0.027984202, 0.026564758, 0.026991254, 0.030417657, 0.021787865, 0.027268084, 0.036490638, 0.048032448, 0.061048076, 0.09010549, 0.06960841, 0.043740567, 0.043056257, 0.07097257, 0.10068326, 0.07152816, 0.07181988, 0.066014804, 0.030905608, 0.044079173, 0.03824017, 0.038807485, 0.043719, 0.03021233, 0.04910889, 0.03450953, 0.030607672, 0.0396839, 0.027964441, 0.02710153, 0.025164023, 0.024245147, 0.032118846, 0.024186408, 0.02037293, 0.025167415, 0.027962303, 0.034921397, 0.02452212, 0.030145567, 0.031392448, 0.023243422, 0.028824866, 0.028974107, 0.026858164, 0.02496253, 0.027432099, 0.024964053, 0.023056267, 0.026493013, 0.07654423, 0.095960416, 0.14097305, 0.2075516, 0.18002547, 0.08320385, 0.05253195, 0.10295846, 0.06439308, 0.11327114, 0.12871213, 0.10050438, 0.085016906, 0.056876596, 0.05031686, 0.044337597, 0.046704356, 0.048505727, 0.050023604, 0.03291879, 0.033545326, 0.03763202, 0.033648588, 0.025871167, 0.021860188, 0.045905814, 0.042330194, 0.025270404, 0.029294306, 0.021805644, 0.022991344, 0.023764454, 0.024946705, 0.030040001, 0.024899637, 0.029836783, 0.023847753, 0.030030357, 0.07199077, 0.14798051, 0.08293807, 0.06640284, 0.1039428, 0.040597547, 0.036106396, 0.049822718, 0.073866084, 0.027322697, 0.04339396, 0.057719845, 0.055415384, 0.038311195, 0.02915541, 0.02896984, 0.025574027, 0.03575614, 0.033490527, 0.0368717, 0.035667595, 0.03365637, 0.032844625, 0.028809015, 0.04057952, 0.032329746, 0.025843307, 0.030757928, 0.025066782, 0.02365589, 0.022203907, 0.021146812, 0.02626607, 0.02718454, 0.025268903, 0.027712006, 0.023321074, 0.032760955, 0.26148507, 0.18443535, 0.09504606, 0.08544525, 0.15915234, 0.09888091, 0.08350722, 0.0637551, 0.04357319, 0.046006985, 0.057237707, 0.06511323, 0.060371913, 0.08740842, 0.03763402, 0.04959237, 0.04244561, 0.031395935, 0.037546538, 0.052439854, 0.03494968, 0.022216594, 0.0349858, 0.03270029, 0.024901865, 0.026222918, 0.029846342, 0.024024539, 0.027493333, 0.029064342, 0.029518692, 0.023161951, 0.02623161, 0.029551493, 0.027084602, 0.02522966, 0.025272071, 0.027121255, 0.07754058, 0.18087322, 0.23427105, 0.14019161, 0.07238023, 0.07022958, 0.07742781, 0.05082205, 0.055530537, 0.060329888, 0.0557621, 0.042401478, 0.085045196, 0.07492624, 0.08292797, 0.042333927, 0.04361404, 0.037050933, 0.030817155, 0.023912309, 0.02646736, 0.02702383, 0.031016227, 0.025259215, 0.0337795, 0.022925949, 0.030935828, 0.025478126, 0.025454016, 0.028114786, 0.02855272, 0.023152983, 0.02236407, 0.023367876, 0.02723919, 0.026648944, 0.028167414, 0.02398314, 0.046242423, 0.11500316, 0.107902, 0.108383395, 0.05875556, 0.049142748, 0.046176884, 0.09512931, 0.052171517, 0.09408884, 0.045380205, 0.04742192, 0.06717815, 0.037532695, 0.03715161, 0.035673335, 0.03416612, 0.029858187, 0.025555752, 0.03528452, 0.02675659, 0.025761627, 0.029625429, 0.03173845, 0.021852046, 0.024828093, 0.02685894, 0.04331475, 0.02814834, 0.020765886, 0.034346443, 0.029205803, 0.029972447, 0.022533862, 0.031130351, 0.03753156, 0.0346419, 0.043477863, 0.027132438, 0.023468247, 0.026099682, 0.026581045, 0.01921986, 0.02044034, 0.08432423, 0.092124, 0.14732867, 0.10276383, 0.09730651, 0.07500312, 0.051580936, 0.0689631, 0.05199746, 0.07076484, 0.04062561, 0.022687513, 0.030188745, 0.027653245, 0.027190952, 0.031188603, 0.021529708, 0.031798393, 0.025191663, 0.043268424, 0.03574198, 0.036534484, 0.029959524, 0.037073817, 0.026643198, 0.035897225, 0.033879187, 0.03292496, 0.029032368, 0.028219173, 0.032455176, 0.016964942, 0.027247934, 0.020292377, 0.024287976, 0.028736435, 0.025182413, 0.021735638, 0.11137341, 0.08605908, 0.08027879, 0.10524088, 0.10518112, 0.071231015, 0.08828857, 0.10274397, 0.13776779, 0.072250344, 0.035097253, 0.03623217, 0.040496968, 0.04083668, 0.061600912, 0.02688633, 0.039173752, 0.04973203, 0.033186127, 0.03427412, 0.027030151, 0.028852168, 0.03069762, 0.027229076, 0.02562276, 0.02852831, 0.025335452, 0.020579923, 0.027190154, 0.023922654, 0.028517902, 0.020450214, 0.020971889, 0.02696775, 0.018603139, 0.018457238, 0.018810425, 0.03154765, 0.071719214, 0.0761356, 0.109856464, 0.06376321, 0.05879337, 0.06913511, 0.09283052, 0.10130972, 0.050801266, 0.05511683, 0.04830243, 0.04957152, 0.055264402, 0.034707457, 0.04866361, 0.028910303, 0.029134847, 0.026833544, 0.029448427, 0.030662775, 0.034467362, 0.034961663, 0.02264565, 0.02234193, 0.02450836, 0.025309406, 0.025942042, 0.021536987, 0.017660549, 0.018389743, 0.02660302, 0.023649337, 0.031710263, 0.013773192, 0.022953503, 0.024162458, 0.026434189, 0.016830735, 0.05692779, 0.054599956, 0.122599415, 0.14672343, 0.1327798, 0.083397634, 0.04140058, 0.08792137, 0.08443765, 0.06271556, 0.061328806, 0.046113104, 0.04002804, 0.0454977, 0.025967818, 0.029055601, 0.031653386, 0.03096764, 0.02882615, 0.03170613, 0.034053564, 0.03587435, 0.023680035, 0.017508402, 0.022321666, 0.02389982, 0.018949486, 0.018167306, 0.019494012, 0.02759337, 0.022220062, 0.015165262, 0.024231976, 0.018521374, 0.02926925, 0.014544211, 0.019821987, 0.02206506, 0.074309595, 0.07511431, 0.110525385, 0.07883404, 0.13905017, 0.06589458, 0.057347674, 0.08793137, 0.09138821, 0.13676988, 0.040490236, 0.04438274, 0.05279405, 0.04903834, 0.038654547, 0.02407721, 0.032373678, 0.03134127, 0.04879982, 0.041916985, 0.04481137, 0.033641353, 0.029306004, 0.024924183, 0.036794566, 0.025424207, 0.02042949, 0.028923053, 0.026933447, 0.023487005, 0.02222696, 0.02893104, 0.02624076, 0.017863208, 0.018692577, 0.021553218, 0.01789103, 0.020106513, 0.016554495, 0.023180792, 0.02485649, 0.022025041, 0.02046894, 0.09253075, 0.07554798, 0.073173404, 0.08983519, 0.06863707, 0.075956285, 0.042538136, 0.07298465, 0.11452669, 0.04004076, 0.037033252, 0.019345623, 0.034817643, 0.041472636, 0.048560616, 0.04972552, 0.038347233, 0.041390408, 0.035610862, 0.026944838, 0.031406164, 0.0260155, 0.02408676, 0.018516079, 0.027009528, 0.032724172, 0.02549896, 0.026778096, 0.020971125, 0.027671017, 0.02199297, 0.023114214, 0.018271374, 0.022085119, 0.021981558, 0.013964399, 0.016913796, 0.015800472, 0.048969965, 0.11395577, 0.072772466, 0.09576587, 0.1402918, 0.05071549, 0.107633054, 0.075687505, 0.05730928, 0.06455238, 0.05806207, 0.037191164, 0.05118566, 0.025579564, 0.02920211, 0.053531226, 0.043763407, 0.038169406, 0.025080806, 0.033052802, 0.02165904, 0.025461616, 0.022218244, 0.026893025, 0.021004701, 0.02624554, 0.02373857, 0.016587662, 0.024527902, 0.021516144, 0.023738, 0.022276191, 0.01887853, 0.018179486, 0.019304957, 0.027450053, 0.018498825, 0.022180336, 0.11510394, 0.10821412, 0.14176536, 0.13521571, 0.096330546, 0.115438685, 0.16387856, 0.04601138, 0.04919025, 0.07873406, 0.0443553, 0.03874614, 0.03567489, 0.02801269, 0.030035162, 0.052236903, 0.04023979, 0.041968573, 0.02987898, 0.032996185, 0.018170139, 0.025613276, 0.024470095, 0.029329259, 0.021095378, 0.032823544, 0.03536717, 0.02288335, 0.03156406, 0.023693042, 0.024225127, 0.027644277, 0.031424083, 0.024554498, 0.022160009, 0.029776968, 0.024756191, 0.02048526, 0.0822066, 0.16092436, 0.12093519, 0.11151743, 0.30989066, 0.13677688, 0.09065216, 0.10306472, 0.059792507, 0.06373546, 0.045829378, 0.04905832, 0.04789288, 0.03706048, 0.03432116, 0.074479625, 0.042508118, 0.026643584, 0.030716332, 0.04782425, 0.029387418, 0.043872353, 0.024148628, 0.028201593, 0.033777744, 0.029834546, 0.025324104, 0.022484949, 0.028550386, 0.019025426, 0.024780149, 0.02760987, 0.024547871, 0.025983375, 0.026926352, 0.026593579, 0.023468807, 0.019202145, 0.056552052, 0.1671923, 0.09453221, 0.090883784, 0.11055493, 0.0935964, 0.05920059, 0.03856505, 0.033439826, 0.03743986, 0.03638522, 0.0439715, 0.077850975, 0.044488758, 0.044848822, 0.03087167, 0.02534781, 0.027138907, 0.02954256, 0.02332498, 0.03526303, 0.02277055, 0.044051785, 0.03281686, 0.028312175, 0.028247278, 0.042515326, 0.023535091, 0.028596543, 0.026185669, 0.022860296, 0.022176659, 0.03246674, 0.023637928, 0.026752578, 0.026154736, 0.026418438, 0.031671617, 0.025407389, 0.019956492, 0.022808334, 0.021585627, 0.020109225, 0.025409486, 0.06794268, 0.1428811, 0.1635042, 0.1768234, 0.0839579, 0.080065146, 0.06709831, 0.056057222, 0.08120724, 0.0923764, 0.04009317, 0.043186966, 0.04375014, 0.028849987, 0.041930348, 0.053529933, 0.031452034, 0.036431693, 0.029390391, 0.028211553, 0.0315921, 0.025928639, 0.024754079, 0.028028423, 0.032637525, 0.031723607, 0.02898582, 0.027380614, 0.034847505, 0.019880325, 0.040326398, 0.027667925, 0.029628085, 0.023710318, 0.019378848, 0.030353412, 0.024969555, 0.024180686, 0.05771304, 0.1094254, 0.06994264, 0.1227149, 0.06309121, 0.034783076, 0.054972153, 0.041068137, 0.051787954, 0.048353106, 0.057530634, 0.03125334, 0.030784236, 0.03708271, 0.050104063, 0.032781966, 0.028790317, 0.033742834, 0.039076086, 0.03327283, 0.026753595, 0.040981796, 0.025236165, 0.023074213, 0.024723416, 0.022728259, 0.024838258, 0.02570935, 0.027242001, 0.020016497, 0.022671178, 0.029760059, 0.025777105, 0.028784774, 0.021424418, 0.020569518, 0.027377382, 0.018973768, 0.055404864, 0.06476754, 0.09440177, 0.05558533, 0.07906186, 0.06412557, 0.039139614, 0.05024684, 0.076416045, 0.071970575, 0.06810572, 0.049890283, 0.0397101, 0.045995265, 0.025251472, 0.036793973, 0.03866031, 0.036796536, 0.030270692, 0.02702812, 0.028236242, 0.023993442, 0.02471971, 0.027938021, 0.02415862, 0.01885598, 0.025320437, 0.024851644, 0.019216977, 0.02105346, 0.02465816, 0.020172598, 0.024368726, 0.01765398, 0.019517347, 0.014648483, 0.027717926, 0.016175365, 0.10746587, 0.16777226, 0.17830291, 0.1110906, 0.034602236, 0.07528938, 0.09058079, 0.04314371, 0.09117415, 0.1384389, 0.05208437, 0.039307564, 0.03256981, 0.026739327, 0.029362477, 0.038658485, 0.03137933, 0.028213862, 0.04263375, 0.032824825, 0.038832687, 0.024860762, 0.026408888, 0.027507754, 0.025757123, 0.029697211, 0.028347379, 0.034128156, 0.025950085, 0.021859631, 0.021877324, 0.023581767, 0.022391988, 0.035245895, 0.020193521, 0.01966281, 0.03079711, 0.02403677, 0.05743469, 0.07727658, 0.252894, 0.11873894, 0.047429644, 0.045608904, 0.11228004, 0.083988786, 0.061872255, 0.04294395, 0.04700949, 0.02758806, 0.042099096, 0.04404842, 0.04238652, 0.035016786, 0.032373916, 0.03193624, 0.03722599, 0.027439678, 0.02329349, 0.03633579, 0.029607855, 0.02650905, 0.031089636, 0.029764973, 0.028959826, 0.020848578, 0.024452949, 0.020640451, 0.03507433, 0.02081212, 0.022871876, 0.030925596, 0.02171166, 0.019350812, 0.025118303, 0.02440368, 0.17219605, 0.15384042, 0.12755464, 0.08665758, 0.07677658, 0.082023464, 0.092983045, 0.06874232, 0.055495225, 0.039546404, 0.044495955, 0.052977644, 0.044417366, 0.024105592, 0.042806815, 0.03584578, 0.04999735, 0.02221109, 0.033217028, 0.035205867, 0.03695559, 0.032101978, 0.030294808, 0.026601108, 0.023982953, 0.02450242, 0.023379123, 0.022593781, 0.026741536, 0.01815197, 0.02729079, 0.027965697, 0.026021065, 0.03307463, 0.031900715, 0.018885389, 0.02382758, 0.020536216, 0.074586175, 0.09110357, 0.13581328, 0.09280014, 0.04576887, 0.1200406, 0.09009713, 0.04401974, 0.031180708, 0.04847191, 0.041682735, 0.039682906, 0.0358644, 0.032431353, 0.040992714, 0.027552592, 0.027134184, 0.026866624, 0.032480527, 0.026730642, 0.020893937, 0.029807797, 0.032446016, 0.025353964, 0.02994596, 0.023663802, 0.031923033, 0.02546447, 0.025668398, 0.016751211, 0.026879715, 0.020685628, 0.02419097, 0.021663202, 0.023577537, 0.019019665, 0.019941494, 0.017723724, 0.091609925, 0.12889044, 0.09745984, 0.12716398, 0.07157674, 0.095013015, 0.03240505, 0.035626322, 0.08584056, 0.047424756, 0.089361094, 0.042735778, 0.043073956, 0.03143418, 0.029181374, 0.026153399, 0.026684577, 0.026915932, 0.021178115, 0.066922486, 0.04737144, 0.033557307, 0.018884944, 0.025223022, 0.023047755, 0.02192341, 0.018847605, 0.020960981, 0.01812852, 0.02341853, 0.019467574, 0.021726722, 0.020104438, 0.024316713, 0.019032191, 0.028231287, 0.024461236, 0.031168941, 0.12073447, 0.108525455, 0.20426357, 0.12280915, 0.049613982, 0.049854577, 0.054949194, 0.03148261, 0.036024597, 0.06338217, 0.07543709, 0.07417793, 0.066550314, 0.06358859, 0.037119944, 0.041996162, 0.042480793, 0.042765934, 0.033670615, 0.030838352, 0.05499552, 0.03222099, 0.026617656, 0.036878053, 0.02268272, 0.02629679, 0.023232624, 0.026194228, 0.019685902, 0.021531397, 0.026745962, 0.022599518, 0.02715606, 0.029953782, 0.021654084, 0.020819293, 0.022081897, 0.018423075, 0.06373378, 0.070499204, 0.06573625, 0.14630851, 0.058560777, 0.04064917, 0.036093425, 0.03692433, 0.030401582, 0.061032742, 0.04333075, 0.03386826, 0.038432717, 0.042250924, 0.03494097, 0.028556291, 0.04388795, 0.041748352, 0.030078365, 0.024522077, 0.03137057, 0.02737015, 0.02365807, 0.02838161, 0.016582595, 0.024436086, 0.03340712, 0.028898945, 0.02137217, 0.01900503, 0.029562231, 0.024488553, 0.02321661, 0.01896664, 0.026415257, 0.020436192, 0.026358865, 0.024544166, 0.026894128, 0.02437592, 0.01467344, 0.032941762, 0.029878164, 0.08215043, 0.19626166, 0.15654247, 0.19343624, 0.061581098, 0.05957155, 0.04318912, 0.03507381, 0.04155291, 0.040213123, 0.046812706, 0.03792996, 0.033397075, 0.04378176, 0.034255017, 0.036831986, 0.031608213, 0.052858546, 0.052610192, 0.040907513, 0.03809242, 0.02943321, 0.030313194, 0.028356057, 0.02221249, 0.024900202, 0.031249542, 0.025964445, 0.020156875, 0.026445705, 0.021832595, 0.028544838, 0.022673462, 0.019384395, 0.025516674, 0.022545654, 0.017628297, 0.029723942, 0.07555661, 0.111833036, 0.07232132, 0.18501703, 0.13017929, 0.08717565, 0.06182357, 0.04633394, 0.03927979, 0.11133128, 0.034914006, 0.021274587, 0.025253795, 0.045540933, 0.03394292, 0.030810645, 0.03107164, 0.022966772, 0.017884756, 0.025362963, 0.025161823, 0.028891023, 0.017481199, 0.022793049, 0.027789565, 0.025084274, 0.01823929, 0.024457932, 0.028451389, 0.021979406, 0.014971642, 0.024658449, 0.019618573, 0.021175243, 0.021925258, 0.018247332, 0.019732192, 0.026205158, 0.10461636, 0.18524037, 0.089644425, 0.0955616, 0.10144339, 0.078113295, 0.08185641, 0.07749004, 0.058929373, 0.04597054, 0.04743239, 0.05143952, 0.059154686, 0.059389085, 0.06576105, 0.032425616, 0.061788283, 0.039390445, 0.034758363, 0.029741623, 0.035907444, 0.027402772, 0.021807099, 0.028399674, 0.024605855, 0.02483695, 0.026466534, 0.03416883, 0.019823475, 0.026501393, 0.028989471, 0.01774159, 0.01998916, 0.02152569, 0.019446002, 0.018279806, 0.019787079, 0.022051588, 0.05536561, 0.14109235, 0.09660674, 0.04468126, 0.14570704, 0.11172425, 0.08779643, 0.068981834, 0.07174861, 0.062540285, 0.0317436, 0.033825915, 0.054649327, 0.057957493, 0.035783023, 0.028520135, 0.023938743, 0.026396573, 0.02321195, 0.023763144, 0.022721771, 0.031992678, 0.030060988, 0.028291011, 0.025969744, 0.02744118, 0.027437279, 0.016613543, 0.025004964, 0.02494375, 0.025489168, 0.02438, 0.02893678, 0.016679406, 0.018854616, 0.02264639, 0.021388188, 0.018499717, 0.06427066, 0.07760057, 0.093931966, 0.09609379, 0.045007683, 0.058249444, 0.07184112, 0.03509033, 0.06707749, 0.054532196, 0.03248485, 0.047782384, 0.041626677, 0.02870545, 0.031379133, 0.029086972, 0.02508223, 0.022519404, 0.022088202, 0.025724439, 0.023268212, 0.025255663, 0.025688542, 0.025706159, 0.025092248, 0.015413761, 0.02293748, 0.019562721, 0.01843171, 0.02038044, 0.039075997, 0.023631217, 0.018693702, 0.020968908, 0.028103424, 0.012292366, 0.02108202, 0.021873064, 0.020379506, 0.02140506, 0.017447792, 0.027616547, 0.02193498, 0.015240119, 0.0745375, 0.09951705, 0.12770157, 0.13317445, 0.045362584, 0.06930185, 0.051972177, 0.035591803, 0.039087184, 0.02915194, 0.031966724, 0.028756395, 0.038127653, 0.032667957, 0.023633474, 0.02148018, 0.019430064, 0.035900958, 0.017116679, 0.019458389, 0.020529328, 0.016114796, 0.023719188, 0.01916342, 0.01586488, 0.036688082, 0.022926677, 0.027996233, 0.022389214, 0.021217242, 0.016506938, 0.016732085, 0.019305333, 0.013670931, 0.032053236, 0.022649506, 0.0153768435, 0.020452484, 0.066421814, 0.25575376, 0.19840987, 0.29402208, 0.0863592, 0.04811608, 0.09880632, 0.0752887, 0.045203228, 0.029193832, 0.03528684, 0.063242406, 0.03991297, 0.033225387, 0.026929632, 0.04809174, 0.040888775, 0.02976176, 0.030900255, 0.024741068, 0.026331956, 0.025779415, 0.024550168, 0.024567638, 0.026035327, 0.032723162, 0.029528297, 0.027862875, 0.025340892, 0.02403866, 0.018126892, 0.028415615, 0.02009954, 0.017000556, 0.02113185, 0.024851646, 0.0192585, 0.017569918, 0.09897154, 0.07562519, 0.18446214, 0.12630974, 0.085252464, 0.057023276, 0.08477354, 0.12448016, 0.081602834, 0.06094738, 0.037462268, 0.028733056, 0.026631882, 0.07409175, 0.04885008, 0.029649656, 0.024987467, 0.026633322, 0.019387426, 0.02301042, 0.025308501, 0.026578123, 0.029306209, 0.039694358, 0.028473098, 0.02543914, 0.031363815, 0.021000588, 0.01937862, 0.016943505, 0.0189122, 0.02349892, 0.025661575, 0.017013138, 0.018294618, 0.020218216, 0.023913117, 0.018401477, 0.10899489, 0.057286013, 0.11878025, 0.08806984, 0.04613887, 0.04387384, 0.03454738, 0.035598937, 0.06242326, 0.076234244, 0.041415833, 0.026484082, 0.020357905, 0.06761422, 0.045939256, 0.05474801, 0.038981542, 0.04264875, 0.026495002, 0.024339307, 0.021738537, 0.03161924, 0.017698426, 0.021759292, 0.028070299, 0.015946845, 0.017269725, 0.015521733, 0.014049418, 0.021096032, 0.020472383, 0.017084083, 0.012048169, 0.016666645, 0.020863567, 0.018052295, 0.015366983, 0.015558748, 0.07698779, 0.10118046, 0.12258992, 0.10777481, 0.047841936, 0.03906842, 0.042276748, 0.07301394, 0.07833946, 0.042716507, 0.032623332, 0.0553494, 0.032736275, 0.045394465, 0.027747756, 0.031727165, 0.03959482, 0.04130337, 0.028956808, 0.03081025, 0.045140177, 0.024705019, 0.022406159, 0.02887469, 0.019919414, 0.019811247, 0.019554295, 0.016470207, 0.020596094, 0.024457427, 0.02450553, 0.016670885, 0.016154088, 0.0157668, 0.0161262, 0.018827988, 0.01631999, 0.015241695, 0.024100607, 0.013784347, 0.01359256, 0.022140343, 0.016754214, 0.07640884, 0.09783104, 0.18104367, 0.08816313, 0.08464148, 0.054959178, 0.09123739, 0.03744111, 0.039426938, 0.05570765, 0.017431049, 0.024919119, 0.029610837, 0.02886461, 0.023593444, 0.044601806, 0.03579094, 0.01537672, 0.015149988, 0.02157315, 0.022233197, 0.021846276, 0.018461151, 0.021830756, 0.013875164, 0.021722656, 0.0155780455, 0.021655528, 0.024045028, 0.027063737, 0.015883625, 0.016325787, 0.022841511, 0.02417504, 0.018617982, 0.02027266, 0.015780047, 0.017184194, 0.04254744, 0.11272154, 0.15718941, 0.15636158, 0.1622534, 0.04290528, 0.03282168, 0.07029516, 0.05710576, 0.061680786, 0.03537706, 0.055018194, 0.037490137, 0.040144995, 0.033784334, 0.044767953, 0.024019154, 0.026931904, 0.035153534, 0.031086015, 0.02191336, 0.025416747, 0.02242255, 0.023098553, 0.022417085, 0.018949969, 0.018472446, 0.023007149, 0.02128165, 0.023494488, 0.0143118845, 0.015456862, 0.017979734, 0.01589582, 0.016607946, 0.02381132, 0.017098876, 0.021395996, 0.07424424, 0.4341118, 0.1785057, 0.15230538, 0.10924255, 0.07298321, 0.045066, 0.037256006, 0.07684406, 0.059797764, 0.024567997, 0.04621819, 0.0271898, 0.04713798, 0.029231796, 0.038559437, 0.04630354, 0.035884462, 0.036439117, 0.031648725, 0.02873656, 0.025129342, 0.021575635, 0.026254958, 0.01793945, 0.025876941, 0.027164202, 0.025149662, 0.03177002, 0.021846095, 0.030219426, 0.020426504, 0.019424425, 0.019867238, 0.017890347, 0.020156631, 0.015094457, 0.021819098, 0.067779474, 0.13763265, 0.1454701, 0.087632045, 0.12104144, 0.08845214, 0.1261049, 0.15303819, 0.08123653, 0.053653624, 0.037706226, 0.036721602, 0.033691805, 0.036724735, 0.05842094, 0.030317958, 0.033728093, 0.027304493, 0.027878234, 0.030830396, 0.04862242, 0.024325082, 0.023120169, 0.02918966, 0.022015734, 0.024350252, 0.0164007, 0.01870451, 0.015465368, 0.018691475, 0.019511508, 0.015882641, 0.022577064, 0.018985486, 0.0126991095, 0.014988761, 0.01849924, 0.021538747, 0.03894012, 0.08677067, 0.095787786, 0.057091136, 0.07083734, 0.079670705, 0.070056975, 0.041956216, 0.031463448, 0.053040497, 0.036020163, 0.039471153, 0.03107096, 0.057317108, 0.03238544, 0.0484816, 0.03276424, 0.024830207, 0.025229914, 0.026263213, 0.037328076, 0.02198242, 0.022446645, 0.026828082, 0.026528075, 0.026478752, 0.024292054, 0.027439741, 0.024415078, 0.020558119, 0.02778746, 0.020509064, 0.013053313, 0.018122174, 0.022376398, 0.023341771, 0.0148443645, 0.015612203, 0.012783496, 0.013758114, 0.0125089735, 0.017752334, 0.015082168, 0.012595872, 0.06337071, 0.12664615, 0.062315226, 0.08097316, 0.04516676, 0.06217375, 0.059236847, 0.057596877, 0.06456574, 0.051937822, 0.035272468, 0.03306201, 0.034948815, 0.04296652, 0.054204296, 0.032167293, 0.04638848, 0.033242997, 0.031249082, 0.019343877, 0.022912234, 0.014598531, 0.017872658, 0.019825686, 0.02610982, 0.014762791, 0.019409467, 0.016049983, 0.016654212, 0.015718918, 0.02041223, 0.026237773, 0.018794402, 0.032640904, 0.020719245, 0.016635373, 0.018134465, 0.018555582, 0.061724342, 0.09387611, 0.23908524, 0.117353804, 0.06660104, 0.0732397, 0.03324305, 0.09129343, 0.042379506, 0.07997416, 0.024154313, 0.042540867, 0.024940683, 0.021316232, 0.029157046, 0.02649723, 0.020274792, 0.02240056, 0.020497225, 0.02231867, 0.019931471, 0.025684688, 0.0323273, 0.021550713, 0.019961113, 0.018678376, 0.027531046, 0.022215888, 0.019676022, 0.015985178, 0.033054855, 0.019125583, 0.02267113, 0.014450727, 0.01638606, 0.022593254, 0.01591648, 0.015988488, 0.06909015, 0.18091135, 0.16551012, 0.1350486, 0.061775263, 0.039634973, 0.06008302, 0.059073493, 0.054684658, 0.040701684, 0.04642297, 0.032833006, 0.027829932, 0.06545513, 0.0456706, 0.034418926, 0.040970147, 0.045963217, 0.032174505, 0.033637106, 0.03246894, 0.027355436, 0.01842539, 0.021559095, 0.026042515, 0.025504231, 0.02314173, 0.019528037, 0.017059708, 0.013577381, 0.016719589, 0.019166993, 0.025710322, 0.0150435865, 0.0201094, 0.013752846, 0.021337984, 0.019521577, 0.05241093, 0.04600236, 0.091494635, 0.17205854, 0.1163682, 0.07473011, 0.040067837, 0.031613693, 0.0662433, 0.07633613, 0.08691778, 0.04868663, 0.028507626, 0.034969997, 0.03737943, 0.03175748, 0.034268808, 0.03680135, 0.035428688, 0.021925258, 0.022299318, 0.028343951, 0.01690425, 0.023699904, 0.03322662, 0.023693355, 0.014620763, 0.02056023, 0.018910266, 0.018048748, 0.01884611, 0.01612544, 0.018510863, 0.02277727, 0.014224603, 0.01744269, 0.021120861, 0.01565212, 0.0674518, 0.0972293, 0.12664458, 0.10662578, 0.08177124, 0.071496174, 0.051335327, 0.053417746, 0.06678517, 0.05241667, 0.065254875, 0.03615866, 0.029650748, 0.03291431, 0.035835076, 0.03062849, 0.04256792, 0.017949125, 0.016694471, 0.025882049, 0.01755417, 0.016712328, 0.021460732, 0.01905129, 0.01757043, 0.016787166, 0.024144096, 0.016521813, 0.014683396, 0.011774977, 0.016919142, 0.018777324, 0.020605111, 0.016671792, 0.019160885, 0.023335343, 0.014207107, 0.017468184, 0.018983979, 0.018913008, 0.023310298, 0.019786254, 0.017455967, 0.053352796, 0.085286595, 0.07507647, 0.23672302, 0.07964576, 0.11623423, 0.07844898, 0.040831596, 0.05961496, 0.05708692, 0.034557134, 0.034650125, 0.025918035, 0.054985937, 0.050493684, 0.025402928, 0.032328837, 0.023282738, 0.027992396, 0.027072724, 0.028309712, 0.04044714, 0.024782881, 0.01735503, 0.02265778, 0.020229692, 0.021072138, 0.021091565, 0.019744176, 0.020543016, 0.015312372, 0.020223673, 0.024084223, 0.023865107, 0.019921651, 0.021091597, 0.017907605, 0.023680942, 0.09993122, 0.08965276, 0.07807571, 0.07131336, 0.06589416, 0.11252966, 0.0437913, 0.04285868, 0.045228634, 0.029389437, 0.033748407, 0.031648945, 0.018819094, 0.032357603, 0.019993184, 0.027588345, 0.01733984, 0.025772281, 0.014038508, 0.023509707, 0.027277494, 0.025885636, 0.017343624, 0.020199208, 0.02542315, 0.015529711, 0.019674644, 0.017794348, 0.016986074, 0.02054726, 0.018077603, 0.017691595, 0.014928189, 0.013071304, 0.023758404, 0.02402587, 0.017014038, 0.024242694, 0.04138907, 0.15756068, 0.1058064, 0.1262985, 0.09597809, 0.0564741, 0.048030186, 0.0813115, 0.05476702, 0.05411219, 0.048934925, 0.07930241, 0.0625927, 0.0484928, 0.054144137, 0.029229041, 0.02057, 0.021073766, 0.025019458, 0.044601988, 0.02626592, 0.018778339, 0.019633746, 0.019355904, 0.020458356, 0.015182241, 0.023683157, 0.01727824, 0.02690711, 0.024112111, 0.01763619, 0.017072642, 0.017126981, 0.016252337, 0.013775103, 0.018138487, 0.015922725, 0.013040763, 0.0577859, 0.08535357, 0.10664964, 0.112804666, 0.05464717, 0.061373033, 0.053928364, 0.06461774, 0.07653263, 0.07571914, 0.074428506, 0.051602, 0.04492335, 0.032906443, 0.032411557, 0.03636415, 0.017741278, 0.01962367, 0.011320443, 0.027360493, 0.021688899, 0.017881092, 0.020875076, 0.0188063, 0.021050574, 0.023900518, 0.019758726, 0.014366304, 0.0162581, 0.0164163, 0.023836033, 0.022461044, 0.022183781, 0.02460292, 0.02157275, 0.021909049, 0.013070499, 0.01703478, 0.055924367, 0.084194735, 0.12764226, 0.06624948, 0.08258305, 0.075305566, 0.07886487, 0.09385852, 0.051281285, 0.07190937, 0.07883418, 0.045757636, 0.027359711, 0.02432481, 0.019161846, 0.02673102, 0.023467321, 0.037004188, 0.048918534, 0.018966593, 0.018548938, 0.020608062, 0.021576505, 0.021438396, 0.020061105, 0.02219094, 0.021647668, 0.015830005, 0.022547677, 0.02474118, 0.018607268, 0.021929534, 0.01693556, 0.024251813, 0.018281186, 0.013739263, 0.022874992, 0.023899289, 0.018160423, 0.017474972, 0.020002244, 0.016274417, 0.016777204, 0.019237136, 0.08134427, 0.1556883, 0.11673313, 0.11324431, 0.091477074, 0.046332873, 0.04490508, 0.06886669, 0.06577332, 0.042338435, 0.027376235, 0.055022605, 0.026858632, 0.059803586, 0.054616444, 0.038307346, 0.03594215, 0.024240518, 0.020923825, 0.024788106, 0.027193075, 0.023778897, 0.023975179, 0.027429745, 0.018599339, 0.02380569, 0.020700755, 0.036695, 0.014072801, 0.02524931, 0.022879217, 0.014669616, 0.021204734, 0.018735096, 0.014135778, 0.015344469, 0.015032081, 0.014323525, 0.07708863, 0.14099441, 0.13246526, 0.070978545, 0.057245824, 0.057681173, 0.04994389, 0.054570097, 0.039785396, 0.08207236, 0.03734673, 0.024906887, 0.022515958, 0.03926076, 0.032845568, 0.019217204, 0.021220757, 0.02196062, 0.020004129, 0.027215824, 0.020540483, 0.021755023, 0.024054816, 0.018881889, 0.01805599, 0.021617366, 0.021505004, 0.021161016, 0.014071085, 0.017362216, 0.012940509, 0.023995593, 0.016376426, 0.017738594, 0.012438834, 0.011054033, 0.018222284, 0.014908483, 0.06533939, 0.093685664, 0.08512813, 0.111951925, 0.06570674, 0.09279516, 0.07013095, 0.07681618, 0.042745717, 0.02707609, 0.0246911, 0.029361391, 0.029510012, 0.038608816, 0.03673284, 0.02927501, 0.017168725, 0.026475595, 0.026972648, 0.017475707, 0.02342738, 0.025396898, 0.01607215, 0.01754984, 0.024519835, 0.025603779, 0.023690624, 0.022572333, 0.021005865, 0.016389346, 0.019101521, 0.021458168, 0.025398757, 0.015561216, 0.013888605, 0.021784864, 0.019148694, 0.016092282, 0.045970317, 0.16904393, 0.20784557, 0.10653684, 0.08356217, 0.043942984, 0.050182443, 0.04949596, 0.08680994, 0.04567125, 0.060052633, 0.056043398, 0.038335476, 0.029163765, 0.019650493, 0.018635424, 0.024306454, 0.022967635, 0.028633699, 0.040182482, 0.030801622, 0.02512838, 0.022009518, 0.024041252, 0.01712603, 0.028424812, 0.01626656, 0.015481275, 0.022103585, 0.024032034, 0.025670847, 0.015075047, 0.019203505, 0.013685962, 0.011424934, 0.013442581, 0.017415294, 0.018944917, 0.047618013, 0.0402398, 0.063472815, 0.095091514, 0.11045346, 0.10679162, 0.03070703, 0.03790046, 0.053420205, 0.059682637, 0.059249062, 0.043341983, 0.06983802, 0.06457768, 0.07886453, 0.034858003, 0.027793432, 0.018282557, 0.021953002, 0.0424979, 0.021615243, 0.027204538, 0.024796182, 0.031147229, 0.026912633, 0.024920372, 0.020302469, 0.02250148, 0.015659511, 0.018309934, 0.021748329, 0.031432178, 0.030097237, 0.022532173, 0.02454711, 0.0146606285, 0.015325982, 0.024868824, 0.017572975, 0.020412318, 0.01689257, 0.018753046, 0.018982073, 0.072733924, 0.08561177, 0.13586469, 0.13480358, 0.10565863, 0.080424175, 0.0722259, 0.094941005, 0.05139651, 0.039319254, 0.027754815, 0.025802795, 0.02788358, 0.036618013, 0.03522147, 0.034412596, 0.02483699, 0.023507077, 0.022366263, 0.025292916, 0.017944023, 0.01834047, 0.016922755, 0.034517996, 0.023617243, 0.016613731, 0.01662355, 0.026453178, 0.01773022, 0.01943985, 0.01881235, 0.021150814, 0.024101956, 0.018042928, 0.017111301, 0.014802498, 0.021794759, 0.026724407, 0.02670162, 0.09095617, 0.12607855, 0.040925533, 0.065963514, 0.06558055, 0.06704618, 0.06404989, 0.041667216, 0.06890286, 0.054667685, 0.049921017, 0.0347367, 0.0774396, 0.028794019, 0.022946076, 0.02181563, 0.021897474, 0.019402448, 0.024607822, 0.022373073, 0.017583035, 0.013791235, 0.018455997, 0.028284792, 0.027119765, 0.020256551, 0.033951435, 0.020394985, 0.023886539, 0.014923207, 0.020015096, 0.014587404, 0.017668746, 0.014362645, 0.015604533, 0.017343936, 0.027612956, 0.06310134, 0.15066236, 0.036192227, 0.08235511, 0.12414441, 0.08148984, 0.048319805, 0.039061442, 0.026408557, 0.038687803, 0.024536764, 0.041884486, 0.031448007, 0.03109484, 0.028376698, 0.026218208, 0.027587712, 0.030623367, 0.021852735, 0.023105433, 0.015717003, 0.017511351, 0.025852198, 0.022152806, 0.017945057, 0.025024543, 0.014958294, 0.017570361, 0.022125984, 0.02152498, 0.013616402, 0.014704627, 0.015738597, 0.014697889, 0.013430614, 0.012783301, 0.013952829, 0.013383987, 0.08493754, 0.14996712, 0.0737074, 0.071607165, 0.09583268, 0.067408204, 0.0914135, 0.06820638, 0.056634396, 0.09747793, 0.051343806, 0.036971997, 0.029437166, 0.03182075, 0.029555766, 0.03145796, 0.025779957, 0.028286137, 0.027949894, 0.017731955, 0.022724653, 0.03112421, 0.02997078, 0.019589731, 0.024986101, 0.025634252, 0.026101597, 0.022893686, 0.017652, 0.02316812, 0.020670937, 0.02159757, 0.019598141, 0.016045263, 0.015449144, 0.019322569, 0.019371368, 0.015256458, 0.09760155, 0.15536535, 0.15723321, 0.15374176, 0.07262299, 0.042840157, 0.034827504, 0.082864866, 0.049483463, 0.04670031, 0.032563366, 0.036949616, 0.041552495, 0.043906305, 0.029680267, 0.030154742, 0.02751902, 0.019953592, 0.03186132, 0.030081764, 0.029034091, 0.019609055, 0.015623872, 0.023231873, 0.029590702, 0.017351571, 0.025157377, 0.027393287, 0.016261568, 0.017690621, 0.024346309, 0.024864126, 0.01610257, 0.017485892, 0.015698515, 0.0149318995, 0.016920961, 0.014999944, 0.018592656, 0.016445795, 0.016791716, 0.01516629, 0.021862328, 0.013328951, 0.05142907, 0.06203997, 0.053846728, 0.09638746, 0.031273495, 0.03829661, 0.039116133, 0.033626452, 0.03175969, 0.048092503, 0.028740818, 0.039249808, 0.0450557, 0.017577158, 0.036463648, 0.019520434, 0.015768142, 0.018141545, 0.022023508, 0.03269885, 0.031183137, 0.019786187, 0.019605879, 0.018718317, 0.014698006, 0.021769146, 0.016447678, 0.018746063, 0.01815171, 0.018460432, 0.018399302, 0.016792236, 0.0143125495, 0.022220144, 0.016583666, 0.017549453, 0.0119711505, 0.018249169, 0.076230906, 0.15762256, 0.09868842, 0.22340576, 0.12483175, 0.0773776, 0.03510177, 0.055327855, 0.052719586, 0.048916128, 0.024782445, 0.04804173, 0.041784156, 0.043011107, 0.025879042, 0.027163832, 0.017711228, 0.032493822, 0.025327548, 0.0203309, 0.037847802, 0.029222718, 0.026013402, 0.021386774, 0.014420939, 0.019849125, 0.023912188, 0.017024124, 0.023722515, 0.014188235, 0.020308092, 0.013184872, 0.019324323, 0.01856704, 0.015553073, 0.021936644, 0.02848723, 0.014160865, 0.18437463, 0.24910392, 0.07420043, 0.31808206, 0.0849656, 0.039435476, 0.075410195, 0.16951811, 0.085077025, 0.030771151, 0.02670044, 0.027450081, 0.032620404, 0.03376565, 0.03081461, 0.03293855, 0.037830126, 0.03709659, 0.021061683, 0.021942697, 0.024687197, 0.03171734, 0.0134652825, 0.018684957, 0.01843859, 0.02123559, 0.022010138, 0.018964043, 0.023726666, 0.027416075, 0.022222895, 0.02068147, 0.021129334, 0.025687734, 0.019133054, 0.02435471, 0.020118114, 0.01961263, 0.039872445, 0.07989602, 0.09150627, 0.13512588, 0.05104984, 0.086541675, 0.037120778, 0.057225376, 0.03503483, 0.06673156, 0.041160405, 0.03858019, 0.04311929, 0.037997015, 0.030016074, 0.03594718, 0.024103902, 0.022540215, 0.031157566, 0.016801113, 0.021272745, 0.022787761, 0.021135485, 0.019014774, 0.01486024, 0.02862891, 0.014430065, 0.02333159, 0.017462345, 0.018963551, 0.025652142, 0.013017573, 0.02526168, 0.020723157, 0.015629629, 0.021869525, 0.023810074, 0.013131532, 0.15173689, 0.30271426, 0.13459381, 0.145963, 0.05048445, 0.0556187, 0.114566624, 0.0529485, 0.035577487, 0.052039195, 0.030604495, 0.04674223, 0.044223573, 0.048898585, 0.051143546, 0.024807965, 0.023015672, 0.01940918, 0.024139758, 0.019593237, 0.025036748, 0.024003582, 0.019656189, 0.020921644, 0.021653589, 0.020845642, 0.030057186, 0.02446403, 0.021080231, 0.028241983, 0.017009655, 0.030656379, 0.015987517, 0.019934082, 0.026817238, 0.01601098, 0.025525019, 0.018368818, 0.019498695, 0.024339266, 0.016282968, 0.01539803, 0.017149102, 0.08489502, 0.15926707, 0.08826555, 0.12521844, 0.12654084, 0.12803116, 0.08493508, 0.028908024, 0.041720476, 0.039816964, 0.064512424, 0.033118337, 0.023005554, 0.02437806, 0.03016785, 0.03795788, 0.020512205, 0.029565284, 0.029592887, 0.033362158, 0.017975911, 0.020272052, 0.0197248, 0.021239465, 0.017062854, 0.021758411, 0.019021286, 0.033012137, 0.026971538, 0.017559597, 0.01969548, 0.020319648, 0.016453601, 0.015414623, 0.02413053, 0.018422123, 0.016248837, 0.021676397, 0.04638869, 0.13362555, 0.09777053, 0.09920638, 0.097388186, 0.102859356, 0.07826859, 0.06110849, 0.09276674, 0.0412568, 0.027469322, 0.03349511, 0.042858534, 0.026365684, 0.041062042, 0.03581006, 0.024636973, 0.028441554, 0.026067013, 0.024922436, 0.021001177, 0.01892117, 0.019723957, 0.019983685, 0.01919631, 0.02194063, 0.016949655, 0.017166303, 0.019194916, 0.020090923, 0.025932407, 0.01698284, 0.015972266, 0.01999991, 0.018768366, 0.025509907, 0.013827191, 0.016142989, 0.022304228, 0.05319638, 0.08856724, 0.093702175, 0.16658525, 0.06045038, 0.06368468, 0.050971903, 0.04411143, 0.030773286, 0.034724314, 0.02308814, 0.013755441, 0.035638027, 0.028317217, 0.020660765, 0.021167332, 0.026312146, 0.02002449, 0.029894402, 0.035226043, 0.02849634, 0.023716792, 0.026217267, 0.02748908, 0.03214212, 0.03202764, 0.023706887, 0.022680469, 0.015242928, 0.017566493, 0.018852847, 0.022057617, 0.017826492, 0.023629984, 0.019467305, 0.017244572, 0.010660578, 0.08149108, 0.21692316, 0.1661255, 0.16427673, 0.088676155, 0.059784744, 0.061169535, 0.041852128, 0.048360113, 0.030018916, 0.03898977, 0.06160449, 0.034273554, 0.06611301, 0.025427466, 0.021976316, 0.017360335, 0.030696517, 0.021799788, 0.018718768, 0.027541606, 0.02601281, 0.026684385, 0.020882148, 0.021472251, 0.020329265, 0.014345252, 0.02025116, 0.015319675, 0.019693688, 0.012083498, 0.023079801, 0.01806104, 0.020412317, 0.012160159, 0.017342137, 0.018389393, 0.022795126, 0.050310582, 0.09608334, 0.13387968, 0.0823232, 0.0767176, 0.14442837, 0.066365354, 0.04655211, 0.056080654, 0.03670057, 0.037206765, 0.03242032, 0.021000072, 0.03209753, 0.038578022, 0.028685078, 0.029153984, 0.025353445, 0.020816343, 0.025986845, 0.022096366, 0.027063543, 0.024324927, 0.022732312, 0.017662626, 0.016199922, 0.018854005, 0.01403, 0.021305392, 0.023594653, 0.01528592, 0.014573313, 0.019375207, 0.015300084, 0.020290302, 0.015033071, 0.016837826, 0.024783375, 0.016176227, 0.021601414, 0.020171786, 0.013688268, 0.014910974, 0.014810193, 0.04825293, 0.10098558, 0.10261493, 0.064838156, 0.06211223, 0.05290654, 0.059587266, 0.044313513, 0.042863168, 0.06617636, 0.029571766, 0.030273836, 0.025506957, 0.026163414, 0.026586363, 0.016591305, 0.017259862, 0.021686349, 0.020087436, 0.020881891, 0.02433187, 0.025103236, 0.022874374, 0.026302801, 0.017820256, 0.016159488, 0.029054742, 0.024086475, 0.019184498, 0.01201439, 0.019207126, 0.0208623, 0.015480612, 0.014960564, 0.0150685245, 0.01790878, 0.022540804, 0.014426417, 0.10967693, 0.13234413, 0.08902456, 0.11183647, 0.06335115, 0.05805396, 0.04132807, 0.07381463, 0.055131245, 0.05484989, 0.040641107, 0.05291959, 0.039351817, 0.032323707, 0.023225741, 0.032581765, 0.028463904, 0.023711935, 0.016712548, 0.01902775, 0.01575954, 0.019199764, 0.018665032, 0.02025587, 0.025099646, 0.026735416, 0.017076686, 0.016463192, 0.021679536, 0.022045309, 0.017374182, 0.02762104, 0.020376457, 0.015284162, 0.020819431, 0.016342029, 0.017634455, 0.019356564, 0.091756836, 0.05140647, 0.06782486, 0.082851626, 0.07914633, 0.055938978, 0.05538028, 0.050307482, 0.030346124, 0.034515392, 0.038302124, 0.034125313, 0.026780091, 0.07385814, 0.05323776, 0.04711973, 0.02443052, 0.02739906, 0.038228165, 0.024296787, 0.028962389, 0.031542126, 0.02213208, 0.026041351, 0.01843003, 0.021305963, 0.015809089, 0.017865544, 0.014941764, 0.015366788, 0.013944723, 0.028753698, 0.013098259, 0.01492374, 0.016607478, 0.014366168, 0.018426588, 0.016877659, 0.047310997, 0.08774367, 0.09346157, 0.12217773, 0.038588353, 0.05206961, 0.07309765, 0.051061217, 0.038986757, 0.05110066, 0.07462911, 0.046266567, 0.03754612, 0.0472943, 0.02672021, 0.029644668, 0.015563877, 0.028129216, 0.022195786, 0.018219177, 0.02883217, 0.024857659, 0.023726756, 0.012486507, 0.017908147, 0.014781094, 0.023317222, 0.018384479, 0.017365133, 0.020734154, 0.015533847, 0.024485167, 0.017502567, 0.018991038, 0.016590405, 0.025538046, 0.018339163, 0.01895462, 0.05773272, 0.079821765, 0.0506252, 0.056515265, 0.046571188, 0.054613784, 0.06693524, 0.029407227, 0.05824864, 0.062311612, 0.033651534, 0.020288266, 0.02683751, 0.019387672, 0.035569396, 0.01974019, 0.029855952, 0.028456124, 0.019605746, 0.034823574, 0.02234761, 0.01591814, 0.015532933, 0.014738403, 0.015972333, 0.016026426, 0.02648945, 0.018865356, 0.017582692, 0.015333389, 0.018904474, 0.018256096, 0.01740308, 0.015711794, 0.019600257, 0.018625235, 0.011583305, 0.016212238, 0.1821771, 0.12522125, 0.14229354, 0.14774564, 0.12227243, 0.091857955, 0.08382005, 0.040622227, 0.040160093, 0.0686426, 0.029897548, 0.036152747, 0.035664987, 0.022075482, 0.032359023, 0.024752, 0.019191917, 0.018551154, 0.023031138, 0.020181045, 0.018405613, 0.028954245, 0.025476916, 0.021636559, 0.028513739, 0.025344126, 0.016726727, 0.02324006, 0.029794509, 0.018535618, 0.018266786, 0.019157764, 0.015492387, 0.0179814, 0.019862894, 0.013448422, 0.013334908, 0.01332794, 0.07997802, 0.108130455, 0.13699234, 0.11048438, 0.03748406, 0.039850555, 0.042837486, 0.030788358, 0.03125054, 0.0492257, 0.06085713, 0.037282024, 0.04739327, 0.034596592, 0.046590254, 0.025282487, 0.016546354, 0.01805744, 0.024446154, 0.020021802, 0.020711383, 0.019567266, 0.015085051, 0.021868581, 0.019430911, 0.01970337, 0.013592892, 0.013982917, 0.02085966, 0.012949559, 0.025685137, 0.014268456, 0.022023873, 0.013574103, 0.018432364, 0.030372322, 0.014138942, 0.022537028, 0.08783219, 0.13138027, 0.11793828, 0.06458306, 0.07499948, 0.04954535, 0.13146959, 0.09476601, 0.054681715, 0.04149743, 0.033915583, 0.027933454, 0.030444756, 0.015295574, 0.02793095, 0.030471683, 0.02493755, 0.031194605, 0.028600382, 0.024541572, 0.02814649, 0.01610057, 0.023885835, 0.017584935, 0.017145846, 0.022507096, 0.014358855, 0.023437992, 0.021506716, 0.01429412, 0.019842923, 0.014302683, 0.015549969, 0.012184555, 0.02296377, 0.022686265, 0.015743082, 0.018261336, 0.079365194, 0.16395098, 0.14885867, 0.18059717, 0.06796434, 0.059708357, 0.084591135, 0.04474047, 0.038395554, 0.043568566, 0.03177737, 0.058492154, 0.026479451, 0.019245293, 0.03777056, 0.024878388, 0.020312443, 0.025279725, 0.026479872, 0.02361182, 0.021597937, 0.022020364, 0.023144038, 0.014667529, 0.021195607, 0.019491568, 0.015852528, 0.015054178, 0.02354837, 0.024223877, 0.020209981, 0.0138506275, 0.024001366, 0.03065069, 0.012264065, 0.020834453, 0.018669913, 0.016871868, 0.044635493, 0.19778657, 0.12201137, 0.059008073, 0.04726411, 0.05125476, 0.032021996, 0.037762564, 0.042493585, 0.06322992, 0.067613915, 0.07929063, 0.043290295, 0.035292737, 0.03507609, 0.036062073, 0.019155815, 0.03447688, 0.029962774, 0.026353927, 0.023230905, 0.021407513, 0.024725873, 0.022422655, 0.01796875, 0.015385215, 0.016295463, 0.016767524, 0.021042472, 0.016298683, 0.026795281, 0.019897025, 0.020435447, 0.014347616, 0.01583528, 0.02294229, 0.018237581, 0.014540845, 0.017630361, 0.017274767, 0.016265482, 0.014859472, 0.016885584, 0.039105717, 0.057224248, 0.093728565, 0.09874881, 0.08463918, 0.10792694, 0.10123601, 0.06684481, 0.026015516, 0.033790715, 0.03129883, 0.0411903, 0.05218918, 0.023122907, 0.030457271, 0.07215662, 0.02409562, 0.03229628, 0.020546045, 0.025661817, 0.02297295, 0.029285783, 0.028163537, 0.034850907, 0.02508712, 0.017721644, 0.02569597, 0.013017766, 0.030055488, 0.015246314, 0.020829555, 0.016152278, 0.018993892, 0.02545472, 0.018539255, 0.030706143, 0.01239081, 0.019447291, 0.08756569, 0.06989481, 0.11299659, 0.14829391, 0.061016947, 0.047090244, 0.038421113, 0.057394248, 0.07377599, 0.03321395, 0.043290954, 0.05036669, 0.027931295, 0.029474039, 0.027380401, 0.02536707, 0.018662987, 0.023070036, 0.031523436, 0.0234618, 0.032459926, 0.019901384, 0.017499104, 0.016687639, 0.012817678, 0.019688811, 0.023956666, 0.024760898, 0.0240524, 0.023094624, 0.011062404, 0.01933294, 0.018137457, 0.019829525, 0.02522063, 0.013224781, 0.016999131, 0.022226978, 0.045615196, 0.07360599, 0.07267146, 0.06095855, 0.08360858, 0.05994517, 0.0545103, 0.048971545, 0.044579715, 0.049772613, 0.04176864, 0.049785566, 0.031166438, 0.049873136, 0.035109673, 0.027615868, 0.024170578, 0.020300746, 0.018758046, 0.014852535, 0.02626655, 0.023090657, 0.01545198, 0.018891165, 0.01613202, 0.021509275, 0.016032314, 0.019526837, 0.023382222, 0.018309826, 0.014631576, 0.024283526, 0.014351426, 0.018376665, 0.022341266, 0.017863873, 0.017060183, 0.016805202, 0.054477144, 0.08932656, 0.08185252, 0.101534426, 0.086346455, 0.051994048, 0.0378413, 0.0438485, 0.038415432, 0.05058511, 0.030082304, 0.02769533, 0.021896288, 0.02741334, 0.01862734, 0.022353372, 0.025450611, 0.02570768, 0.026524695, 0.02396887, 0.030870002, 0.027310265, 0.016637335, 0.019920994, 0.016638955, 0.019514034, 0.018869428, 0.020851726, 0.018720066, 0.02284114, 0.0156069575, 0.017775517, 0.014644463, 0.023713343, 0.010107947, 0.022077423, 0.014223739, 0.01930976, 0.08132252, 0.17583805, 0.07244928, 0.07827155, 0.12805815, 0.06347553, 0.059451398, 0.09036981, 0.10195118, 0.08222419, 0.034165554, 0.030539563, 0.021273717, 0.0332565, 0.0368517, 0.03893708, 0.023678748, 0.017601307, 0.021268666, 0.020537054, 0.013843692, 0.018955583, 0.019918468, 0.015796855, 0.02541158, 0.028469246, 0.016279373, 0.020299634, 0.021138847, 0.018420821, 0.020114359, 0.012976078, 0.015712297, 0.013293869, 0.015554229, 0.021219002, 0.022479955, 0.015919503, 0.013243834, 0.017525526, 0.014531303, 0.01728573, 0.014000166, 0.012456318, 0.03472757, 0.07807762, 0.046560016, 0.060443155, 0.067104146, 0.021488622, 0.03332387, 0.037765764, 0.07677443, 0.03439986, 0.025404472, 0.032515854, 0.043279063, 0.028718764, 0.029058442, 0.033428095, 0.032224867, 0.023830656, 0.03686296, 0.039112378, 0.023340968, 0.022580473, 0.016668169, 0.020384403, 0.014604731, 0.027385674, 0.015370823, 0.018286178, 0.01800679, 0.0146769285, 0.016653523, 0.019314025, 0.011877192, 0.013971102, 0.0166284, 0.015189001, 0.013777179, 0.014034679, 0.05534302, 0.09542271, 0.11867749, 0.046877753, 0.06729535, 0.13666171, 0.080257066, 0.0647491, 0.056933634, 0.05397732, 0.07704616, 0.047636583, 0.035751123, 0.03398085, 0.028187595, 0.042625133, 0.0198816, 0.028770365, 0.02568541, 0.02561506, 0.02595515, 0.032107614, 0.03329399, 0.018402757, 0.018754179, 0.019759873, 0.020564178, 0.017634733, 0.017518844, 0.023437412, 0.022284895, 0.021623548, 0.015420198, 0.019118778, 0.015752576, 0.014417401, 0.0235907, 0.013159598, 0.06746821, 0.13840497, 0.14922433, 0.11591266, 0.052601397, 0.09195026, 0.07001899, 0.060478963, 0.053657897, 0.05667383, 0.027140187, 0.05025686, 0.030322548, 0.027190877, 0.03851359, 0.034619395, 0.031150768, 0.039039835, 0.02423663, 0.029340869, 0.021411441, 0.027270174, 0.021069393, 0.020758048, 0.022326587, 0.02949948, 0.014987072, 0.023085032, 0.012967125, 0.019176736, 0.02009691, 0.016737217, 0.019822074, 0.018612456, 0.01758262, 0.016099349, 0.016133849, 0.01695525, 0.10822533, 0.124171786, 0.12233081, 0.13251488, 0.06264508, 0.06273295, 0.089592785, 0.049889017, 0.063926585, 0.073435165, 0.049425475, 0.044598438, 0.03343266, 0.027340466, 0.030198533, 0.021912074, 0.024834903, 0.024479771, 0.025397703, 0.02845532, 0.025117539, 0.015183032, 0.022091871, 0.013623145, 0.028736468, 0.026043875, 0.015011877, 0.020050136, 0.024868134, 0.01875201, 0.020594189, 0.014578686, 0.013057381, 0.018476948, 0.017730063, 0.018671086, 0.013464537, 0.022386165, 0.054451905, 0.08791738, 0.11566281, 0.054332703, 0.058534734, 0.08597171, 0.048193015, 0.06571355, 0.06407596, 0.055005588, 0.029019292, 0.037384003, 0.05219162, 0.021389896, 0.015139, 0.015757756, 0.028833421, 0.02476462, 0.029813403, 0.018790884, 0.034512695, 0.017611833, 0.036376357, 0.024606887, 0.021191996, 0.017598959, 0.024211137, 0.025980106, 0.025313145, 0.022485135, 0.021072458, 0.02462289, 0.018853081, 0.029940464, 0.012878996, 0.014183462, 0.016604358, 0.021633001, 0.015491791, 0.019971326, 0.018977422, 0.020054996, 0.014752489, 0.049878657, 0.07318027, 0.057766683, 0.123702176, 0.11083032, 0.04507227, 0.047368947, 0.09933142, 0.05256244, 0.056936953, 0.02811784, 0.03348894, 0.021654421, 0.019381858, 0.026013423, 0.021228261, 0.021894423, 0.027274722, 0.015076355, 0.026228163, 0.018843187, 0.015013206, 0.022964073, 0.0191824, 0.021120584, 0.023855025, 0.019825947, 0.013409353, 0.009257758, 0.023733003, 0.011602404, 0.014472436, 0.016920054, 0.013582014, 0.017690469, 0.012235249, 0.013634828, 0.01686511, 0.023679094, 0.049396347, 0.05738605, 0.053459812, 0.14581881, 0.043794483, 0.052593026, 0.05533827, 0.10083828, 0.11709212, 0.041565057, 0.044856843, 0.049800914, 0.05273486, 0.030803166, 0.01786919, 0.019455062, 0.023711218, 0.027115408, 0.022436308, 0.020075575, 0.015776385, 0.023471136, 0.022322154, 0.02973777, 0.042130083, 0.01904825, 0.014958292, 0.019545702, 0.014047872, 0.030274382, 0.020923326, 0.017449258, 0.015728742, 0.016972894, 0.025991794, 0.014759206, 0.017850185, 0.07784046, 0.067725345, 0.07329693, 0.04660946, 0.08630393, 0.030740242, 0.05435995, 0.039012197, 0.04357134, 0.0820013, 0.028605346, 0.023971539, 0.024620792, 0.04709189, 0.029483415, 0.037342835, 0.02198049, 0.020569121, 0.031655777, 0.017686624, 0.029129885, 0.024595356, 0.011503865, 0.020999443, 0.017809836, 0.017192189, 0.014930556, 0.017412446, 0.019993898, 0.016305814, 0.020369444, 0.01905865, 0.01556322, 0.018710429, 0.017650682, 0.012348252, 0.015562177, 0.015541074, 0.052196454, 0.20281991, 0.09611923, 0.18586896, 0.17176086, 0.06653896, 0.048472546, 0.08541253, 0.0370766, 0.03767798, 0.038835373, 0.02698935, 0.021219032, 0.02813119, 0.03553248, 0.02132722, 0.02711693, 0.022460254, 0.022350669, 0.026849862, 0.038075835, 0.028963594, 0.03568952, 0.016287256, 0.01578713, 0.021262508, 0.013957391, 0.013732433, 0.017097453, 0.01838004, 0.013360306, 0.017989963, 0.028797608, 0.020620352, 0.01996536, 0.015600695, 0.012210016, 0.012019192, 0.07210005, 0.11294145, 0.1378427, 0.11904518, 0.08182215, 0.047080874, 0.119959004, 0.040393718, 0.041240428, 0.06200994, 0.031025592, 0.016959175, 0.033446543, 0.02275462, 0.04142492, 0.017782737, 0.026097806, 0.019297922, 0.019689485, 0.026906503, 0.027638664, 0.021222165, 0.017113255, 0.025043035, 0.021036651, 0.025342446, 0.030606586, 0.023001218, 0.01735892, 0.01571693, 0.024454696, 0.018366115, 0.019106314, 0.015057962, 0.02223722, 0.016532406, 0.023924151, 0.013566322, 0.014413893, 0.020171748, 0.017438062, 0.012769157, 0.0139398575, 0.018020933, 0.044082507, 0.053194717, 0.11926359, 0.112537295, 0.048142552, 0.06631669, 0.05584794, 0.05739233, 0.038523354, 0.043606, 0.022852834, 0.029687589, 0.0399056, 0.041210447, 0.03635535, 0.021383557, 0.022232056, 0.024681391, 0.038799006, 0.022281924, 0.028818814, 0.021831686, 0.024205834, 0.011306118, 0.012634372, 0.013514996, 0.009374681, 0.015392785, 0.018407786, 0.02450739, 0.017082885, 0.01832451, 0.021923555, 0.019354753, 0.017886737, 0.016673895, 0.026048109, 0.013607525, 0.062388532, 0.08815266, 0.073262446, 0.123133205, 0.15461206, 0.04769029, 0.034929868, 0.02963036, 0.03819315, 0.07051158, 0.024963267, 0.020962179, 0.017367285, 0.061768632, 0.028976921, 0.02117705, 0.014941501, 0.024296807, 0.02351997, 0.022740645, 0.034232967, 0.026053427, 0.033047054, 0.02959737, 0.016930595, 0.025045957, 0.017892698, 0.01405986, 0.014340194, 0.012752679, 0.019469183, 0.013483934, 0.012472576, 0.019241944, 0.015769228, 0.0154927485, 0.015263103, 0.019744232, 0.057288602, 0.10840372, 0.083649315, 0.11755887, 0.07138219, 0.053570427, 0.058958724, 0.045637857, 0.07171992, 0.038542572, 0.0370507, 0.04103742, 0.048626047, 0.02721575, 0.036564577, 0.052276284, 0.0215413, 0.035324167, 0.020713665, 0.02929551, 0.023021847, 0.0164259, 0.02358843, 0.014798919, 0.020807719, 0.014072489, 0.016092189, 0.013828378, 0.018273987, 0.020503646, 0.032022268, 0.0131898895, 0.020724962, 0.015196454, 0.015099414, 0.01731307, 0.018674418, 0.0177986, 0.029795295, 0.08519816, 0.09682508, 0.06061576, 0.061896563, 0.040162634, 0.041334882, 0.027437367, 0.019070765, 0.02620831, 0.034087695, 0.027447475, 0.020292819, 0.036402445, 0.016866717, 0.022495115, 0.022413038, 0.01464706, 0.035186734, 0.030973779, 0.01724795, 0.016015982, 0.015546932, 0.019400332, 0.019813778, 0.011864769, 0.0204757, 0.020956589, 0.014040506, 0.013493941, 0.022323765, 0.02105662, 0.01505043, 0.02340343, 0.018335992, 0.018728122, 0.017050566, 0.015924467, 0.033279076, 0.10733395, 0.07262497, 0.10080803, 0.08525645, 0.12060708, 0.04644258, 0.035460424, 0.041799746, 0.07096329, 0.03756734, 0.060580354, 0.029654726, 0.029228067, 0.018297559, 0.031254303, 0.029085146, 0.026753765, 0.016185615, 0.027826255, 0.02330394, 0.030571796, 0.02254305, 0.023048602, 0.019776922, 0.036884397, 0.026151707, 0.012537917, 0.012258623, 0.020208435, 0.024610933, 0.018495742, 0.020328365, 0.014722567, 0.013478798, 0.02393856, 0.018347, 0.024022814, 0.012695184, 0.016177898, 0.012801418, 0.01762498, 0.020438265, 0.1380752, 0.11723477, 0.14093196, 0.0935716, 0.08435688, 0.03667011, 0.045511078, 0.044542585, 0.07063437, 0.05376187, 0.04211993, 0.03139917, 0.025742466, 0.03267146, 0.032383867, 0.034313817, 0.029813726, 0.022194441, 0.021465214, 0.016337046, 0.026659636, 0.021626208, 0.013899108, 0.0143901985, 0.017806109, 0.017524047, 0.013047502, 0.017493114, 0.02191178, 0.023629533, 0.020412818, 0.013424379, 0.01340798, 0.014619592, 0.017540747, 0.015194076, 0.009987005, 0.017210627, 0.051646993, 0.06654203, 0.13093196, 0.1533267, 0.16377623, 0.08142579, 0.037277106, 0.0601722, 0.06323431, 0.06167365, 0.07619383, 0.031089718, 0.04006426, 0.040693827, 0.044516947, 0.03619654, 0.030877782, 0.022258515, 0.029270763, 0.024163991, 0.024627766, 0.0119973235, 0.017628225, 0.017541056, 0.022495903, 0.033337656, 0.02051458, 0.020631945, 0.014532886, 0.009604561, 0.0152051225, 0.018167414, 0.018044215, 0.023419363, 0.017714167, 0.026455892, 0.013058032, 0.014345318, 0.029383728, 0.11839977, 0.08497133, 0.09348868, 0.08830343, 0.06126271, 0.039103787, 0.05192337, 0.05061599, 0.040388543, 0.02606892, 0.029796202, 0.026950927, 0.030888585, 0.06108476, 0.047016133, 0.021217646, 0.030726144, 0.02451574, 0.024888868, 0.0226103, 0.025601845, 0.019044947, 0.015308626, 0.01337178, 0.020369453, 0.019751484, 0.011857584, 0.01586429, 0.016687253, 0.017490013, 0.017275536, 0.014003473, 0.018484684, 0.01791501, 0.012666239, 0.012807404, 0.014313395, 0.049306173, 0.075259626, 0.17114961, 0.13994662, 0.10628941, 0.079103775, 0.036896978, 0.061651085, 0.06499157, 0.09794961, 0.07514105, 0.039563563, 0.036802135, 0.023233173, 0.024607077, 0.030066477, 0.022899523, 0.020787397, 0.021215344, 0.027009452, 0.013795257, 0.023863824, 0.017005537, 0.013029073, 0.014793738, 0.015675938, 0.020150773, 0.010123479, 0.015810126, 0.018342253, 0.012768849, 0.013105047, 0.016678339, 0.015528795, 0.014758537, 0.01632441, 0.025349772, 0.020667419, 0.052441586, 0.056403913, 0.06656232, 0.06480421, 0.07343075, 0.046162475, 0.042042296, 0.031392016, 0.040157825, 0.03584722, 0.036726963, 0.031453095, 0.034533884, 0.03441013, 0.02107776, 0.031756856, 0.021382343, 0.022567376, 0.035729267, 0.035369802, 0.03640341, 0.025565509, 0.022805855, 0.019394116, 0.021018036, 0.018486151, 0.019410541, 0.02321588, 0.017908936, 0.016848518, 0.015675794, 0.014782151, 0.015578806, 0.01777507, 0.014635361, 0.017508524, 0.018630778, 0.01919155, 0.0115499245, 0.020749522, 0.016839255, 0.01568758, 0.014040194, 0.017531846, 0.060744535, 0.1436432, 0.15938048, 0.08610139, 0.092570014, 0.08678637, 0.038677365, 0.03678323, 0.05102383, 0.048118103, 0.030853534, 0.017830463, 0.031786222, 0.026787218, 0.028327227, 0.024694106, 0.019720087, 0.02397128, 0.023759395, 0.017498465, 0.022508338, 0.028559731, 0.020280277, 0.019167924, 0.017331036, 0.0146747595, 0.018198235, 0.016918521, 0.016688997, 0.01223474, 0.01142406, 0.019004753, 0.016782483, 0.016207622, 0.014700684, 0.01672221, 0.019941851, 0.011841932, 0.046462424, 0.052627694, 0.050125018, 0.049756516, 0.03907958, 0.025598627, 0.024858085, 0.057989705, 0.041663457, 0.039543122, 0.03257061, 0.025685273, 0.030173237, 0.026859747, 0.031142853, 0.022643043, 0.018091213, 0.024862373, 0.038675725, 0.023450887, 0.015546225, 0.031555478, 0.025815826, 0.015889846, 0.016626574, 0.016041502, 0.016633898, 0.015236114, 0.013170311, 0.016886497, 0.017801516, 0.033104602, 0.016605532, 0.0125903245, 0.017446782, 0.012278437, 0.017470106, 0.014730342, 0.060004704, 0.06730111, 0.065938056, 0.082968414, 0.106416576, 0.060118787, 0.04414134, 0.049332242, 0.06589184, 0.035327066, 0.020930162, 0.016538857, 0.021784756, 0.021178192, 0.018432941, 0.01563333, 0.03427285, 0.026472885, 0.025393283, 0.033305738, 0.030489702, 0.020359507, 0.02184909, 0.020061469, 0.026415179, 0.030108053, 0.01755982, 0.014070076, 0.012091317, 0.016862249, 0.018079799, 0.017766368, 0.01924745, 0.012262714, 0.015707666, 0.013739306, 0.013248268, 0.01700057, 0.096735895, 0.14229877, 0.11731021, 0.14684756, 0.045493454, 0.051400453, 0.044999227, 0.022518564, 0.05048318, 0.039879464, 0.027107744, 0.033465825, 0.052410256, 0.041681923, 0.035434652, 0.029845642, 0.030529458, 0.018147573, 0.033370607, 0.044367798, 0.02360871, 0.016396293, 0.016816096, 0.016369835, 0.013402316, 0.02135323, 0.017014815, 0.014741723, 0.014597824, 0.022820257, 0.01962254, 0.017731793, 0.017577207, 0.01889993, 0.011517254, 0.016228097, 0.015712881, 0.020275403, 0.033297513, 0.13001493, 0.08804511, 0.13652156, 0.083996005, 0.084737, 0.06615034, 0.04634143, 0.018898763, 0.03323533, 0.050312117, 0.024362624, 0.023116909, 0.025608849, 0.03191448, 0.044150192, 0.023139823, 0.023933489, 0.021902427, 0.020748656, 0.018121205, 0.017018294, 0.02710486, 0.0143768005, 0.0142156305, 0.021777794, 0.020669768, 0.019427406, 0.01319564, 0.016231334, 0.017443918, 0.014071872, 0.012516769, 0.016774004, 0.011842919, 0.016819766, 0.017796412, 0.024770612, 0.017792895, 0.013615794, 0.016921956, 0.01283461, 0.017496869, 0.030749513, 0.090216324, 0.063457906, 0.11840898, 0.10403929, 0.07622722, 0.06195051, 0.045869492, 0.064114526, 0.05636431, 0.046232138, 0.034899384, 0.027147193, 0.01947523, 0.027932212, 0.044537246, 0.022499936, 0.018610515, 0.020488208, 0.022911524, 0.04207346, 0.022588259, 0.020385092, 0.020803144, 0.021231819, 0.023355806, 0.017473195, 0.015317249, 0.019849962, 0.013669798, 0.023371596, 0.017173914, 0.02152626, 0.015654549, 0.013414711, 0.017627766, 0.017385475, 0.025617346, 0.07841802, 0.17685488, 0.057359256, 0.14232635, 0.11695055, 0.07570869, 0.050005946, 0.058521766, 0.06278186, 0.057063717, 0.036302455, 0.027050907, 0.048417326, 0.045872632, 0.0330729, 0.024532145, 0.021237724, 0.025418598, 0.020583333, 0.019441612, 0.03656461, 0.018344898, 0.01815341, 0.019025141, 0.010811278, 0.018992994, 0.018032357, 0.01950293, 0.013170895, 0.015075445, 0.023846053, 0.02321446, 0.017942006, 0.014029219, 0.01634356, 0.016352836, 0.013186963, 0.01978524, 0.07322557, 0.07553909, 0.085431874, 0.11491691, 0.047820665, 0.050115235, 0.03972826, 0.033069324, 0.08545942, 0.040896207, 0.028321937, 0.01987448, 0.02784592, 0.014502362, 0.026373094, 0.020906132, 0.014790946, 0.019770531, 0.03304378, 0.024946978, 0.029846812, 0.019515332, 0.021509128, 0.022839988, 0.015719352, 0.014605957, 0.018774062, 0.023547947, 0.018219193, 0.012891638, 0.012116149, 0.013616043, 0.014569764, 0.02068507, 0.016268417, 0.013372006, 0.022380456, 0.016531471, 0.07406564, 0.1182578, 0.07273387, 0.17259641, 0.16351481, 0.06256059, 0.038845383, 0.036631692, 0.043829653, 0.07281839, 0.04232216, 0.046782184, 0.041752644, 0.04855547, 0.021735976, 0.03892822, 0.03894194, 0.03455174, 0.019176148, 0.02496433, 0.01948219, 0.024488628, 0.017941928, 0.015154037, 0.023808155, 0.016902968, 0.033153374, 0.016625846, 0.01212649, 0.016883729, 0.018175896, 0.0137587115, 0.019233894, 0.01579415, 0.017677596, 0.016628493, 0.015024252, 0.015961174, 0.026902849, 0.07687563, 0.11949426, 0.10924196, 0.05676506, 0.06391274, 0.057367444, 0.07111045, 0.060374618, 0.07615851, 0.086721726, 0.035225317, 0.03145801, 0.03065129, 0.021934154, 0.023130568, 0.023456551, 0.0154278185, 0.016193377, 0.023122335, 0.022751525, 0.019313568, 0.017264584, 0.014816021, 0.01623924, 0.01953884, 0.022138646, 0.017724896, 0.026271466, 0.017301649, 0.023332978, 0.013950678, 0.02564788, 0.0140710315, 0.0129914805, 0.015579233, 0.021788845, 0.020163128, 0.0136028575, 0.014614197, 0.017394407, 0.016196817, 0.016919555, 0.01566817, 0.06501748, 0.19197135, 0.09673824, 0.08533509, 0.09072323, 0.047954258, 0.062282287, 0.044228088, 0.032530837, 0.0935424, 0.047367185, 0.022785291, 0.034417044, 0.018639898, 0.02618384, 0.023943366, 0.021913916, 0.027202496, 0.02283886, 0.016770415, 0.02072574, 0.02328361, 0.02718169, 0.022656249, 0.014082466, 0.020297311, 0.02366329, 0.01825101, 0.01621969, 0.015926307, 0.016893024, 0.018219626, 0.02121029, 0.01749205, 0.016090171, 0.016581368, 0.015131119, 0.0198717, 0.039520957, 0.1545038, 0.25090313, 0.07547162, 0.043951243, 0.039263587, 0.042057756, 0.042763975, 0.042620517, 0.043171037, 0.0420318, 0.046885706, 0.035473563, 0.037886787, 0.05611416, 0.038697883, 0.016848022, 0.016225472, 0.027201584, 0.027562728, 0.02948829, 0.028849464, 0.021021636, 0.018652717, 0.024018502, 0.015857354, 0.014301238, 0.0154226795, 0.014137524, 0.015768254, 0.0155023215, 0.015463841, 0.016230686, 0.019937597, 0.027451193, 0.015492231, 0.01546072, 0.02096645, 0.05197975, 0.054306608, 0.055428118, 0.030141752, 0.047087025, 0.031226933, 0.028944567, 0.11409658, 0.04143528, 0.026560301, 0.037196428, 0.02302594, 0.026464054, 0.028990507, 0.043264877, 0.03443007, 0.04358885, 0.028428687, 0.024519142, 0.028413361, 0.019693568, 0.014132431, 0.021669589, 0.018771838, 0.021586595, 0.020306865, 0.026656426, 0.01949671, 0.015600984, 0.01710347, 0.016700268, 0.02112089, 0.017575586, 0.010689339, 0.0143481195, 0.018272648, 0.018712921, 0.012545304, 0.11428312, 0.062223434, 0.13324566, 0.09101337, 0.048845906, 0.043997694, 0.04888685, 0.053253125, 0.049624015, 0.07532924, 0.043978855, 0.03472135, 0.033208698, 0.016720423, 0.03150355, 0.025727678, 0.02229823, 0.015831875, 0.016463673, 0.017832045, 0.02327057, 0.021011278, 0.017449787, 0.015666395, 0.02081274, 0.016646545, 0.016246421, 0.01996471, 0.02210248, 0.021013487, 0.012771501, 0.0234638, 0.017178994, 0.01951203, 0.014068089, 0.012419938, 0.016916065, 0.015576424, 0.07885625, 0.12323952, 0.22403795, 0.13979177, 0.042278975, 0.03958915, 0.07901925, 0.051627256, 0.076684214, 0.03428081, 0.032526206, 0.04581017, 0.03737496, 0.038008105, 0.051605713, 0.022252226, 0.026375435, 0.028954534, 0.026434366, 0.030126082, 0.026696177, 0.017419372, 0.02037789, 0.03174416, 0.020859871, 0.024621157, 0.014580813, 0.018835438, 0.018650668, 0.01569267, 0.019254265, 0.016299896, 0.018707681, 0.026440721, 0.015383475, 0.022152135, 0.021605583, 0.021117082, 0.014773399, 0.016751617, 0.02079539, 0.013815376, 0.020874625, 0.07571349, 0.11952523, 0.14032201, 0.12135075, 0.111619495, 0.077275366, 0.14632751, 0.07265288, 0.032038193, 0.05825956, 0.036681343, 0.07495477, 0.059098728, 0.057968836, 0.036352478, 0.04052254, 0.038013227, 0.025566295, 0.03844001, 0.033358965, 0.019953106, 0.025649237, 0.02060583, 0.020101903, 0.020061778, 0.01825519, 0.028816901, 0.019206949, 0.0123266885, 0.014305218, 0.016177889, 0.024515662, 0.016359761, 0.020936295, 0.01147123, 0.01742758, 0.015422271, 0.0187775, 0.056243557, 0.18622954, 0.08449395, 0.119921185, 0.08989802, 0.057124425, 0.042348176, 0.06042136, 0.06112237, 0.060576703, 0.05711039, 0.04685928, 0.03871646, 0.027757676, 0.036631044, 0.03207535, 0.025591997, 0.019997433, 0.03241256, 0.023915246, 0.030987656, 0.0202531, 0.01559703, 0.012677513, 0.012255238, 0.021139009, 0.018140333, 0.017404966, 0.027545346, 0.014749789, 0.023972625, 0.018805321, 0.018630832, 0.0137883695, 0.02694896, 0.019756868, 0.0149995275, 0.016111469, 0.043009475, 0.08348114, 0.05724821, 0.05159526, 0.065112814, 0.062132504, 0.022420673, 0.039639473, 0.033303317, 0.04312755, 0.03651, 0.03162433, 0.027770767, 0.02661938, 0.03083065, 0.03714271, 0.02673593, 0.019564556, 0.01986264, 0.019592484, 0.02333358, 0.018751377, 0.015642392, 0.02331086, 0.020311499, 0.016493037, 0.015568458, 0.015603425, 0.014735931, 0.023080515, 0.011524054, 0.018904602, 0.01056857, 0.0096898265, 0.017270025, 0.016054248, 0.01636764, 0.013697188, 0.05650558, 0.11533543, 0.176422, 0.13025764, 0.08835482, 0.058811672, 0.044698864, 0.065718725, 0.059554745, 0.032272857, 0.019470582, 0.019970197, 0.026720777, 0.017716711, 0.026717992, 0.023242712, 0.020795941, 0.01777554, 0.026487185, 0.023280459, 0.020904457, 0.026573205, 0.016088432, 0.019712219, 0.022195563, 0.019344553, 0.018089669, 0.01607959, 0.025433496, 0.015273742, 0.017443419, 0.021172008, 0.011232445, 0.02010895, 0.013456362, 0.013769395, 0.017511794, 0.016322922, 0.056686226, 0.09749847, 0.06842861, 0.10341599, 0.10380798, 0.04520678, 0.030676443, 0.07505738, 0.061708935, 0.056801815, 0.052898854, 0.04022003, 0.028605474, 0.05521773, 0.026722487, 0.03229192, 0.022905802, 0.014405537, 0.015035885, 0.020880442, 0.019959273, 0.017102405, 0.016672714, 0.020812085, 0.0153282285, 0.02052848, 0.016723284, 0.020529073, 0.020211658, 0.01656879, 0.018486911, 0.018382436, 0.014414069, 0.0165694, 0.02347679, 0.020557975, 0.024417654, 0.019535989, 0.012084028, 0.016341725, 0.02027113, 0.022881646, 0.009788283, 0.013642681, 0.12379923, 0.1911594, 0.18855129, 0.15964727, 0.0746739, 0.057522923, 0.08952145, 0.03710662, 0.08359469, 0.08207173, 0.05220597, 0.028172662, 0.025396368, 0.022563387, 0.026120683, 0.027987348, 0.020608908, 0.020698199, 0.033977184, 0.031986266, 0.023756262, 0.027691782, 0.025486723, 0.024339218, 0.029302245, 0.018868877, 0.017596846, 0.02008673, 0.016601173, 0.02506772, 0.0118900435, 0.017571643, 0.016678382, 0.022543093, 0.018274972, 0.018943477, 0.015825855, 0.016291661, 0.0753504, 0.14144057, 0.20853779, 0.088737436, 0.12306023, 0.08874637, 0.052112184, 0.060278628, 0.041247126, 0.09159885, 0.039834168, 0.030782158, 0.036008403, 0.02281861, 0.026806056, 0.027443493, 0.021326555, 0.016887184, 0.020670235, 0.027328366, 0.017487418, 0.02604059, 0.02730336, 0.033083994, 0.018945057, 0.0280893, 0.03126499, 0.022600368, 0.014289347, 0.013410156, 0.018131288, 0.018835712, 0.019583238, 0.019143075, 0.013843195, 0.016464433, 0.01604254, 0.015357336, 0.07500719, 0.06953988, 0.07533926, 0.07333777, 0.07312677, 0.04515758, 0.028339518, 0.052648056, 0.03530026, 0.034020726, 0.042984765, 0.03598144, 0.028194973, 0.048362058, 0.037401248, 0.023319168, 0.020388793, 0.023903755, 0.020482676, 0.021592544, 0.025592938, 0.017077, 0.01489439, 0.021190004, 0.02202422, 0.018772854, 0.022397889, 0.020046677, 0.025297875, 0.01238442, 0.020238405, 0.02288167, 0.024020756, 0.022383742, 0.012368587, 0.019176615, 0.015342786, 0.016262658, 0.052867506, 0.10768361, 0.119173266, 0.08521707, 0.054045733, 0.029665314, 0.028366214, 0.046890263, 0.02687714, 0.043338977, 0.02670979, 0.031029083, 0.026864642, 0.03642097, 0.041235656, 0.018555598, 0.022078754, 0.022552058, 0.022840805, 0.01830317, 0.020373601, 0.020038677, 0.016289981, 0.016603725, 0.019403571, 0.018001487, 0.014172641, 0.024972862, 0.01919237, 0.014855373, 0.013399223, 0.01582774, 0.01893291, 0.0175989, 0.014485737, 0.015418341, 0.017461007, 0.018735077, 0.06887275, 0.062355336, 0.058693126, 0.060926445, 0.027971152, 0.030672168, 0.051277075, 0.05361148, 0.06365862, 0.02469861, 0.0213835, 0.024048144, 0.021254538, 0.026660947, 0.028128183, 0.024490848, 0.022313055, 0.027394941, 0.02511776, 0.024566265, 0.020021275, 0.020429935, 0.01659132, 0.016752496, 0.018894764, 0.018943306, 0.022726694, 0.023107892, 0.015994165, 0.014877312, 0.015493858, 0.021364933, 0.022144275, 0.011382105, 0.020805325, 0.019898515, 0.019047039, 0.017749874, 0.05976709, 0.12323508, 0.10750193, 0.08937084, 0.041971054, 0.037458763, 0.039308276, 0.043199424, 0.04495659, 0.036672316, 0.023914266, 0.024696607, 0.045495957, 0.032732833, 0.03192175, 0.030305203, 0.017739214, 0.02675531, 0.02738073, 0.01692881, 0.01798754, 0.023958111, 0.021276295, 0.014786186, 0.013608473, 0.01627497, 0.017177645, 0.015254174, 0.014100912, 0.017591871, 0.013694708, 0.016757393, 0.025516218, 0.017531123, 0.016269483, 0.016712204, 0.02073192, 0.013424621, 0.10012233, 0.054520458, 0.06215742, 0.10009045, 0.08417537, 0.06671438, 0.055655222, 0.03464511, 0.017956702, 0.034278043, 0.019554533, 0.023405807, 0.024164641, 0.036531694, 0.024733562, 0.018595496, 0.03195287, 0.018187977, 0.018701484, 0.01921259, 0.025799409, 0.020723665, 0.020068679, 0.019108674, 0.020118944, 0.01558677, 0.015652848, 0.020002037, 0.01655571, 0.015599175, 0.012456489, 0.014735125, 0.01707189, 0.013514911, 0.012973659, 0.013854788, 0.015946157, 0.017298074, 0.08119051, 0.08976075, 0.10687759, 0.087786235, 0.0679707, 0.034918636, 0.05460933, 0.09005133, 0.045686744, 0.034598246, 0.036307365, 0.022035887, 0.032494117, 0.03336697, 0.025349919, 0.026195576, 0.032247055, 0.027806502, 0.025907906, 0.018264573, 0.024856625, 0.018090868, 0.023521058, 0.02136876, 0.014644323, 0.020945286, 0.019242669, 0.021243589, 0.02092802, 0.01328024, 0.021429403, 0.019203737, 0.015586647, 0.021424273, 0.018576376, 0.019225676, 0.020768458, 0.018175134, 0.0801983, 0.17516887, 0.1060009, 0.05587365, 0.047245, 0.04295001, 0.05405158, 0.044914573, 0.047660723, 0.052676145, 0.023897806, 0.021441763, 0.037714552, 0.03019875, 0.020295814, 0.024934491, 0.01994635, 0.02537933, 0.03707228, 0.038678244, 0.027695373, 0.013491745, 0.02430615, 0.021101365, 0.02167301, 0.021604853, 0.021763181, 0.013089362, 0.016572809, 0.024931038, 0.009415951, 0.013697303, 0.024085594, 0.02202306, 0.017743591, 0.018931072, 0.019910628, 0.021817671, 0.063217975, 0.056114227, 0.078314684, 0.095105454, 0.08033694, 0.038461618, 0.046787947, 0.04598338, 0.051480554, 0.05206694, 0.022321964, 0.029241966, 0.023433354, 0.020394467, 0.026739383, 0.0191036, 0.03040697, 0.030738762, 0.031104198, 0.023423295, 0.01998721, 0.015786683, 0.026086425, 0.017917655]

Ascent losses:

{63: -0.0001542427, 103: -0.00016520423, 143: -0.00015807466, 182: -0.0001422191, 221: -0.0002006704, 261: -0.00013796888, 300: -0.00015125822, 339: -0.00012884068, 378: -9.6179014e-05, 423: -0.0001205688, 462: -0.00012332803, 501: -0.0001001934, 540: -7.596972e-05, 579: -0.00012329961, 623: -0.00010299939, 662: -0.00010655329, 701: -7.668294e-05, 740: -7.822665e-05, 779: -0.000115413146, 824: -0.0001115267, 863: -8.477544e-05, 902: -8.7894055e-05, 941: -0.00010666282, 980: -6.351779e-05, 1019: -8.0673395e-05, 1058: -6.551546e-05, 1097: -7.410459e-05, 1136: -5.7166253e-05, 1175: -9.36833e-05, 1219: -6.4911415e-05, 1258: -9.428404e-05, 1297: -8.0016536e-05, 1336: -6.3945394e-05, 1375: -7.832339e-05, 1420: -7.0871414e-05, 1459: -7.69588e-05, 1498: -6.319208e-05, 1537: -4.995492e-05, 1576: -5.9354028e-05, 1620: -5.277962e-05, 1659: -3.772474e-05, 1698: -6.811869e-05, 1737: -8.540709e-05, 1776: -5.416635e-05, 1821: -5.2233765e-05, 1860: -5.405595e-05, 1899: -7.4240605e-05, 1938: -4.8696947e-05, 1977: -6.12694e-05, 2021: -4.937333e-05, 2060: -5.924306e-05, 2099: -5.7958172e-05, 2138: -4.8387355e-05, 2177: -4.685583e-05, 2222: -4.5137193e-05, 2261: -4.048324e-05, 2300: -4.6554305e-05, 2339: -3.8298003e-05, 2378: -5.1825504e-05, 2422: -4.168758e-05, 2461: -3.859395e-05, 2500: -3.664299e-05, 2539: -3.371141e-05, 2578: -4.046866e-05, 2623: -3.1061743e-05, 2662: -2.4019775e-05, 2701: -4.6100373e-05, 2740: -4.8645717e-05, 2779: -2.6327214e-05, 2823: -2.7472468e-05, 2862: -3.7388847e-05, 2901: -2.5344285e-05, 2940: -3.81948e-05, 2979: -2.6297934e-05, 3024: -3.2601634e-05, 3063: -3.695149e-05, 3102: -2.2747385e-05, 3141: -2.1003565e-05, 3180: -2.8971403e-05, 3219: -2.9105862e-05, 3258: -4.020016e-05, 3297: -2.7714132e-05, 3336: -3.5591762e-05, 3375: -2.8921946e-05, 3419: -2.6884887e-05, 3458: -3.502453e-05, 3497: -3.1741267e-05, 3536: -2.534342e-05, 3575: -2.4277835e-05, 3620: -3.116103e-05, 3659: -2.4923827e-05, 3698: -2.8437335e-05, 3737: -2.2513623e-05, 3776: -2.3008377e-05, 3820: -2.8274542e-05, 3859: -2.7772765e-05, 3898: -2.9839772e-05, 3937: -3.0264822e-05, 3976: -2.497671e-05, 4021: -2.5352098e-05, 4060: -2.6727856e-05, 4099: -2.0881142e-05, 4138: -1.720097e-05, 4177: -2.6266194e-05, 4221: -1.9052315e-05, 4260: -3.0544947e-05, 4299: -2.2252058e-05, 4338: -2.185539e-05, 4377: -1.938176e-05, 4422: -2.4355866e-05, 4461: -1.9491892e-05, 4500: -3.2886885e-05, 4539: -2.4237184e-05, 4578: -2.4444478e-05, 4622: -2.7327627e-05, 4661: -2.128547e-05, 4700: -2.6999367e-05, 4739: -2.2450722e-05, 4778: -2.1375738e-05, 4823: -2.5735291e-05, 4862: -2.5391699e-05, 4901: -1.8731236e-05, 4940: -1.900566e-05, 4979: -1.8202087e-05, 5023: -2.2751543e-05, 5062: -1.6412681e-05, 5101: -1.9682906e-05, 5140: -2.045243e-05, 5179: -2.0477362e-05, 5224: -2.1665008e-05, 5263: -3.0261062e-05, 5302: -2.8054485e-05, 5341: -2.5891617e-05, 5380: -2.2251736e-05, 5419: -2.1289234e-05, 5458: -1.92944e-05, 5497: -2.242793e-05, 5536: -2.496669e-05, 5575: -2.264877e-05, 5619: -2.1968015e-05, 5658: -2.4118113e-05, 5697: -2.209681e-05, 5736: -2.3176546e-05, 5775: -2.1497037e-05, 5820: -3.6338468e-05, 5859: -2.4013827e-05, 5898: -2.1591757e-05, 5937: -1.4449058e-05, 5976: -1.7136339e-05, 6020: -1.5787016e-05, 6059: -1.7948782e-05, 6098: -1.7220314e-05, 6137: -1.8018107e-05, 6176: -1.7951692e-05, 6221: -1.5902417e-05, 6260: -1.3521058e-05, 6299: -1.873163e-05, 6338: -2.3180419e-05, 6377: -1.31886445e-05, 6421: -1.8334547e-05, 6460: -1.2160593e-05, 6499: -1.3255239e-05, 6538: -2.7056658e-05, 6577: -1.9604033e-05, 6622: -2.271501e-05, 6661: -1.6502507e-05, 6700: -2.1258238e-05, 6739: -1.4300547e-05, 6778: -2.1907525e-05, 6822: -1.8080866e-05, 6861: -1.9262836e-05, 6900: -1.6273625e-05, 6939: -1.5213906e-05, 6978: -1.36719145e-05, 7023: -1.4581851e-05, 7062: -1.3655554e-05, 7101: -1.580976e-05, 7140: -1.7313749e-05, 7179: -1.6112603e-05, 7223: -1.5709984e-05, 7262: -1.6319804e-05, 7301: -1.97888e-05, 7340: -1.9094967e-05, 7379: -2.1415159e-05, 7424: -2.5760146e-05, 7463: -1.63907e-05, 7502: -1.582288e-05, 7541: -1.4594802e-05, 7580: -2.5709522e-05, 7619: -1.5977754e-05, 7658: -1.6533137e-05, 7697: -1.686012e-05, 7736: -1.5663813e-05, 7775: -1.6899357e-05, 7819: -1.5819427e-05, 7858: -2.052479e-05, 7897: -1.8253357e-05, 7936: -1.6854587e-05, 7975: -1.7081436e-05, 8020: -1.09240245e-05, 8059: -1.3581582e-05, 8098: -1.08986405e-05, 8137: -1.2809831e-05, 8176: -2.0909758e-05, 8220: -2.219718e-05, 8259: -1.4999308e-05, 8298: -1.2986017e-05, 8337: -1.7365117e-05, 8376: -1.4748916e-05, 8421: -1.4736718e-05, 8460: -1.8576058e-05, 8499: -1.5333562e-05, 8538: -1.4975931e-05, 8577: -2.0466983e-05, 8621: -1.1919787e-05, 8660: -1.5641157e-05, 8699: -1.28047905e-05, 8738: -1.1164916e-05, 8777: -1.467341e-05, 8822: -1.4203521e-05, 8861: -1.5339223e-05, 8900: -1.6629701e-05, 8939: -1.0960244e-05, 8978: -1.3359064e-05, 9022: -1.4538667e-05, 9061: -1.7957065e-05, 9100: -1.5982761e-05, 9139: -1.6351132e-05, 9178: -1.26085015e-05, 9223: -1.6865295e-05, 9262: -1.2026256e-05, 9301: -1.9863635e-05, 9340: -1.5597754e-05, 9379: -1.4713996e-05, 9423: -1.874649e-05, 9462: -1.6227217e-05, 9501: -1.8762326e-05, 9540: -1.917037e-05, 9579: -1.657575e-05, 9624: -1.5727079e-05, 9663: -1.6093918e-05, 9702: -1.49205225e-05, 9741: -1.8073066e-05, 9780: -1.6491758e-05, 9819: -1.6310136e-05, 9858: -1.0857958e-05, 9897: -1.3732763e-05, 9936: -1.5335785e-05, 9975: -1.3743279e-05}
Importing data files...
['mm1d_1.csv', 'mm1d_4.csv', 'mm1d_0.csv', 'mm1d_2.csv', 'mm1d_3.csv', 'mm1d_5.csv']
Splitting data into training and test sets with a ratio of: 0.2
Total number of training samples is 38400
Total number of test samples is 9600
Length of an output spectrum is 300
Generating torch datasets
training using gradient ascent
Starting training process
Doing Evaluation on the model now
This is Epoch 0, training loss 7.28701, validation loss 3.61454
Resetting learning rate to 0.01000
Doing Evaluation on the model now
This is Epoch 10, training loss 0.23043, validation loss 0.17798
Epoch    38: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 20, training loss 0.10754, validation loss 0.10108
Doing Evaluation on the model now
This is Epoch 30, training loss 0.15795, validation loss 0.13667
Doing Evaluation on the model now
This is Epoch 40, training loss 0.13233, validation loss 0.06528
Epoch    82: reducing learning rate of group 0 to 2.5000e-04.
Epoch    94: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 50, training loss 0.06161, validation loss 0.05342
Epoch   105: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 52
Mean train loss for ascent epoch 53: -0.0009486133349128067
Mean eval for ascent epoch 53: 0.08167154341936111
Epoch   118: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 60, training loss 0.45021, validation loss 0.60400
Epoch   129: reducing learning rate of group 0 to 2.5000e-03.
Epoch   140: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 70, training loss 0.16919, validation loss 0.11055
Epoch   151: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 80, training loss 0.06378, validation loss 0.11026
Epoch   162: reducing learning rate of group 0 to 3.1250e-04.
Epoch   173: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 90, training loss 0.02838, validation loss 0.02559
Epoch   184: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 92
Mean train loss for ascent epoch 93: -0.0006041451706551015
Mean eval for ascent epoch 93: 0.03766174986958504
Epoch   195: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 100, training loss 0.54138, validation loss 1.08209
Epoch   206: reducing learning rate of group 0 to 2.5000e-03.
Epoch   217: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 110, training loss 0.23393, validation loss 0.07395
Epoch   228: reducing learning rate of group 0 to 6.2500e-04.
Epoch   239: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 120, training loss 0.04114, validation loss 0.03524
Epoch   250: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 130, training loss 0.02422, validation loss 0.04180
Epoch   261: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 131
Mean train loss for ascent epoch 132: -0.0006896635750308633
Mean eval for ascent epoch 132: 0.02266944758594036
Epoch   272: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 140, training loss 0.17705, validation loss 0.09129
Epoch   283: reducing learning rate of group 0 to 2.5000e-03.
Epoch   294: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 150, training loss 0.08110, validation loss 0.04325
Epoch   305: reducing learning rate of group 0 to 6.2500e-04.
Epoch   316: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 160, training loss 0.04068, validation loss 0.04530
Epoch   327: reducing learning rate of group 0 to 1.5625e-04.
Epoch   338: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 170
Doing Evaluation on the model now
This is Epoch 170, training loss 0.02112, validation loss 0.01969
Mean train loss for ascent epoch 171: -0.0006659700884483755
Mean eval for ascent epoch 171: 0.021776249632239342
Epoch   349: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 180, training loss 0.30259, validation loss 0.08360
Epoch   360: reducing learning rate of group 0 to 2.5000e-03.
Epoch   371: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 190, training loss 0.06973, validation loss 0.08193
Epoch   382: reducing learning rate of group 0 to 6.2500e-04.
Epoch   393: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 200, training loss 0.02683, validation loss 0.02860
Resetting learning rate to 0.01000
Epoch   404: reducing learning rate of group 0 to 5.0000e-04.
Epoch   415: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 210, training loss 0.02686, validation loss 0.01997
Epoch   426: reducing learning rate of group 0 to 1.2500e-04.
Epoch   437: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 220
Doing Evaluation on the model now
This is Epoch 220, training loss 0.01549, validation loss 0.01495
Mean train loss for ascent epoch 221: -0.0006197176990099251
Mean eval for ascent epoch 221: 0.017018619924783707
Epoch   448: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 230, training loss 0.16347, validation loss 0.08531
Epoch   459: reducing learning rate of group 0 to 2.5000e-03.
Epoch   470: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 240, training loss 0.04421, validation loss 0.03351
Epoch   481: reducing learning rate of group 0 to 6.2500e-04.
Epoch   492: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 250, training loss 0.03035, validation loss 0.02072
Epoch   503: reducing learning rate of group 0 to 1.5625e-04.
Epoch   514: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 259
Mean train loss for ascent epoch 260: -0.0005660250899381936
Mean eval for ascent epoch 260: 0.024624347686767578
Doing Evaluation on the model now
This is Epoch 260, training loss 0.02462, validation loss 0.02424
Epoch   525: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 270, training loss 0.16317, validation loss 0.06431
Epoch   536: reducing learning rate of group 0 to 2.5000e-03.
Epoch   547: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 280, training loss 0.07452, validation loss 0.11626
Epoch   558: reducing learning rate of group 0 to 6.2500e-04.
Epoch   569: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 290, training loss 0.03137, validation loss 0.01643
Epoch   580: reducing learning rate of group 0 to 1.5625e-04.
Epoch   591: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 298
Mean train loss for ascent epoch 299: -0.0006087893852964044
Mean eval for ascent epoch 299: 0.030845798552036285
Doing Evaluation on the model now
This is Epoch 300, training loss 0.88862, validation loss 1.69782
Epoch   602: reducing learning rate of group 0 to 5.0000e-03.
Epoch   613: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 310, training loss 0.18441, validation loss 0.10014
Epoch   624: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 320, training loss 0.14625, validation loss 0.06484
Epoch   635: reducing learning rate of group 0 to 6.2500e-04.
Epoch   646: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 330, training loss 0.02312, validation loss 0.01852
Epoch   657: reducing learning rate of group 0 to 1.5625e-04.
Epoch   668: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 337
Mean train loss for ascent epoch 338: -0.0006917798891663551
Mean eval for ascent epoch 338: 0.023170096799731255
Doing Evaluation on the model now
This is Epoch 340, training loss 0.74884, validation loss 0.56745
Epoch   679: reducing learning rate of group 0 to 5.0000e-03.
Epoch   690: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 350, training loss 0.09477, validation loss 0.25755
Epoch   701: reducing learning rate of group 0 to 1.2500e-03.
Epoch   712: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 360, training loss 0.03618, validation loss 0.02954
Epoch   723: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 370, training loss 0.04311, validation loss 0.06503
Epoch   734: reducing learning rate of group 0 to 1.5625e-04.
Epoch   745: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 376
Mean train loss for ascent epoch 377: -0.0005374738830141723
Mean eval for ascent epoch 377: 0.017475087195634842
Doing Evaluation on the model now
This is Epoch 380, training loss 0.40786, validation loss 0.19253
Epoch   756: reducing learning rate of group 0 to 5.0000e-03.
Epoch   767: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 390, training loss 0.22654, validation loss 0.21436
Epoch   778: reducing learning rate of group 0 to 1.2500e-03.
Epoch   789: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 400, training loss 0.03022, validation loss 0.03219
Resetting learning rate to 0.01000
Epoch   800: reducing learning rate of group 0 to 5.0000e-04.
Epoch   811: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 410, training loss 0.03310, validation loss 0.03755
Epoch   822: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 420, training loss 0.01400, validation loss 0.01489
Epoch   833: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 421
Mean train loss for ascent epoch 422: -0.0007358735310845077
Mean eval for ascent epoch 422: 0.016632651910185814
Epoch   844: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 430, training loss 0.18496, validation loss 0.07834
Epoch   855: reducing learning rate of group 0 to 2.5000e-03.
Epoch   866: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 440, training loss 0.10846, validation loss 0.07826
Epoch   877: reducing learning rate of group 0 to 6.2500e-04.
Epoch   888: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 450, training loss 0.01863, validation loss 0.01645
Epoch   899: reducing learning rate of group 0 to 1.5625e-04.
Epoch   910: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 460
Doing Evaluation on the model now
This is Epoch 460, training loss 0.01962, validation loss 0.01535
Mean train loss for ascent epoch 461: -0.0007249162881635129
Mean eval for ascent epoch 461: 0.018784020096063614
Epoch   921: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 470, training loss 0.38625, validation loss 0.31333
Epoch   932: reducing learning rate of group 0 to 2.5000e-03.
Epoch   943: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 480, training loss 0.04446, validation loss 0.04138
Epoch   954: reducing learning rate of group 0 to 6.2500e-04.
Epoch   965: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 490, training loss 0.02697, validation loss 0.02183
Epoch   976: reducing learning rate of group 0 to 1.5625e-04.
Epoch   987: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 499
Mean train loss for ascent epoch 500: -0.0007282309234142303
Mean eval for ascent epoch 500: 0.01743059791624546
Doing Evaluation on the model now
This is Epoch 500, training loss 0.01743, validation loss 0.02006
Epoch   998: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1009: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 510, training loss 0.17530, validation loss 0.07841
Epoch  1020: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 520, training loss 0.05839, validation loss 0.11132
Epoch  1031: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1042: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 530, training loss 0.01210, validation loss 0.01238
Epoch  1053: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1064: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 538
Mean train loss for ascent epoch 539: -0.0005332535947673023
Mean eval for ascent epoch 539: 0.014520668424665928
Doing Evaluation on the model now
This is Epoch 540, training loss 0.37565, validation loss 0.19500
Epoch  1075: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1086: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 550, training loss 0.12530, validation loss 0.06711
Epoch  1097: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1108: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 560, training loss 0.06451, validation loss 0.15589
Epoch  1119: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 570, training loss 0.02132, validation loss 0.01658
Epoch  1130: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1141: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 577
Mean train loss for ascent epoch 578: -0.00038981082616373897
Mean eval for ascent epoch 578: 0.01365606114268303
Doing Evaluation on the model now
This is Epoch 580, training loss 0.42922, validation loss 0.81658
Epoch  1152: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1163: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 590, training loss 0.16682, validation loss 0.21037
Epoch  1174: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1185: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 600, training loss 0.04004, validation loss 0.02669
Resetting learning rate to 0.01000
Epoch  1196: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1207: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 610, training loss 0.02625, validation loss 0.03316
Epoch  1218: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 620, training loss 0.01402, validation loss 0.01243
Epoch  1229: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 621
Mean train loss for ascent epoch 622: -0.0004069858114235103
Mean eval for ascent epoch 622: 0.013747593387961388
Epoch  1240: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 630, training loss 0.19500, validation loss 0.33348
Epoch  1251: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1262: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 640, training loss 0.04996, validation loss 0.02996
Epoch  1273: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1284: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 650, training loss 0.01975, validation loss 0.02598
Epoch  1295: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1306: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 660
Doing Evaluation on the model now
This is Epoch 660, training loss 0.01635, validation loss 0.01674
Mean train loss for ascent epoch 661: -0.000634198950137943
Mean eval for ascent epoch 661: 0.018932456150650978
Epoch  1317: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 670, training loss 0.15898, validation loss 0.11538
Epoch  1328: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1339: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 680, training loss 0.04521, validation loss 0.02314
Epoch  1350: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1361: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 690, training loss 0.01610, validation loss 0.01351
Epoch  1372: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1383: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 699
Mean train loss for ascent epoch 700: -0.0006699537625536323
Mean eval for ascent epoch 700: 0.014931190758943558
Doing Evaluation on the model now
This is Epoch 700, training loss 0.01493, validation loss 0.01243
Epoch  1394: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 710, training loss 0.15536, validation loss 0.19578
Epoch  1405: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1416: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 720, training loss 0.08857, validation loss 0.08082
Epoch  1427: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1438: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 730, training loss 0.01736, validation loss 0.02189
Epoch  1449: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1460: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 738
Mean train loss for ascent epoch 739: -0.0004172852204646915
Mean eval for ascent epoch 739: 0.012462981045246124
Doing Evaluation on the model now
This is Epoch 740, training loss 0.50555, validation loss 0.21598
Epoch  1471: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1482: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 750, training loss 0.09152, validation loss 0.03650
Epoch  1493: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 760, training loss 0.06243, validation loss 0.06244
Epoch  1504: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1515: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 770, training loss 0.01706, validation loss 0.01624
Epoch  1526: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1537: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 777
Mean train loss for ascent epoch 778: -0.0006893064710311592
Mean eval for ascent epoch 778: 0.016618866473436356
Doing Evaluation on the model now
This is Epoch 780, training loss 0.39418, validation loss 0.19872
Epoch  1548: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1559: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 790, training loss 0.09268, validation loss 0.08442
Epoch  1570: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1581: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 800, training loss 0.02398, validation loss 0.03028
Resetting learning rate to 0.01000
Epoch  1592: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 810, training loss 0.01724, validation loss 0.01586
Epoch  1603: reducing learning rate of group 0 to 2.5000e-04.
Epoch  1614: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 820, training loss 0.01273, validation loss 0.01342
Epoch  1625: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 822
Mean train loss for ascent epoch 823: -0.000527702271938324
Mean eval for ascent epoch 823: 0.013524469919502735
Epoch  1636: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 830, training loss 0.25591, validation loss 0.11962
Epoch  1647: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1658: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 840, training loss 0.04124, validation loss 0.06769
Epoch  1669: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1680: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 850, training loss 0.01971, validation loss 0.01431
Epoch  1691: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 860, training loss 0.01363, validation loss 0.01312
Epoch  1702: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 861
Mean train loss for ascent epoch 862: -0.000333383068209514
Mean eval for ascent epoch 862: 0.013030695728957653
Epoch  1713: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 870, training loss 0.21496, validation loss 0.08497
Epoch  1724: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1735: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 880, training loss 0.08586, validation loss 0.11907
Epoch  1746: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1757: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 890, training loss 0.01827, validation loss 0.01469
Epoch  1768: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1779: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 900
Doing Evaluation on the model now
This is Epoch 900, training loss 0.01282, validation loss 0.01096
Mean train loss for ascent epoch 901: -0.00048136760597117245
Mean eval for ascent epoch 901: 0.013932849280536175
Epoch  1790: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 910, training loss 0.09534, validation loss 0.06683
Epoch  1801: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1812: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 920, training loss 0.18060, validation loss 0.12056
Epoch  1823: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1834: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 930, training loss 0.02091, validation loss 0.01662
Epoch  1845: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1856: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 939
Mean train loss for ascent epoch 940: -0.00039796746568754315
Mean eval for ascent epoch 940: 0.012146097607910633
Doing Evaluation on the model now
This is Epoch 940, training loss 0.01215, validation loss 0.01135
Epoch  1867: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1878: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 950, training loss 0.15675, validation loss 0.20092
Epoch  1889: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 960, training loss 0.05506, validation loss 0.07787
Epoch  1900: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1911: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 970, training loss 0.01319, validation loss 0.01477
Epoch  1922: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1933: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 978
Mean train loss for ascent epoch 979: -0.0005359434871934354
Mean eval for ascent epoch 979: 0.013361643999814987
Doing Evaluation on the model now
This is Epoch 980, training loss 0.27443, validation loss 0.30032
Epoch  1944: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1955: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 990, training loss 0.07533, validation loss 0.04060
Epoch  1966: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1977: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1000, training loss 0.04600, validation loss 0.03854
Resetting learning rate to 0.01000
Epoch  1988: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 1010, training loss 0.01555, validation loss 0.01230
Epoch  1999: reducing learning rate of group 0 to 2.5000e-04.
Epoch  2010: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1020, training loss 0.01281, validation loss 0.01612
Epoch  2021: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1022
Mean train loss for ascent epoch 1023: -0.00036640081088989973
Mean eval for ascent epoch 1023: 0.012348425574600697
Epoch  2032: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1030, training loss 0.26915, validation loss 0.54563
Epoch  2043: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2054: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1040, training loss 0.10034, validation loss 0.06832
Epoch  2065: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2076: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1050, training loss 0.02161, validation loss 0.02743
Epoch  2087: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1060, training loss 0.01228, validation loss 0.01417
Epoch  2098: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1061
Mean train loss for ascent epoch 1062: -0.0004565291746985167
Mean eval for ascent epoch 1062: 0.011937386356294155
Epoch  2109: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1070, training loss 0.14365, validation loss 0.06059
Epoch  2120: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2131: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1080, training loss 0.02499, validation loss 0.02996
Epoch  2142: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2153: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1090, training loss 0.01827, validation loss 0.02196
Epoch  2164: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2175: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1100
Doing Evaluation on the model now
This is Epoch 1100, training loss 0.01368, validation loss 0.01270
Mean train loss for ascent epoch 1101: -0.0003510574751999229
Mean eval for ascent epoch 1101: 0.014222773723304272
Epoch  2186: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1110, training loss 0.09426, validation loss 0.10941
Epoch  2197: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2208: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1120, training loss 0.04018, validation loss 0.02617
Epoch  2219: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2230: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1130, training loss 0.01614, validation loss 0.01668
Epoch  2241: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2252: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1139
Mean train loss for ascent epoch 1140: -0.00032066035782918334
Mean eval for ascent epoch 1140: 0.012046433053910732
Doing Evaluation on the model now
This is Epoch 1140, training loss 0.01205, validation loss 0.01149
Epoch  2263: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1150, training loss 0.16021, validation loss 0.05561
Epoch  2274: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2285: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1160, training loss 0.05028, validation loss 0.04164
Epoch  2296: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2307: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1170, training loss 0.01352, validation loss 0.01317
Epoch  2318: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2329: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1178
Mean train loss for ascent epoch 1179: -0.00029149826150387526
Mean eval for ascent epoch 1179: 0.012554130516946316
Doing Evaluation on the model now
This is Epoch 1180, training loss 0.36552, validation loss 0.45642
Epoch  2340: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2351: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1190, training loss 0.06930, validation loss 0.08604
Epoch  2362: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1200, training loss 0.03585, validation loss 0.03022
Epoch  2373: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  2384: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 1210, training loss 0.01581, validation loss 0.01260
Epoch  2395: reducing learning rate of group 0 to 2.5000e-04.
Epoch  2406: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1220, training loss 0.01121, validation loss 0.01355
Epoch  2417: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1223
Mean train loss for ascent epoch 1224: -0.0003124132053926587
Mean eval for ascent epoch 1224: 0.012014550156891346
Epoch  2428: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1230, training loss 0.09136, validation loss 0.14955
Epoch  2439: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2450: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1240, training loss 0.03095, validation loss 0.03638
Epoch  2461: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1250, training loss 0.02074, validation loss 0.02500
Epoch  2472: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2483: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1260, training loss 0.01248, validation loss 0.01306
Epoch  2494: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1262
Mean train loss for ascent epoch 1263: -0.00033011066261678934
Mean eval for ascent epoch 1263: 0.01225821953266859
Epoch  2505: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1270, training loss 0.14103, validation loss 0.03280
Epoch  2516: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2527: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1280, training loss 0.01891, validation loss 0.01340
Epoch  2538: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2549: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1290, training loss 0.01607, validation loss 0.01274
Epoch  2560: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1300, training loss 0.01309, validation loss 0.01026
Epoch  2571: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1301
Mean train loss for ascent epoch 1302: -0.0004565226554404944
Mean eval for ascent epoch 1302: 0.012301299721002579
Epoch  2582: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1310, training loss 0.11304, validation loss 0.06677
Epoch  2593: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2604: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1320, training loss 0.02068, validation loss 0.03787
Epoch  2615: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2626: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1330, training loss 0.01487, validation loss 0.01667
Epoch  2637: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2648: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1340
Doing Evaluation on the model now
This is Epoch 1340, training loss 0.01096, validation loss 0.01036
Mean train loss for ascent epoch 1341: -0.0004842891066800803
Mean eval for ascent epoch 1341: 0.012131628580391407
Epoch  2659: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1350, training loss 0.08364, validation loss 0.10336
Epoch  2670: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2681: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1360, training loss 0.08831, validation loss 0.06272
Epoch  2692: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2703: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1370, training loss 0.01365, validation loss 0.01433
Epoch  2714: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2725: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1379
Mean train loss for ascent epoch 1380: -0.0003302221011836082
Mean eval for ascent epoch 1380: 0.011672776192426682
Doing Evaluation on the model now
This is Epoch 1380, training loss 0.01167, validation loss 0.01013
Epoch  2736: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2747: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1390, training loss 0.14918, validation loss 0.07245
Epoch  2758: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1400, training loss 0.03911, validation loss 0.02310
Resetting learning rate to 0.01000
Epoch  2769: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2780: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1410, training loss 0.01367, validation loss 0.01539
Epoch  2791: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2802: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1418
Mean train loss for ascent epoch 1419: -0.0002510219637770206
Mean eval for ascent epoch 1419: 0.009741812013089657
Doing Evaluation on the model now
This is Epoch 1420, training loss 0.20950, validation loss 0.82293
Epoch  2813: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2824: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1430, training loss 0.16250, validation loss 0.23998
Epoch  2835: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2846: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1440, training loss 0.02883, validation loss 0.02418
Epoch  2857: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1450, training loss 0.01097, validation loss 0.01107
Epoch  2868: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2879: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1457
Mean train loss for ascent epoch 1458: -0.0003080943424720317
Mean eval for ascent epoch 1458: 0.01048506423830986
Doing Evaluation on the model now
This is Epoch 1460, training loss 0.40871, validation loss 0.57150
Epoch  2890: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2901: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1470, training loss 0.03148, validation loss 0.03968
Epoch  2912: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2923: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1480, training loss 0.02337, validation loss 0.01352
Epoch  2934: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2945: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1490, training loss 0.00945, validation loss 0.00955
Epoch  2956: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1496
Mean train loss for ascent epoch 1497: -0.00036604763590730727
Mean eval for ascent epoch 1497: 0.009898037649691105
Doing Evaluation on the model now
This is Epoch 1500, training loss 0.19507, validation loss 0.37623
Epoch  2967: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2978: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1510, training loss 0.15519, validation loss 0.05133
Epoch  2989: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3000: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1520, training loss 0.02981, validation loss 0.05432
Epoch  3011: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3022: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1530, training loss 0.00985, validation loss 0.00966
Epoch  3033: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1535
Mean train loss for ascent epoch 1536: -0.00031779147684574127
Mean eval for ascent epoch 1536: 0.009565196931362152
Doing Evaluation on the model now
This is Epoch 1540, training loss 0.53627, validation loss 0.29183
Epoch  3044: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3055: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1550, training loss 0.04233, validation loss 0.03104
Epoch  3066: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3077: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1560, training loss 0.01807, validation loss 0.02158
Epoch  3088: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3099: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1570, training loss 0.01069, validation loss 0.00973
Epoch  3110: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1574
Mean train loss for ascent epoch 1575: -0.000371951493434608
Mean eval for ascent epoch 1575: 0.0094140088185668
Epoch  3121: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1580, training loss 0.19301, validation loss 0.06429
Epoch  3132: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1590, training loss 0.06828, validation loss 0.06115
Epoch  3143: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3154: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1600, training loss 0.01284, validation loss 0.01201
Resetting learning rate to 0.01000
Epoch  3165: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3176: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1610, training loss 0.01018, validation loss 0.00968
Epoch  3187: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3198: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1618
Mean train loss for ascent epoch 1619: -0.0003729207383003086
Mean eval for ascent epoch 1619: 0.011110848747193813
Doing Evaluation on the model now
This is Epoch 1620, training loss 0.17932, validation loss 0.10572
Epoch  3209: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3220: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1630, training loss 0.05112, validation loss 0.14269
Epoch  3231: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1640, training loss 0.02270, validation loss 0.03122
Epoch  3242: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3253: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1650, training loss 0.01236, validation loss 0.01176
Epoch  3264: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3275: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1657
Mean train loss for ascent epoch 1658: -0.0002732936118263751
Mean eval for ascent epoch 1658: 0.008727136999368668
Doing Evaluation on the model now
This is Epoch 1660, training loss 0.32327, validation loss 0.32385
Epoch  3286: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3297: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1670, training loss 0.06579, validation loss 0.03058
Epoch  3308: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3319: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1680, training loss 0.02151, validation loss 0.01231
Epoch  3330: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1690, training loss 0.01019, validation loss 0.00858
Epoch  3341: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3352: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1696
Mean train loss for ascent epoch 1697: -0.0003651916340459138
Mean eval for ascent epoch 1697: 0.00915518682450056
Doing Evaluation on the model now
This is Epoch 1700, training loss 0.23502, validation loss 0.18534
Epoch  3363: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3374: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1710, training loss 0.06990, validation loss 0.05446
Epoch  3385: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3396: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1720, training loss 0.01602, validation loss 0.01473
Epoch  3407: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3418: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1730, training loss 0.00989, validation loss 0.00928
Epoch  3429: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1735
Mean train loss for ascent epoch 1736: -0.00031893077539280057
Mean eval for ascent epoch 1736: 0.011770017445087433
Doing Evaluation on the model now
This is Epoch 1740, training loss 0.21599, validation loss 0.23503
Epoch  3440: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3451: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1750, training loss 0.05071, validation loss 0.08832
Epoch  3462: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3473: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1760, training loss 0.01507, validation loss 0.01156
Epoch  3484: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3495: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1770, training loss 0.01199, validation loss 0.01138
Epoch  3506: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1774
Mean train loss for ascent epoch 1775: -0.0004252740473020822
Mean eval for ascent epoch 1775: 0.011559095233678818
Epoch  3517: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1780, training loss 0.43005, validation loss 0.24473
Epoch  3528: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1790, training loss 0.05044, validation loss 0.04337
Epoch  3539: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3550: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1800, training loss 0.01731, validation loss 0.01214
Resetting learning rate to 0.01000
Epoch  3561: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3572: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1810, training loss 0.01097, validation loss 0.00891
Epoch  3583: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3594: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1819
Mean train loss for ascent epoch 1820: -0.0004449281841516495
Mean eval for ascent epoch 1820: 0.010405960492789745
Doing Evaluation on the model now
This is Epoch 1820, training loss 0.01041, validation loss 0.00997
Epoch  3605: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3616: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1830, training loss 0.08435, validation loss 0.09066
Epoch  3627: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1840, training loss 0.01863, validation loss 0.02371
Epoch  3638: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3649: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1850, training loss 0.01143, validation loss 0.00937
Epoch  3660: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3671: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1858
Mean train loss for ascent epoch 1859: -0.0003693340113386512
Mean eval for ascent epoch 1859: 0.010298804379999638
Doing Evaluation on the model now
This is Epoch 1860, training loss 0.13504, validation loss 0.15753
Epoch  3682: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3693: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1870, training loss 0.04665, validation loss 0.05007
Epoch  3704: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3715: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1880, training loss 0.03271, validation loss 0.03290
Epoch  3726: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1890, training loss 0.01837, validation loss 0.01313
Epoch  3737: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3748: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1897
Mean train loss for ascent epoch 1898: -0.0002873335324693471
Mean eval for ascent epoch 1898: 0.007761203218251467
Doing Evaluation on the model now
This is Epoch 1900, training loss 0.17025, validation loss 0.20687
Epoch  3759: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3770: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1910, training loss 0.07357, validation loss 0.06815
Epoch  3781: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3792: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1920, training loss 0.01971, validation loss 0.01310
Epoch  3803: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3814: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1930, training loss 0.01108, validation loss 0.00873
Epoch  3825: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1936
Mean train loss for ascent epoch 1937: -0.00028206384740769863
Mean eval for ascent epoch 1937: 0.008586101233959198
Doing Evaluation on the model now
This is Epoch 1940, training loss 0.57258, validation loss 1.05725
Epoch  3836: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3847: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1950, training loss 0.06202, validation loss 0.05213
Epoch  3858: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3869: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1960, training loss 0.01251, validation loss 0.01423
Epoch  3880: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3891: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1970, training loss 0.01026, validation loss 0.00738
Epoch  3902: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1975
Mean train loss for ascent epoch 1976: -0.00022927978716325015
Mean eval for ascent epoch 1976: 0.009572586975991726
Doing Evaluation on the model now
This is Epoch 1980, training loss 1.36960, validation loss 3.60942
Epoch  3913: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3924: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1990, training loss 0.07792, validation loss 0.05413
Epoch  3935: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3946: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2000, training loss 0.01784, validation loss 0.02158
Resetting learning rate to 0.01000
Epoch  3957: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3968: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2010, training loss 0.01037, validation loss 0.00965
Epoch  3979: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3990: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2019
Mean train loss for ascent epoch 2020: -0.00024313043104484677
Mean eval for ascent epoch 2020: 0.009208181872963905
Doing Evaluation on the model now
This is Epoch 2020, training loss 0.00921, validation loss 0.00810
Epoch  4001: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2030, training loss 0.14832, validation loss 0.25654
Epoch  4012: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4023: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2040, training loss 0.03268, validation loss 0.06270
Epoch  4034: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4045: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2050, training loss 0.00889, validation loss 0.00970
Epoch  4056: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4067: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2058
Mean train loss for ascent epoch 2059: -0.0003378510009497404
Mean eval for ascent epoch 2059: 0.009612157009541988
Doing Evaluation on the model now
This is Epoch 2060, training loss 0.29359, validation loss 0.27226
Epoch  4078: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4089: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2070, training loss 0.04964, validation loss 0.07544
Epoch  4100: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2080, training loss 0.02085, validation loss 0.01747
Epoch  4111: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4122: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2090, training loss 0.01189, validation loss 0.00896
Epoch  4133: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4144: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2097
Mean train loss for ascent epoch 2098: -0.0002334762248210609
Mean eval for ascent epoch 2098: 0.013233604840934277
Doing Evaluation on the model now
This is Epoch 2100, training loss 0.39252, validation loss 0.37574
Epoch  4155: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4166: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2110, training loss 0.03200, validation loss 0.03386
Epoch  4177: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4188: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2120, training loss 0.02136, validation loss 0.00844
Epoch  4199: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2130, training loss 0.01115, validation loss 0.00863
Epoch  4210: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4221: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2136
Mean train loss for ascent epoch 2137: -0.0002723757643252611
Mean eval for ascent epoch 2137: 0.010385053232312202
Doing Evaluation on the model now
This is Epoch 2140, training loss 0.53108, validation loss 0.34534
Epoch  4232: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4243: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2150, training loss 0.07001, validation loss 0.05986
Epoch  4254: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4265: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2160, training loss 0.01576, validation loss 0.00860
Epoch  4276: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4287: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2170, training loss 0.01107, validation loss 0.00985
Epoch  4298: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2175
Mean train loss for ascent epoch 2176: -0.00020308408420532942
Mean eval for ascent epoch 2176: 0.008330035023391247
Doing Evaluation on the model now
This is Epoch 2180, training loss 0.42868, validation loss 0.09324
Epoch  4309: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4320: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2190, training loss 0.06699, validation loss 0.05780
Epoch  4331: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4342: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2200, training loss 0.01306, validation loss 0.01503
Resetting learning rate to 0.01000
Epoch  4353: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4364: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2210, training loss 0.00816, validation loss 0.00701
Epoch  4375: reducing learning rate of group 0 to 1.2500e-04.
Epoch  4386: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2220
Doing Evaluation on the model now
This is Epoch 2220, training loss 0.00840, validation loss 0.00740
Mean train loss for ascent epoch 2221: -0.00018475193064659834
Mean eval for ascent epoch 2221: 0.008535902947187424
Epoch  4397: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2230, training loss 0.06534, validation loss 0.03050
Epoch  4408: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4419: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2240, training loss 0.04419, validation loss 0.07127
Epoch  4430: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4441: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2250, training loss 0.00932, validation loss 0.00858
Epoch  4452: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4463: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2259
Mean train loss for ascent epoch 2260: -0.00023836120089981705
Mean eval for ascent epoch 2260: 0.008283107541501522
Doing Evaluation on the model now
This is Epoch 2260, training loss 0.00828, validation loss 0.00676
Epoch  4474: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4485: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2270, training loss 0.15230, validation loss 0.04482
Epoch  4496: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2280, training loss 0.04006, validation loss 0.01862
Epoch  4507: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4518: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2290, training loss 0.00912, validation loss 0.00895
Epoch  4529: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4540: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2298
Mean train loss for ascent epoch 2299: -0.00026267304201610386
Mean eval for ascent epoch 2299: 0.00902634672820568
Doing Evaluation on the model now
This is Epoch 2300, training loss 0.59069, validation loss 0.34997
Epoch  4551: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4562: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2310, training loss 0.06427, validation loss 0.04901
Epoch  4573: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4584: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2320, training loss 0.02514, validation loss 0.01215
Epoch  4595: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2330, training loss 0.00964, validation loss 0.00950
Epoch  4606: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4617: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2337
Mean train loss for ascent epoch 2338: -0.0002716963645070791
Mean eval for ascent epoch 2338: 0.01036914438009262
Doing Evaluation on the model now
This is Epoch 2340, training loss 0.31583, validation loss 0.15625
Epoch  4628: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4639: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2350, training loss 0.06828, validation loss 0.03121
Epoch  4650: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4661: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2360, training loss 0.01327, validation loss 0.00817
Epoch  4672: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4683: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2370, training loss 0.00873, validation loss 0.00892
Epoch  4694: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2376
Mean train loss for ascent epoch 2377: -0.00020495832723099738
Mean eval for ascent epoch 2377: 0.008318331092596054
Doing Evaluation on the model now
This is Epoch 2380, training loss 0.44990, validation loss 0.17557
Epoch  4705: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4716: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2390, training loss 0.02920, validation loss 0.02446
Epoch  4727: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4738: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2400, training loss 0.01550, validation loss 0.02360
Resetting learning rate to 0.01000
Epoch  4749: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4760: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2410, training loss 0.01377, validation loss 0.01401
Epoch  4771: reducing learning rate of group 0 to 1.2500e-04.
Epoch  4782: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2420
Doing Evaluation on the model now
This is Epoch 2420, training loss 0.00982, validation loss 0.00853
Mean train loss for ascent epoch 2421: -0.00017837315681390464
Mean eval for ascent epoch 2421: 0.008299789391458035
Epoch  4793: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2430, training loss 0.07116, validation loss 0.16799
Epoch  4804: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4815: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2440, training loss 0.02261, validation loss 0.06199
Epoch  4826: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4837: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2450, training loss 0.01011, validation loss 0.00972
Epoch  4848: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4859: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2459
Mean train loss for ascent epoch 2460: -0.0004225697775837034
Mean eval for ascent epoch 2460: 0.00989840179681778
Doing Evaluation on the model now
This is Epoch 2460, training loss 0.00990, validation loss 0.00911
Epoch  4870: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2470, training loss 0.10878, validation loss 0.08613
Epoch  4881: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4892: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2480, training loss 0.01388, validation loss 0.01697
Epoch  4903: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4914: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2490, training loss 0.00830, validation loss 0.00895
Epoch  4925: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4936: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2498
Mean train loss for ascent epoch 2499: -0.00020828878041356802
Mean eval for ascent epoch 2499: 0.007332171313464642
Doing Evaluation on the model now
This is Epoch 2500, training loss 0.10759, validation loss 0.08366
Epoch  4947: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4958: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2510, training loss 0.02863, validation loss 0.01125
Epoch  4969: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2520, training loss 0.02140, validation loss 0.01553
Epoch  4980: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4991: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2530, training loss 0.00765, validation loss 0.00734
Epoch  5002: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5013: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2537
Mean train loss for ascent epoch 2538: -0.00020704085181932896
Mean eval for ascent epoch 2538: 0.007550737354904413
Doing Evaluation on the model now
This is Epoch 2540, training loss 0.13482, validation loss 0.22705
Epoch  5024: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5035: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2550, training loss 0.03847, validation loss 0.02300
Epoch  5046: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5057: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2560, training loss 0.01445, validation loss 0.01091
Epoch  5068: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2570, training loss 0.00744, validation loss 0.00877
Epoch  5079: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5090: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2576
Mean train loss for ascent epoch 2577: -0.00021714161266572773
Mean eval for ascent epoch 2577: 0.006919813808053732
Doing Evaluation on the model now
This is Epoch 2580, training loss 0.50229, validation loss 0.42041
Epoch  5101: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5112: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2590, training loss 0.03744, validation loss 0.02192
Epoch  5123: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5134: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2600, training loss 0.00857, validation loss 0.00934
Resetting learning rate to 0.01000
Epoch  5145: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5156: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2610, training loss 0.00872, validation loss 0.00822
Epoch  5167: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2620, training loss 0.00741, validation loss 0.00657
Epoch  5178: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2621
Mean train loss for ascent epoch 2622: -0.00023755144502501935
Mean eval for ascent epoch 2622: 0.007433020975440741
Epoch  5189: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2630, training loss 0.10407, validation loss 0.02594
Epoch  5200: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5211: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2640, training loss 0.01885, validation loss 0.01668
Epoch  5222: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5233: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2650, training loss 0.00753, validation loss 0.00782
Epoch  5244: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5255: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2660
Doing Evaluation on the model now
This is Epoch 2660, training loss 0.00751, validation loss 0.00691
Mean train loss for ascent epoch 2661: -0.000252897065365687
Mean eval for ascent epoch 2661: 0.008097827434539795
Epoch  5266: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2670, training loss 0.09377, validation loss 0.05739
Epoch  5277: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5288: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2680, training loss 0.01811, validation loss 0.02666
Epoch  5299: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5310: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2690, training loss 0.00930, validation loss 0.00780
Epoch  5321: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5332: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2699
Mean train loss for ascent epoch 2700: -0.00021674108575098217
Mean eval for ascent epoch 2700: 0.008805644698441029
Doing Evaluation on the model now
This is Epoch 2700, training loss 0.00881, validation loss 0.00817
Epoch  5343: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5354: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2710, training loss 0.13098, validation loss 0.09807
Epoch  5365: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2720, training loss 0.02615, validation loss 0.01323
Epoch  5376: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5387: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2730, training loss 0.00842, validation loss 0.00863
Epoch  5398: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5409: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2738
Mean train loss for ascent epoch 2739: -0.0002554977254476398
Mean eval for ascent epoch 2739: 0.007939831353724003
Doing Evaluation on the model now
This is Epoch 2740, training loss 0.17191, validation loss 0.22472
Epoch  5420: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5431: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2750, training loss 0.06583, validation loss 0.06053
Epoch  5442: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5453: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2760, training loss 0.01932, validation loss 0.02029
Epoch  5464: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2770, training loss 0.01005, validation loss 0.00800
Epoch  5475: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5486: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2777
Mean train loss for ascent epoch 2778: -0.00018982523761224002
Mean eval for ascent epoch 2778: 0.008230545558035374
Doing Evaluation on the model now
This is Epoch 2780, training loss 0.30562, validation loss 0.42094
Epoch  5497: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5508: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2790, training loss 0.04655, validation loss 0.03526
Epoch  5519: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5530: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2800, training loss 0.01525, validation loss 0.00847
Resetting learning rate to 0.01000
Epoch  5541: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5552: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2810, training loss 0.01211, validation loss 0.01287
Epoch  5563: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2820, training loss 0.00692, validation loss 0.00929
Epoch  5574: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2821
Mean train loss for ascent epoch 2822: -0.00026966113364323974
Mean eval for ascent epoch 2822: 0.009840480983257294
Epoch  5585: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2830, training loss 0.05644, validation loss 0.06254
Epoch  5596: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5607: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2840, training loss 0.01533, validation loss 0.01737
Epoch  5618: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5629: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2850, training loss 0.00992, validation loss 0.00832
Epoch  5640: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5651: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2860
Doing Evaluation on the model now
This is Epoch 2860, training loss 0.00683, validation loss 0.00733
Mean train loss for ascent epoch 2861: -0.0001561004319228232
Mean eval for ascent epoch 2861: 0.006823997013270855
Epoch  5662: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2870, training loss 0.12499, validation loss 0.15701
Epoch  5673: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5684: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2880, training loss 0.04206, validation loss 0.07951
Epoch  5695: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5706: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2890, training loss 0.00801, validation loss 0.00793
Epoch  5717: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5728: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2899
Mean train loss for ascent epoch 2900: -0.0002505182346794754
Mean eval for ascent epoch 2900: 0.006699742283672094
Doing Evaluation on the model now
This is Epoch 2900, training loss 0.00670, validation loss 0.00617
Epoch  5739: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2910, training loss 0.14067, validation loss 0.14015
Epoch  5750: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5761: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2920, training loss 0.01706, validation loss 0.01727
Epoch  5772: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5783: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2930, training loss 0.00951, validation loss 0.00732
Epoch  5794: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5805: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2938
Mean train loss for ascent epoch 2939: -0.00021607436065096408
Mean eval for ascent epoch 2939: 0.0073798783123493195
Doing Evaluation on the model now
This is Epoch 2940, training loss 0.11984, validation loss 0.02890
Epoch  5816: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5827: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2950, training loss 0.02150, validation loss 0.01630
Epoch  5838: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2960, training loss 0.02774, validation loss 0.00881
Epoch  5849: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5860: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2970, training loss 0.00763, validation loss 0.00786
Epoch  5871: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5882: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2977
Mean train loss for ascent epoch 2978: -0.0001780797028914094
Mean eval for ascent epoch 2978: 0.0066252960823476315
Doing Evaluation on the model now
This is Epoch 2980, training loss 0.24004, validation loss 0.50669
Epoch  5893: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5904: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2990, training loss 0.03999, validation loss 0.01435
Epoch  5915: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5926: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3000, training loss 0.01178, validation loss 0.01329
Resetting learning rate to 0.01000
Epoch  5937: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 3010, training loss 0.00927, validation loss 0.00917
Epoch  5948: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5959: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3020, training loss 0.00697, validation loss 0.00625
Epoch  5970: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3022
Mean train loss for ascent epoch 3023: -0.0002674520656000823
Mean eval for ascent epoch 3023: 0.006716872565448284
Epoch  5981: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3030, training loss 0.07612, validation loss 0.34596
Epoch  5992: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6003: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3040, training loss 0.01771, validation loss 0.01295
Epoch  6014: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6025: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3050, training loss 0.00789, validation loss 0.00565
Epoch  6036: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3060, training loss 0.00644, validation loss 0.00730
Epoch  6047: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3061
Mean train loss for ascent epoch 3062: -0.00025231397012248635
Mean eval for ascent epoch 3062: 0.006693619769066572
Epoch  6058: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3070, training loss 0.07998, validation loss 0.09146
Epoch  6069: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6080: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3080, training loss 0.01893, validation loss 0.01597
Epoch  6091: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6102: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3090, training loss 0.00765, validation loss 0.00622
Epoch  6113: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6124: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3100
Doing Evaluation on the model now
This is Epoch 3100, training loss 0.00663, validation loss 0.00692
Mean train loss for ascent epoch 3101: -0.00024923012824729085
Mean eval for ascent epoch 3101: 0.006544430274516344
Epoch  6135: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3110, training loss 0.08294, validation loss 0.06123
Epoch  6146: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6157: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3120, training loss 0.01438, validation loss 0.00881
Epoch  6168: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6179: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3130, training loss 0.00779, validation loss 0.00641
Epoch  6190: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6201: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3139
Mean train loss for ascent epoch 3140: -0.00023489646264351904
Mean eval for ascent epoch 3140: 0.006465099286288023
Doing Evaluation on the model now
This is Epoch 3140, training loss 0.00647, validation loss 0.00584
Epoch  6212: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6223: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3150, training loss 0.10860, validation loss 0.14009
Epoch  6234: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3160, training loss 0.02570, validation loss 0.03486
Epoch  6245: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6256: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3170, training loss 0.00754, validation loss 0.00705
Epoch  6267: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6278: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3178
Mean train loss for ascent epoch 3179: -0.00018668676784727722
Mean eval for ascent epoch 3179: 0.0075888438150286674
Doing Evaluation on the model now
This is Epoch 3180, training loss 0.13707, validation loss 0.09772
Epoch  6289: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6300: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3190, training loss 0.02868, validation loss 0.02076
Epoch  6311: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6322: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3200, training loss 0.01640, validation loss 0.02307
Resetting learning rate to 0.01000
Epoch  6333: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 3210, training loss 0.01105, validation loss 0.01681
Epoch  6344: reducing learning rate of group 0 to 2.5000e-04.
Epoch  6355: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3220, training loss 0.00596, validation loss 0.00565
Epoch  6366: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3222
Mean train loss for ascent epoch 3223: -0.00032498061773367226
Mean eval for ascent epoch 3223: 0.005765409674495459
Epoch  6377: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3230, training loss 0.05977, validation loss 0.05271
Epoch  6388: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6399: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3240, training loss 0.00988, validation loss 0.01023
Epoch  6410: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6421: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3250, training loss 0.00910, validation loss 0.00665
Epoch  6432: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3260, training loss 0.00593, validation loss 0.00599
Epoch  6443: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3261
Mean train loss for ascent epoch 3262: -0.00025873410049825907
Mean eval for ascent epoch 3262: 0.008290317840874195
Epoch  6454: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3270, training loss 0.04941, validation loss 0.03355
Epoch  6465: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6476: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3280, training loss 0.01718, validation loss 0.01982
Epoch  6487: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6498: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3290, training loss 0.00767, validation loss 0.00882
Epoch  6509: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6520: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3300
Doing Evaluation on the model now
This is Epoch 3300, training loss 0.00600, validation loss 0.00614
Mean train loss for ascent epoch 3301: -0.00023642466112505645
Mean eval for ascent epoch 3301: 0.006083793472498655
Epoch  6531: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3310, training loss 0.05527, validation loss 0.08767
Epoch  6542: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6553: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3320, training loss 0.00894, validation loss 0.00884
Epoch  6564: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6575: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3330, training loss 0.00596, validation loss 0.00654
Epoch  6586: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6597: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3339
Mean train loss for ascent epoch 3340: -0.0001745788613334298
Mean eval for ascent epoch 3340: 0.005724071059376001
Doing Evaluation on the model now
This is Epoch 3340, training loss 0.00572, validation loss 0.00940
Epoch  6608: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3350, training loss 0.08020, validation loss 0.10237
Epoch  6619: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6630: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3360, training loss 0.01365, validation loss 0.01867
Epoch  6641: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6652: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3370, training loss 0.00649, validation loss 0.00597
Epoch  6663: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6674: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3378
Mean train loss for ascent epoch 3379: -0.00019201230315957218
Mean eval for ascent epoch 3379: 0.006620722357183695
Doing Evaluation on the model now
This is Epoch 3380, training loss 0.04543, validation loss 0.05182
Epoch  6685: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6696: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3390, training loss 0.02505, validation loss 0.04113
Epoch  6707: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3400, training loss 0.01759, validation loss 0.02398
Epoch  6718: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  6729: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 3410, training loss 0.00627, validation loss 0.00655
Epoch  6740: reducing learning rate of group 0 to 2.5000e-04.
Epoch  6751: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3420, training loss 0.00536, validation loss 0.00611
Epoch  6762: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3423
Mean train loss for ascent epoch 3424: -0.0002366649714531377
Mean eval for ascent epoch 3424: 0.005232173949480057
Epoch  6773: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3430, training loss 0.08321, validation loss 0.16444
Epoch  6784: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6795: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3440, training loss 0.04736, validation loss 0.06287
Epoch  6806: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3450, training loss 0.01673, validation loss 0.02383
Epoch  6817: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6828: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3460, training loss 0.00585, validation loss 0.00515
Epoch  6839: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3462
Mean train loss for ascent epoch 3463: -0.0002675841096788645
Mean eval for ascent epoch 3463: 0.007554264273494482
Epoch  6850: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3470, training loss 0.05906, validation loss 0.02157
Epoch  6861: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6872: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3480, training loss 0.02662, validation loss 0.00941
Epoch  6883: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6894: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3490, training loss 0.00555, validation loss 0.00531
Epoch  6905: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3500, training loss 0.00654, validation loss 0.00681
Epoch  6916: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3501
Mean train loss for ascent epoch 3502: -0.00018461959552951157
Mean eval for ascent epoch 3502: 0.006354017648845911
Epoch  6927: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3510, training loss 0.08593, validation loss 0.09250
Epoch  6938: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6949: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3520, training loss 0.02478, validation loss 0.02031
Epoch  6960: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6971: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3530, training loss 0.00614, validation loss 0.00630
Epoch  6982: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6993: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3540
Doing Evaluation on the model now
This is Epoch 3540, training loss 0.00751, validation loss 0.00572
Mean train loss for ascent epoch 3541: -0.00020634540123865008
Mean eval for ascent epoch 3541: 0.005934859625995159
Epoch  7004: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3550, training loss 0.03932, validation loss 0.03267
Epoch  7015: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7026: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3560, training loss 0.00832, validation loss 0.00785
Epoch  7037: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7048: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3570, training loss 0.00560, validation loss 0.00555
Epoch  7059: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7070: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3579
Mean train loss for ascent epoch 3580: -0.00023051702009979635
Mean eval for ascent epoch 3580: 0.005474678240716457
Doing Evaluation on the model now
This is Epoch 3580, training loss 0.00547, validation loss 0.00486
Epoch  7081: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7092: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3590, training loss 0.04109, validation loss 0.04614
Epoch  7103: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3600, training loss 0.01894, validation loss 0.01294
Resetting learning rate to 0.01000
Epoch  7114: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7125: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3610, training loss 0.00642, validation loss 0.00648
Epoch  7136: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7147: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3618
Mean train loss for ascent epoch 3619: -0.0001508997374912724
Mean eval for ascent epoch 3619: 0.005940679926425219
Doing Evaluation on the model now
This is Epoch 3620, training loss 0.11546, validation loss 0.10708
Epoch  7158: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7169: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3630, training loss 0.06979, validation loss 0.08336
Epoch  7180: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7191: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3640, training loss 0.01314, validation loss 0.00903
Epoch  7202: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3650, training loss 0.00524, validation loss 0.00698
Epoch  7213: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7224: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3657
Mean train loss for ascent epoch 3658: -0.0001144182970165275
Mean eval for ascent epoch 3658: 0.005016296170651913
Doing Evaluation on the model now
This is Epoch 3660, training loss 0.07641, validation loss 0.11785
Epoch  7235: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7246: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3670, training loss 0.02408, validation loss 0.03454
Epoch  7257: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7268: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3680, training loss 0.00983, validation loss 0.01306
Epoch  7279: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7290: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3690, training loss 0.00624, validation loss 0.00746
Epoch  7301: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3696
Mean train loss for ascent epoch 3697: -0.0001487962290411815
Mean eval for ascent epoch 3697: 0.0050608329474925995
Doing Evaluation on the model now
This is Epoch 3700, training loss 0.57451, validation loss 0.27327
Epoch  7312: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7323: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3710, training loss 0.02920, validation loss 0.02613
Epoch  7334: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7345: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3720, training loss 0.00805, validation loss 0.00611
Epoch  7356: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7367: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3730, training loss 0.00698, validation loss 0.00520
Epoch  7378: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3735
Mean train loss for ascent epoch 3736: -0.00022206261928658932
Mean eval for ascent epoch 3736: 0.005915225949138403
Doing Evaluation on the model now
This is Epoch 3740, training loss 0.57937, validation loss 0.67491
Epoch  7389: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7400: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3750, training loss 0.04562, validation loss 0.06848
Epoch  7411: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7422: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3760, training loss 0.00676, validation loss 0.00900
Epoch  7433: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7444: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3770, training loss 0.00573, validation loss 0.00718
Epoch  7455: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3774
Mean train loss for ascent epoch 3775: -0.0001654407533351332
Mean eval for ascent epoch 3775: 0.006384433712810278
Epoch  7466: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3780, training loss 0.07549, validation loss 0.06469
Epoch  7477: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3790, training loss 0.01568, validation loss 0.00972
Epoch  7488: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7499: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3800, training loss 0.00855, validation loss 0.01551
Resetting learning rate to 0.01000
Epoch  7510: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7521: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3810, training loss 0.00564, validation loss 0.00510
Epoch  7532: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7543: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3818
Mean train loss for ascent epoch 3819: -0.00014853513857815415
Mean eval for ascent epoch 3819: 0.0050123040564358234
Doing Evaluation on the model now
This is Epoch 3820, training loss 0.06258, validation loss 0.04547
Epoch  7554: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7565: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3830, training loss 0.08330, validation loss 0.08265
Epoch  7576: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3840, training loss 0.02272, validation loss 0.02894
Epoch  7587: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7598: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3850, training loss 0.00604, validation loss 0.00869
Epoch  7609: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7620: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3857
Mean train loss for ascent epoch 3858: -0.00019224817515350878
Mean eval for ascent epoch 3858: 0.007110006641596556
Doing Evaluation on the model now
This is Epoch 3860, training loss 0.58997, validation loss 0.73781
Epoch  7631: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7642: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3870, training loss 0.02186, validation loss 0.01938
Epoch  7653: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7664: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3880, training loss 0.00720, validation loss 0.00736
Epoch  7675: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3890, training loss 0.00602, validation loss 0.00693
Epoch  7686: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7697: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3896
Mean train loss for ascent epoch 3897: -0.00019643969426397234
Mean eval for ascent epoch 3897: 0.005358292255550623
Doing Evaluation on the model now
This is Epoch 3900, training loss 0.14788, validation loss 0.39312
Epoch  7708: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7719: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3910, training loss 0.02591, validation loss 0.03375
Epoch  7730: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7741: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3920, training loss 0.00673, validation loss 0.00615
Epoch  7752: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7763: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3930, training loss 0.00596, validation loss 0.00628
Epoch  7774: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3935
Mean train loss for ascent epoch 3936: -0.00016545646940357983
Mean eval for ascent epoch 3936: 0.005676817148923874
Doing Evaluation on the model now
This is Epoch 3940, training loss 1.00752, validation loss 0.94118
Epoch  7785: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7796: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3950, training loss 0.03496, validation loss 0.05168
Epoch  7807: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7818: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3960, training loss 0.00953, validation loss 0.01310
Epoch  7829: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7840: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3970, training loss 0.00663, validation loss 0.00501
Epoch  7851: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3974
Mean train loss for ascent epoch 3975: -0.00016459084872622043
Mean eval for ascent epoch 3975: 0.005623320583254099
Epoch  7862: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3980, training loss 0.73617, validation loss 1.00117
Epoch  7873: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3990, training loss 0.04392, validation loss 0.01838
Epoch  7884: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7895: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4000, training loss 0.01167, validation loss 0.01546
Resetting learning rate to 0.01000
Epoch  7906: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7917: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4010, training loss 0.00603, validation loss 0.00769
Epoch  7928: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7939: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4019
Mean train loss for ascent epoch 4020: -0.0002253653365187347
Mean eval for ascent epoch 4020: 0.005891565699130297
Doing Evaluation on the model now
This is Epoch 4020, training loss 0.00589, validation loss 0.00550
Epoch  7950: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7961: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4030, training loss 0.12472, validation loss 0.24598
Epoch  7972: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4040, training loss 0.01684, validation loss 0.02381
Epoch  7983: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7994: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4050, training loss 0.00648, validation loss 0.00558
Epoch  8005: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8016: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4058
Mean train loss for ascent epoch 4059: -0.00016720897110644728
Mean eval for ascent epoch 4059: 0.005795599427074194
Doing Evaluation on the model now
This is Epoch 4060, training loss 0.20031, validation loss 0.30031
Epoch  8027: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8038: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4070, training loss 0.01873, validation loss 0.03304
Epoch  8049: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8060: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4080, training loss 0.02020, validation loss 0.04441
Epoch  8071: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4090, training loss 0.00643, validation loss 0.00775
Epoch  8082: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8093: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4097
Mean train loss for ascent epoch 4098: -0.0001763582113198936
Mean eval for ascent epoch 4098: 0.0075872414745390415
Doing Evaluation on the model now
This is Epoch 4100, training loss 0.31376, validation loss 0.57931
Epoch  8104: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8115: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4110, training loss 0.03217, validation loss 0.01507
Epoch  8126: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8137: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4120, training loss 0.00992, validation loss 0.01303
Epoch  8148: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8159: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4130, training loss 0.00787, validation loss 0.00670
Epoch  8170: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4136
Mean train loss for ascent epoch 4137: -0.00022611460008192807
Mean eval for ascent epoch 4137: 0.005586667917668819
Doing Evaluation on the model now
This is Epoch 4140, training loss 0.38899, validation loss 0.28548
Epoch  8181: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8192: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4150, training loss 0.04375, validation loss 0.02592
Epoch  8203: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8214: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4160, training loss 0.00881, validation loss 0.00582
Epoch  8225: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8236: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4170, training loss 0.00628, validation loss 0.00789
Epoch  8247: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4175
Mean train loss for ascent epoch 4176: -0.0001778881560312584
Mean eval for ascent epoch 4176: 0.005519770551472902
Doing Evaluation on the model now
This is Epoch 4180, training loss 0.52742, validation loss 0.24682
Epoch  8258: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8269: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4190, training loss 0.06977, validation loss 0.08279
Epoch  8280: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8291: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4200, training loss 0.00742, validation loss 0.00762
Resetting learning rate to 0.01000
Epoch  8302: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8313: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4210, training loss 0.00572, validation loss 0.00537
Epoch  8324: reducing learning rate of group 0 to 1.2500e-04.
Epoch  8335: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4219
Mean train loss for ascent epoch 4220: -0.00013323299936018884
Mean eval for ascent epoch 4220: 0.006230698432773352
Doing Evaluation on the model now
This is Epoch 4220, training loss 0.00623, validation loss 0.00433
Epoch  8346: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4230, training loss 0.07000, validation loss 0.04395
Epoch  8357: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8368: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4240, training loss 0.02449, validation loss 0.01806
Epoch  8379: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8390: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4250, training loss 0.00583, validation loss 0.00655
Epoch  8401: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8412: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4258
Mean train loss for ascent epoch 4259: -0.00017229402146767825
Mean eval for ascent epoch 4259: 0.005258369259536266
Doing Evaluation on the model now
This is Epoch 4260, training loss 0.20020, validation loss 0.56351
Epoch  8423: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8434: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4270, training loss 0.03889, validation loss 0.01740
Epoch  8445: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4280, training loss 0.01766, validation loss 0.02025
Epoch  8456: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8467: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4290, training loss 0.00756, validation loss 0.00610
Epoch  8478: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8489: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4297
Mean train loss for ascent epoch 4298: -0.00024608627427369356
Mean eval for ascent epoch 4298: 0.00669544842094183
Doing Evaluation on the model now
This is Epoch 4300, training loss 0.43924, validation loss 0.98012
Epoch  8500: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8511: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4310, training loss 0.03313, validation loss 0.01979
Epoch  8522: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8533: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4320, training loss 0.00795, validation loss 0.00787
Epoch  8544: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4330, training loss 0.00617, validation loss 0.00622
Epoch  8555: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8566: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4336
Mean train loss for ascent epoch 4337: -0.0002838531509041786
Mean eval for ascent epoch 4337: 0.007520821411162615
Doing Evaluation on the model now
This is Epoch 4340, training loss 0.25324, validation loss 0.85948
Epoch  8577: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8588: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4350, training loss 0.02038, validation loss 0.04361
Epoch  8599: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8610: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4360, training loss 0.01634, validation loss 0.02072
Epoch  8621: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8632: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4370, training loss 0.00658, validation loss 0.00630
Epoch  8643: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4375
Mean train loss for ascent epoch 4376: -0.00039666015072725713
Mean eval for ascent epoch 4376: 0.007803985383361578
Doing Evaluation on the model now
This is Epoch 4380, training loss 0.17337, validation loss 0.14731
Epoch  8654: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8665: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4390, training loss 0.04035, validation loss 0.02557
Epoch  8676: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8687: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4400, training loss 0.01001, validation loss 0.00728
Resetting learning rate to 0.01000
Epoch  8698: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8709: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4410, training loss 0.00652, validation loss 0.00667
Epoch  8720: reducing learning rate of group 0 to 1.2500e-04.
Epoch  8731: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4420
Doing Evaluation on the model now
This is Epoch 4420, training loss 0.00563, validation loss 0.00652
Mean train loss for ascent epoch 4421: -0.00012816312664654106
Mean eval for ascent epoch 4421: 0.005570793524384499
Epoch  8742: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4430, training loss 0.07425, validation loss 0.16203
Epoch  8753: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8764: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4440, training loss 0.03786, validation loss 0.03540
Epoch  8775: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8786: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4450, training loss 0.00779, validation loss 0.00934
Epoch  8797: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8808: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4459
Mean train loss for ascent epoch 4460: -0.00019048024842049927
Mean eval for ascent epoch 4460: 0.006705011241137981
Doing Evaluation on the model now
This is Epoch 4460, training loss 0.00671, validation loss 0.00693
Epoch  8819: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8830: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4470, training loss 0.04996, validation loss 0.03282
Epoch  8841: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4480, training loss 0.01608, validation loss 0.01920
Epoch  8852: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8863: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4490, training loss 0.00742, validation loss 0.00649
Epoch  8874: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8885: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4498
Mean train loss for ascent epoch 4499: -0.0001648241450311616
Mean eval for ascent epoch 4499: 0.005969167221337557
Doing Evaluation on the model now
This is Epoch 4500, training loss 0.30508, validation loss 1.31963
Epoch  8896: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8907: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4510, training loss 0.04192, validation loss 0.03286
Epoch  8918: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8929: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4520, training loss 0.02737, validation loss 0.03121
Epoch  8940: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4530, training loss 0.00680, validation loss 0.00620
Epoch  8951: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8962: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4537
Mean train loss for ascent epoch 4538: -0.00016889547987375408
Mean eval for ascent epoch 4538: 0.006375519558787346
Doing Evaluation on the model now
This is Epoch 4540, training loss 0.18650, validation loss 0.33603
Epoch  8973: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8984: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4550, training loss 0.05138, validation loss 0.01658
Epoch  8995: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9006: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4560, training loss 0.01039, validation loss 0.00698
Epoch  9017: reducing learning rate of group 0 to 3.1250e-04.
Epoch  9028: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4570, training loss 0.00851, validation loss 0.00781
Epoch  9039: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4576
Mean train loss for ascent epoch 4577: -0.0002499941037967801
Mean eval for ascent epoch 4577: 0.006211492232978344
Doing Evaluation on the model now
This is Epoch 4580, training loss 0.24285, validation loss 0.21104
Epoch  9050: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9061: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4590, training loss 0.04335, validation loss 0.04879
Epoch  9072: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9083: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4600, training loss 0.00791, validation loss 0.00599
Resetting learning rate to 0.01000
Epoch  9094: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9105: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4610, training loss 0.00733, validation loss 0.00691
Epoch  9116: reducing learning rate of group 0 to 1.2500e-04.
Epoch  9127: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4620
Doing Evaluation on the model now
This is Epoch 4620, training loss 0.00589, validation loss 0.00532
Mean train loss for ascent epoch 4621: -0.00013838184531778097
Mean eval for ascent epoch 4621: 0.006130988243967295
Epoch  9138: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4630, training loss 0.03125, validation loss 0.09519
Epoch  9149: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9160: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4640, training loss 0.02197, validation loss 0.02053
Epoch  9171: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9182: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4650, training loss 0.00645, validation loss 0.00556
Epoch  9193: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9204: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4659
Mean train loss for ascent epoch 4660: -0.00015929577057249844
Mean eval for ascent epoch 4660: 0.005266430322080851
Doing Evaluation on the model now
This is Epoch 4660, training loss 0.00527, validation loss 0.00540
Epoch  9215: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4670, training loss 0.04190, validation loss 0.09735
Epoch  9226: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9237: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4680, training loss 0.02062, validation loss 0.00794
Epoch  9248: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9259: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4690, training loss 0.00604, validation loss 0.00800
Epoch  9270: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9281: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4698
Mean train loss for ascent epoch 4699: -0.00020662258611992002
Mean eval for ascent epoch 4699: 0.006447126157581806
Doing Evaluation on the model now
This is Epoch 4700, training loss 0.18411, validation loss 0.45809
Epoch  9292: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9303: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4710, training loss 0.02801, validation loss 0.02851
Epoch  9314: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4720, training loss 0.01515, validation loss 0.00792
Epoch  9325: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9336: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4730, training loss 0.00666, validation loss 0.01131
Epoch  9347: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9358: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4737
Mean train loss for ascent epoch 4738: -0.00015690512373112142
Mean eval for ascent epoch 4738: 0.005631708074361086
Doing Evaluation on the model now
This is Epoch 4740, training loss 0.21438, validation loss 0.11476
Epoch  9369: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9380: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4750, training loss 0.03509, validation loss 0.05472
Epoch  9391: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9402: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4760, training loss 0.01177, validation loss 0.00718
Epoch  9413: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4770, training loss 0.00592, validation loss 0.00726
Epoch  9424: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9435: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4776
Mean train loss for ascent epoch 4777: -0.00018701398221310228
Mean eval for ascent epoch 4777: 0.006131191737949848
Doing Evaluation on the model now
This is Epoch 4780, training loss 0.67765, validation loss 0.53269
Epoch  9446: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9457: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4790, training loss 0.04024, validation loss 0.02511
Epoch  9468: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9479: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4800, training loss 0.00725, validation loss 0.00651
Resetting learning rate to 0.01000
Epoch  9490: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9501: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4810, training loss 0.00822, validation loss 0.00617
Epoch  9512: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4820, training loss 0.00540, validation loss 0.00574
Epoch  9523: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4821
Mean train loss for ascent epoch 4822: -0.00016233888163696975
Mean eval for ascent epoch 4822: 0.005204497836530209
Epoch  9534: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4830, training loss 0.13648, validation loss 0.18990
Epoch  9545: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9556: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4840, training loss 0.01514, validation loss 0.01565
Epoch  9567: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9578: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4850, training loss 0.00583, validation loss 0.00610
Epoch  9589: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9600: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4860
Doing Evaluation on the model now
This is Epoch 4860, training loss 0.00447, validation loss 0.00461
Mean train loss for ascent epoch 4861: -0.00017744569049682468
Mean eval for ascent epoch 4861: 0.00494248466566205
Epoch  9611: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4870, training loss 0.08831, validation loss 0.14768
Epoch  9622: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9633: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4880, training loss 0.01080, validation loss 0.00676
Epoch  9644: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9655: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4890, training loss 0.00451, validation loss 0.00457
Epoch  9666: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9677: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4899
Mean train loss for ascent epoch 4900: -0.00015868243644945323
Mean eval for ascent epoch 4900: 0.004378172568976879
Doing Evaluation on the model now
This is Epoch 4900, training loss 0.00438, validation loss 0.00450
Epoch  9688: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9699: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4910, training loss 0.03760, validation loss 0.04371
Epoch  9710: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4920, training loss 0.01350, validation loss 0.00887
Epoch  9721: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9732: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4930, training loss 0.00650, validation loss 0.00487
Epoch  9743: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9754: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4938
Mean train loss for ascent epoch 4939: -0.00016244415019173175
Mean eval for ascent epoch 4939: 0.005205006338655949
Doing Evaluation on the model now
This is Epoch 4940, training loss 0.14428, validation loss 0.20779
Epoch  9765: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9776: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4950, training loss 0.05102, validation loss 0.01666
Epoch  9787: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9798: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4960, training loss 0.01135, validation loss 0.00757
Epoch  9809: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4970, training loss 0.00685, validation loss 0.00574
Epoch  9820: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9831: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4977
Mean train loss for ascent epoch 4978: -0.00016035181761253625
Mean eval for ascent epoch 4978: 0.004503127187490463
Doing Evaluation on the model now
This is Epoch 4980, training loss 0.27809, validation loss 0.34731
Epoch  9842: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9853: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4990, training loss 0.03831, validation loss 0.02216
Epoch  9864: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9875: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5000, training loss 0.00751, validation loss 0.00586
Resetting learning rate to 0.01000
Epoch  9886: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9897: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5010, training loss 0.00625, validation loss 0.00918
Epoch  9908: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5020, training loss 0.00409, validation loss 0.00535
Epoch  9919: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5021
Mean train loss for ascent epoch 5022: -0.0001835298171499744
Mean eval for ascent epoch 5022: 0.004318809136748314
Epoch  9930: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5030, training loss 0.08915, validation loss 0.06594
Epoch  9941: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9952: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5040, training loss 0.01472, validation loss 0.01390
Epoch  9963: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9974: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5050, training loss 0.00587, validation loss 0.00535
Epoch  9985: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9996: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5060
Doing Evaluation on the model now
This is Epoch 5060, training loss 0.00519, validation loss 0.00576
Mean train loss for ascent epoch 5061: -0.00014266786456573755
Mean eval for ascent epoch 5061: 0.004788039717823267
Epoch 10007: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5070, training loss 0.05991, validation loss 0.11063
Epoch 10018: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10029: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5080, training loss 0.01592, validation loss 0.01166
Epoch 10040: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10051: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5090, training loss 0.00607, validation loss 0.00499
Epoch 10062: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10073: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5099
Mean train loss for ascent epoch 5100: -0.0001188203677884303
Mean eval for ascent epoch 5100: 0.004702830221503973
Doing Evaluation on the model now
This is Epoch 5100, training loss 0.00470, validation loss 0.00403
Epoch 10084: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5110, training loss 0.07615, validation loss 0.12215
Epoch 10095: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10106: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5120, training loss 0.02876, validation loss 0.04569
Epoch 10117: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10128: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5130, training loss 0.00671, validation loss 0.00612
Epoch 10139: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10150: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5138
Mean train loss for ascent epoch 5139: -0.0001327792415395379
Mean eval for ascent epoch 5139: 0.006719876080751419
Doing Evaluation on the model now
This is Epoch 5140, training loss 0.10957, validation loss 0.04702
Epoch 10161: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10172: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5150, training loss 0.07855, validation loss 0.03550
Epoch 10183: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5160, training loss 0.01235, validation loss 0.01202
Epoch 10194: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10205: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5170, training loss 0.00448, validation loss 0.00744
Epoch 10216: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10227: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5177
Mean train loss for ascent epoch 5178: -0.00014524732250720263
Mean eval for ascent epoch 5178: 0.004371679853647947
Doing Evaluation on the model now
This is Epoch 5180, training loss 1.62442, validation loss 1.34030
Epoch 10238: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10249: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5190, training loss 0.02651, validation loss 0.04325
Epoch 10260: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10271: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5200, training loss 0.01238, validation loss 0.01418
Resetting learning rate to 0.01000
Epoch 10282: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5210, training loss 0.00643, validation loss 0.00992
Epoch 10293: reducing learning rate of group 0 to 2.5000e-04.
Epoch 10304: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5220, training loss 0.00445, validation loss 0.00502
Epoch 10315: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5222
Mean train loss for ascent epoch 5223: -0.00015613601135555655
Mean eval for ascent epoch 5223: 0.0041469489224255085
Epoch 10326: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5230, training loss 0.06945, validation loss 0.05437
Epoch 10337: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10348: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5240, training loss 0.02492, validation loss 0.03754
Epoch 10359: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10370: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5250, training loss 0.00555, validation loss 0.00466
Epoch 10381: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5260, training loss 0.00430, validation loss 0.00466
Epoch 10392: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5261
Mean train loss for ascent epoch 5262: -0.0001698731502983719
Mean eval for ascent epoch 5262: 0.0043573095463216305
Epoch 10403: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5270, training loss 0.04669, validation loss 0.04226
Epoch 10414: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10425: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5280, training loss 0.01229, validation loss 0.01915
Epoch 10436: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10447: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5290, training loss 0.00812, validation loss 0.00740
Epoch 10458: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10469: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5300
Doing Evaluation on the model now
This is Epoch 5300, training loss 0.00393, validation loss 0.00381
Mean train loss for ascent epoch 5301: -0.000182439194759354
Mean eval for ascent epoch 5301: 0.0042018950916826725
Epoch 10480: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5310, training loss 0.06880, validation loss 0.02397
Epoch 10491: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10502: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5320, training loss 0.01042, validation loss 0.01244
Epoch 10513: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10524: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5330, training loss 0.00460, validation loss 0.00540
Epoch 10535: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10546: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5339
Mean train loss for ascent epoch 5340: -0.00016894245345611125
Mean eval for ascent epoch 5340: 0.004748881328850985
Doing Evaluation on the model now
This is Epoch 5340, training loss 0.00475, validation loss 0.00498
Epoch 10557: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10568: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5350, training loss 0.05056, validation loss 0.05611
Epoch 10579: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5360, training loss 0.01396, validation loss 0.02126
Epoch 10590: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10601: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5370, training loss 0.00423, validation loss 0.00399
Epoch 10612: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10623: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5378
Mean train loss for ascent epoch 5379: -0.00010701832798076794
Mean eval for ascent epoch 5379: 0.004177498631179333
Doing Evaluation on the model now
This is Epoch 5380, training loss 0.09196, validation loss 0.12799
Epoch 10634: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10645: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5390, training loss 0.03774, validation loss 0.06698
Epoch 10656: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10667: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5400, training loss 0.01064, validation loss 0.00761
Resetting learning rate to 0.01000
Epoch 10678: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5410, training loss 0.00677, validation loss 0.00591
Epoch 10689: reducing learning rate of group 0 to 2.5000e-04.
Epoch 10700: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5420, training loss 0.00477, validation loss 0.00475
Epoch 10711: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5422
Mean train loss for ascent epoch 5423: -0.00014963271678425372
Mean eval for ascent epoch 5423: 0.005481827538460493
Epoch 10722: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5430, training loss 0.04849, validation loss 0.03397
Epoch 10733: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10744: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5440, training loss 0.01587, validation loss 0.02728
Epoch 10755: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10766: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5450, training loss 0.01164, validation loss 0.00646
Epoch 10777: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5460, training loss 0.00453, validation loss 0.00388
Epoch 10788: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5461
Mean train loss for ascent epoch 5462: -0.00017009164730552584
Mean eval for ascent epoch 5462: 0.004102089907974005
Epoch 10799: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5470, training loss 0.03852, validation loss 0.04542
Epoch 10810: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10821: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5480, training loss 0.02202, validation loss 0.01753
Epoch 10832: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10843: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5490, training loss 0.00598, validation loss 0.00414
Epoch 10854: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10865: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5500
Doing Evaluation on the model now
This is Epoch 5500, training loss 0.00403, validation loss 0.00398
Mean train loss for ascent epoch 5501: -0.00018676860781852156
Mean eval for ascent epoch 5501: 0.00447879871353507
Epoch 10876: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5510, training loss 0.03637, validation loss 0.02792
Epoch 10887: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10898: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5520, training loss 0.01773, validation loss 0.00946
Epoch 10909: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10920: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5530, training loss 0.00520, validation loss 0.00415
Epoch 10931: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10942: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5539
Mean train loss for ascent epoch 5540: -0.0001283642341149971
Mean eval for ascent epoch 5540: 0.003935800399631262
Doing Evaluation on the model now
This is Epoch 5540, training loss 0.00394, validation loss 0.00382
Epoch 10953: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5550, training loss 0.05458, validation loss 0.02586
Epoch 10964: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10975: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5560, training loss 0.01192, validation loss 0.01304
Epoch 10986: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10997: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5570, training loss 0.00513, validation loss 0.00476
Epoch 11008: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11019: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5578
Mean train loss for ascent epoch 5579: -9.340532415080816e-05
Mean eval for ascent epoch 5579: 0.004660466685891151
Doing Evaluation on the model now
This is Epoch 5580, training loss 0.16491, validation loss 0.31736
Epoch 11030: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11041: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5590, training loss 0.02125, validation loss 0.02646
Epoch 11052: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5600, training loss 0.01720, validation loss 0.01053
Epoch 11063: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 11074: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5610, training loss 0.00932, validation loss 0.01026
Epoch 11085: reducing learning rate of group 0 to 2.5000e-04.
Epoch 11096: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5620, training loss 0.00502, validation loss 0.00442
Epoch 11107: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5623
Mean train loss for ascent epoch 5624: -0.00014183114399202168
Mean eval for ascent epoch 5624: 0.004936484154313803
Epoch 11118: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5630, training loss 0.14199, validation loss 0.08385
Epoch 11129: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11140: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5640, training loss 0.04166, validation loss 0.01018
Epoch 11151: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5650, training loss 0.00806, validation loss 0.00571
Epoch 11162: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11173: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5660, training loss 0.00477, validation loss 0.00442
Epoch 11184: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5662
Mean train loss for ascent epoch 5663: -0.00014228399959392846
Mean eval for ascent epoch 5663: 0.005397783126682043
Epoch 11195: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5670, training loss 0.09817, validation loss 0.10617
Epoch 11206: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11217: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5680, training loss 0.01760, validation loss 0.00957
Epoch 11228: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11239: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5690, training loss 0.00619, validation loss 0.00568
Epoch 11250: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5700, training loss 0.00424, validation loss 0.00619
Epoch 11261: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5701
Mean train loss for ascent epoch 5702: -9.196026076097041e-05
Mean eval for ascent epoch 5702: 0.004085966851562262
Epoch 11272: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5710, training loss 0.03824, validation loss 0.02734
Epoch 11283: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11294: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5720, training loss 0.01454, validation loss 0.01251
Epoch 11305: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11316: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5730, training loss 0.00508, validation loss 0.00424
Epoch 11327: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11338: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5740
Doing Evaluation on the model now
This is Epoch 5740, training loss 0.00510, validation loss 0.00456
Mean train loss for ascent epoch 5741: -0.0001288180792471394
Mean eval for ascent epoch 5741: 0.004366957116872072
Epoch 11349: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5750, training loss 0.09862, validation loss 0.16959
Epoch 11360: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11371: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5760, training loss 0.01518, validation loss 0.02490
Epoch 11382: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11393: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5770, training loss 0.00513, validation loss 0.00657
Epoch 11404: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11415: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5779
Mean train loss for ascent epoch 5780: -0.00014166142500471324
Mean eval for ascent epoch 5780: 0.004465117119252682
Doing Evaluation on the model now
This is Epoch 5780, training loss 0.00447, validation loss 0.00476
Epoch 11426: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11437: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5790, training loss 0.06240, validation loss 0.04311
Epoch 11448: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5800, training loss 0.01185, validation loss 0.01414
Resetting learning rate to 0.01000
Epoch 11459: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11470: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5810, training loss 0.00553, validation loss 0.00529
Epoch 11481: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11492: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5818
Mean train loss for ascent epoch 5819: -0.00010083829693030566
Mean eval for ascent epoch 5819: 0.004952363204210997
Doing Evaluation on the model now
This is Epoch 5820, training loss 0.09358, validation loss 0.05627
Epoch 11503: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11514: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5830, training loss 0.02354, validation loss 0.02024
Epoch 11525: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11536: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5840, training loss 0.00841, validation loss 0.01087
Epoch 11547: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5850, training loss 0.00452, validation loss 0.00458
Epoch 11558: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11569: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5857
Mean train loss for ascent epoch 5858: -0.00012466921180021018
Mean eval for ascent epoch 5858: 0.003968313802033663
Doing Evaluation on the model now
This is Epoch 5860, training loss 2.31678, validation loss 1.87156
Epoch 11580: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11591: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5870, training loss 0.05247, validation loss 0.02923
Epoch 11602: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11613: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5880, training loss 0.00699, validation loss 0.00619
Epoch 11624: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11635: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5890, training loss 0.00510, validation loss 0.00399
Epoch 11646: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5896
Mean train loss for ascent epoch 5897: -0.0001418679894413799
Mean eval for ascent epoch 5897: 0.004900102969259024
Doing Evaluation on the model now
This is Epoch 5900, training loss 0.27754, validation loss 0.22141
Epoch 11657: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11668: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5910, training loss 0.03582, validation loss 0.04013
Epoch 11679: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11690: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5920, training loss 0.00571, validation loss 0.00788
Epoch 11701: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11712: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5930, training loss 0.00456, validation loss 0.00477
Epoch 11723: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5935
Mean train loss for ascent epoch 5936: -0.00018145970534533262
Mean eval for ascent epoch 5936: 0.004433079622685909
Doing Evaluation on the model now
This is Epoch 5940, training loss 0.18163, validation loss 0.08665
Epoch 11734: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11745: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5950, training loss 0.03251, validation loss 0.04515
Epoch 11756: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11767: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5960, training loss 0.01169, validation loss 0.00822
Epoch 11778: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11789: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5970, training loss 0.00374, validation loss 0.00386
Epoch 11800: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5974
Mean train loss for ascent epoch 5975: -0.00013649901666212827
Mean eval for ascent epoch 5975: 0.00434349849820137
Epoch 11811: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5980, training loss 0.08934, validation loss 0.11732
Epoch 11822: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5990, training loss 0.03177, validation loss 0.03895
Epoch 11833: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11844: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6000, training loss 0.00695, validation loss 0.00527
Resetting learning rate to 0.01000
Epoch 11855: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11866: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6010, training loss 0.00434, validation loss 0.00406
Epoch 11877: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11888: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6018
Mean train loss for ascent epoch 6019: -0.0002520472335163504
Mean eval for ascent epoch 6019: 0.004865200258791447
Doing Evaluation on the model now
This is Epoch 6020, training loss 0.57306, validation loss 1.15812
Epoch 11899: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11910: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6030, training loss 0.05879, validation loss 0.03301
Epoch 11921: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6040, training loss 0.01469, validation loss 0.01902
Epoch 11932: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11943: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6050, training loss 0.00627, validation loss 0.00691
Epoch 11954: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11965: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6057
Mean train loss for ascent epoch 6058: -0.00015290612645912915
Mean eval for ascent epoch 6058: 0.0048936838284134865
Doing Evaluation on the model now
This is Epoch 6060, training loss 0.29369, validation loss 0.39036
Epoch 11976: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11987: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6070, training loss 0.02528, validation loss 0.02831
Epoch 11998: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12009: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6080, training loss 0.01004, validation loss 0.01440
Epoch 12020: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6090, training loss 0.00419, validation loss 0.00399
Epoch 12031: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12042: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6096
Mean train loss for ascent epoch 6097: -0.00021082485909573734
Mean eval for ascent epoch 6097: 0.00370453461073339
Doing Evaluation on the model now
This is Epoch 6100, training loss 0.35421, validation loss 0.19973
Epoch 12053: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12064: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6110, training loss 0.03239, validation loss 0.03045
Epoch 12075: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12086: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6120, training loss 0.00784, validation loss 0.00675
Epoch 12097: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12108: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6130, training loss 0.00455, validation loss 0.00372
Epoch 12119: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6135
Mean train loss for ascent epoch 6136: -0.00014906642900314182
Mean eval for ascent epoch 6136: 0.0035038194619119167
Doing Evaluation on the model now
This is Epoch 6140, training loss 0.18543, validation loss 0.09810
Epoch 12130: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12141: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6150, training loss 0.04877, validation loss 0.03292
Epoch 12152: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12163: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6160, training loss 0.00565, validation loss 0.00814
Epoch 12174: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12185: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6170, training loss 0.00356, validation loss 0.00332
Epoch 12196: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6174
Mean train loss for ascent epoch 6175: -0.00012751345639117062
Mean eval for ascent epoch 6175: 0.00390331051312387
Epoch 12207: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6180, training loss 0.40743, validation loss 0.22901
Epoch 12218: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6190, training loss 0.01872, validation loss 0.01614
Epoch 12229: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12240: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6200, training loss 0.00655, validation loss 0.00831
Resetting learning rate to 0.01000
Epoch 12251: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12262: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6210, training loss 0.00403, validation loss 0.00432
Epoch 12273: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12284: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6219
Mean train loss for ascent epoch 6220: -0.00010989073052769527
Mean eval for ascent epoch 6220: 0.003593253903090954
Doing Evaluation on the model now
This is Epoch 6220, training loss 0.00359, validation loss 0.00357
Epoch 12295: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12306: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6230, training loss 0.02917, validation loss 0.03784
Epoch 12317: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6240, training loss 0.01058, validation loss 0.01062
Epoch 12328: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12339: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6250, training loss 0.00724, validation loss 0.00611
Epoch 12350: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12361: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6258
Mean train loss for ascent epoch 6259: -0.00011041537072742358
Mean eval for ascent epoch 6259: 0.0034805666655302048
Doing Evaluation on the model now
This is Epoch 6260, training loss 0.09631, validation loss 0.05039
Epoch 12372: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12383: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6270, training loss 0.02245, validation loss 0.02536
Epoch 12394: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12405: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6280, training loss 0.00773, validation loss 0.00844
Epoch 12416: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6290, training loss 0.00565, validation loss 0.00576
Epoch 12427: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12438: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6297
Mean train loss for ascent epoch 6298: -0.00014254059351515025
Mean eval for ascent epoch 6298: 0.003926909063011408
Doing Evaluation on the model now
This is Epoch 6300, training loss 0.76776, validation loss 1.43341
Epoch 12449: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12460: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6310, training loss 0.03448, validation loss 0.02209
Epoch 12471: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12482: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6320, training loss 0.01060, validation loss 0.00702
Epoch 12493: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12504: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6330, training loss 0.00486, validation loss 0.00475
Epoch 12515: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6336
Mean train loss for ascent epoch 6337: -0.00012059062282787636
Mean eval for ascent epoch 6337: 0.004099399317055941
Doing Evaluation on the model now
This is Epoch 6340, training loss 0.26646, validation loss 0.59597
Epoch 12526: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12537: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6350, training loss 0.02984, validation loss 0.03878
Epoch 12548: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12559: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6360, training loss 0.00743, validation loss 0.00708
Epoch 12570: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12581: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6370, training loss 0.00407, validation loss 0.00339
Epoch 12592: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6375
Mean train loss for ascent epoch 6376: -9.275925549445674e-05
Mean eval for ascent epoch 6376: 0.0038085579872131348
Doing Evaluation on the model now
This is Epoch 6380, training loss 0.18704, validation loss 0.12156
Epoch 12603: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12614: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6390, training loss 0.03226, validation loss 0.02589
Epoch 12625: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12636: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6400, training loss 0.00641, validation loss 0.00595
Resetting learning rate to 0.01000
Epoch 12647: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12658: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6410, training loss 0.00521, validation loss 0.00458
Epoch 12669: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12680: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6419
Mean train loss for ascent epoch 6420: -0.00023202110605780035
Mean eval for ascent epoch 6420: 0.0036908090114593506
Doing Evaluation on the model now
This is Epoch 6420, training loss 0.00369, validation loss 0.00446
Epoch 12691: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6430, training loss 0.10841, validation loss 0.21723
Epoch 12702: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12713: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6440, training loss 0.00950, validation loss 0.01389
Epoch 12724: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12735: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6450, training loss 0.00388, validation loss 0.00414
Epoch 12746: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12757: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6458
Mean train loss for ascent epoch 6459: -8.455701754428446e-05
Mean eval for ascent epoch 6459: 0.003653724677860737
Doing Evaluation on the model now
This is Epoch 6460, training loss 0.29834, validation loss 0.51609
Epoch 12768: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12779: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6470, training loss 0.04646, validation loss 0.01945
Epoch 12790: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6480, training loss 0.01012, validation loss 0.01007
Epoch 12801: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12812: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6490, training loss 0.00449, validation loss 0.00373
Epoch 12823: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12834: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6497
Mean train loss for ascent epoch 6498: -0.00010467679385328665
Mean eval for ascent epoch 6498: 0.003650498343631625
Doing Evaluation on the model now
This is Epoch 6500, training loss 0.19657, validation loss 0.14524
Epoch 12845: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12856: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6510, training loss 0.01760, validation loss 0.03006
Epoch 12867: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12878: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6520, training loss 0.00771, validation loss 0.00608
Epoch 12889: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6530, training loss 0.00545, validation loss 0.00351
Epoch 12900: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12911: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6536
Mean train loss for ascent epoch 6537: -0.00015350714966189116
Mean eval for ascent epoch 6537: 0.004178951494395733
Doing Evaluation on the model now
This is Epoch 6540, training loss 0.83190, validation loss 0.37673
Epoch 12922: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12933: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6550, training loss 0.05574, validation loss 0.07061
Epoch 12944: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12955: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6560, training loss 0.00637, validation loss 0.00657
Epoch 12966: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12977: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6570, training loss 0.00417, validation loss 0.00383
Epoch 12988: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6575
Mean train loss for ascent epoch 6576: -0.0001698737032711506
Mean eval for ascent epoch 6576: 0.004154312424361706
Doing Evaluation on the model now
This is Epoch 6580, training loss 0.52147, validation loss 0.08369
Epoch 12999: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13010: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6590, training loss 0.01884, validation loss 0.02959
Epoch 13021: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13032: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6600, training loss 0.00585, validation loss 0.00492
Resetting learning rate to 0.01000
Epoch 13043: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13054: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6610, training loss 0.00354, validation loss 0.00328
Epoch 13065: reducing learning rate of group 0 to 1.2500e-04.
Epoch 13076: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6620
Doing Evaluation on the model now
This is Epoch 6620, training loss 0.00349, validation loss 0.00291
Mean train loss for ascent epoch 6621: -0.00019272492500022054
Mean eval for ascent epoch 6621: 0.0039656213484704494
Epoch 13087: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6630, training loss 0.04658, validation loss 0.03088
Epoch 13098: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13109: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6640, training loss 0.00878, validation loss 0.00561
Epoch 13120: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13131: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6650, training loss 0.00407, validation loss 0.00477
Epoch 13142: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13153: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6659
Mean train loss for ascent epoch 6660: -0.0001865618396550417
Mean eval for ascent epoch 6660: 0.004071087576448917
Doing Evaluation on the model now
This is Epoch 6660, training loss 0.00407, validation loss 0.00369
Epoch 13164: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13175: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6670, training loss 0.04787, validation loss 0.05472
Epoch 13186: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6680, training loss 0.01242, validation loss 0.00855
Epoch 13197: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13208: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6690, training loss 0.00408, validation loss 0.00466
Epoch 13219: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13230: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6698
Mean train loss for ascent epoch 6699: -0.00016570916341152042
Mean eval for ascent epoch 6699: 0.0040483009070158005
Doing Evaluation on the model now
This is Epoch 6700, training loss 0.25170, validation loss 0.52579
Epoch 13241: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13252: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6710, training loss 0.03398, validation loss 0.03684
Epoch 13263: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13274: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6720, training loss 0.01905, validation loss 0.00884
Epoch 13285: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6730, training loss 0.00418, validation loss 0.00426
Epoch 13296: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13307: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6737
Mean train loss for ascent epoch 6738: -0.00013076198229100555
Mean eval for ascent epoch 6738: 0.003606123384088278
Doing Evaluation on the model now
This is Epoch 6740, training loss 0.20721, validation loss 0.16556
Epoch 13318: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13329: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6750, training loss 0.01851, validation loss 0.02265
Epoch 13340: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13351: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6760, training loss 0.00780, validation loss 0.00431
Epoch 13362: reducing learning rate of group 0 to 3.1250e-04.
Epoch 13373: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6770, training loss 0.00348, validation loss 0.00324
Epoch 13384: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6776
Mean train loss for ascent epoch 6777: -0.00011838423233712092
Mean eval for ascent epoch 6777: 0.003279763041064143
Doing Evaluation on the model now
This is Epoch 6780, training loss 1.15240, validation loss 1.21940
Epoch 13395: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13406: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6790, training loss 0.02790, validation loss 0.03347
Epoch 13417: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13428: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6800, training loss 0.00533, validation loss 0.00460
Resetting learning rate to 0.01000
Epoch 13439: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13450: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6810, training loss 0.00330, validation loss 0.00289
Epoch 13461: reducing learning rate of group 0 to 1.2500e-04.
Epoch 13472: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6820
Doing Evaluation on the model now
This is Epoch 6820, training loss 0.00307, validation loss 0.00312
Mean train loss for ascent epoch 6821: -0.00013917831529397517
Mean eval for ascent epoch 6821: 0.003094177693128586
Epoch 13483: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6830, training loss 0.09758, validation loss 0.09415
Epoch 13494: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13505: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6840, training loss 0.01374, validation loss 0.01162
Epoch 13516: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13527: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6850, training loss 0.00369, validation loss 0.00331
Epoch 13538: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13549: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6859
Mean train loss for ascent epoch 6860: -9.26315551623702e-05
Mean eval for ascent epoch 6860: 0.0038706024643033743
Doing Evaluation on the model now
This is Epoch 6860, training loss 0.00387, validation loss 0.00481
Epoch 13560: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6870, training loss 0.05728, validation loss 0.03093
Epoch 13571: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13582: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6880, training loss 0.01004, validation loss 0.00864
Epoch 13593: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13604: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6890, training loss 0.00410, validation loss 0.00418
Epoch 13615: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13626: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6898
Mean train loss for ascent epoch 6899: -0.00013255230442155153
Mean eval for ascent epoch 6899: 0.003984447568655014
Doing Evaluation on the model now
This is Epoch 6900, training loss 0.17215, validation loss 0.18472
Epoch 13637: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13648: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6910, training loss 0.03121, validation loss 0.03476
Epoch 13659: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6920, training loss 0.00923, validation loss 0.01064
Epoch 13670: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13681: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6930, training loss 0.00648, validation loss 0.00909
Epoch 13692: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13703: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6937
Mean train loss for ascent epoch 6938: -0.0001227716711582616
Mean eval for ascent epoch 6938: 0.003472207812592387
Doing Evaluation on the model now
This is Epoch 6940, training loss 0.06511, validation loss 0.07195
Epoch 13714: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13725: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6950, training loss 0.02649, validation loss 0.02639
Epoch 13736: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13747: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6960, training loss 0.00874, validation loss 0.00509
Epoch 13758: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6970, training loss 0.00453, validation loss 0.00480
Epoch 13769: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13780: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6976
Mean train loss for ascent epoch 6977: -0.00012058867287123576
Mean eval for ascent epoch 6977: 0.0032932860776782036
Doing Evaluation on the model now
This is Epoch 6980, training loss 0.57729, validation loss 0.70896
Epoch 13791: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13802: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6990, training loss 0.03988, validation loss 0.02232
Epoch 13813: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13824: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7000, training loss 0.00509, validation loss 0.00357
Resetting learning rate to 0.01000
Epoch 13835: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13846: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7010, training loss 0.00482, validation loss 0.00550
Epoch 13857: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7020, training loss 0.00348, validation loss 0.00422
Epoch 13868: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7021
Mean train loss for ascent epoch 7022: -0.00015818598330952227
Mean eval for ascent epoch 7022: 0.0034082457423210144
Epoch 13879: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7030, training loss 0.05594, validation loss 0.02275
Epoch 13890: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13901: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7040, training loss 0.01047, validation loss 0.01546
Epoch 13912: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13923: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7050, training loss 0.00465, validation loss 0.00339
Epoch 13934: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13945: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7060
Doing Evaluation on the model now
This is Epoch 7060, training loss 0.00433, validation loss 0.00550
Mean train loss for ascent epoch 7061: -0.0001519176730653271
Mean eval for ascent epoch 7061: 0.0037565052043646574
Epoch 13956: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7070, training loss 0.04710, validation loss 0.02734
Epoch 13967: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13978: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7080, training loss 0.01182, validation loss 0.02850
Epoch 13989: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14000: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7090, training loss 0.00558, validation loss 0.00559
Epoch 14011: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14022: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7099
Mean train loss for ascent epoch 7100: -0.0001507000415585935
Mean eval for ascent epoch 7100: 0.00335501367226243
Doing Evaluation on the model now
This is Epoch 7100, training loss 0.00336, validation loss 0.00302
Epoch 14033: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14044: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7110, training loss 0.06422, validation loss 0.08016
Epoch 14055: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7120, training loss 0.01231, validation loss 0.01124
Epoch 14066: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14077: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7130, training loss 0.00365, validation loss 0.00323
Epoch 14088: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14099: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7138
Mean train loss for ascent epoch 7139: -0.00010345793998567387
Mean eval for ascent epoch 7139: 0.003382985945791006
Doing Evaluation on the model now
This is Epoch 7140, training loss 0.14094, validation loss 0.04796
Epoch 14110: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14121: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7150, training loss 0.07876, validation loss 0.01631
Epoch 14132: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14143: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7160, training loss 0.01270, validation loss 0.01694
Epoch 14154: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7170, training loss 0.00408, validation loss 0.00713
Epoch 14165: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14176: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7177
Mean train loss for ascent epoch 7178: -0.00011723653005901724
Mean eval for ascent epoch 7178: 0.0031599707435816526
Doing Evaluation on the model now
This is Epoch 7180, training loss 0.78090, validation loss 1.31318
Epoch 14187: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14198: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7190, training loss 0.02168, validation loss 0.02037
Epoch 14209: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14220: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7200, training loss 0.01375, validation loss 0.01407
Resetting learning rate to 0.01000
Epoch 14231: reducing learning rate of group 0 to 5.0000e-04.
Epoch 14242: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7210, training loss 0.00889, validation loss 0.01091
Epoch 14253: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7220, training loss 0.00325, validation loss 0.00378
Epoch 14264: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7221
Mean train loss for ascent epoch 7222: -0.00010337485582567751
Mean eval for ascent epoch 7222: 0.0034124364610761404
Epoch 14275: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7230, training loss 0.06140, validation loss 0.05874
Epoch 14286: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14297: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7240, training loss 0.00868, validation loss 0.00527
Epoch 14308: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14319: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7250, training loss 0.00447, validation loss 0.00506
Epoch 14330: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14341: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7260
Doing Evaluation on the model now
This is Epoch 7260, training loss 0.00327, validation loss 0.00285
Mean train loss for ascent epoch 7261: -0.00010876887972699478
Mean eval for ascent epoch 7261: 0.0028582492377609015
Epoch 14352: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7270, training loss 0.12047, validation loss 0.14621
Epoch 14363: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14374: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7280, training loss 0.01203, validation loss 0.01935
Epoch 14385: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14396: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7290, training loss 0.00398, validation loss 0.00364
Epoch 14407: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14418: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7299
Mean train loss for ascent epoch 7300: -0.00013513636076822877
Mean eval for ascent epoch 7300: 0.0032977324444800615
Doing Evaluation on the model now
This is Epoch 7300, training loss 0.00330, validation loss 0.00301
Epoch 14429: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7310, training loss 0.13134, validation loss 0.05454
Epoch 14440: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14451: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7320, training loss 0.01555, validation loss 0.00589
Epoch 14462: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14473: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7330, training loss 0.00381, validation loss 0.00349
Epoch 14484: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14495: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7338
Mean train loss for ascent epoch 7339: -0.00013470691919792444
Mean eval for ascent epoch 7339: 0.003104942385107279
Doing Evaluation on the model now
This is Epoch 7340, training loss 0.18372, validation loss 0.50344
Epoch 14506: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14517: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7350, training loss 0.03755, validation loss 0.02770
Epoch 14528: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7360, training loss 0.01218, validation loss 0.00984
Epoch 14539: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14550: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7370, training loss 0.00390, validation loss 0.00454
Epoch 14561: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14572: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7377
Mean train loss for ascent epoch 7378: -0.00012442054867278785
Mean eval for ascent epoch 7378: 0.003415307030081749
Doing Evaluation on the model now
This is Epoch 7380, training loss 2.20310, validation loss 1.29911
Epoch 14583: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14594: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7390, training loss 0.02887, validation loss 0.06547
Epoch 14605: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14616: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7400, training loss 0.01089, validation loss 0.00809
Resetting learning rate to 0.01000
Epoch 14627: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7410, training loss 0.00559, validation loss 0.00443
Epoch 14638: reducing learning rate of group 0 to 2.5000e-04.
Epoch 14649: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7420, training loss 0.00415, validation loss 0.00436
Epoch 14660: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7422
Mean train loss for ascent epoch 7423: -0.00014893639308866113
Mean eval for ascent epoch 7423: 0.0038396508898586035
Epoch 14671: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7430, training loss 0.05831, validation loss 0.03064
Epoch 14682: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14693: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7440, training loss 0.00845, validation loss 0.01317
Epoch 14704: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14715: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7450, training loss 0.00499, validation loss 0.00481
Epoch 14726: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7460, training loss 0.00472, validation loss 0.00436
Epoch 14737: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7461
Mean train loss for ascent epoch 7462: -0.00011346634710207582
Mean eval for ascent epoch 7462: 0.003844432532787323
Epoch 14748: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7470, training loss 0.06431, validation loss 0.09277
Epoch 14759: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14770: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7480, training loss 0.01019, validation loss 0.01004
Epoch 14781: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14792: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7490, training loss 0.00463, validation loss 0.00539
Epoch 14803: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14814: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7500
Doing Evaluation on the model now
This is Epoch 7500, training loss 0.00401, validation loss 0.00376
Mean train loss for ascent epoch 7501: -0.0001930670259753242
Mean eval for ascent epoch 7501: 0.004022208508104086
Epoch 14825: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7510, training loss 0.07760, validation loss 0.08048
Epoch 14836: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14847: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7520, training loss 0.00857, validation loss 0.01053
Epoch 14858: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14869: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7530, training loss 0.00388, validation loss 0.00330
Epoch 14880: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14891: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7539
Mean train loss for ascent epoch 7540: -0.00012055735714966431
Mean eval for ascent epoch 7540: 0.0037628368008881807
Doing Evaluation on the model now
This is Epoch 7540, training loss 0.00376, validation loss 0.00325
Epoch 14902: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14913: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7550, training loss 0.05574, validation loss 0.03038
Epoch 14924: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7560, training loss 0.01807, validation loss 0.01673
Epoch 14935: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14946: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7570, training loss 0.00475, validation loss 0.00344
Epoch 14957: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14968: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7578
Mean train loss for ascent epoch 7579: -8.663323387736455e-05
Mean eval for ascent epoch 7579: 0.003178549464792013
Doing Evaluation on the model now
This is Epoch 7580, training loss 0.26929, validation loss 0.27149
Epoch 14979: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14990: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7590, training loss 0.02953, validation loss 0.01656
Epoch 15001: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15012: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7600, training loss 0.01290, validation loss 0.01543
Resetting learning rate to 0.01000
Epoch 15023: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7610, training loss 0.00468, validation loss 0.00437
Epoch 15034: reducing learning rate of group 0 to 2.5000e-04.
Epoch 15045: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7620, training loss 0.00319, validation loss 0.00669
Epoch 15056: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7622
Mean train loss for ascent epoch 7623: -0.00012207584222778678
Mean eval for ascent epoch 7623: 0.0035621649585664272
Epoch 15067: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7630, training loss 0.16654, validation loss 0.08378
Epoch 15078: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15089: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7640, training loss 0.01489, validation loss 0.01358
Epoch 15100: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15111: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7650, training loss 0.00690, validation loss 0.00408
Epoch 15122: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7660, training loss 0.00295, validation loss 0.00288
Epoch 15133: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7661
Mean train loss for ascent epoch 7662: -0.00010807177750393748
Mean eval for ascent epoch 7662: 0.002945535583421588
Epoch 15144: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7670, training loss 0.06745, validation loss 0.05272
Epoch 15155: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15166: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7680, training loss 0.01162, validation loss 0.01643
Epoch 15177: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15188: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7690, training loss 0.00495, validation loss 0.00400
Epoch 15199: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15210: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7700
Doing Evaluation on the model now
This is Epoch 7700, training loss 0.00331, validation loss 0.00330
Mean train loss for ascent epoch 7701: -0.00013848987873643637
Mean eval for ascent epoch 7701: 0.0033516688272356987
Epoch 15221: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7710, training loss 0.04512, validation loss 0.01717
Epoch 15232: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15243: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7720, training loss 0.00713, validation loss 0.00527
Epoch 15254: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15265: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7730, training loss 0.00515, validation loss 0.00746
Epoch 15276: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15287: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7739
Mean train loss for ascent epoch 7740: -0.00012415503442753106
Mean eval for ascent epoch 7740: 0.004341674502938986
Doing Evaluation on the model now
This is Epoch 7740, training loss 0.00434, validation loss 0.00393
Epoch 15298: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7750, training loss 0.07186, validation loss 0.07708
Epoch 15309: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15320: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7760, training loss 0.01571, validation loss 0.02020
Epoch 15331: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15342: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7770, training loss 0.00399, validation loss 0.00450
Epoch 15353: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15364: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7778
Mean train loss for ascent epoch 7779: -9.811839845497161e-05
Mean eval for ascent epoch 7779: 0.00370787363499403
Doing Evaluation on the model now
This is Epoch 7780, training loss 0.09369, validation loss 0.10620
Epoch 15375: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15386: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7790, training loss 0.03096, validation loss 0.02090
Epoch 15397: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7800, training loss 0.00912, validation loss 0.00703
Epoch 15408: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 15419: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7810, training loss 0.00592, validation loss 0.00525
Epoch 15430: reducing learning rate of group 0 to 2.5000e-04.
Epoch 15441: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7820, training loss 0.00336, validation loss 0.00485
Epoch 15452: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7823
Mean train loss for ascent epoch 7824: -8.608243661001325e-05
Mean eval for ascent epoch 7824: 0.003270160173997283
Epoch 15463: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7830, training loss 0.07033, validation loss 0.05507
Epoch 15474: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15485: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7840, training loss 0.01411, validation loss 0.01115
Epoch 15496: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7850, training loss 0.00580, validation loss 0.00421
Epoch 15507: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15518: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7860, training loss 0.00317, validation loss 0.00345
Epoch 15529: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7862
Mean train loss for ascent epoch 7863: -0.00011732240818673745
Mean eval for ascent epoch 7863: 0.0034463994670659304
Epoch 15540: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7870, training loss 0.09937, validation loss 0.06864
Epoch 15551: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15562: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7880, training loss 0.01061, validation loss 0.00656
Epoch 15573: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15584: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7890, training loss 0.00344, validation loss 0.00305
Epoch 15595: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7900, training loss 0.00340, validation loss 0.00545
Epoch 15606: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7901
Mean train loss for ascent epoch 7902: -0.0001379480236209929
Mean eval for ascent epoch 7902: 0.0031595781911164522
Epoch 15617: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7910, training loss 0.06629, validation loss 0.06481
Epoch 15628: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15639: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7920, training loss 0.01028, validation loss 0.00524
Epoch 15650: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15661: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7930, training loss 0.00381, validation loss 0.00541
Epoch 15672: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15683: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7940
Doing Evaluation on the model now
This is Epoch 7940, training loss 0.00318, validation loss 0.00305
Mean train loss for ascent epoch 7941: -0.00010384753841208294
Mean eval for ascent epoch 7941: 0.002848458243533969
Epoch 15694: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7950, training loss 0.06960, validation loss 0.13747
Epoch 15705: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15716: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7960, training loss 0.01116, validation loss 0.01278
Epoch 15727: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15738: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7970, training loss 0.00392, validation loss 0.00325
Epoch 15749: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15760: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7979
Mean train loss for ascent epoch 7980: -0.0001140150852734223
Mean eval for ascent epoch 7980: 0.0033395455684512854
Doing Evaluation on the model now
This is Epoch 7980, training loss 0.00334, validation loss 0.00332
Epoch 15771: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15782: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7990, training loss 0.05694, validation loss 0.04873
Epoch 15793: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8000, training loss 0.01591, validation loss 0.00775
Resetting learning rate to 0.01000
Epoch 15804: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15815: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8010, training loss 0.00385, validation loss 0.00411
Epoch 15826: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15837: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8018
Mean train loss for ascent epoch 8019: -0.00015658250777050853
Mean eval for ascent epoch 8019: 0.004513324238359928
Doing Evaluation on the model now
This is Epoch 8020, training loss 0.22095, validation loss 0.42162
Epoch 15848: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15859: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8030, training loss 0.04901, validation loss 0.04836
Epoch 15870: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15881: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8040, training loss 0.01033, validation loss 0.01573
Epoch 15892: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8050, training loss 0.00380, validation loss 0.00368
Epoch 15903: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15914: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8057
Mean train loss for ascent epoch 8058: -0.00010832327825482935
Mean eval for ascent epoch 8058: 0.0037562926299870014
Doing Evaluation on the model now
This is Epoch 8060, training loss 0.27125, validation loss 0.23410
Epoch 15925: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15936: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8070, training loss 0.02782, validation loss 0.01886
Epoch 15947: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15958: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8080, training loss 0.00843, validation loss 0.00553
Epoch 15969: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15980: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8090, training loss 0.00381, validation loss 0.00357
Epoch 15991: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8096
Mean train loss for ascent epoch 8097: -0.00013327630585990846
Mean eval for ascent epoch 8097: 0.0035930864978581667
Doing Evaluation on the model now
This is Epoch 8100, training loss 0.17991, validation loss 0.13676
Epoch 16002: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16013: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8110, training loss 0.02378, validation loss 0.01083
Epoch 16024: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16035: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8120, training loss 0.00564, validation loss 0.01007
Epoch 16046: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16057: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8130, training loss 0.00333, validation loss 0.00332
Epoch 16068: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8135
Mean train loss for ascent epoch 8136: -0.00010148862929781899
Mean eval for ascent epoch 8136: 0.0034476181026548147
Doing Evaluation on the model now
This is Epoch 8140, training loss 0.80739, validation loss 0.16827
Epoch 16079: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16090: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8150, training loss 0.02830, validation loss 0.01471
Epoch 16101: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16112: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8160, training loss 0.00538, validation loss 0.00471
Epoch 16123: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16134: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8170, training loss 0.00350, validation loss 0.00380
Epoch 16145: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8174
Mean train loss for ascent epoch 8175: -0.00012220549979247153
Mean eval for ascent epoch 8175: 0.00361808342859149
Epoch 16156: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8180, training loss 0.09860, validation loss 0.01969
Epoch 16167: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8190, training loss 0.02173, validation loss 0.02072
Epoch 16178: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16189: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8200, training loss 0.00500, validation loss 0.00480
Resetting learning rate to 0.01000
Epoch 16200: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16211: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8210, training loss 0.00430, validation loss 0.00422
Epoch 16222: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16233: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8218
Mean train loss for ascent epoch 8219: -0.00010285191820003092
Mean eval for ascent epoch 8219: 0.0037037667352706194
Doing Evaluation on the model now
This is Epoch 8220, training loss 0.17960, validation loss 0.13329
Epoch 16244: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16255: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8230, training loss 0.04152, validation loss 0.03478
Epoch 16266: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8240, training loss 0.00799, validation loss 0.00750
Epoch 16277: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16288: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8250, training loss 0.00455, validation loss 0.00606
Epoch 16299: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16310: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8257
Mean train loss for ascent epoch 8258: -0.00020988415053579956
Mean eval for ascent epoch 8258: 0.004277416970580816
Doing Evaluation on the model now
This is Epoch 8260, training loss 0.25722, validation loss 0.23534
Epoch 16321: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16332: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8270, training loss 0.03275, validation loss 0.02595
Epoch 16343: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16354: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8280, training loss 0.00629, validation loss 0.01154
Epoch 16365: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8290, training loss 0.00451, validation loss 0.00506
Epoch 16376: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16387: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8296
Mean train loss for ascent epoch 8297: -0.00011905415158253163
Mean eval for ascent epoch 8297: 0.00434898491948843
Doing Evaluation on the model now
This is Epoch 8300, training loss 0.26326, validation loss 0.07176
Epoch 16398: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16409: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8310, training loss 0.03420, validation loss 0.02554
Epoch 16420: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16431: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8320, training loss 0.00647, validation loss 0.00445
Epoch 16442: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16453: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8330, training loss 0.00453, validation loss 0.00488
Epoch 16464: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8335
Mean train loss for ascent epoch 8336: -0.00012238322233315557
Mean eval for ascent epoch 8336: 0.0037936889566481113
Doing Evaluation on the model now
This is Epoch 8340, training loss 0.33767, validation loss 0.16058
Epoch 16475: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16486: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8350, training loss 0.02032, validation loss 0.00844
Epoch 16497: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16508: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8360, training loss 0.00720, validation loss 0.00780
Epoch 16519: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16530: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8370, training loss 0.00441, validation loss 0.00508
Epoch 16541: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8374
Mean train loss for ascent epoch 8375: -0.00012900603178422898
Mean eval for ascent epoch 8375: 0.00414838083088398
Epoch 16552: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8380, training loss 0.25414, validation loss 0.12582
Epoch 16563: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8390, training loss 0.01968, validation loss 0.02701
Epoch 16574: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16585: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8400, training loss 0.00634, validation loss 0.00894
Resetting learning rate to 0.01000
Epoch 16596: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16607: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8410, training loss 0.00494, validation loss 0.00443
Epoch 16618: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16629: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8419
Mean train loss for ascent epoch 8420: -0.00014234900299925357
Mean eval for ascent epoch 8420: 0.003727039322257042
Doing Evaluation on the model now
This is Epoch 8420, training loss 0.00373, validation loss 0.00360
Epoch 16640: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16651: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8430, training loss 0.06077, validation loss 0.09402
Epoch 16662: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8440, training loss 0.01417, validation loss 0.01545
Epoch 16673: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16684: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8450, training loss 0.00516, validation loss 0.00390
Epoch 16695: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16706: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8458
Mean train loss for ascent epoch 8459: -9.229166607838124e-05
Mean eval for ascent epoch 8459: 0.0035848335828632116
Doing Evaluation on the model now
This is Epoch 8460, training loss 0.11560, validation loss 0.03920
Epoch 16717: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16728: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8470, training loss 0.02235, validation loss 0.02114
Epoch 16739: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16750: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8480, training loss 0.00745, validation loss 0.00833
Epoch 16761: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8490, training loss 0.00399, validation loss 0.00460
Epoch 16772: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16783: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8497
Mean train loss for ascent epoch 8498: -8.108803740469739e-05
Mean eval for ascent epoch 8498: 0.0034111954737454653
Doing Evaluation on the model now
This is Epoch 8500, training loss 0.28817, validation loss 0.27132
Epoch 16794: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16805: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8510, training loss 0.01704, validation loss 0.02321
Epoch 16816: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16827: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8520, training loss 0.00756, validation loss 0.00552
Epoch 16838: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16849: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8530, training loss 0.00417, validation loss 0.00463
Epoch 16860: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8536
Mean train loss for ascent epoch 8537: -0.00010225819278275594
Mean eval for ascent epoch 8537: 0.003721272572875023
Doing Evaluation on the model now
This is Epoch 8540, training loss 0.78592, validation loss 0.19600
Epoch 16871: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16882: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8550, training loss 0.02298, validation loss 0.01246
Epoch 16893: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16904: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8560, training loss 0.00489, validation loss 0.00751
Epoch 16915: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16926: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8570, training loss 0.00346, validation loss 0.00321
Epoch 16937: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8575
Mean train loss for ascent epoch 8576: -0.00010015614680014551
Mean eval for ascent epoch 8576: 0.003572407877072692
Doing Evaluation on the model now
This is Epoch 8580, training loss 0.17173, validation loss 0.06415
Epoch 16948: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16959: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8590, training loss 0.03321, validation loss 0.01726
Epoch 16970: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16981: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8600, training loss 0.00924, validation loss 0.01486
Resetting learning rate to 0.01000
Epoch 16992: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17003: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8610, training loss 0.00407, validation loss 0.00439
Epoch 17014: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17025: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8619
Mean train loss for ascent epoch 8620: -0.00012797753151971847
Mean eval for ascent epoch 8620: 0.003209039568901062
Doing Evaluation on the model now
This is Epoch 8620, training loss 0.00321, validation loss 0.00303
Epoch 17036: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8630, training loss 0.05591, validation loss 0.02838
Epoch 17047: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17058: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8640, training loss 0.00690, validation loss 0.01008
Epoch 17069: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17080: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8650, training loss 0.00451, validation loss 0.00498
Epoch 17091: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17102: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8658
Mean train loss for ascent epoch 8659: -0.00010935249156318605
Mean eval for ascent epoch 8659: 0.003396877320483327
Doing Evaluation on the model now
This is Epoch 8660, training loss 0.25728, validation loss 1.24536
Epoch 17113: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17124: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8670, training loss 0.05650, validation loss 0.06063
Epoch 17135: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8680, training loss 0.01370, validation loss 0.01027
Epoch 17146: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17157: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8690, training loss 0.00384, validation loss 0.00421
Epoch 17168: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17179: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8697
Mean train loss for ascent epoch 8698: -0.00011878802615683526
Mean eval for ascent epoch 8698: 0.0036167921498417854
Doing Evaluation on the model now
This is Epoch 8700, training loss 0.15658, validation loss 0.18068
Epoch 17190: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17201: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8710, training loss 0.02513, validation loss 0.01718
Epoch 17212: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17223: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8720, training loss 0.01019, validation loss 0.00485
Epoch 17234: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8730, training loss 0.00365, validation loss 0.00364
Epoch 17245: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17256: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8736
Mean train loss for ascent epoch 8737: -0.00010058740735985339
Mean eval for ascent epoch 8737: 0.0036662197671830654
Doing Evaluation on the model now
This is Epoch 8740, training loss 1.09795, validation loss 0.81637
Epoch 17267: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17278: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8750, training loss 0.02924, validation loss 0.02022
Epoch 17289: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17300: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8760, training loss 0.00589, validation loss 0.00413
Epoch 17311: reducing learning rate of group 0 to 3.1250e-04.
Epoch 17322: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8770, training loss 0.00313, validation loss 0.00300
Epoch 17333: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8775
Mean train loss for ascent epoch 8776: -0.00011625697516137734
Mean eval for ascent epoch 8776: 0.003293201792985201
Doing Evaluation on the model now
This is Epoch 8780, training loss 0.14955, validation loss 0.08640
Epoch 17344: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17355: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8790, training loss 0.02571, validation loss 0.01417
Epoch 17366: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17377: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8800, training loss 0.00470, validation loss 0.00351
Resetting learning rate to 0.01000
Epoch 17388: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17399: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8810, training loss 0.00313, validation loss 0.00333
Epoch 17410: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17421: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8820
Doing Evaluation on the model now
This is Epoch 8820, training loss 0.00279, validation loss 0.00265
Mean train loss for ascent epoch 8821: -0.00012792344205081463
Mean eval for ascent epoch 8821: 0.0027353372424840927
Epoch 17432: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8830, training loss 0.09135, validation loss 0.15822
Epoch 17443: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17454: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8840, training loss 0.01177, validation loss 0.00983
Epoch 17465: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17476: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8850, training loss 0.00474, validation loss 0.00694
Epoch 17487: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17498: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8859
Mean train loss for ascent epoch 8860: -0.00011686267680488527
Mean eval for ascent epoch 8860: 0.0038796458393335342
Doing Evaluation on the model now
This is Epoch 8860, training loss 0.00388, validation loss 0.00405
Epoch 17509: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17520: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8870, training loss 0.04787, validation loss 0.10726
Epoch 17531: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8880, training loss 0.00923, validation loss 0.01195
Epoch 17542: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17553: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8890, training loss 0.00467, validation loss 0.00483
Epoch 17564: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17575: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8898
Mean train loss for ascent epoch 8899: -0.00011438532965257764
Mean eval for ascent epoch 8899: 0.004237744957208633
Doing Evaluation on the model now
This is Epoch 8900, training loss 0.07478, validation loss 0.03635
Epoch 17586: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17597: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8910, training loss 0.02549, validation loss 0.04472
Epoch 17608: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17619: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8920, training loss 0.01088, validation loss 0.01103
Epoch 17630: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8930, training loss 0.00426, validation loss 0.00400
Epoch 17641: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17652: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8937
Mean train loss for ascent epoch 8938: -7.944518438307568e-05
Mean eval for ascent epoch 8938: 0.004088853485882282
Doing Evaluation on the model now
This is Epoch 8940, training loss 0.72772, validation loss 0.60580
Epoch 17663: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17674: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8950, training loss 0.01491, validation loss 0.01551
Epoch 17685: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17696: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8960, training loss 0.00805, validation loss 0.00847
Epoch 17707: reducing learning rate of group 0 to 3.1250e-04.
Epoch 17718: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8970, training loss 0.00445, validation loss 0.00420
Epoch 17729: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8976
Mean train loss for ascent epoch 8977: -9.16490753297694e-05
Mean eval for ascent epoch 8977: 0.0037704212591052055
Doing Evaluation on the model now
This is Epoch 8980, training loss 0.07002, validation loss 0.05830
Epoch 17740: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17751: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8990, training loss 0.02616, validation loss 0.01038
Epoch 17762: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17773: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9000, training loss 0.00578, validation loss 0.00669
Resetting learning rate to 0.01000
Epoch 17784: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17795: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9010, training loss 0.00376, validation loss 0.00393
Epoch 17806: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17817: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9020
Doing Evaluation on the model now
This is Epoch 9020, training loss 0.00328, validation loss 0.00320
Mean train loss for ascent epoch 9021: -0.00013792238314636052
Mean eval for ascent epoch 9021: 0.003335852874442935
Epoch 17828: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9030, training loss 0.10431, validation loss 0.15404
Epoch 17839: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17850: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9040, training loss 0.01235, validation loss 0.01597
Epoch 17861: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17872: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9050, training loss 0.00387, validation loss 0.00480
Epoch 17883: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17894: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9059
Mean train loss for ascent epoch 9060: -0.00016329721256624907
Mean eval for ascent epoch 9060: 0.004299398511648178
Doing Evaluation on the model now
This is Epoch 9060, training loss 0.00430, validation loss 0.00333
Epoch 17905: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9070, training loss 0.04889, validation loss 0.02441
Epoch 17916: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17927: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9080, training loss 0.01264, validation loss 0.00824
Epoch 17938: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17949: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9090, training loss 0.00457, validation loss 0.00410
Epoch 17960: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17971: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9098
Mean train loss for ascent epoch 9099: -9.754453640198335e-05
Mean eval for ascent epoch 9099: 0.0037713616620749235
Doing Evaluation on the model now
This is Epoch 9100, training loss 0.54122, validation loss 1.05249
Epoch 17982: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17993: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9110, training loss 0.02654, validation loss 0.05395
Epoch 18004: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9120, training loss 0.00851, validation loss 0.01642
Epoch 18015: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18026: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9130, training loss 0.00393, validation loss 0.00552
Epoch 18037: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18048: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9137
Mean train loss for ascent epoch 9138: -0.00011406477278796956
Mean eval for ascent epoch 9138: 0.0036000728141516447
Doing Evaluation on the model now
This is Epoch 9140, training loss 0.19714, validation loss 0.03560
Epoch 18059: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18070: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9150, training loss 0.03935, validation loss 0.01782
Epoch 18081: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18092: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9160, training loss 0.00862, validation loss 0.00858
Epoch 18103: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9170, training loss 0.00393, validation loss 0.00367
Epoch 18114: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18125: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9176
Mean train loss for ascent epoch 9177: -9.746564319357276e-05
Mean eval for ascent epoch 9177: 0.0036540497094392776
Doing Evaluation on the model now
This is Epoch 9180, training loss 0.16420, validation loss 0.10486
Epoch 18136: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18147: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9190, training loss 0.03429, validation loss 0.05825
Epoch 18158: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18169: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9200, training loss 0.00894, validation loss 0.00461
Resetting learning rate to 0.01000
Epoch 18180: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18191: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9210, training loss 0.00485, validation loss 0.00329
Epoch 18202: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9220, training loss 0.00361, validation loss 0.00347
Epoch 18213: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9221
Mean train loss for ascent epoch 9222: -0.00010898397886194289
Mean eval for ascent epoch 9222: 0.003476989921182394
Epoch 18224: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9230, training loss 0.30907, validation loss 0.11664
Epoch 18235: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18246: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9240, training loss 0.01347, validation loss 0.00736
Epoch 18257: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18268: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9250, training loss 0.00429, validation loss 0.00383
Epoch 18279: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18290: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9260
Doing Evaluation on the model now
This is Epoch 9260, training loss 0.00372, validation loss 0.00303
Mean train loss for ascent epoch 9261: -9.490704542258754e-05
Mean eval for ascent epoch 9261: 0.0034106604289263487
Epoch 18301: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9270, training loss 0.04507, validation loss 0.04671
Epoch 18312: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18323: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9280, training loss 0.01019, validation loss 0.02135
Epoch 18334: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18345: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9290, training loss 0.00416, validation loss 0.00413
Epoch 18356: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18367: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9299
Mean train loss for ascent epoch 9300: -8.436027565039694e-05
Mean eval for ascent epoch 9300: 0.0034576088655740023
Doing Evaluation on the model now
This is Epoch 9300, training loss 0.00346, validation loss 0.00314
Epoch 18378: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18389: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9310, training loss 0.10611, validation loss 0.01804
Epoch 18400: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9320, training loss 0.01679, validation loss 0.02163
Epoch 18411: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18422: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9330, training loss 0.00378, validation loss 0.00334
Epoch 18433: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18444: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9338
Mean train loss for ascent epoch 9339: -7.160888344515115e-05
Mean eval for ascent epoch 9339: 0.0033947781194001436
Doing Evaluation on the model now
This is Epoch 9340, training loss 0.26367, validation loss 0.71151
Epoch 18455: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18466: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9350, training loss 0.03601, validation loss 0.01630
Epoch 18477: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18488: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9360, training loss 0.01149, validation loss 0.00578
Epoch 18499: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9370, training loss 0.00388, validation loss 0.00358
Epoch 18510: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18521: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9377
Mean train loss for ascent epoch 9378: -0.0001451131101930514
Mean eval for ascent epoch 9378: 0.003376319305971265
Doing Evaluation on the model now
This is Epoch 9380, training loss 0.37803, validation loss 0.49097
Epoch 18532: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18543: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9390, training loss 0.02994, validation loss 0.02310
Epoch 18554: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18565: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9400, training loss 0.00592, validation loss 0.00636
Resetting learning rate to 0.01000
Epoch 18576: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18587: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9410, training loss 0.00573, validation loss 0.00463
Epoch 18598: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9420, training loss 0.00358, validation loss 0.00343
Epoch 18609: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9421
Mean train loss for ascent epoch 9422: -8.539986447431147e-05
Mean eval for ascent epoch 9422: 0.00318918377161026
Epoch 18620: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9430, training loss 0.05731, validation loss 0.10177
Epoch 18631: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18642: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9440, training loss 0.01197, validation loss 0.00764
Epoch 18653: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18664: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9450, training loss 0.00410, validation loss 0.00353
Epoch 18675: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18686: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9460
Doing Evaluation on the model now
This is Epoch 9460, training loss 0.00299, validation loss 0.00316
Mean train loss for ascent epoch 9461: -0.0001451817952329293
Mean eval for ascent epoch 9461: 0.0035647170152515173
Epoch 18697: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9470, training loss 0.05582, validation loss 0.06888
Epoch 18708: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18719: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9480, training loss 0.01126, validation loss 0.01594
Epoch 18730: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18741: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9490, training loss 0.00340, validation loss 0.00310
Epoch 18752: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18763: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9499
Mean train loss for ascent epoch 9500: -8.647514914628118e-05
Mean eval for ascent epoch 9500: 0.003157669445499778
Doing Evaluation on the model now
This is Epoch 9500, training loss 0.00316, validation loss 0.00304
Epoch 18774: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9510, training loss 0.05929, validation loss 0.06184
Epoch 18785: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18796: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9520, training loss 0.01315, validation loss 0.01166
Epoch 18807: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18818: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9530, training loss 0.00474, validation loss 0.00338
Epoch 18829: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18840: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9538
Mean train loss for ascent epoch 9539: -0.00010180770186707377
Mean eval for ascent epoch 9539: 0.003271362977102399
Doing Evaluation on the model now
This is Epoch 9540, training loss 0.08277, validation loss 0.11319
Epoch 18851: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18862: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9550, training loss 0.02645, validation loss 0.03045
Epoch 18873: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9560, training loss 0.01538, validation loss 0.02218
Epoch 18884: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18895: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9570, training loss 0.00397, validation loss 0.00492
Epoch 18906: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18917: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9577
Mean train loss for ascent epoch 9578: -0.0001352718536509201
Mean eval for ascent epoch 9578: 0.00349139585159719
Doing Evaluation on the model now
This is Epoch 9580, training loss 1.04168, validation loss 1.80838
Epoch 18928: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18939: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9590, training loss 0.02076, validation loss 0.01908
Epoch 18950: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18961: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9600, training loss 0.01399, validation loss 0.01215
Resetting learning rate to 0.01000
Epoch 18972: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9610, training loss 0.00501, validation loss 0.00423
Epoch 18983: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18994: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9620, training loss 0.00360, validation loss 0.00367
Epoch 19005: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9622
Mean train loss for ascent epoch 9623: -8.508165046805516e-05
Mean eval for ascent epoch 9623: 0.003697935724630952
Epoch 19016: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9630, training loss 0.04754, validation loss 0.02254
Epoch 19027: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19038: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9640, training loss 0.01639, validation loss 0.01556
Epoch 19049: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19060: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9650, training loss 0.00471, validation loss 0.00420
Epoch 19071: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9660, training loss 0.00336, validation loss 0.00379
Epoch 19082: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9661
Mean train loss for ascent epoch 9662: -0.00010762971214717254
Mean eval for ascent epoch 9662: 0.003428564639762044
Epoch 19093: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9670, training loss 0.05883, validation loss 0.09523
Epoch 19104: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19115: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9680, training loss 0.01563, validation loss 0.00726
Epoch 19126: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19137: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9690, training loss 0.00385, validation loss 0.00385
Epoch 19148: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19159: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9700
Doing Evaluation on the model now
This is Epoch 9700, training loss 0.00334, validation loss 0.00410
Mean train loss for ascent epoch 9701: -0.00011251666728639975
Mean eval for ascent epoch 9701: 0.003642287338152528
Epoch 19170: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9710, training loss 0.04580, validation loss 0.07701
Epoch 19181: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19192: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9720, training loss 0.01141, validation loss 0.01018
Epoch 19203: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19214: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9730, training loss 0.00467, validation loss 0.00471
Epoch 19225: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19236: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9739
Mean train loss for ascent epoch 9740: -0.00011525234731379896
Mean eval for ascent epoch 9740: 0.003973284270614386
Doing Evaluation on the model now
This is Epoch 9740, training loss 0.00397, validation loss 0.00432
Epoch 19247: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19258: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9750, training loss 0.26774, validation loss 0.40760
Epoch 19269: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9760, training loss 0.01230, validation loss 0.02293
Epoch 19280: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19291: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9770, training loss 0.00454, validation loss 0.00421
Epoch 19302: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19313: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9778
Mean train loss for ascent epoch 9779: -0.00018365627329330891
Mean eval for ascent epoch 9779: 0.004668834153562784
Doing Evaluation on the model now
This is Epoch 9780, training loss 0.26947, validation loss 0.36128
Epoch 19324: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19335: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9790, training loss 0.02562, validation loss 0.01492
Epoch 19346: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19357: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9800, training loss 0.01643, validation loss 0.01426
Resetting learning rate to 0.01000
Epoch 19368: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9810, training loss 0.00476, validation loss 0.00571
Epoch 19379: reducing learning rate of group 0 to 2.5000e-04.
Epoch 19390: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9820, training loss 0.00399, validation loss 0.00360
Epoch 19401: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9822
Mean train loss for ascent epoch 9823: -9.00771192391403e-05
Mean eval for ascent epoch 9823: 0.004152962937951088
Epoch 19412: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9830, training loss 0.24063, validation loss 0.06977
Epoch 19423: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19434: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9840, training loss 0.02013, validation loss 0.00909
Epoch 19445: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19456: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9850, training loss 0.00544, validation loss 0.00530
Epoch 19467: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9860, training loss 0.00390, validation loss 0.00343
Epoch 19478: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9861
Mean train loss for ascent epoch 9862: -0.00011093407374573871
Mean eval for ascent epoch 9862: 0.004294755402952433
Epoch 19489: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9870, training loss 0.04135, validation loss 0.04980
Epoch 19500: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19511: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9880, training loss 0.01431, validation loss 0.02023
Epoch 19522: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19533: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9890, training loss 0.00602, validation loss 0.00589
Epoch 19544: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19555: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9900
Doing Evaluation on the model now
This is Epoch 9900, training loss 0.00401, validation loss 0.00362
Mean train loss for ascent epoch 9901: -0.00010915415623458102
Mean eval for ascent epoch 9901: 0.0034854256082326174
Epoch 19566: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9910, training loss 0.03918, validation loss 0.02434
Epoch 19577: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19588: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9920, training loss 0.00963, validation loss 0.00768
Epoch 19599: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19610: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9930, training loss 0.00401, validation loss 0.00387
Epoch 19621: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19632: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9939
Mean train loss for ascent epoch 9940: -0.00011064026330132037
Mean eval for ascent epoch 9940: 0.004573664627969265
Doing Evaluation on the model now
This is Epoch 9940, training loss 0.00457, validation loss 0.00365
Epoch 19643: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9950, training loss 0.07636, validation loss 0.04655
Epoch 19654: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19665: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9960, training loss 0.01171, validation loss 0.00667
Epoch 19676: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19687: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9970, training loss 0.00432, validation loss 0.00389
Epoch 19698: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19709: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9978
Mean train loss for ascent epoch 9979: -9.799140389077365e-05
Mean eval for ascent epoch 9979: 0.004054799675941467
Doing Evaluation on the model now
This is Epoch 9980, training loss 0.09477, validation loss 0.16194
Epoch 19720: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19731: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9990, training loss 0.04435, validation loss 0.02817
Epoch 19742: reducing learning rate of group 0 to 1.2500e-03.
POS: 
[6.1488023, 2.248186, 1.3445522, 0.9210198, 0.60460615, 0.54031485, 0.49207577, 0.45607382, 0.36618543, 0.36875668, 0.30571568, 0.24727194, 0.3252559, 0.24065216, 0.3870181, 0.575082, 0.4627692, 0.42643252, 0.25280795, 0.23015484, 0.24731947, 0.23671876, 0.37180963, 0.26686883, 0.22915682, 0.32089698, 0.31855437, 0.35964862, 0.22093992, 0.22997434, 0.32225874, 0.21464837, 0.19233637, 0.23247026, 0.27832308, 0.19107811, 0.28830957, 0.30677187, 0.23098835, 0.391456, 0.23385535, 0.16323325, 0.2186097, 0.26144886, 0.17505762, 0.25679594, 0.20456584, 0.1971882, 0.1759696, 0.20362595, 0.18898615, 0.26179692, 0.222163, 2.8366947, 1.0293534, 1.3194616, 0.9223696, 1.3591003, 0.9860069, 0.713758, 0.48456925, 0.69475746, 0.5252207, 0.5334999, 0.4899552, 0.45539373, 0.5959048, 0.32881716, 0.39052758, 0.2775344, 0.20135343, 0.19912909, 0.25765094, 0.15143032, 0.2091866, 0.21397766, 0.25414863, 0.21338052, 0.19664016, 0.18099767, 0.20889987, 0.19891447, 0.14448828, 0.19186158, 0.21782885, 0.13452916, 0.17170644, 0.15622099, 0.19353156, 0.11705907, 0.12783796, 0.13020343, 0.6686144, 0.9090851, 1.2960314, 1.0657656, 0.7682605, 0.4785576, 0.6687131, 1.1242975, 0.9195573, 0.3452051, 0.521975, 0.48107815, 0.35500634, 0.3677381, 0.45465052, 0.57112616, 0.36631277, 0.24359262, 0.17420173, 0.21150163, 0.31393403, 0.16298257, 0.14758019, 0.22307888, 0.21202539, 0.1909365, 0.1438371, 0.21132986, 0.14210823, 0.19327962, 0.121198416, 0.16248983, 0.11763936, 0.13555118, 0.16965802, 0.18871738, 0.17915086, 0.14517593, 0.36970517, 1.0183022, 0.9841695, 0.71626496, 0.49945942, 0.25014645, 0.33997503, 0.30340806, 0.6183624, 0.83421123, 0.23578322, 0.24036817, 0.29717982, 0.2985358, 0.38155222, 0.7067823, 0.2979418, 0.16780886, 0.14580703, 0.17987025, 0.20423895, 0.14319077, 0.2311428, 0.21289027, 0.2044067, 0.2439686, 0.16947758, 0.19117375, 0.16635391, 0.10996934, 0.109315485, 0.14188124, 0.17971162, 0.13845865, 0.1494653, 0.15101252, 0.17143235, 0.1147025, 0.37902904, 0.8037874, 0.7461445, 1.0964856, 0.574893, 0.7737342, 0.4200047, 0.6764532, 0.36908397, 0.41339612, 0.2340105, 0.18411984, 0.23393649, 0.26557973, 0.2772902, 0.36493623, 0.30375314, 0.3115731, 0.17444798, 0.15949953, 0.24625543, 0.14260858, 0.152236, 0.12849009, 0.17160432, 0.18041006, 0.13780707, 0.21401621, 0.15980984, 0.22134726, 0.20351781, 0.15867372, 0.20199414, 0.14982156, 0.13819486, 0.20767061, 0.21590434, 0.19994062, 0.13481249, 0.16212343, 0.14025603, 0.12933525, 0.13330376, 0.13884652, 0.14186691, 0.18615948, 0.12411238, 0.13214897, 0.09696069, 0.49881017, 0.64418334, 0.59990984, 0.5494961, 0.6371698, 0.28104118, 0.35953525, 0.4280533, 0.30386257, 0.39175394, 0.221592, 0.15643077, 0.2930703, 0.33672714, 0.32222328, 0.27281404, 0.31433013, 0.3098433, 0.1995753, 0.30552956, 0.2071337, 0.19297174, 0.21699876, 0.23960683, 0.14356118, 0.11860356, 0.18637621, 0.20234965, 0.1404936, 0.21465018, 0.1507076, 0.14326888, 0.09031106, 0.08406766, 0.14075848, 0.13167258, 0.15648384, 0.18143809, 0.86195916, 0.87760675, 0.70376056, 0.9181724, 0.48295864, 0.48583406, 0.29825345, 0.3072425, 0.18584335, 0.25086623, 0.16728626, 0.41440263, 0.18318653, 0.22074719, 0.31119362, 0.34184435, 0.08899499, 0.15008132, 0.21936058, 0.31091484, 0.16960554, 0.17632653, 0.17381002, 0.1955192, 0.14964303, 0.20863347, 0.14559121, 0.14868383, 0.17696767, 0.14571603, 0.19336382, 0.14239703, 0.13343158, 0.13509327, 0.15688159, 0.07797273, 0.119017676, 0.1397751, 0.9945617, 0.52366745, 0.4822831, 0.5055838, 1.1198809, 0.9181934, 0.6505286, 0.5457388, 0.47591376, 0.576927, 0.349965, 0.22045459, 0.27935314, 0.32904962, 0.23682305, 0.21051574, 0.3126845, 0.1882033, 0.13477172, 0.17747055, 0.19275399, 0.15013236, 0.16528206, 0.19688818, 0.16536161, 0.16254197, 0.17551869, 0.15017925, 0.14174743, 0.19094294, 0.1485653, 0.15495527, 0.115117125, 0.10138215, 0.17161013, 0.13322692, 0.19058104, 0.1141068, 0.5814388, 1.0212889, 0.86658686, 0.54281783, 0.43943098, 0.46513698, 0.42087266, 0.34959924, 0.46484506, 0.6813074, 0.3091492, 0.25867692, 0.42444903, 0.27339655, 0.16103092, 0.19525842, 0.17693484, 0.17660442, 0.26066115, 0.18830124, 0.25499174, 0.12579337, 0.16824497, 0.15103008, 0.17007673, 0.16311546, 0.12638783, 0.12332751, 0.08259597, 0.0942888, 0.19022305, 0.15902127, 0.13411643, 0.1323653, 0.1308705, 0.12182606, 0.1585493, 0.13487883, 0.59361356, 0.5637031, 0.46184805, 0.6803472, 0.4056368, 0.41768345, 0.3151514, 0.35389712, 0.35699952, 0.39527282, 0.14985448, 0.28959537, 0.40984818, 0.22444274, 0.29816267, 0.44760388, 0.27053627, 0.18793055, 0.21017173, 0.13161273, 0.18177585, 0.11913226, 0.18518923, 0.1556804, 0.1360512, 0.27051523, 0.21413675, 0.13823673, 0.116690814, 0.18963754, 0.13401075, 0.21618414, 0.17827326, 0.10963282, 0.13856001, 0.16764683, 0.13509844, 0.14289723, 0.09075652, 0.10293955, 0.15849756, 0.11901628, 0.06880112, 0.08667066, 0.40313262, 0.82651883, 0.49045554, 0.28895482, 0.2764648, 0.22388852, 0.2565899, 0.21539964, 0.28109527, 0.28156608, 0.1920509, 0.18648756, 0.29055482, 0.24918112, 0.24989876, 0.19949481, 0.11996108, 0.22370958, 0.24990878, 0.15997517, 0.19142973, 0.14572772, 0.15547661, 0.16926587, 0.17619835, 0.14706102, 0.15543765, 0.12537797, 0.2063851, 0.14812092, 0.14725092, 0.1430887, 0.105813764, 0.14172259, 0.11182839, 0.116419464, 0.13111293, 0.1665601, 0.52329046, 0.5724149, 0.60041994, 1.0171306, 0.35957563, 0.42058107, 0.27641407, 0.53316545, 0.49926978, 0.32315654, 0.18219495, 0.14807278, 0.23522843, 0.26142806, 0.19311187, 0.17164426, 0.20200494, 0.14602154, 0.10408132, 0.19237298, 0.20664585, 0.1233414, 0.11030078, 0.22350228, 0.1499862, 0.27425683, 0.15349941, 0.14544325, 0.15223646, 0.15036666, 0.15552056, 0.11327619, 0.12231242, 0.1351454, 0.0987147, 0.09193666, 0.0943039, 0.11882705, 0.38635185, 0.78287435, 0.58726263, 0.39264083, 0.5638373, 0.2604096, 0.27419534, 0.57173526, 0.29117328, 0.2963232, 0.1713473, 0.30678844, 0.24358921, 0.17992614, 0.34724337, 0.2103202, 0.15663, 0.12549119, 0.15946381, 0.20102772, 0.16797698, 0.109564506, 0.12760319, 0.093382485, 0.130833, 0.16382925, 0.10744021, 0.09802161, 0.09864961, 0.07988902, 0.12369989, 0.10467103, 0.08161412, 0.10131186, 0.08258259, 0.112302005, 0.16047649, 0.1310612, 0.447441, 0.5368212, 0.70635986, 0.42429262, 0.35346416, 0.27350268, 0.5550145, 0.51908326, 0.28413787, 0.5811121, 0.18122028, 0.14931528, 0.16444588, 0.40221894, 0.32265723, 0.1851231, 0.15391482, 0.17552139, 0.09194627, 0.19207454, 0.18987598, 0.19607446, 0.14651826, 0.17164627, 0.13554804, 0.1441978, 0.16082348, 0.09364078, 0.16104099, 0.14555512, 0.12730952, 0.13457486, 0.14591405, 0.0960744, 0.084490426, 0.15030971, 0.093135886, 0.12355643, 0.28120753, 0.47993284, 0.56048524, 0.46530876, 0.80057395, 0.44498673, 0.4953683, 0.35632697, 0.15464213, 0.48913565, 0.61359227, 0.25672454, 0.23235619, 0.2323816, 0.1235853, 0.1633071, 0.14627531, 0.16714913, 0.11487737, 0.16972987, 0.28788003, 0.14308272, 0.14370617, 0.17767002, 0.16180904, 0.19763482, 0.16129212, 0.09669345, 0.15784279, 0.16901414, 0.12969458, 0.18220334, 0.085392594, 0.114397705, 0.088532306, 0.11925153, 0.10366568, 0.09532724, 0.1556964, 0.106232546, 0.10251227, 0.077553846, 0.117968656, 0.82956034, 0.4685859, 0.7772339, 0.6179053, 0.48072404, 0.33892667, 0.3933679, 0.3228767, 0.299564, 0.26667017, 0.19641952, 0.33449292, 0.20724723, 0.16244091, 0.21583132, 0.1429783, 0.16844681, 0.13666354, 0.14168058, 0.21087284, 0.28324482, 0.25490248, 0.19351836, 0.13798414, 0.1467469, 0.08449894, 0.115677305, 0.17199156, 0.14097172, 0.13226342, 0.1526473, 0.13444047, 0.11728179, 0.16644832, 0.08366838, 0.17365508, 0.10625394, 0.1352344, 0.2829866, 0.261172, 0.49616522, 0.36791554, 0.28942752, 0.37849692, 0.2892201, 0.31447387, 0.31451455, 0.33166468, 0.33815575, 0.31923482, 0.15093441, 0.23557954, 0.17492387, 0.16742408, 0.10625452, 0.12924485, 0.10960515, 0.11810204, 0.23020642, 0.12342911, 0.13496718, 0.15709534, 0.10494949, 0.111737266, 0.12888515, 0.13377497, 0.11119269, 0.09398451, 0.13379568, 0.15892677, 0.121392, 0.10812191, 0.16169794, 0.11663541, 0.11186236, 0.10217918, 0.23915662, 0.4670505, 0.24909995, 0.4956693, 0.5241356, 0.48233515, 0.2882925, 0.59614587, 0.38621736, 0.20479524, 0.14671543, 0.16732216, 0.18105163, 0.20168161, 0.119525306, 0.091119766, 0.21936214, 0.124760225, 0.15086614, 0.23894122, 0.18450765, 0.18397118, 0.16032423, 0.12698038, 0.088989, 0.13342366, 0.21352525, 0.1519795, 0.10133058, 0.13338982, 0.07868574, 0.09064894, 0.106890716, 0.100787856, 0.10595514, 0.09171931, 0.088074155, 0.077369876, 0.5067299, 0.4221198, 0.76912135, 0.7388765, 0.63144225, 0.40316132, 0.23312007, 0.44862086, 0.4858474, 0.1758873, 0.23542783, 0.23582089, 0.15363652, 0.19892485, 0.20691134, 0.20294885, 0.21097466, 0.25515988, 0.18755105, 0.17150387, 0.16065003, 0.10342532, 0.07398721, 0.12579818, 0.12536529, 0.110046685, 0.17138577, 0.11348961, 0.10647717, 0.10926511, 0.12691768, 0.10596967, 0.13266803, 0.08032503, 0.10436445, 0.109170325, 0.102419056, 0.119613595, 0.30269378, 0.5601387, 0.6912188, 0.40787807, 0.38853505, 0.32371268, 0.2774764, 0.17406067, 0.35707995, 0.28116032, 0.16682981, 0.26724282, 0.260194, 0.23612137, 0.1263651, 0.14100908, 0.11906725, 0.111706, 0.14647226, 0.107358456, 0.11677132, 0.11130471, 0.073148824, 0.11203205, 0.14502631, 0.1045918, 0.129571, 0.087145664, 0.15155226, 0.18527293, 0.09235791, 0.093317196, 0.10204693, 0.11870614, 0.11447059, 0.12205775, 0.09331457, 0.109984696, 0.08552864, 0.09804346, 0.08507391, 0.11075666, 0.078567944, 0.13422313, 0.65956616, 0.7791783, 0.31029868, 0.7156136, 0.31111407, 0.3694483, 0.39747486, 0.23809528, 0.49513796, 0.49284297, 0.16384026, 0.12961997, 0.15357196, 0.14913706, 0.2044363, 0.11302739, 0.19558792, 0.15740936, 0.23050527, 0.22531025, 0.15209363, 0.102599375, 0.14086305, 0.1034542, 0.100664675, 0.13843985, 0.09901531, 0.11535195, 0.08676171, 0.09201857, 0.13412619, 0.101471044, 0.10273975, 0.13159396, 0.105942786, 0.10130556, 0.07371695, 0.07979339, 0.24908854, 0.3739051, 0.3526205, 0.57200253, 0.16902757, 0.27131352, 0.39928752, 0.34018654, 0.50711256, 0.3701036, 0.38895288, 0.2227004, 0.27400443, 0.11623349, 0.106010176, 0.1293379, 0.09273265, 0.17284216, 0.11399315, 0.139254, 0.13722111, 0.14200334, 0.16501066, 0.19166608, 0.10213673, 0.12588158, 0.083618745, 0.09938877, 0.1344304, 0.09879593, 0.109346054, 0.14567184, 0.08830327, 0.08100245, 0.10643141, 0.07522891, 0.09266128, 0.098138615, 0.15583049, 0.46955064, 0.7281818, 0.4398519, 0.47794187, 0.21296321, 0.53572804, 0.39869678, 0.18800384, 0.3116771, 0.20543538, 0.13417366, 0.1624954, 0.1569622, 0.15715976, 0.20989805, 0.13229358, 0.11808214, 0.29506978, 0.2302194, 0.124574006, 0.104433, 0.117391534, 0.13648076, 0.09499556, 0.12260993, 0.10265596, 0.079763226, 0.121939406, 0.17079559, 0.076843835, 0.05140187, 0.08993013, 0.08059762, 0.11698472, 0.122238815, 0.08801058, 0.08776147, 0.2539001, 0.5029282, 0.29329747, 0.51885533, 0.3282617, 0.16695908, 0.17701685, 0.21788652, 0.38688284, 0.35562035, 0.18409362, 0.16410151, 0.18789428, 0.16767274, 0.15625145, 0.15122084, 0.20729275, 0.15130764, 0.120495304, 0.16928011, 0.14447625, 0.13651091, 0.10837364, 0.12406343, 0.09874163, 0.10069387, 0.0792184, 0.07902474, 0.09960872, 0.11241298, 0.09606394, 0.15133758, 0.08490824, 0.101920806, 0.12069365, 0.0988917, 0.08024611, 0.09098502, 0.3225057, 0.4493641, 0.37562105, 0.23546939, 0.27437657, 0.29920927, 0.19239022, 0.30451077, 0.4677867, 0.2250827, 0.12471531, 0.20902681, 0.11156656, 0.14339507, 0.101017736, 0.12547608, 0.08717342, 0.14944082, 0.12798418, 0.10838441, 0.124200456, 0.11916842, 0.14488806, 0.15066649, 0.124558836, 0.15537708, 0.12500226, 0.088957965, 0.09840484, 0.09482702, 0.071474805, 0.093423426, 0.10478678, 0.209231, 0.10503073, 0.07546305, 0.09464289, 0.06549841, 0.102425955, 0.06823667, 0.07710956, 0.079648495, 0.05786616, 0.18373552, 0.23029086, 0.28817707, 0.42087284, 0.32400948, 0.2248505, 0.30548736, 0.4413874, 0.42265856, 0.18367207, 0.1927673, 0.11564114, 0.105452225, 0.09723487, 0.12049922, 0.120480716, 0.1595858, 0.11917204, 0.17795178, 0.1559214, 0.16158777, 0.053197302, 0.08431985, 0.12533718, 0.076005556, 0.0899527, 0.099056974, 0.14604814, 0.095133424, 0.06989528, 0.12266018, 0.0702673, 0.04558503, 0.10185561, 0.11926602, 0.10271885, 0.078000456, 0.061377972, 0.46066198, 0.5530176, 0.2322675, 0.3079779, 0.56820923, 0.2892111, 0.2032452, 0.31299302, 0.20754854, 0.1765123, 0.18136792, 0.10833384, 0.103650935, 0.15925418, 0.14092481, 0.11251717, 0.11171001, 0.11460145, 0.16553392, 0.20375149, 0.088033654, 0.11916517, 0.116995275, 0.13519444, 0.15206616, 0.11218494, 0.09961855, 0.12222717, 0.09891165, 0.06521111, 0.07556277, 0.08760996, 0.110189855, 0.14217, 0.11945175, 0.15613611, 0.08461163, 0.10733777, 0.34289423, 0.31780317, 0.33760333, 0.39201725, 0.55072147, 0.2146106, 0.15407969, 0.28298846, 0.12886983, 0.22666578, 0.22849508, 0.18415093, 0.14248852, 0.2092707, 0.15553576, 0.1856218, 0.20143545, 0.109918356, 0.11311414, 0.15358625, 0.09695039, 0.09619775, 0.101183616, 0.1024582, 0.12080479, 0.07092964, 0.1326213, 0.11432108, 0.10178801, 0.09179435, 0.10224154, 0.09884744, 0.061188966, 0.08430116, 0.11382758, 0.076138, 0.08576562, 0.095109336, 0.6075816, 0.87366676, 0.76246786, 0.4001645, 0.34350404, 0.24014337, 0.29293108, 0.25699496, 0.12176989, 0.2289187, 0.14596485, 0.12551957, 0.16410632, 0.15111332, 0.18361586, 0.17636001, 0.13802989, 0.13544914, 0.10203388, 0.1882574, 0.1481945, 0.11073233, 0.12867308, 0.11537871, 0.104125604, 0.10121643, 0.102974236, 0.08805182, 0.0742564, 0.07487502, 0.11044724, 0.11395018, 0.0780946, 0.080357455, 0.13475667, 0.101880305, 0.06656196, 0.05075986, 0.4529466, 0.58262765, 0.4442848, 0.72942114, 0.8303803, 0.26709908, 0.15940543, 0.34681648, 0.28614837, 0.11883097, 0.16630201, 0.1851173, 0.11631443, 0.22767922, 0.19533806, 0.15952496, 0.19439901, 0.122703865, 0.08650122, 0.11581919, 0.14063197, 0.09678574, 0.09783008, 0.10295066, 0.06548501, 0.0662015, 0.0922497, 0.10586887, 0.10795478, 0.10953661, 0.07889665, 0.07274474, 0.100705475, 0.08774986, 0.09000422, 0.1010429, 0.05310188, 0.11392654, 0.08892231, 0.08063266, 0.07200482, 0.093123615, 0.066895045, 0.06935417, 0.22278768, 0.399598, 0.19259034, 0.5749546, 0.23694375, 0.22703299, 0.18888076, 0.24431716, 0.19688712, 0.11647464, 0.11875439, 0.13562632, 0.13868524, 0.19145286, 0.23872127, 0.11470292, 0.09635458, 0.078399405, 0.12026829, 0.16263528, 0.14457935, 0.1333276, 0.11029563, 0.12621555, 0.075747415, 0.12980334, 0.08756486, 0.07542676, 0.06776229, 0.08634088, 0.08693589, 0.07466769, 0.12679173, 0.06082092, 0.095013425, 0.0720223, 0.11120363, 0.0798575, 0.3590729, 0.26883024, 0.14388286, 0.19909026, 0.35800317, 0.17825985, 0.17853187, 0.17907357, 0.37859505, 0.5217143, 0.216358, 0.08234906, 0.13299008, 0.124965645, 0.16748486, 0.06861461, 0.06083237, 0.07637717, 0.07228563, 0.12117465, 0.10799344, 0.118390575, 0.060628194, 0.121208355, 0.08578957, 0.09008246, 0.102826394, 0.07959469, 0.07195223, 0.09343094, 0.08020981, 0.110409096, 0.05593721, 0.09360585, 0.08116096, 0.117105976, 0.09298501, 0.075843625, 0.30463684, 0.3984851, 0.38717115, 0.3621632, 0.29413748, 0.3008112, 0.38912416, 0.23642352, 0.14546359, 0.20812447, 0.12957473, 0.098348975, 0.112143464, 0.1277942, 0.13917434, 0.10430283, 0.11512362, 0.12962937, 0.07194457, 0.1029337, 0.120104335, 0.11986218, 0.0678636, 0.0729792, 0.111088514, 0.08235651, 0.08462362, 0.101910464, 0.07076921, 0.10193514, 0.04632634, 0.07072403, 0.046646327, 0.07975719, 0.14016831, 0.065648444, 0.058464058, 0.091531165, 0.16785538, 0.3356046, 0.5311478, 0.2865967, 0.387539, 0.2713587, 0.1814869, 0.12463925, 0.15057155, 0.1488689, 0.16670962, 0.14433283, 0.14399643, 0.17712304, 0.15891102, 0.1456268, 0.14264742, 0.08702031, 0.22120637, 0.1281169, 0.08734769, 0.09161982, 0.09537518, 0.07127851, 0.10361022, 0.15692478, 0.13982196, 0.108382076, 0.0782908, 0.07988083, 0.06861604, 0.10588473, 0.07876227, 0.11577153, 0.07040009, 0.08167268, 0.068201385, 0.1008958, 0.17353435, 0.42007372, 0.33499998, 0.41961172, 0.35659093, 0.12336533, 0.31308293, 0.15896349, 0.107424036, 0.19013198, 0.13127016, 0.15143427, 0.123804115, 0.09388923, 0.16134456, 0.13779889, 0.16280827, 0.09973005, 0.09842847, 0.09576283, 0.104618825, 0.08956736, 0.0676623, 0.09370926, 0.07406522, 0.08292761, 0.055875897, 0.083056115, 0.05124944, 0.12267169, 0.07858472, 0.057516374, 0.06038467, 0.07172942, 0.09875278, 0.058949582, 0.06324672, 0.060071908, 0.22304113, 0.371769, 0.28717923, 0.25250524, 0.20736519, 0.12918076, 0.16988425, 0.1332567, 0.111855805, 0.18403599, 0.26269105, 0.15989195, 0.14816646, 0.08003605, 0.17467804, 0.085280925, 0.068678364, 0.12921472, 0.09866182, 0.09513631, 0.088150986, 0.08208295, 0.08516296, 0.06249175, 0.14053433, 0.09427698, 0.083343245, 0.090450644, 0.076524444, 0.06680353, 0.09545561, 0.12951834, 0.070130765, 0.055385817, 0.06708576, 0.083504654, 0.05886341, 0.058516447, 0.23008756, 0.3723235, 0.30889308, 0.16418737, 0.122147925, 0.20718502, 0.21224137, 0.3658533, 0.4382793, 0.23739477, 0.0906852, 0.08446846, 0.17748569, 0.183609, 0.09560652, 0.10894968, 0.095853806, 0.108744085, 0.1557981, 0.083281524, 0.09431717, 0.079893745, 0.08660246, 0.104849026, 0.08306821, 0.06566762, 0.077079505, 0.06690545, 0.08704862, 0.07979558, 0.050239563, 0.06485826, 0.07468967, 0.09380764, 0.09000163, 0.073391646, 0.06291833, 0.060273107, 0.16647463, 0.20793132, 0.2283388, 0.32021743, 0.12487457, 0.19877556, 0.21117413, 0.29603028, 0.16838057, 0.31507546, 0.1589216, 0.115199074, 0.2236299, 0.24893951, 0.12038288, 0.082702324, 0.1045598, 0.093353115, 0.101452924, 0.079383515, 0.11434032, 0.087959364, 0.13430905, 0.08455522, 0.0988595, 0.101399355, 0.083332226, 0.056739036, 0.081062935, 0.059477136, 0.088713266, 0.08745439, 0.07794292, 0.079421826, 0.07399328, 0.08864496, 0.07993941, 0.06981995, 0.18869501, 0.2668372, 0.28381354, 0.41008818, 0.18948963, 0.15079236, 0.12489326, 0.112469055, 0.15347938, 0.18103907, 0.08551413, 0.09360354, 0.124233544, 0.084610835, 0.10512807, 0.13721846, 0.060549546, 0.12676457, 0.077640705, 0.060442034, 0.13066182, 0.092794076, 0.113161236, 0.06719287, 0.088018514, 0.1015378, 0.07036573, 0.07843153, 0.06185793, 0.10029045, 0.090126805, 0.073669694, 0.08706448, 0.077098586, 0.06513053, 0.04158997, 0.047980063, 0.044085253, 0.2247446, 0.13675332, 0.2870388, 0.17025605, 0.1738443, 0.30256078, 0.16161954, 0.14368884, 0.17478256, 0.14165917, 0.116935804, 0.108763196, 0.0942211, 0.13388526, 0.13161512, 0.08973029, 0.14550743, 0.18872397, 0.10227633, 0.08244517, 0.102463305, 0.0970003, 0.09320069, 0.080137014, 0.07917348, 0.13123094, 0.08811635, 0.12896971, 0.06478264, 0.0957814, 0.0801242, 0.06869525, 0.060941007, 0.051175427, 0.068208896, 0.06626087, 0.08384397, 0.063493736, 0.03902655, 0.048070837, 0.05575998, 0.08449967, 0.09118227, 0.20006713, 0.2572728, 0.30492365, 0.25029758, 0.22022463, 0.224062, 0.14256911, 0.083657205, 0.1291529, 0.17828368, 0.10075126, 0.10047096, 0.106975205, 0.08076963, 0.13371837, 0.092921235, 0.10530108, 0.07712196, 0.14489682, 0.08172021, 0.09520765, 0.12800293, 0.09880183, 0.07476797, 0.084456176, 0.10404864, 0.09127288, 0.05365083, 0.07112517, 0.056451514, 0.08790845, 0.057278205, 0.057620436, 0.07287989, 0.040053394, 0.066969536, 0.047451604, 0.060102783, 0.19569671, 0.19895501, 0.523369, 0.40670905, 0.24050166, 0.14588723, 0.1328445, 0.119690105, 0.11174389, 0.13574214, 0.13221306, 0.12909007, 0.09766087, 0.1546835, 0.15072511, 0.15539819, 0.08329966, 0.078640416, 0.077030465, 0.10585183, 0.11612929, 0.07793806, 0.10016024, 0.075230315, 0.06764229, 0.05769219, 0.0592471, 0.07070047, 0.076034665, 0.0625996, 0.07088215, 0.06561392, 0.06546568, 0.06766729, 0.050248116, 0.056539703, 0.060926907, 0.058335762, 0.36237836, 0.22638175, 0.2654121, 0.44705907, 0.34444913, 0.2437265, 0.22520888, 0.2862237, 0.3119947, 0.13856499, 0.116856374, 0.107602134, 0.20233396, 0.095668614, 0.089663275, 0.07521562, 0.12294068, 0.096720956, 0.09594879, 0.06793521, 0.07104812, 0.09289453, 0.071199864, 0.07284011, 0.081627294, 0.07089041, 0.055407476, 0.0601796, 0.072095454, 0.0537637, 0.066727616, 0.05322199, 0.07333744, 0.06822428, 0.05900829, 0.05380377, 0.06594226, 0.07388248, 0.31194872, 0.32701856, 0.22782014, 0.15872769, 0.30163056, 0.11325561, 0.09082511, 0.176714, 0.24886362, 0.16463742, 0.13379817, 0.121214844, 0.13609377, 0.10968108, 0.14093022, 0.2190222, 0.13110918, 0.108911656, 0.05865348, 0.08693967, 0.09993337, 0.0789612, 0.06887681, 0.086149305, 0.053494353, 0.050596476, 0.09498051, 0.09692401, 0.07982555, 0.07584754, 0.09258298, 0.10194262, 0.06167059, 0.06643762, 0.056648403, 0.05630581, 0.059748262, 0.06913956, 0.2535676, 0.42534903, 0.42155513, 0.376297, 0.29157323, 0.15503559, 0.1104448, 0.1435542, 0.1276621, 0.12826543, 0.107088946, 0.11484834, 0.10541135, 0.1383776, 0.114994526, 0.10453937, 0.10381396, 0.09678456, 0.10595702, 0.080464415, 0.07636436, 0.060362056, 0.05897978, 0.09294756, 0.08266653, 0.07893231, 0.06164085, 0.08387449, 0.061070085, 0.06871259, 0.06914935, 0.067075565, 0.061431296, 0.08295173, 0.055008404, 0.04615437, 0.06280133, 0.077635705, 0.072075106, 0.061097354, 0.070380375, 0.040533178, 0.09258002, 0.057274718, 0.24649715, 0.5406279, 0.28020307, 0.11240609, 0.15948787, 0.14761183, 0.13369992, 0.11943436, 0.11125739, 0.13404629, 0.08752745, 0.072153024, 0.05804878, 0.08885002, 0.079799525, 0.08736994, 0.09802624, 0.12048127, 0.10991465, 0.073692925, 0.06348266, 0.06662582, 0.048599828, 0.06710086, 0.060957797, 0.07048482, 0.061149754, 0.07692657, 0.10331908, 0.058356673, 0.046210386, 0.064103365, 0.08626868, 0.07185268, 0.051917482, 0.07258584, 0.06867355, 0.08191935, 0.125223, 0.23564796, 0.21901615, 0.18845212, 0.2501761, 0.1621312, 0.20326936, 0.1783401, 0.19580576, 0.12126294, 0.07812593, 0.090900354, 0.11683218, 0.10697745, 0.07117867, 0.1147115, 0.06625943, 0.09055268, 0.0975621, 0.07290111, 0.09353268, 0.06246689, 0.060257345, 0.054804213, 0.082523964, 0.07333162, 0.07937157, 0.0631131, 0.038928114, 0.07687079, 0.07413411, 0.07214953, 0.0644055, 0.07063178, 0.04736494, 0.05028874, 0.07978237, 0.06570498, 0.2051713, 0.16891588, 0.28695872, 0.16119282, 0.17317316, 0.13879979, 0.11840791, 0.22331083, 0.1355073, 0.12226667, 0.119477525, 0.11593239, 0.1064105, 0.0912888, 0.10427803, 0.09052565, 0.07683823, 0.09275125, 0.07052826, 0.09632823, 0.07852321, 0.10108, 0.07648017, 0.08455484, 0.048170082, 0.058564555, 0.08670962, 0.06363603, 0.06455931, 0.071204245, 0.06367528, 0.055040378, 0.06341189, 0.07057557, 0.06928625, 0.06407088, 0.08920933, 0.047030143, 0.24540801, 0.15302655, 0.3256018, 0.17706574, 0.1438027, 0.08356661, 0.1535819, 0.11393791, 0.19366379, 0.10477271, 0.1297128, 0.1282031, 0.08554468, 0.15084606, 0.13152662, 0.111657836, 0.054162916, 0.097248666, 0.10876964, 0.070123255, 0.08463603, 0.05626969, 0.05825105, 0.062286943, 0.07578412, 0.046034824, 0.08104029, 0.06093956, 0.058401275, 0.05183289, 0.0930326, 0.056114107, 0.05037631, 0.054093298, 0.07059605, 0.06169973, 0.05102563, 0.05637408, 0.22970489, 0.22765689, 0.25285485, 0.28484434, 0.15008146, 0.081372, 0.18952174, 0.14635225, 0.13634963, 0.3622547, 0.10729232, 0.103113696, 0.15369327, 0.10643552, 0.14018084, 0.091950856, 0.09010614, 0.090828046, 0.09970888, 0.11547305, 0.10355147, 0.096517585, 0.066293105, 0.08569645, 0.09109505, 0.10709539, 0.12803976, 0.06685121, 0.046453085, 0.052256256, 0.054678, 0.051595338, 0.07965806, 0.055082787, 0.06322478, 0.07392711, 0.056460857, 0.07081352, 0.062391948, 0.058894277, 0.053953107, 0.043339096, 0.05414519, 0.27244925, 0.2819771, 0.15049991, 0.293963, 0.22906601, 0.16005774, 0.1485589, 0.21483403, 0.15162651, 0.2167664, 0.21638122, 0.3302833, 0.09089575, 0.078361824, 0.115864895, 0.16419895, 0.08854417, 0.044055488, 0.09607679, 0.09583977, 0.07350428, 0.053184994, 0.056282427, 0.09462222, 0.09944053, 0.062174786, 0.05436256, 0.04856068, 0.056363847, 0.047234517, 0.07429657, 0.068446696, 0.0556082, 0.054539096, 0.046497967, 0.0402826, 0.03889536, 0.05930474, 0.14926168, 0.14687291, 0.43635383, 0.24446689, 0.34100148, 0.31544375, 0.16014618, 0.14495377, 0.11266609, 0.12744376, 0.07700705, 0.060889985, 0.07743301, 0.10222094, 0.1674529, 0.13413616, 0.07661084, 0.09783171, 0.102660395, 0.10136547, 0.06698854, 0.063588254, 0.053135954, 0.060236335, 0.10226262, 0.097462855, 0.06434243, 0.069889754, 0.057441123, 0.07779334, 0.053920966, 0.039813675, 0.07084042, 0.08019043, 0.05347686, 0.05108886, 0.07661321, 0.06349446, 0.14750785, 0.35872146, 0.34517136, 0.32536972, 0.1609216, 0.09773244, 0.13664703, 0.12469659, 0.187606, 0.13197093, 0.11216849, 0.08653241, 0.09885484, 0.100509666, 0.061064728, 0.11138615, 0.10089262, 0.08300461, 0.10272125, 0.06937274, 0.054354083, 0.08868352, 0.08306179, 0.05542799, 0.07054041, 0.09647888, 0.062367633, 0.053634197, 0.065936625, 0.051614188, 0.068723515, 0.07026929, 0.05311811, 0.06242254, 0.05363414, 0.06461682, 0.046851136, 0.05663979, 0.28943515, 0.19902557, 0.33378813, 0.33021003, 0.16371416, 0.12622824, 0.1302681, 0.11432761, 0.18893296, 0.16262387, 0.2694324, 0.18389839, 0.09352259, 0.070357084, 0.07354198, 0.063453, 0.06633841, 0.059314426, 0.06285619, 0.06810442, 0.057901, 0.08695267, 0.05028811, 0.09934411, 0.05564243, 0.04841979, 0.051893227, 0.056967203, 0.06886452, 0.059812263, 0.06824791, 0.081462204, 0.056535885, 0.06784804, 0.055065442, 0.05790376, 0.06259161, 0.045328587, 0.1989115, 0.26960766, 0.18778509, 0.23742892, 0.17239904, 0.18930835, 0.11956833, 0.120514594, 0.06917599, 0.15791596, 0.1096732, 0.08582448, 0.06309466, 0.085553706, 0.08058991, 0.1588493, 0.10947864, 0.060140546, 0.04468253, 0.08793159, 0.11728441, 0.090198964, 0.055050015, 0.054722913, 0.0937025, 0.052384526, 0.040238198, 0.056752782, 0.04466298, 0.04911487, 0.06132414, 0.069343545, 0.04567934, 0.050354924, 0.05351783, 0.059304357, 0.04783078, 0.030148432, 0.04126561, 0.045895603, 0.047156602, 0.053170003, 0.052823257, 0.058132492, 0.22941744, 0.27921432, 0.21315901, 0.13535981, 0.14458027, 0.0970916, 0.09528945, 0.086092554, 0.08763292, 0.102351554, 0.062370747, 0.079286955, 0.116810285, 0.08845258, 0.13189161, 0.10514426, 0.13610978, 0.10951272, 0.08382502, 0.10146266, 0.058398355, 0.053427864, 0.051783387, 0.055197828, 0.069444984, 0.06934249, 0.05630188, 0.07564229, 0.057026874, 0.09031119, 0.058037426, 0.05362717, 0.058094043, 0.0639506, 0.07227669, 0.045447934, 0.051155657, 0.048694044, 0.09674612, 0.2576379, 0.31043828, 0.19011559, 0.20527454, 0.10979337, 0.10087135, 0.11068775, 0.113263346, 0.187044, 0.06963367, 0.07391883, 0.07413292, 0.049762785, 0.091226414, 0.07772607, 0.07620882, 0.045790862, 0.07331033, 0.09073309, 0.09111297, 0.062043183, 0.095310174, 0.05155271, 0.08341399, 0.045041863, 0.070375405, 0.062027756, 0.06989664, 0.047274835, 0.06402078, 0.05148721, 0.057963997, 0.059379548, 0.063459426, 0.052598376, 0.092939846, 0.04629876, 0.604063, 0.3589202, 0.16320908, 0.5109721, 0.16649309, 0.15586223, 0.1679478, 0.14745413, 0.10653483, 0.06299523, 0.118315406, 0.16271517, 0.081004545, 0.076336205, 0.06445923, 0.11448062, 0.089928195, 0.049709372, 0.052514773, 0.072032616, 0.06613308, 0.06733441, 0.0668701, 0.06837076, 0.06456496, 0.06111297, 0.049573675, 0.066048704, 0.05802676, 0.06953285, 0.042244095, 0.043494362, 0.06712451, 0.07506659, 0.060041673, 0.07664884, 0.046311468, 0.057198394, 0.20447578, 0.31286073, 0.24179968, 0.2535284, 0.2216156, 0.13588183, 0.124621406, 0.13484068, 0.10464989, 0.1324344, 0.077117614, 0.102098644, 0.10479906, 0.07155817, 0.08856225, 0.056224063, 0.041172534, 0.05675792, 0.059737418, 0.05251362, 0.06331272, 0.044100076, 0.043383736, 0.07462628, 0.055535387, 0.05402875, 0.039631236, 0.072715856, 0.05733942, 0.032929078, 0.041947924, 0.042593703, 0.06458672, 0.050903518, 0.07957854, 0.0573137, 0.06404406, 0.050291106, 0.11779914, 0.2338113, 0.15767363, 0.13216741, 0.14445357, 0.15650938, 0.10360207, 0.14137962, 0.109986156, 0.08318786, 0.11637715, 0.071765415, 0.07841389, 0.06353611, 0.0679176, 0.09803993, 0.094693325, 0.07150194, 0.0760297, 0.050829004, 0.080026254, 0.047634967, 0.051592216, 0.071180426, 0.057811886, 0.09211039, 0.07012226, 0.055988405, 0.06288968, 0.04411562, 0.050665535, 0.07408143, 0.06445847, 0.054720905, 0.06156747, 0.053874403, 0.06938026, 0.04733355, 0.049480833, 0.059223633, 0.057906277, 0.045711506, 0.05794041, 0.144, 0.16871405, 0.2164044, 0.23472944, 0.17642911, 0.13364436, 0.08475715, 0.08480911, 0.12353974, 0.1049748, 0.061759163, 0.064288855, 0.09054309, 0.12381047, 0.07640983, 0.12967303, 0.08673944, 0.0916742, 0.094190896, 0.07026553, 0.07487214, 0.0687796, 0.07092636, 0.06333716, 0.051095463, 0.050653066, 0.041585244, 0.06284149, 0.058044158, 0.069712974, 0.038914297, 0.07096472, 0.0547191, 0.040748585, 0.08450881, 0.046903234, 0.057379354, 0.046462916, 0.14395148, 0.24986935, 0.20672068, 0.23104931, 0.1793158, 0.09136467, 0.07998146, 0.09536883, 0.07019011, 0.15380791, 0.08455528, 0.12077302, 0.07970032, 0.055047292, 0.08718915, 0.06808963, 0.055806912, 0.056925498, 0.05652192, 0.051343083, 0.080094114, 0.10066793, 0.07226434, 0.051375233, 0.04678242, 0.06768851, 0.046026208, 0.049423873, 0.058780324, 0.047576156, 0.071459286, 0.053506374, 0.043717537, 0.048437215, 0.054990135, 0.04248734, 0.056657303, 0.036523912, 0.113117635, 0.22773816, 0.15386464, 0.14949699, 0.16989143, 0.13270555, 0.15422249, 0.2645976, 0.09327275, 0.12752755, 0.09151251, 0.057309434, 0.06433239, 0.08342051, 0.049689878, 0.05656627, 0.1021631, 0.054705307, 0.0705749, 0.0721004, 0.05973149, 0.04789856, 0.055328544, 0.0673123, 0.04584284, 0.059469312, 0.03533827, 0.0450668, 0.048731327, 0.056133408, 0.04389421, 0.054728728, 0.05525676, 0.049469814, 0.044373874, 0.047455255, 0.048754014, 0.05714567, 0.13747323, 0.18380882, 0.13590737, 0.25619283, 0.19048043, 0.113887146, 0.16504817, 0.103232175, 0.08095184, 0.11995992, 0.091967225, 0.071369305, 0.05911848, 0.08572985, 0.0843461, 0.08724714, 0.070454635, 0.050945967, 0.0525421, 0.05888044, 0.06247363, 0.051055886, 0.043771494, 0.06529798, 0.074497715, 0.055176407, 0.06179118, 0.047052667, 0.04593007, 0.0509105, 0.045706227, 0.037585925, 0.056669053, 0.042925704, 0.043151744, 0.041971024, 0.04233396, 0.04670551, 0.13043685, 0.37090483, 0.11390879, 0.12432223, 0.14851885, 0.116595685, 0.095676206, 0.16530597, 0.092644796, 0.07036231, 0.15492271, 0.08663233, 0.07726254, 0.05717803, 0.083249524, 0.09237655, 0.07313316, 0.06904577, 0.06620082, 0.05505053, 0.06788428, 0.03842748, 0.05812084, 0.05129237, 0.062204994, 0.050701894, 0.04653175, 0.042013012, 0.06302189, 0.054186575, 0.048625983, 0.04272329, 0.069170214, 0.052072234, 0.060222965, 0.034418885, 0.05776531, 0.055689383, 0.038204253, 0.044439863, 0.050083846, 0.056506146, 0.047631003, 0.040042877, 0.08467197, 0.12529579, 0.2207422, 0.21835661, 0.12351047, 0.12272204, 0.061793536, 0.123929754, 0.10841008, 0.07441411, 0.090416014, 0.06899982, 0.063573115, 0.07040761, 0.0816397, 0.036422264, 0.03686002, 0.06591465, 0.083889775, 0.056245033, 0.08803222, 0.048627693, 0.05162527, 0.04472092, 0.06458933, 0.064810015, 0.047789775, 0.057695307, 0.0647925, 0.03981224, 0.03474868, 0.06305303, 0.054615926, 0.053923212, 0.05862158, 0.042018443, 0.0505304, 0.03556302, 0.11715977, 0.28631377, 0.16897984, 0.14608523, 0.084726386, 0.0872311, 0.12448075, 0.21721543, 0.11104538, 0.1018458, 0.07502308, 0.10241121, 0.05379402, 0.094244905, 0.10127029, 0.095185675, 0.06742106, 0.08436625, 0.059051406, 0.104557164, 0.046791792, 0.04315955, 0.04905914, 0.050609335, 0.05474019, 0.059684873, 0.045631126, 0.056025725, 0.06447006, 0.058189567, 0.058559474, 0.07331815, 0.06618483, 0.047446154, 0.049421314, 0.047617923, 0.061222326, 0.04306388, 0.11627254, 0.24363221, 0.09065485, 0.14298701, 0.14811447, 0.096883856, 0.10105758, 0.11737268, 0.10337124, 0.14480619, 0.09414252, 0.05851137, 0.059912913, 0.05615988, 0.09030225, 0.08279667, 0.06426578, 0.09849747, 0.044891946, 0.08607203, 0.061064105, 0.058532484, 0.037797548, 0.055788487, 0.05649361, 0.06371405, 0.044607036, 0.04423712, 0.044848938, 0.04764454, 0.03839487, 0.036400825, 0.045227338, 0.056610152, 0.039036147, 0.048831977, 0.042059485, 0.0415233, 0.05325598, 0.13697973, 0.24319737, 0.1281864, 0.198437, 0.12648238, 0.061646875, 0.24765572, 0.15947896, 0.17407751, 0.10616812, 0.07643697, 0.054916162, 0.075643, 0.0881522, 0.10937959, 0.07688656, 0.081040084, 0.057814956, 0.072166026, 0.060996406, 0.07671827, 0.05639208, 0.062185094, 0.052218944, 0.05791322, 0.07662461, 0.046841282, 0.05934221, 0.06132178, 0.05001268, 0.06806499, 0.05115898, 0.043896317, 0.05247261, 0.048102632, 0.028956303, 0.044830482, 0.06158505, 0.12532012, 0.19277272, 0.43004808, 0.2252184, 0.17157686, 0.18904626, 0.100770295, 0.09160315, 0.10642773, 0.084994294, 0.07151203, 0.07531083, 0.08647674, 0.08389641, 0.055332057, 0.043658387, 0.044872936, 0.0955804, 0.06660452, 0.04288152, 0.0663703, 0.05883866, 0.04096929, 0.034514096, 0.06149271, 0.04431202, 0.077917, 0.05195375, 0.059887253, 0.02952331, 0.06747682, 0.0524852, 0.049095638, 0.049712665, 0.046414435, 0.030966232, 0.05357258, 0.029149221, 0.03980831, 0.051904686, 0.05154042, 0.0526658, 0.09867482, 0.36207318, 0.18935366, 0.18093279, 0.13974208, 0.111032926, 0.11361311, 0.12302107, 0.14602405, 0.0944187, 0.10510209, 0.07505635, 0.04234422, 0.10159319, 0.06553764, 0.07174622, 0.046805374, 0.04215023, 0.058387905, 0.05769063, 0.055280026, 0.049533974, 0.051385716, 0.050274815, 0.0329361, 0.08392841, 0.062475927, 0.050433088, 0.038557522, 0.048008017, 0.05294709, 0.07458212, 0.039017938, 0.045864843, 0.047512244, 0.049925577, 0.04062431, 0.037117183, 0.16358545, 0.2940634, 0.17403354, 0.10056177, 0.17426501, 0.09403933, 0.12504329, 0.1286198, 0.16508804, 0.15653159, 0.07879915, 0.052390914, 0.047308832, 0.11929908, 0.08507749, 0.059636757, 0.05811995, 0.0778481, 0.07850619, 0.10297885, 0.0718568, 0.058692243, 0.06331973, 0.044891983, 0.06867478, 0.050573938, 0.06260814, 0.040157977, 0.0415977, 0.06916189, 0.044048723, 0.04847834, 0.045579266, 0.03243783, 0.04918024, 0.036648404, 0.053638488, 0.028646516, 0.16032244, 0.15278836, 0.11489898, 0.13442732, 0.1426865, 0.09100767, 0.043873955, 0.05790054, 0.13519394, 0.19449274, 0.09266016, 0.069796525, 0.093995266, 0.08666227, 0.08331959, 0.06307755, 0.07110502, 0.050615735, 0.06980096, 0.0484275, 0.06035649, 0.050308153, 0.061859895, 0.032467097, 0.047153227, 0.042015344, 0.04795264, 0.055155624, 0.05745662, 0.0653818, 0.050562304, 0.043674055, 0.047228158, 0.04415731, 0.059020225, 0.055756815, 0.038543582, 0.055467907, 0.082512856, 0.1502883, 0.2100807, 0.23945056, 0.12271395, 0.065111324, 0.13394526, 0.09756801, 0.12208699, 0.1041613, 0.055624083, 0.07560226, 0.06427357, 0.069271386, 0.057148118, 0.059087332, 0.073131606, 0.04789515, 0.045421712, 0.0834098, 0.07100562, 0.03339429, 0.050416727, 0.04054802, 0.054083716, 0.044533182, 0.054577827, 0.049435712, 0.03240523, 0.06365147, 0.033858165, 0.042011213, 0.04221007, 0.033713173, 0.058691714, 0.05430608, 0.040017903, 0.076223545, 0.09117528, 0.13173647, 0.1602547, 0.15235607, 0.18055223, 0.3158071, 0.2414611, 0.10552659, 0.12721694, 0.10544482, 0.14841934, 0.07830484, 0.078475244, 0.10254715, 0.05489158, 0.060438443, 0.06083242, 0.05609994, 0.051195685, 0.05066119, 0.03852048, 0.05537771, 0.05278054, 0.05761509, 0.047618877, 0.06297483, 0.056639403, 0.038781416, 0.042996313, 0.05220752, 0.0515284, 0.05651019, 0.04180676, 0.040754564, 0.04714305, 0.05180924, 0.045235395, 0.04507159, 0.09258813, 0.046566952, 0.034317035, 0.049046323, 0.052275196, 0.035897885, 0.09627782, 0.10861017, 0.109576225, 0.15360689, 0.094651915, 0.08228671, 0.085917495, 0.2100573, 0.13874574, 0.12288845, 0.07248402, 0.10601134, 0.11374734, 0.073191315, 0.057465177, 0.05632913, 0.049169358, 0.056960545, 0.053853977, 0.04125314, 0.06498744, 0.049089104, 0.051852103, 0.06348384, 0.043913852, 0.039972834, 0.046711892, 0.02689527, 0.045044, 0.081877574, 0.046729922, 0.038276087, 0.03864647, 0.034204714, 0.05734828, 0.06404668, 0.050382227, 0.05207161, 0.18337579, 0.3174584, 0.11724437, 0.11427408, 0.14037502, 0.1467828, 0.24309455, 0.119609304, 0.15394264, 0.09433804, 0.06616733, 0.04752033, 0.066142015, 0.06809547, 0.0936986, 0.06430318, 0.07463837, 0.05343594, 0.06791534, 0.06658291, 0.053269632, 0.045917463, 0.033433568, 0.060725864, 0.06626648, 0.0479242, 0.04783346, 0.053724125, 0.049638856, 0.025916373, 0.060040843, 0.032912057, 0.03941905, 0.047731727, 0.053736255, 0.04019247, 0.0329159, 0.052800823, 0.11177417, 0.1874045, 0.12985565, 0.20544697, 0.14818716, 0.16188656, 0.18030035, 0.18403302, 0.13497452, 0.15709181, 0.08139806, 0.071102045, 0.050531387, 0.04919402, 0.055564713, 0.051025953, 0.05374396, 0.08013938, 0.060181763, 0.046980623, 0.032663796, 0.039280966, 0.050070453, 0.043044746, 0.040094357, 0.052692596, 0.050277457, 0.06656869, 0.048225347, 0.038711842, 0.03865794, 0.05841598, 0.053261813, 0.036481217, 0.05006592, 0.03941386, 0.03418417, 0.03871796, 0.1112628, 0.1703392, 0.21758626, 0.1600414, 0.10017393, 0.07691715, 0.08300544, 0.08803025, 0.10946908, 0.09430193, 0.14840941, 0.07441801, 0.07816626, 0.10627685, 0.048327167, 0.06337198, 0.065485865, 0.045421008, 0.048793256, 0.112660944, 0.07966218, 0.05308721, 0.059817106, 0.05515874, 0.047895983, 0.08123674, 0.039514184, 0.057521995, 0.054780565, 0.04458088, 0.03159318, 0.030272638, 0.035849843, 0.03293388, 0.041099902, 0.07774013, 0.027218685, 0.059451792, 0.11104182, 0.17938897, 0.18424165, 0.19005214, 0.1305134, 0.1199421, 0.095882595, 0.11097657, 0.2379899, 0.09738874, 0.07428616, 0.055781234, 0.04215325, 0.055490725, 0.08880556, 0.06192687, 0.13911007, 0.08912778, 0.064756036, 0.045976717, 0.050819375, 0.042563885, 0.07398553, 0.051343232, 0.062443763, 0.061616182, 0.038566906, 0.040552393, 0.06603668, 0.04915399, 0.060679007, 0.039403822, 0.0449235, 0.031532187, 0.035918817, 0.035718165, 0.04247502, 0.03547699, 0.04133867, 0.035784427, 0.04323283, 0.050740115, 0.039430812, 0.085020795, 0.1409275, 0.16560502, 0.14223182, 0.20778011, 0.12935272, 0.11749802, 0.08338068, 0.0584444, 0.06999642, 0.09660687, 0.06004371, 0.07405541, 0.082997054, 0.064481996, 0.054888688, 0.05021985, 0.034950987, 0.03857107, 0.086727574, 0.07846274, 0.059390992, 0.053293, 0.037360173, 0.047093075, 0.041388225, 0.0539024, 0.03885932, 0.03439476, 0.030489549, 0.05878562, 0.042434122, 0.033145677, 0.03984338, 0.03490668, 0.04128196, 0.040888082, 0.05999425, 0.08183576, 0.10143446, 0.10915312, 0.21939783, 0.115801334, 0.07931432, 0.07758973, 0.05832852, 0.13281627, 0.07189882, 0.08403882, 0.092131875, 0.0932249, 0.06962929, 0.065988064, 0.039735354, 0.049503032, 0.051177744, 0.116149135, 0.04299894, 0.0621299, 0.051111907, 0.030749312, 0.029927332, 0.044664163, 0.055867095, 0.050184, 0.039997477, 0.03481893, 0.052127708, 0.030517217, 0.040031258, 0.030514225, 0.05678608, 0.042198073, 0.043694545, 0.049551677, 0.047130354, 0.20430674, 0.17492886, 0.13662009, 0.13026585, 0.23171633, 0.11286286, 0.05240974, 0.04669071, 0.06590067, 0.09004771, 0.06763546, 0.05189325, 0.042300064, 0.08895036, 0.064543575, 0.044959255, 0.05049198, 0.05386244, 0.06518598, 0.04552713, 0.038015105, 0.08008568, 0.04316955, 0.05914503, 0.05486257, 0.052638337, 0.05724137, 0.032468125, 0.03848802, 0.043890357, 0.05167115, 0.04243375, 0.04431286, 0.05136808, 0.048924174, 0.04509299, 0.03533563, 0.045095425, 0.14485022, 0.13090105, 0.09933588, 0.12427174, 0.19828553, 0.1388445, 0.13267632, 0.094006754, 0.1083407, 0.13429636, 0.077334076, 0.046159077, 0.03991096, 0.053148765, 0.06805602, 0.07952129, 0.06793548, 0.06361151, 0.045759432, 0.059335608, 0.05472727, 0.067995526, 0.03597918, 0.06822654, 0.053158887, 0.053461783, 0.06455809, 0.0667665, 0.038345415, 0.03763681, 0.039909933, 0.042882267, 0.0386141, 0.04846701, 0.04197263, 0.031388905, 0.060803365, 0.06596769, 0.08121251, 0.08920393, 0.11544255, 0.15380429, 0.1213115, 0.12986332, 0.070008464, 0.056871433, 0.08510638, 0.10899204, 0.06686797, 0.100422755, 0.064064674, 0.0739444, 0.08579508, 0.07430204, 0.07961928, 0.054081988, 0.051253233, 0.05194712, 0.04986843, 0.06514228, 0.057501875, 0.036637414, 0.061650865, 0.058993485, 0.048031174, 0.065669276, 0.04942485, 0.037426043, 0.03701439, 0.05352919, 0.05981691, 0.05585442, 0.029614056, 0.04572337, 0.020825237, 0.031386927, 0.04002441, 0.035727166, 0.03548682, 0.034614764, 0.025470676, 0.031319503, 0.117531665, 0.27834776, 0.2463986, 0.21016453, 0.09729851, 0.08067297, 0.089491785, 0.0755408, 0.07955517, 0.09232779, 0.10960414, 0.057168134, 0.050273944, 0.066304654, 0.12991412, 0.072246276, 0.080283895, 0.06484716, 0.062181365, 0.05142227, 0.055421088, 0.053594157, 0.042799395, 0.045857478, 0.04971551, 0.06985667, 0.081842706, 0.0444158, 0.039268192, 0.03206995, 0.045625545, 0.034023225, 0.04370131, 0.046561375, 0.04275367, 0.044425417, 0.0313229, 0.041461617, 0.08548248, 0.1156197, 0.19786057, 0.20917198, 0.07210362, 0.062576674, 0.10026223, 0.091648735, 0.12922949, 0.11763659, 0.061643098, 0.07426518, 0.11338973, 0.06430199, 0.05793468, 0.043904435, 0.090338744, 0.07433649, 0.052450195, 0.0318647, 0.028118802, 0.060087837, 0.03999393, 0.04004088, 0.04958482, 0.052038103, 0.047834177, 0.0378519, 0.042009007, 0.03607113, 0.032961257, 0.038141105, 0.053376023, 0.034977052, 0.04857643, 0.04914241, 0.05123667, 0.0334537, 0.11284276, 0.108544804, 0.12656112, 0.18364681, 0.0705242, 0.11814398, 0.115236536, 0.10707027, 0.1343692, 0.124290995, 0.070105284, 0.03981181, 0.059336398, 0.065585434, 0.06469476, 0.053347375, 0.057115838, 0.0719326, 0.052646548, 0.04005596, 0.06611366, 0.04609383, 0.06338168, 0.057282694, 0.04838102, 0.03357086, 0.04109993, 0.040876906, 0.033584733, 0.05292725, 0.03913458, 0.040632132, 0.054592308, 0.04970695, 0.035564497, 0.059689753, 0.047717102, 0.05999759, 0.104866624, 0.14475521, 0.110196926, 0.14373545, 0.11805529, 0.093793094, 0.1475429, 0.078723684, 0.10418024, 0.11494806, 0.06964777, 0.06298398, 0.056686986, 0.044929177, 0.09054863, 0.07193937, 0.07456694, 0.03517031, 0.040302884, 0.06888404, 0.07857758, 0.055753928, 0.061378527, 0.055039663, 0.04116703, 0.03697379, 0.026522486, 0.035777606, 0.043396655, 0.03136544, 0.04186249, 0.044687454, 0.039747253, 0.05373313, 0.029997142, 0.024801707, 0.04243907, 0.039566044, 0.16259305, 0.14242096, 0.10262535, 0.14700523, 0.11801681, 0.11356907, 0.123026185, 0.08288505, 0.10348663, 0.071181394, 0.07750008, 0.05456754, 0.0713978, 0.042381234, 0.055106018, 0.060142435, 0.05010931, 0.046176583, 0.07726403, 0.06995731, 0.059977613, 0.04672606, 0.056836072, 0.027688915, 0.043701004, 0.0637399, 0.04294957, 0.04326685, 0.04881322, 0.050128773, 0.043558035, 0.053596046, 0.0347529, 0.032376766, 0.032131758, 0.038794562, 0.051467467, 0.030106371, 0.121487476, 0.14311758, 0.14164604, 0.20957728, 0.10388764, 0.2012779, 0.0961638, 0.11078125, 0.12171533, 0.112105995, 0.13420609, 0.0721141, 0.057206366, 0.047390122, 0.05137208, 0.062166527, 0.056908935, 0.046322294, 0.04194984, 0.05443554, 0.04372341, 0.047392093, 0.046033848, 0.026386939, 0.054490697, 0.034050137, 0.033598486, 0.026890017, 0.037559368, 0.035169493, 0.030614678, 0.040995013, 0.04320545, 0.045821562, 0.04015238, 0.034812316, 0.048152857, 0.040124044, 0.12623794, 0.099512674, 0.09091326, 0.07910019, 0.0724894, 0.0679344, 0.06708589, 0.116398826, 0.13456552, 0.07241305, 0.06889259, 0.07261429, 0.09756224, 0.069036946, 0.07708969, 0.076996125, 0.045894206, 0.05708286, 0.075312845, 0.038018018, 0.030871272, 0.031365216, 0.04607003, 0.051539257, 0.032862492, 0.052460648, 0.04094589, 0.056895185, 0.055657838, 0.055818822, 0.03727042, 0.04177658, 0.037167106, 0.028522111, 0.029467331, 0.028579488, 0.040465996, 0.045252122, 0.1916668, 0.277902, 0.14346407, 0.22686636, 0.19818732, 0.14807734, 0.08198233, 0.06158204, 0.058575537, 0.070812315, 0.061188888, 0.045649152, 0.07480436, 0.054580063, 0.05830764, 0.054022606, 0.06116151, 0.086488746, 0.05150601, 0.057351496, 0.052669436, 0.057101727, 0.03917071, 0.043017264, 0.05335373, 0.050659087, 0.03947002, 0.033052675, 0.04973969, 0.048066832, 0.039771486, 0.047884554, 0.038410652, 0.06606493, 0.043654528, 0.038014602, 0.034067597, 0.05030625, 0.094324164, 0.11098033, 0.13290909, 0.23083112, 0.12502714, 0.07920701, 0.16299199, 0.11174438, 0.08066295, 0.08252019, 0.047825105, 0.05570596, 0.077729076, 0.089027226, 0.061010845, 0.06867323, 0.04685328, 0.061237518, 0.04978359, 0.047898315, 0.06692742, 0.047858942, 0.032871157, 0.041526847, 0.0468071, 0.04260323, 0.039427325, 0.048007943, 0.028779162, 0.06192353, 0.03581797, 0.03297694, 0.044513304, 0.03503564, 0.070531674, 0.04696757, 0.05669484, 0.04777683, 0.1161119, 0.15623577, 0.118634954, 0.077321164, 0.103092976, 0.16468088, 0.07860341, 0.08250588, 0.17548926, 0.072685115, 0.06148408, 0.042724866, 0.069623895, 0.05427475, 0.040032215, 0.06250116, 0.05412482, 0.049791425, 0.05512617, 0.061346482, 0.06136054, 0.034522425, 0.04324358, 0.026840264, 0.032378286, 0.061145175, 0.05651548, 0.053751897, 0.032215998, 0.027315652, 0.045975495, 0.045181263, 0.02724438, 0.056574453, 0.037990183, 0.049491145, 0.036271077, 0.039144486, 0.02707038, 0.04278748, 0.04449283, 0.038879577, 0.034245785, 0.03244013, 0.14430979, 0.13476977, 0.21450193, 0.13938016, 0.08181412, 0.07709632, 0.07017667, 0.11460196, 0.100075394, 0.14918324, 0.06738243, 0.056410223, 0.06479681, 0.044847384, 0.031909045, 0.054838784, 0.033706553, 0.046692804, 0.062033102, 0.04852641, 0.04991308, 0.04932633, 0.048337024, 0.05780509, 0.03642729, 0.041160256, 0.03799464, 0.035434987, 0.024464846, 0.03692628, 0.035570342, 0.041267846, 0.05184645, 0.028620174, 0.056100763, 0.046102613, 0.047448754, 0.23667009, 0.18803695, 0.07645792, 0.1336211, 0.1906903, 0.10051281, 0.118550636, 0.07439336, 0.09938841, 0.067545325, 0.0815779, 0.045056242, 0.052848388, 0.041591182, 0.036349595, 0.072144575, 0.05794208, 0.08257427, 0.04119282, 0.03570643, 0.03620316, 0.035981484, 0.045391586, 0.034629714, 0.054166216, 0.037478935, 0.044248793, 0.042727552, 0.036221154, 0.031086808, 0.039568316, 0.048918076, 0.028133593, 0.032428723, 0.03104939, 0.04136183, 0.04176732, 0.045488935, 0.15591474, 0.122398436, 0.10348785, 0.08189513, 0.16805111, 0.14185728, 0.10694263, 0.21601453, 0.07279589, 0.07647094, 0.077530034, 0.079500966, 0.05853278, 0.053518213, 0.08270419, 0.066534854, 0.062236536, 0.046040274, 0.05913781, 0.05108324, 0.052659243, 0.039293006, 0.036498982, 0.030639423, 0.054857217, 0.026852084, 0.04147301, 0.023560047, 0.044249266, 0.050855134, 0.041369416, 0.03183153, 0.03953258, 0.03316644, 0.04645636, 0.03637426, 0.074080124, 0.035032474, 0.11987866, 0.10846556, 0.13610332, 0.1353918, 0.24009581, 0.098422416, 0.05243007, 0.077486366, 0.07686684, 0.14977154, 0.063184954, 0.074612714, 0.045889154, 0.07266269, 0.06350124, 0.053245775, 0.049990702, 0.038565397, 0.059982494, 0.039754804, 0.049844183, 0.042550333, 0.032668583, 0.06430596, 0.045514546, 0.038294706, 0.037450444, 0.054457676, 0.04015627, 0.03817291, 0.049221314, 0.048128363, 0.039633077, 0.03208889, 0.03560773, 0.038670972, 0.06880036, 0.029564826, 0.11392493, 0.08317469, 0.1201281, 0.1375182, 0.22756797, 0.095884874, 0.21368328, 0.08646948, 0.07119731, 0.099653885, 0.0713261, 0.05026522, 0.051374305, 0.07926653, 0.09354488, 0.07795831, 0.088684775, 0.05284157, 0.059996523, 0.049318384, 0.04952175, 0.045241833, 0.047871154, 0.049624544, 0.04477883, 0.04183803, 0.04910611, 0.0514118, 0.038176868, 0.04037661, 0.054559883, 0.051025216, 0.05054295, 0.02856388, 0.048659712, 0.036025003, 0.05024505, 0.0479131, 0.030175677, 0.032075614, 0.04552409, 0.049252793, 0.05981232, 0.038128544, 0.13365622, 0.12038857, 0.080914795, 0.09410584, 0.11306979, 0.13299842, 0.07897753, 0.070503056, 0.11301992, 0.18825485, 0.1233409, 0.04991366, 0.075654924, 0.065020315, 0.066309415, 0.07066123, 0.050765473, 0.027667904, 0.03620379, 0.068437874, 0.067354284, 0.043081433, 0.04775043, 0.035271, 0.030826697, 0.043928094, 0.036801685, 0.03325909, 0.045736432, 0.029416608, 0.03197248, 0.03524981, 0.04666209, 0.054880112, 0.030956334, 0.033522945, 0.048911314, 0.038099702, 0.16324878, 0.17097495, 0.07748185, 0.098432, 0.1187891, 0.1134183, 0.096833795, 0.13966711, 0.18915991, 0.11191553, 0.045841884, 0.0500724, 0.034793984, 0.06383798, 0.07273594, 0.05148728, 0.039664917, 0.033408307, 0.050799392, 0.08118218, 0.06328051, 0.065454505, 0.04586513, 0.055654757, 0.044612214, 0.05116709, 0.027171226, 0.041506063, 0.034844905, 0.04550609, 0.036747225, 0.02775217, 0.033349406, 0.04506361, 0.026649302, 0.035025988, 0.037824314, 0.039871488, 0.071289, 0.12561221, 0.109229475, 0.11765299, 0.08663809, 0.11076056, 0.09581953, 0.072662584, 0.10556111, 0.09810837, 0.049624085, 0.050256897, 0.06269948, 0.04908785, 0.05915781, 0.054579634, 0.049143147, 0.08129533, 0.07152782, 0.05217104, 0.046209704, 0.049080867, 0.083814204, 0.046713233, 0.039138738, 0.044146843, 0.031949174, 0.051567085, 0.04716946, 0.03306053, 0.035723798, 0.058400486, 0.04041972, 0.052001204, 0.022367343, 0.029874023, 0.048240393, 0.03785558, 0.13910949, 0.17697364, 0.12534228, 0.13490665, 0.08676577, 0.091698274, 0.12394865, 0.08379878, 0.06763203, 0.06879547, 0.15382649, 0.10935919, 0.07583796, 0.08205006, 0.07928764, 0.04576674, 0.04051977, 0.053972173, 0.04547484, 0.04242965, 0.033514768, 0.04396193, 0.045704294, 0.03984535, 0.026148254, 0.04552295, 0.058983434, 0.047000498, 0.044458687, 0.06340337, 0.033380896, 0.04499622, 0.041643925, 0.04462411, 0.037617356, 0.035663594, 0.03792986, 0.027954891, 0.05097444, 0.1468713, 0.1352485, 0.18679392, 0.05902939, 0.15669416, 0.095812105, 0.06844418, 0.11084647, 0.1477007, 0.054464504, 0.06177197, 0.05345717, 0.11589314, 0.101247646, 0.041768495, 0.07397192, 0.05360024, 0.046783682, 0.045832764, 0.04811395, 0.034984343, 0.035656765, 0.031687345, 0.039400686, 0.048223052, 0.04503438, 0.03726193, 0.040984944, 0.025170831, 0.029006138, 0.05462041, 0.041484077, 0.041029092, 0.041348934, 0.04591642, 0.032335125, 0.025366679, 0.059224796, 0.0305508, 0.04574705, 0.025888119, 0.04156205, 0.08895572, 0.092307985, 0.09140638, 0.14259633, 0.091198534, 0.06937867, 0.09694096, 0.08586817, 0.097889654, 0.09040597, 0.07421238, 0.11184325, 0.06867715, 0.04823194, 0.06845867, 0.04546247, 0.049224522, 0.057175763, 0.038197804, 0.04986643, 0.031800073, 0.057989478, 0.054678787, 0.04704343, 0.04860157, 0.052434314, 0.043206908, 0.04964205, 0.030183017, 0.04239713, 0.025266543, 0.034286205, 0.03599362, 0.024262547, 0.053611793, 0.030616287, 0.04651916, 0.030626448, 0.07657159, 0.10881068, 0.33782634, 0.27419367, 0.107225955, 0.11205851, 0.10377576, 0.110616535, 0.064742655, 0.12258552, 0.06348841, 0.046051178, 0.05228569, 0.054952346, 0.059538014, 0.07124088, 0.062386587, 0.06377103, 0.048494115, 0.043432888, 0.04987597, 0.053861927, 0.057716105, 0.053346597, 0.03480242, 0.038507793, 0.040687963, 0.06432891, 0.029061794, 0.056279056, 0.061805174, 0.07131063, 0.044080008, 0.03857106, 0.022245057, 0.04056966, 0.06841972, 0.039213765, 0.07209375, 0.21337824, 0.116302565, 0.1362086, 0.21285482, 0.06931124, 0.109303534, 0.077975646, 0.1337372, 0.089828245, 0.07021939, 0.06627925, 0.05380416, 0.05327755, 0.05911945, 0.059254464, 0.036686797, 0.041477513, 0.037590876, 0.04642206, 0.02842691, 0.03311562, 0.04131101, 0.033593886, 0.04592573, 0.05277438, 0.05592793, 0.043610495, 0.03915604, 0.03440814, 0.028828446, 0.055577885, 0.030882487, 0.03813949, 0.03485876, 0.06613658, 0.042339433, 0.04393158, 0.09553219, 0.11199537, 0.0767477, 0.18227053, 0.16492304, 0.1431821, 0.10087758, 0.075498395, 0.05996539, 0.11603179, 0.07906391, 0.08409321, 0.058375288, 0.056940418, 0.045658756, 0.04957174, 0.051025532, 0.060885422, 0.05469985, 0.045138665, 0.046184458, 0.030510899, 0.06310675, 0.06474545, 0.057992917, 0.039542437, 0.030817471, 0.031693313, 0.040150553, 0.06615128, 0.028624883, 0.03912188, 0.038164422, 0.066956095, 0.038722634, 0.04091442, 0.030160684, 0.03846698, 0.15475182, 0.13819118, 0.09117365, 0.069244355, 0.1856872, 0.09324341, 0.045488395, 0.071463525, 0.09258475, 0.1304869, 0.061992694, 0.0644293, 0.07055515, 0.06411347, 0.0755668, 0.06331514, 0.072085924, 0.071142405, 0.072183944, 0.030969432, 0.044403445, 0.045606446, 0.03981137, 0.03562179, 0.05641808, 0.04910956, 0.04937191, 0.03778952, 0.03566504, 0.029162554, 0.049535282, 0.047572106, 0.038914412, 0.03406599, 0.034966264, 0.035234917, 0.035430342, 0.039586265, 0.02842662, 0.047481224, 0.032299574, 0.0620761, 0.038640216, 0.0295004, 0.13322422, 0.11472997, 0.19481221, 0.119440295, 0.10546893, 0.054908622, 0.055516124, 0.074176826, 0.06776869, 0.100267306, 0.08540669, 0.05806456, 0.067726724, 0.050681885, 0.054774817, 0.05434308, 0.04690166, 0.074448965, 0.06553061, 0.052481204, 0.057898212, 0.03676337, 0.041213065, 0.059363823, 0.04678561, 0.045128167, 0.026123963, 0.044451077, 0.04583348, 0.045522988, 0.0388774, 0.030467432, 0.062438156, 0.02981429, 0.04839176, 0.04616831, 0.043761324, 0.0316072, 0.09703331, 0.15143365, 0.1456293, 0.15462352, 0.17011052, 0.091875374, 0.09322431, 0.07393172, 0.06744953, 0.0635034, 0.055890962, 0.0462757, 0.048481032, 0.05386323, 0.051487356, 0.066230156, 0.038596183, 0.044133633, 0.037417836, 0.033488672, 0.040342107, 0.06341409, 0.03893428, 0.03726228, 0.04354645, 0.035356797, 0.031515263, 0.043778766, 0.047055908, 0.047009647, 0.026075661, 0.044481684, 0.044733584, 0.02912884, 0.020634746, 0.05317567, 0.056449484, 0.043041028, 0.12009292, 0.145577, 0.21318872, 0.15761471, 0.07373915, 0.10163206, 0.07554502, 0.1350881, 0.056507617, 0.083942, 0.099501714, 0.06080485, 0.055950075, 0.06117046, 0.050757654, 0.050985787, 0.042870995, 0.055093262, 0.063297056, 0.06538772, 0.063263275, 0.059599545, 0.03497444, 0.039733462, 0.044837072, 0.045109674, 0.052624244, 0.03218481, 0.0318304, 0.031519867, 0.043792695, 0.041244622, 0.03862803, 0.036670957, 0.033157084, 0.042181954, 0.038944196, 0.031296384, 0.07515726, 0.12750013, 0.15044749, 0.15236029, 0.06755346, 0.03493815, 0.073560074, 0.07911726, 0.13852042, 0.09433329, 0.045534935, 0.06657645, 0.041544806, 0.06311645, 0.06744134, 0.063369825, 0.042652335, 0.04191079, 0.048101652, 0.04792634, 0.055739257, 0.048953403, 0.03792541, 0.059189394, 0.047012746, 0.055163056, 0.04384427, 0.04807644, 0.04134612, 0.030288609, 0.039163206, 0.05697244, 0.036959324, 0.049054492, 0.032719176, 0.03881177, 0.036696944, 0.044186868, 0.14996131, 0.09703267, 0.082415335, 0.120288044, 0.082296446, 0.09776765, 0.09905911, 0.054180767, 0.10083729, 0.05041327, 0.074658915, 0.0327411, 0.046396185, 0.059457082, 0.04141657, 0.068236455, 0.044865653, 0.06252179, 0.038946513, 0.05036385, 0.034281023, 0.030459756, 0.039243538, 0.053186197, 0.07601546, 0.036240302, 0.04443422, 0.03604139, 0.032414816, 0.044417616, 0.048745874, 0.049125124, 0.031055588, 0.033167273, 0.052761134, 0.031829894, 0.030740825, 0.038045835, 0.04144521, 0.046760138, 0.031682614, 0.0331279, 0.049726125, 0.12183947, 0.0838061, 0.09156144, 0.061269414, 0.17153136, 0.07854918, 0.07548775, 0.06837137, 0.0517915, 0.1319005, 0.049435727, 0.07697135, 0.03669753, 0.044063717, 0.08055366, 0.061677266, 0.07418571, 0.052017536, 0.04749474, 0.04823464, 0.041944697, 0.044252682, 0.04295648, 0.051933955, 0.037023116, 0.05549759, 0.03974758, 0.034514997, 0.03653813, 0.028935112, 0.036808226, 0.03888576, 0.04536435, 0.047732484, 0.038064983, 0.038023334, 0.058022913, 0.034031976, 0.090271294, 0.060199987, 0.101678364, 0.087732926, 0.17696683, 0.10120544, 0.098361075, 0.09554559, 0.06854653, 0.061567217, 0.06949386, 0.08029027, 0.07514583, 0.04342525, 0.05218505, 0.10183071, 0.03845048, 0.048892524, 0.059725504, 0.09823827, 0.048056174, 0.034028832, 0.039386746, 0.027356597, 0.032846063, 0.047124863, 0.04127771, 0.030370837, 0.032144636, 0.04006186, 0.033634417, 0.034445353, 0.03173245, 0.04954936, 0.038685832, 0.050510544, 0.052793134, 0.028664859, 0.11453375, 0.19086358, 0.096725166, 0.24473813, 0.114289224, 0.10106232, 0.10230161, 0.12324753, 0.08712623, 0.082650125, 0.057802916, 0.064717405, 0.08628731, 0.055624887, 0.053072285, 0.057190374, 0.041323353, 0.048955582, 0.039827686, 0.06649815, 0.050377406, 0.049765248, 0.025872428, 0.033931207, 0.032802135, 0.07541408, 0.045592885, 0.030362152, 0.03509157, 0.030503755, 0.054253727, 0.039995104, 0.037022084, 0.039819162, 0.034160946, 0.036956202, 0.022752969, 0.034030914, 0.09086802, 0.11044401, 0.15063392, 0.1918186, 0.19936557, 0.08815627, 0.1566458, 0.07413894, 0.11226885, 0.06932392, 0.039211653, 0.059868764, 0.07448472, 0.06361375, 0.06272113, 0.0656808, 0.064960144, 0.06247745, 0.040921297, 0.06543395, 0.055804223, 0.038164627, 0.048541367, 0.040474042, 0.046671014, 0.032428116, 0.05382837, 0.03142941, 0.020652538, 0.0328438, 0.025160218, 0.053679753, 0.036604807, 0.02714076, 0.03358264, 0.03353883, 0.0297174, 0.045297895, 0.06584493, 0.08078319, 0.22367628, 0.20225208, 0.1268741, 0.0903336, 0.063514926, 0.06645717, 0.07296705, 0.0761446, 0.059038576, 0.06777764, 0.058076072, 0.05086984, 0.0284317, 0.051529568, 0.035082467, 0.025591698, 0.033850342, 0.06237993, 0.047200426, 0.03243701, 0.031427007, 0.027450355, 0.033362802, 0.04246304, 0.05630008, 0.023282116, 0.04576714, 0.06356267, 0.035488993, 0.048854336, 0.030158972, 0.03973558, 0.03714204, 0.020173585, 0.031631004, 0.0381173, 0.04589659, 0.036734585, 0.031262573, 0.02855858, 0.035354394, 0.03673918, 0.07242343, 0.09630942, 0.10548388, 0.07424127, 0.05777605, 0.078877114, 0.054215916, 0.13514613, 0.17764698, 0.13361251, 0.07364021, 0.07132396, 0.0610993, 0.06942928, 0.07261027, 0.08103096, 0.0588685, 0.04297059, 0.0374355, 0.035376407, 0.04356348, 0.061080705, 0.0331502, 0.03159085, 0.03946638, 0.032621432, 0.027079962, 0.0387157, 0.038121507, 0.026121616, 0.033576254, 0.032458503, 0.026343593, 0.031425692, 0.0423754, 0.023683941, 0.033580437, 0.027965399, 0.19297878, 0.19606103, 0.08549299, 0.2513385, 0.16469212, 0.06822807, 0.11161155, 0.06711237, 0.06289295, 0.13945799, 0.09097053, 0.06392558, 0.06512382, 0.035883166, 0.055804998, 0.036816686, 0.031945392, 0.035202764, 0.0394251, 0.046374846, 0.03273123, 0.027042344, 0.024788743, 0.04161007, 0.044249225, 0.04034555, 0.0334256, 0.04638211, 0.019473646, 0.023216514, 0.03384315, 0.0252381, 0.030243805, 0.03978967, 0.033353537, 0.04457586, 0.025652029, 0.0209385, 0.049578007, 0.16102867, 0.19188884, 0.11086774, 0.08304044, 0.06854484, 0.05558906, 0.11364286, 0.106234126, 0.049405843, 0.031521413, 0.0680746, 0.04964063, 0.07720483, 0.095235795, 0.032640107, 0.039672077, 0.058153857, 0.059916805, 0.056067493, 0.044255797, 0.034449723, 0.044057306, 0.044911098, 0.03295046, 0.032503705, 0.0324161, 0.040043086, 0.040929966, 0.045890946, 0.033972435, 0.027644685, 0.03633395, 0.02676382, 0.032677267, 0.028538521, 0.03797958, 0.028836984, 0.11256924, 0.17644075, 0.20411654, 0.19302191, 0.073450506, 0.072236545, 0.051262707, 0.18871506, 0.07723048, 0.098191954, 0.07538808, 0.033849817, 0.033459686, 0.038414665, 0.038176447, 0.04167657, 0.04673496, 0.027401017, 0.04593954, 0.042471256, 0.035187095, 0.034460098, 0.033632368, 0.045244686, 0.031354666, 0.03706361, 0.039267477, 0.036771066, 0.029400747, 0.032673556, 0.039172746, 0.029193044, 0.036471047, 0.028991558, 0.03797723, 0.032270383, 0.0391716, 0.030590102, 0.13154255, 0.11080368, 0.12926036, 0.18712658, 0.1341407, 0.085845426, 0.036902726, 0.061530206, 0.059443045, 0.041233163, 0.053830933, 0.05723853, 0.027864253, 0.026883455, 0.045400545, 0.046043903, 0.046935413, 0.07789073, 0.04025087, 0.051802825, 0.028882045, 0.027083017, 0.025771352, 0.038996752, 0.028342793, 0.042864464, 0.03757213, 0.049398728, 0.059029542, 0.035376523, 0.03535669, 0.034200482, 0.041268397, 0.027317131, 0.039844185, 0.03591549, 0.026157692, 0.03250432, 0.022865448, 0.025446462, 0.025667842, 0.036967907, 0.026690483, 0.13685903, 0.2658079, 0.21251902, 0.19251245, 0.118167125, 0.070912495, 0.070155725, 0.10175731, 0.16027184, 0.056462478, 0.054115698, 0.057683248, 0.066026, 0.04795092, 0.055267543, 0.060463995, 0.038804848, 0.050406743, 0.0501269, 0.055135697, 0.024587214, 0.042665303, 0.0291784, 0.023826635, 0.031604994, 0.032317333, 0.039696563, 0.027491525, 0.041733824, 0.037154425, 0.028225714, 0.03836595, 0.038563233, 0.03442274, 0.04232361, 0.02639263, 0.039457604, 0.03221437, 0.26510897, 0.08425144, 0.10029902, 0.14944057, 0.12345793, 0.13567628, 0.09984559, 0.058031697, 0.06627901, 0.058548376, 0.054515895, 0.06912551, 0.06034538, 0.048141763, 0.05140398, 0.06496069, 0.08060742, 0.03023749, 0.050004315, 0.028611628, 0.03894542, 0.0287645, 0.056389093, 0.027615882, 0.040865894, 0.05192936, 0.042311173, 0.03846289, 0.0414105, 0.04121291, 0.027968494, 0.039336786, 0.034499396, 0.05517627, 0.024616966, 0.042561125, 0.025770292, 0.019833848, 0.14458781, 0.21274886, 0.13613562, 0.09373713, 0.29686382, 0.11285663, 0.07537045, 0.0518445, 0.17032938, 0.1161526, 0.066630036, 0.062353153, 0.060465924, 0.07062556, 0.08202543, 0.058278993, 0.053666227, 0.039113715, 0.06681222, 0.04513593, 0.036359757, 0.027120745, 0.053748894, 0.028188266, 0.032980777, 0.040471144, 0.03235343, 0.03185609, 0.029961824, 0.03334578, 0.04142135, 0.02367088, 0.029335247, 0.044810195, 0.04440155, 0.037098996, 0.042103685, 0.031878375, 0.08212315, 0.10207314, 0.15032706, 0.09848939, 0.1234556, 0.098426245, 0.045590702, 0.062088244, 0.058982264, 0.16179104, 0.12055538, 0.056104552, 0.051878408, 0.04639406, 0.05662752, 0.08676877, 0.04490834, 0.047663618, 0.030929267, 0.027156211, 0.04090179, 0.036440846, 0.027067743, 0.027667211, 0.038556207, 0.03082699, 0.034792382, 0.028385794, 0.029087529, 0.038645905, 0.04142512, 0.040567838, 0.046124913, 0.032499555, 0.028592473, 0.021238374, 0.027899059, 0.020318963, 0.16573784, 0.39445043, 0.10100578, 0.23272306, 0.23914237, 0.11943054, 0.07157286, 0.08910453, 0.074685894, 0.15162264, 0.070859276, 0.036985766, 0.08948885, 0.07205981, 0.054063007, 0.025778212, 0.027959682, 0.028114712, 0.05143987, 0.0339755, 0.035029914, 0.074839175, 0.05268517, 0.030053562, 0.04383535, 0.035369705, 0.026252203, 0.030964576, 0.041201275, 0.045507394, 0.028902957, 0.021929994, 0.030824794, 0.05733149, 0.04429216, 0.02654054, 0.02563808, 0.025050832, 0.027779914, 0.046615213, 0.025655454, 0.026602928, 0.027568983, 0.022271788, 0.10459752, 0.07352353, 0.084752515, 0.100598246, 0.05182017, 0.040412374, 0.06447511, 0.05870167, 0.049984537, 0.15098555, 0.06293788, 0.043713003, 0.061344814, 0.03417724, 0.031497605, 0.03932355, 0.07100463, 0.04510187, 0.050370187, 0.040151313, 0.048018847, 0.041537743, 0.035532724, 0.02952433, 0.028574234, 0.030198418, 0.024881683, 0.03990732, 0.019787515, 0.023917394, 0.02583841, 0.029108725, 0.020292437, 0.020202167, 0.027480556, 0.024742268, 0.031940825, 0.04035796, 0.089256264, 0.2097224, 0.38473913, 0.091567695, 0.05802418, 0.08115869, 0.07318737, 0.066631585, 0.07420889, 0.049156405, 0.051551018, 0.04266118, 0.07454014, 0.07145219, 0.08700932, 0.066856645, 0.049243677, 0.037729442, 0.04495548, 0.033088755, 0.03978909, 0.04623904, 0.032505978, 0.031397194, 0.028652223, 0.03892294, 0.036702234, 0.03897486, 0.029480215, 0.033050805, 0.02696643, 0.030079389, 0.030733163, 0.03751608, 0.03074115, 0.036264088, 0.019545076, 0.016264804, 0.098087914, 0.08554564, 0.070255905, 0.061741196, 0.11193224, 0.08093378, 0.086145334, 0.1480762, 0.0718017, 0.09232088, 0.08412354, 0.039238058, 0.04624698, 0.0437387, 0.034941405, 0.042726956, 0.059335, 0.04511429, 0.03717087, 0.028795049, 0.04504318, 0.036176864, 0.021113222, 0.045824654, 0.026887719, 0.028313674, 0.024483748, 0.033784088, 0.03393696, 0.02920329, 0.049248707, 0.02407909, 0.03539619, 0.029134031, 0.024230355, 0.02559436, 0.029162223, 0.024203386, 0.053921662, 0.09798973, 0.07332702, 0.11582273, 0.059101757, 0.072424315, 0.09244613, 0.058158994, 0.052595586, 0.037986636, 0.04673094, 0.057789236, 0.065079674, 0.056350917, 0.04165435, 0.028297082, 0.03966665, 0.04360502, 0.024515089, 0.032788724, 0.045169964, 0.028775984, 0.056700833, 0.025009453, 0.04085228, 0.025549468, 0.0384551, 0.033349052, 0.029412718, 0.022281678, 0.025543824, 0.035291135, 0.024932114, 0.058069844, 0.026288189, 0.039116334, 0.030155448, 0.038576763, 0.04777487, 0.11549135, 0.25618306, 0.14947733, 0.0969275, 0.08471496, 0.0778905, 0.055708088, 0.0525004, 0.043993082, 0.03812425, 0.034932237, 0.042273663, 0.038186878, 0.0433111, 0.053500522, 0.02750771, 0.037747312, 0.048265487, 0.03523609, 0.036822822, 0.030910166, 0.048023798, 0.05721032, 0.02937785, 0.03507353, 0.0354115, 0.03633445, 0.032949343, 0.033593822, 0.033374127, 0.019150987, 0.025048744, 0.028673071, 0.032212336, 0.027115693, 0.047589194, 0.018650608, 0.027411941, 0.030274363, 0.022047147, 0.02039592, 0.032048237, 0.1920801, 0.15716352, 0.20535092, 0.083710805, 0.10431962, 0.07544107, 0.06071771, 0.058986515, 0.061938018, 0.085546635, 0.05918635, 0.065862715, 0.06707623, 0.1094649, 0.05897684, 0.05986328, 0.031956755, 0.032906562, 0.027851723, 0.018973285, 0.036514815, 0.033264678, 0.024131522, 0.022731915, 0.027130883, 0.035939846, 0.0368676, 0.026574567, 0.026415968, 0.021634081, 0.029765612, 0.031436488, 0.04545269, 0.0285994, 0.03129339, 0.019140836, 0.031037817, 0.022241443, 0.054296758, 0.08458016, 0.082960285, 0.10343857, 0.10651288, 0.11455235, 0.08516961, 0.055680852, 0.07198078, 0.04834178, 0.037340704, 0.04592107, 0.035911977, 0.047142945, 0.04864195, 0.029329332, 0.03958862, 0.056670822, 0.03818765, 0.03844576, 0.04747952, 0.031078292, 0.029047733, 0.03517603, 0.042031415, 0.03261476, 0.035118636, 0.030939564, 0.039391827, 0.037598554, 0.035319325, 0.040109526, 0.027733976, 0.034195468, 0.022862315, 0.027192974, 0.028740786, 0.021199372, 0.08514899, 0.07772607, 0.12794259, 0.18830463, 0.12973018, 0.05484855, 0.06542943, 0.09880902, 0.057981573, 0.068045974, 0.08859749, 0.051744334, 0.03216046, 0.04661709, 0.064767085, 0.036533214, 0.023024475, 0.034518927, 0.047537904, 0.024153255, 0.03729885, 0.032079548, 0.024896549, 0.04176893, 0.03340084, 0.02789392, 0.027868673, 0.023512261, 0.031944696, 0.026635578, 0.022887006, 0.031514734, 0.049643364, 0.024339916, 0.02218699, 0.030376539, 0.03423834, 0.031978715, 0.08605651, 0.06499356, 0.09709583, 0.13084698, 0.15678589, 0.10317364, 0.069658704, 0.049232244, 0.040928632, 0.054192394, 0.06060186, 0.058188844, 0.072995044, 0.04867564, 0.035311848, 0.030873867, 0.031116106, 0.03364476, 0.032902405, 0.037470017, 0.046518672, 0.034043465, 0.032534346, 0.025970925, 0.037574105, 0.043714333, 0.030122105, 0.03306124, 0.03200726, 0.030571664, 0.033096008, 0.028917924, 0.033683598, 0.033496488, 0.02795577, 0.020548051, 0.026693875, 0.024269259, 0.06596466, 0.078536294, 0.1748982, 0.10834453, 0.1007817, 0.06089172, 0.10902146, 0.14955665, 0.09584756, 0.08386578, 0.061532762, 0.027980851, 0.08733446, 0.06370018, 0.049487296, 0.04654075, 0.074127436, 0.040449258, 0.051224384, 0.08065676, 0.049952134, 0.028453607, 0.04368172, 0.02859731, 0.04213204, 0.046726782, 0.03614609, 0.044817276, 0.027002953, 0.03200957, 0.039720226, 0.027932897, 0.03211215, 0.031210724, 0.024876833, 0.027572045, 0.027197141, 0.035179496, 0.02794987, 0.02530452, 0.035073828, 0.025063748, 0.026585776, 0.03340167, 0.11046146, 0.13148464, 0.14380832, 0.13679947, 0.07164915, 0.07727661, 0.09627447, 0.0664584, 0.08358799, 0.1257885, 0.102275096, 0.059970748, 0.04001533, 0.059141774, 0.048477117, 0.040577393, 0.03428964, 0.028617615, 0.050766483, 0.06830128, 0.049998257, 0.03561617, 0.030989807, 0.020998713, 0.031034702, 0.04081111, 0.031631105, 0.038403537, 0.036337268, 0.03257583, 0.034132205, 0.04320174, 0.03747126, 0.038691197, 0.026577404, 0.029393306, 0.06954073, 0.02775592, 0.13177772, 0.07431, 0.05617369, 0.062191695, 0.04797327, 0.062648155, 0.047672767, 0.079286486, 0.08941933, 0.06359139, 0.08113837, 0.049958557, 0.054177925, 0.08166904, 0.0552845, 0.032542087, 0.03684799, 0.026677765, 0.028748374, 0.030818388, 0.029080924, 0.037603747, 0.030444141, 0.03505281, 0.043822143, 0.046313472, 0.022669867, 0.023780636, 0.028403549, 0.039275035, 0.024652638, 0.027495906, 0.033054892, 0.032808352, 0.025903447, 0.029271837, 0.034086403, 0.025069395, 0.059339434, 0.14157113, 0.1875439, 0.06858271, 0.050041806, 0.052696217, 0.12070423, 0.058936805, 0.06969736, 0.05584031, 0.02813971, 0.037023973, 0.035987, 0.037603013, 0.058495175, 0.028814875, 0.04940242, 0.030412005, 0.02775051, 0.0263087, 0.06032061, 0.029227257, 0.021271437, 0.034177866, 0.028003339, 0.035074513, 0.029238412, 0.03474859, 0.03598216, 0.03387508, 0.044747163, 0.028609043, 0.025099592, 0.02456684, 0.018846167, 0.03270076, 0.024162669, 0.026933894, 0.051476914, 0.08013437, 0.13743699, 0.119822465, 0.07728758, 0.078498565, 0.07906155, 0.04301473, 0.09836204, 0.09565801, 0.08613644, 0.056504387, 0.04188154, 0.03304302, 0.028438196, 0.02874355, 0.035747003, 0.029714901, 0.02741285, 0.044164106, 0.036978062, 0.043758575, 0.027899686, 0.018636221, 0.02624369, 0.030862307, 0.026409782, 0.030712187, 0.028840922, 0.028098298, 0.025521899, 0.02343777, 0.029518146, 0.020133285, 0.021754706, 0.029726204, 0.036304723, 0.026696935, 0.17255493, 0.12727378, 0.30923897, 0.16819037, 0.095083974, 0.07868245, 0.063201666, 0.11244691, 0.111917146, 0.08970065, 0.07250797, 0.03848488, 0.041209687, 0.06191501, 0.04856382, 0.03761643, 0.02127763, 0.052193288, 0.029438818, 0.028981602, 0.05060459, 0.031838965, 0.02747275, 0.041040078, 0.034657422, 0.03389634, 0.021803174, 0.029703155, 0.027312333, 0.027536448, 0.027128344, 0.027803706, 0.022460008, 0.028545069, 0.033870615, 0.022747062, 0.017654791, 0.02490999, 0.08507719, 0.09220579, 0.12817466, 0.13370788, 0.16874954, 0.09294695, 0.10257266, 0.059950974, 0.059912335, 0.05326616, 0.04223721, 0.045711275, 0.026471606, 0.04596517, 0.0499915, 0.053964514, 0.05268074, 0.03933954, 0.024063496, 0.039809562, 0.03309465, 0.026130605, 0.03900239, 0.024488758, 0.027539933, 0.051684648, 0.028726565, 0.02571319, 0.020144477, 0.030751828, 0.024093501, 0.03871397, 0.025670538, 0.019949973, 0.01853989, 0.034201298, 0.04204434, 0.024111941, 0.16629998, 0.16091238, 0.11211901, 0.068203196, 0.03838268, 0.034101043, 0.06488786, 0.072448365, 0.13215539, 0.06841463, 0.04474743, 0.08325611, 0.06617193, 0.06061033, 0.04754517, 0.03335749, 0.038190346, 0.05459183, 0.054315045, 0.035378404, 0.03519333, 0.019873563, 0.03867128, 0.026450666, 0.025717426, 0.029889332, 0.020034486, 0.024851922, 0.03709031, 0.025212761, 0.031112397, 0.024254052, 0.027004147, 0.022494486, 0.028398568, 0.027424304, 0.034961622, 0.03619963, 0.13845287, 0.15689515, 0.08746946, 0.13087142, 0.078766316, 0.09391108, 0.07076486, 0.05911752, 0.052289438, 0.07598305, 0.051419824, 0.046578247, 0.05582674, 0.07337166, 0.047553327, 0.049656995, 0.03497254, 0.024934214, 0.029842092, 0.032718018, 0.04323371, 0.030983344, 0.025009654, 0.027164325, 0.031554546, 0.026272625, 0.02217529, 0.027148485, 0.034275312, 0.022644777, 0.028552981, 0.027533347, 0.023052916, 0.026414169, 0.03588884, 0.021468023, 0.037204243, 0.032159645, 0.13908656, 0.121399105, 0.13654158, 0.06866558, 0.080402434, 0.048062395, 0.07780172, 0.09931266, 0.03521158, 0.070092306, 0.05038028, 0.04393797, 0.0658324, 0.06582738, 0.057624158, 0.044985898, 0.032518335, 0.056899182, 0.045940507, 0.054049823, 0.040138867, 0.03667748, 0.023489235, 0.036271483, 0.026805881, 0.022480361, 0.027340678, 0.03288524, 0.020202875, 0.03034448, 0.02598957, 0.030227235, 0.02032493, 0.021762095, 0.019115819, 0.022490712, 0.03144596, 0.025037412, 0.062175464, 0.075898275, 0.13734178, 0.11076929, 0.09816636, 0.08209621, 0.07409994, 0.06373066, 0.06788129, 0.04584726, 0.080621935, 0.044679325, 0.034293704, 0.028892884, 0.01897964, 0.08275654, 0.032005697, 0.030361747, 0.026706314, 0.05520631, 0.036024153, 0.032557588, 0.029416818, 0.0400108, 0.025839651, 0.033853073, 0.030875871, 0.025004508, 0.028432934, 0.029396499, 0.028588776, 0.053739835, 0.026244817, 0.027273577, 0.02092031, 0.036290046, 0.024903074, 0.019685313, 0.023117114, 0.025882183, 0.019206569, 0.02214505, 0.021645827, 0.1256824, 0.11110111, 0.19625738, 0.1204517, 0.10746666, 0.037965458, 0.051375072, 0.11118106, 0.073495135, 0.05755616, 0.051783226, 0.05641475, 0.033921786, 0.041923173, 0.04690083, 0.07109226, 0.027854543, 0.032809418, 0.04529041, 0.034475774, 0.03502351, 0.030362343, 0.029334627, 0.037756138, 0.033184517, 0.03273876, 0.03256869, 0.027834471, 0.03246972, 0.027030949, 0.030767823, 0.022814345, 0.025044464, 0.024607504, 0.033457283, 0.017822847, 0.024940664, 0.018568033, 0.08247738, 0.13608204, 0.11481487, 0.17222553, 0.17277904, 0.0642809, 0.058318634, 0.123018414, 0.09630078, 0.036380917, 0.038635403, 0.054628298, 0.034335807, 0.03396829, 0.037155043, 0.027348822, 0.038204, 0.043011703, 0.027031189, 0.026553744, 0.028744506, 0.02956185, 0.021201376, 0.018382197, 0.028007075, 0.033139914, 0.026838707, 0.040285062, 0.03567913, 0.017128874, 0.035398107, 0.019007703, 0.020943616, 0.01893747, 0.022265151, 0.030188492, 0.019844942, 0.032429483, 0.054213524, 0.07564261, 0.17215359, 0.22493978, 0.121300586, 0.12709492, 0.07103525, 0.029228779, 0.05960861, 0.047355283, 0.04592411, 0.038507774, 0.038618546, 0.040930692, 0.037330717, 0.035800092, 0.06630067, 0.056145493, 0.03483338, 0.029319929, 0.044219203, 0.02759702, 0.028840156, 0.034352962, 0.029768962, 0.029066136, 0.025889244, 0.027638659, 0.025201961, 0.023391291, 0.027470717, 0.025023539, 0.032375228, 0.031204063, 0.02229991, 0.017118962, 0.022295693, 0.022057706, 0.17103559, 0.12076141, 0.1280903, 0.061106373, 0.11362567, 0.11686398, 0.11869738, 0.05672001, 0.122494675, 0.16765912, 0.06325484, 0.02804151, 0.060189318, 0.045931805, 0.0476527, 0.04405591, 0.024302052, 0.022329807, 0.03921917, 0.0333722, 0.03206439, 0.033051968, 0.037938394, 0.03045359, 0.02796672, 0.025046011, 0.023081431, 0.02382379, 0.02398907, 0.032141656, 0.021209884, 0.0340023, 0.03248069, 0.021314198, 0.023309823, 0.032878112, 0.023945952, 0.026139895, 0.08752015, 0.13829213, 0.10082656, 0.12923634, 0.18237527, 0.095961615, 0.0578152, 0.055982947, 0.07959754, 0.09080573, 0.05310321, 0.03656665, 0.046680078, 0.057035476, 0.040305905, 0.02823163, 0.031012485, 0.035362355, 0.023090774, 0.025348403, 0.036506105, 0.028169058, 0.026470387, 0.031529922, 0.033384822, 0.024258306, 0.029708674, 0.025809212, 0.029222576, 0.026953565, 0.037976455, 0.026858257, 0.023645207, 0.038937308, 0.01580664, 0.028767196, 0.024226844, 0.021252966, 0.02605538, 0.035980426, 0.046138063, 0.021085706, 0.023048481, 0.025446117, 0.06554798, 0.048597947, 0.106025234, 0.11351762, 0.06895321, 0.0538828, 0.07279737, 0.038244035, 0.04074143, 0.045508854, 0.046466023, 0.038270365, 0.030060694, 0.03352989, 0.040000997, 0.032765035, 0.033701897, 0.03845312, 0.024112253, 0.030333752, 0.035021354, 0.038539566, 0.026547277, 0.038726795, 0.024806956, 0.032159258, 0.03567257, 0.025217336, 0.03452932, 0.037430026, 0.026186885, 0.02921942, 0.016447907, 0.023940416, 0.028235838, 0.020313246, 0.018071609, 0.019354226, 0.089119725, 0.13676362, 0.13170652, 0.14919344, 0.13891572, 0.08770884, 0.058172867, 0.05160328, 0.041212603, 0.074145935, 0.039746474, 0.05843444, 0.05847178, 0.05334305, 0.055576336, 0.03632808, 0.02880749, 0.04803177, 0.037216473, 0.025012217, 0.027054338, 0.031107906, 0.033008732, 0.02597801, 0.040217947, 0.04515777, 0.031342268, 0.0273372, 0.024855828, 0.034814965, 0.022391837, 0.030473562, 0.02695823, 0.01931782, 0.030226277, 0.018022958, 0.032524157, 0.021484569, 0.05653631, 0.10372049, 0.20048213, 0.15601106, 0.073864706, 0.062010396, 0.065618195, 0.061562024, 0.065866105, 0.10017117, 0.060578216, 0.056824017, 0.028087093, 0.08871104, 0.056253344, 0.027937334, 0.02808094, 0.04651193, 0.024921045, 0.033467792, 0.03018774, 0.03413281, 0.0333701, 0.028154138, 0.044402845, 0.029310832, 0.024131333, 0.028926643, 0.032744557, 0.02003221, 0.02677121, 0.03646838, 0.024194516, 0.021489376, 0.021252511, 0.020040851, 0.023008801, 0.027415436, 0.04662926, 0.06656268, 0.07741335, 0.09986485, 0.109587945, 0.079337746, 0.05792033, 0.08102338, 0.054126, 0.07847807, 0.06723586, 0.0687794, 0.02742617, 0.044847026, 0.04681009, 0.03315227, 0.037895054, 0.029197661, 0.061944477, 0.027959093, 0.025067868, 0.03359591, 0.024997225, 0.027314516, 0.034590192, 0.03376974, 0.053591803, 0.030257193, 0.02407279, 0.024946505, 0.022577055, 0.032033958, 0.019319499, 0.033339806, 0.019231051, 0.019647116, 0.042409707, 0.03411745, 0.06652348, 0.09624899, 0.07769222, 0.07091377, 0.06918481, 0.048508037, 0.10603882, 0.09623581, 0.100136735, 0.058249895, 0.14776023, 0.12833314, 0.058063634, 0.048597485, 0.029184306, 0.032258138, 0.028198823, 0.04458818, 0.030729914, 0.047406264, 0.030697461, 0.031006731, 0.02318465, 0.027976485, 0.022863304, 0.023618162, 0.030463642, 0.023295775, 0.03023824, 0.03279065, 0.043095477, 0.028761026, 0.029237138, 0.021092776, 0.02117113, 0.024978234, 0.020884065, 0.017871212, 0.036822174, 0.030393666, 0.022586312, 0.026129115, 0.032391705, 0.051808495, 0.09014281, 0.0680802, 0.050644618, 0.09277079, 0.051435772, 0.048560947, 0.039806493, 0.043219354, 0.07545843, 0.1006434, 0.050817657, 0.03769519, 0.03604131, 0.03903739, 0.050990637, 0.027552871, 0.029979821, 0.030214993, 0.04371657, 0.035658807, 0.03436081, 0.028606374, 0.023675656, 0.019835407, 0.03600517, 0.03947078, 0.04027861, 0.017938986, 0.026876261, 0.022696922, 0.021179145, 0.022171892, 0.022435939, 0.025562117, 0.02878158, 0.024275139, 0.019570911, 0.06324253, 0.058034603, 0.057730585, 0.12540929, 0.23520136, 0.12997352, 0.07623494, 0.10030728, 0.061342433, 0.06998294, 0.039757494, 0.061323468, 0.06402739, 0.0924705, 0.06265065, 0.045736656, 0.055775452, 0.03485927, 0.027160462, 0.029834032, 0.042155623, 0.030211078, 0.024302509, 0.030683532, 0.02861305, 0.023413237, 0.06117921, 0.033730436, 0.030546047, 0.022971367, 0.0298499, 0.027584607, 0.03133352, 0.02659871, 0.028775645, 0.029800255, 0.027756333, 0.029659962, 0.11615652, 0.112261154, 0.2060358, 0.1308015, 0.19140086, 0.1904008, 0.09696474, 0.05667791, 0.11179334, 0.10080354, 0.040503714, 0.0291495, 0.06963566, 0.052032504, 0.05251654, 0.034689788, 0.025622698, 0.028858772, 0.03786891, 0.04782234, 0.03999602, 0.021624895, 0.01680417, 0.020690136, 0.026293611, 0.042798232, 0.02401025, 0.035342246, 0.038525578, 0.034330558, 0.039338317, 0.032983746, 0.025775062, 0.029859725, 0.0294333, 0.045614336, 0.019076312, 0.03181619, 0.090544775, 0.082386285, 0.241318, 0.19209234, 0.106088415, 0.057426225, 0.10338723, 0.06871963, 0.03917121, 0.11823845, 0.06626994, 0.034958735, 0.054607812, 0.055030778, 0.04380219, 0.038769215, 0.044792186, 0.039269976, 0.043840054, 0.03477853, 0.033733565, 0.03279249, 0.038865376, 0.033008274, 0.029078484, 0.030278083, 0.028862083, 0.031562913, 0.03396106, 0.029097673, 0.028443063, 0.041550487, 0.02701343, 0.025802338, 0.028410329, 0.02890391, 0.019142238, 0.029931752, 0.07277582, 0.13803487, 0.16206715, 0.07700338, 0.13326102, 0.10520352, 0.091459095, 0.070226766, 0.059297234, 0.036459725, 0.037498575, 0.03184834, 0.045265984, 0.030998189, 0.036987044, 0.03979318, 0.024598544, 0.027745629, 0.04647586, 0.03453057, 0.049307626, 0.0338071, 0.048355505, 0.03314845, 0.022699088, 0.03151666, 0.04090293, 0.032113012, 0.020387853, 0.028619748, 0.037101354, 0.023741465, 0.037857246, 0.022811368, 0.033842254, 0.021155512, 0.021908378, 0.030422794, 0.022371568, 0.02679785, 0.020086303, 0.027921708, 0.025405949, 0.024989828, 0.10782554, 0.04207016, 0.10452369, 0.058065, 0.07414949, 0.050251864, 0.048254207, 0.05439177, 0.04058407, 0.057192635, 0.05408584, 0.05262672, 0.04685636, 0.059110787, 0.07285557, 0.03380842, 0.03464138, 0.025588524, 0.023077346, 0.03582516, 0.035295434, 0.027843788, 0.0266745, 0.032250382, 0.04137804, 0.02369834, 0.031700596, 0.034559555, 0.024523992, 0.03151865, 0.025906395, 0.030262137, 0.025431484, 0.03033797, 0.026299777, 0.021034455, 0.023250518, 0.02064467, 0.045664277, 0.11632457, 0.10821245, 0.08322318, 0.06315447, 0.038122106, 0.076646805, 0.08526536, 0.078695595, 0.059098978, 0.03707792, 0.035561908, 0.024063932, 0.022983657, 0.051106673, 0.03314635, 0.03683449, 0.044248957, 0.047269855, 0.04931108, 0.029243419, 0.02877686, 0.034767862, 0.038763855, 0.023213588, 0.04786882, 0.04222408, 0.024610355, 0.016275482, 0.02247339, 0.028750943, 0.02667417, 0.023492647, 0.021181647, 0.021108463, 0.02020134, 0.030383427, 0.023295261, 0.15460606, 0.113635704, 0.116227664, 0.081891365, 0.055687603, 0.035687648, 0.0351256, 0.04705342, 0.15213592, 0.09607461, 0.050148513, 0.038815927, 0.085403405, 0.067673236, 0.03025583, 0.04339769, 0.03846861, 0.056610234, 0.03164166, 0.023227965, 0.029151674, 0.020071203, 0.023111813, 0.023759924, 0.024938766, 0.0267501, 0.02280142, 0.03021778, 0.03116504, 0.03905124, 0.034873914, 0.025091052, 0.018278664, 0.020911628, 0.023334067, 0.023560274, 0.021546924, 0.014965731, 0.06191691, 0.12297219, 0.14878893, 0.21680303, 0.15297545, 0.13005547, 0.05699201, 0.056535162, 0.05146846, 0.051993325, 0.025658093, 0.039297607, 0.06447939, 0.030422801, 0.042160954, 0.027899241, 0.030299986, 0.036240924, 0.033974625, 0.030685378, 0.024086427, 0.023479942, 0.030204993, 0.019090429, 0.034472696, 0.037511464, 0.03704232, 0.02809768, 0.026472751, 0.027056584, 0.026184969, 0.016709864, 0.024111696, 0.026017614, 0.015734369, 0.026974596, 0.01584503, 0.021822676, 0.11429831, 0.14335935, 0.23712657, 0.14903988, 0.06456292, 0.053047426, 0.045679368, 0.08807334, 0.06702526, 0.0666773, 0.083572105, 0.04219065, 0.03677005, 0.05530261, 0.06380679, 0.033610735, 0.03507437, 0.026426319, 0.032658346, 0.043076634, 0.034423187, 0.03475325, 0.016402373, 0.025179952, 0.026144093, 0.035085574, 0.029131632, 0.030183688, 0.026561612, 0.035593856, 0.025164232, 0.026406642, 0.020100558, 0.025287336, 0.023835827, 0.015229466, 0.026904056, 0.026765252, 0.02591872, 0.028267141, 0.027742555, 0.02730236, 0.019567415, 0.06626894, 0.09075143, 0.17784594, 0.15608178, 0.0797984, 0.06002794, 0.03355467, 0.04253198, 0.06647268, 0.067416504, 0.10521347, 0.06434877, 0.050706945, 0.047152497, 0.02948036, 0.05837179, 0.034516674, 0.041603178, 0.03195917, 0.033554703, 0.020984853, 0.025518553, 0.027592095, 0.03657289, 0.025856158, 0.018933017, 0.021332953, 0.02111871, 0.026688525, 0.020441132, 0.025319442, 0.024036327, 0.026084492, 0.030952165, 0.026573341, 0.021454755, 0.03138711, 0.02839888, 0.03745288, 0.10114298, 0.15006118, 0.10955176, 0.10033694, 0.051993977, 0.08206941, 0.063291855, 0.03200976, 0.05261972, 0.05305138, 0.02963509, 0.040103015, 0.033239115, 0.033395544, 0.035716485, 0.033043135, 0.03755265, 0.026802357, 0.024055004, 0.04297031, 0.045063492, 0.047750384, 0.022510804, 0.028521772, 0.055311635, 0.036885213, 0.026708854, 0.021583969, 0.029273903, 0.022492195, 0.020284772, 0.019996956, 0.03267495, 0.027217159, 0.034030262, 0.022047538, 0.022313215, 0.07091712, 0.11175913, 0.101958685, 0.085839495, 0.20092063, 0.05147625, 0.0849688, 0.080471925, 0.038073946, 0.06853547, 0.0408655, 0.055510383, 0.042610705, 0.030212035, 0.048451867, 0.031588167, 0.040325154, 0.02974263, 0.0385575, 0.0542403, 0.018721832, 0.024568573, 0.03796052, 0.033055015, 0.021023804, 0.029112631, 0.031355653, 0.021813463, 0.030899616, 0.032690275, 0.043553114, 0.022455193, 0.027199768, 0.030872872, 0.027184715, 0.021727974, 0.0217147, 0.018991465, 0.049670212, 0.06684944, 0.09177669, 0.10007533, 0.11640578, 0.05697495, 0.0313447, 0.04865704, 0.056240786, 0.080375694, 0.043941084, 0.031496275, 0.034124754, 0.035703454, 0.05508696, 0.047687024, 0.03260321, 0.05021678, 0.034946736, 0.041992296, 0.040751178, 0.053220063, 0.027863925, 0.02909178, 0.030468198, 0.026752463, 0.028050557, 0.027224656, 0.032759618, 0.029656524, 0.019669041, 0.03198277, 0.030031782, 0.035390507, 0.017030675, 0.025208786, 0.02203063, 0.023736358, 0.06429395, 0.1926902, 0.08802396, 0.12789758, 0.08911286, 0.048886582, 0.039698366, 0.030626494, 0.038701713, 0.09449151, 0.052305683, 0.021672195, 0.049188554, 0.033892468, 0.03130266, 0.05056759, 0.039131545, 0.050901275, 0.03334607, 0.026472028, 0.04455737, 0.022730883, 0.029345324, 0.034875553, 0.02216385, 0.028145652, 0.03358479, 0.03648905, 0.017589662, 0.027837496, 0.032945815, 0.031423748, 0.039333235, 0.028127009, 0.021366114, 0.01842538, 0.029905375, 0.01677922, 0.021926075, 0.01960166, 0.019818252, 0.028699238, 0.029072981, 0.021604937, 0.05940354, 0.08874009, 0.10544414, 0.16311102, 0.089344844, 0.082821675, 0.074194334, 0.049191345, 0.052295912, 0.07486186, 0.06410664, 0.08152091, 0.04335627, 0.034020253, 0.053200196, 0.027859958, 0.030903641, 0.03509667, 0.03076933, 0.03223529, 0.03307146, 0.036636256, 0.03492767, 0.017659796, 0.055995375, 0.041360956, 0.026610961, 0.022281008, 0.022990083, 0.03224647, 0.018651547, 0.035998188, 0.024465831, 0.032985613, 0.024207288, 0.02444993, 0.029571442, 0.029324338, 0.045809753, 0.072903864, 0.14471304, 0.07448246, 0.07807735, 0.07751221, 0.09471246, 0.036945272, 0.04849668, 0.048785966, 0.08041688, 0.0437234, 0.033859562, 0.05238779, 0.06557741, 0.0448513, 0.038533717, 0.036823988, 0.034599096, 0.036137965, 0.031760477, 0.037620332, 0.034204353, 0.016278233, 0.030369952, 0.023063095, 0.026264215, 0.029294271, 0.029771017, 0.026901666, 0.02531732, 0.02644708, 0.023447774, 0.028320514, 0.022209717, 0.025118249, 0.026103176, 0.018436417, 0.0720374, 0.07018656, 0.20951942, 0.10280697, 0.053329635, 0.07982313, 0.05820917, 0.05832858, 0.042080797, 0.04371362, 0.04950477, 0.079823956, 0.04434788, 0.053661924, 0.056509804, 0.023748634, 0.037826963, 0.032893144, 0.047503702, 0.0280599, 0.027516305, 0.042368855, 0.024965169, 0.029676419, 0.02499606, 0.017360251, 0.035959497, 0.022974031, 0.024304025, 0.018322159, 0.024338767, 0.04414954, 0.016275695, 0.030315518, 0.019948557, 0.024648003, 0.030638067, 0.029254153, 0.08272596, 0.05235426, 0.13264565, 0.082177706, 0.04041741, 0.05491869, 0.160291, 0.09697987, 0.07168431, 0.055097103, 0.046569932, 0.03212282, 0.04315998, 0.035253786, 0.04336476, 0.03649761, 0.029345842, 0.026288806, 0.02351147, 0.02533794, 0.021475656, 0.02245317, 0.013760212, 0.015614171, 0.02569084, 0.038882274, 0.029526198, 0.025127271, 0.019910395, 0.0317179, 0.022964025, 0.036601685, 0.036753196, 0.023979433, 0.018780518, 0.022767281, 0.014051309, 0.016987203, 0.08860851, 0.16025189, 0.11744734, 0.09419932, 0.08913092, 0.03749765, 0.042977158, 0.037706446, 0.06420036, 0.14122038, 0.05057173, 0.028751906, 0.029355664, 0.042028792, 0.036537573, 0.020300163, 0.038545277, 0.044749547, 0.030249095, 0.022145055, 0.038693946, 0.058047432, 0.039583977, 0.030155705, 0.026719166, 0.04014525, 0.024763387, 0.024502134, 0.021147637, 0.028794285, 0.022205576, 0.043301553, 0.023353612, 0.018823799, 0.01658671, 0.024967467, 0.025458543, 0.024522638, 0.022589026, 0.014159166, 0.03233145, 0.032235503, 0.017609194, 0.13277312, 0.19016585, 0.26188785, 0.1124515, 0.17210996, 0.07504063, 0.046254367, 0.053608645, 0.055645514, 0.03274434, 0.026805116, 0.034718465, 0.055721313, 0.04694845, 0.030262772, 0.022448981, 0.021284223, 0.021230483, 0.025493974, 0.03137057, 0.02381594, 0.028378569, 0.034979522, 0.029617168, 0.031065935, 0.02564213, 0.0247043, 0.026411273, 0.024879195, 0.023791037, 0.029577164, 0.024210162, 0.02064813, 0.019487884, 0.020538984, 0.025161445, 0.023794355, 0.025415389, 0.04521488, 0.18700567, 0.061807677, 0.19511165, 0.07876287, 0.060340423, 0.042303592, 0.05540936, 0.03726123, 0.042661417, 0.029838057, 0.043181848, 0.052429, 0.028868195, 0.043591645, 0.044793345, 0.023808379, 0.040246878, 0.028455794, 0.033851523, 0.024304414, 0.036570262, 0.026840605, 0.0230493, 0.022030266, 0.031906176, 0.026248572, 0.023672199, 0.023597056, 0.021645056, 0.026875982, 0.040529925, 0.020461842, 0.027024297, 0.027116781, 0.02340995, 0.015008205, 0.026044859, 0.08746133, 0.07137197, 0.09635673, 0.099796094, 0.17717241, 0.10805009, 0.067615114, 0.099401236, 0.06834695, 0.12911832, 0.06789981, 0.040179685, 0.053643372, 0.076456316, 0.042257495, 0.0265699, 0.029617015, 0.03255452, 0.039993986, 0.03553149, 0.03197423, 0.029504271, 0.019925782, 0.034310866, 0.023219744, 0.026534561, 0.030380286, 0.034100242, 0.02166982, 0.03623351, 0.027919766, 0.020634139, 0.030770838, 0.035829928, 0.043248046, 0.023818342, 0.024843847, 0.016914263, 0.062972404, 0.056016006, 0.05169605, 0.048135847, 0.15842839, 0.07042255, 0.037077863, 0.04594662, 0.097416185, 0.11278158, 0.03731503, 0.042990457, 0.0357288, 0.037322823, 0.044584386, 0.031717163, 0.044595756, 0.040150806, 0.020995865, 0.035258245, 0.029420411, 0.020885339, 0.034453064, 0.04228616, 0.026362356, 0.02992818, 0.028816998, 0.023810333, 0.038547844, 0.030513795, 0.029188558, 0.02379929, 0.023002097, 0.016183484, 0.01971331, 0.02181307, 0.025029486, 0.020596957, 0.17477387, 0.26930663, 0.18180768, 0.08651757, 0.093930334, 0.060560774, 0.07384227, 0.105189964, 0.058336478, 0.06961791, 0.09214545, 0.06711657, 0.04802073, 0.053865865, 0.04569394, 0.049709667, 0.03757679, 0.029806802, 0.042639565, 0.03295949, 0.042799067, 0.04416985, 0.029785188, 0.028178077, 0.024863865, 0.029146686, 0.03220989, 0.04249196, 0.023021473, 0.025374532, 0.03361755, 0.032995805, 0.025532402, 0.025308467, 0.026213324, 0.026507406, 0.02892337, 0.030072952, 0.04667561, 0.02641579, 0.024935916, 0.03453738, 0.022787446, 0.021793487, 0.089925334, 0.09369974, 0.06249061, 0.081566714, 0.07206788, 0.12144622, 0.05019508, 0.06312827, 0.06485798, 0.06655103, 0.050958302, 0.027120842, 0.043398533, 0.06853316, 0.03549275, 0.043513577, 0.024171956, 0.024462711, 0.04070583, 0.037240814, 0.031242292, 0.038808886, 0.0233521, 0.044519704, 0.020520536, 0.02465671, 0.051740397, 0.023341242, 0.023236394, 0.041557655, 0.030091321, 0.032896068, 0.023661446, 0.018960241, 0.023936735, 0.02499861, 0.022478506, 0.040229697, 0.060229786, 0.11762171, 0.109178804, 0.06842001, 0.04682832, 0.055899713, 0.0397336, 0.037750676, 0.04250695, 0.03156835, 0.054140586, 0.08057091, 0.06819091, 0.038171656, 0.039064854, 0.032443628, 0.03526457, 0.03056588, 0.034780953, 0.043510765, 0.02302231, 0.030127754, 0.027390731, 0.038982373, 0.034048438, 0.042197175, 0.032119177, 0.037325248, 0.026508046, 0.03903903, 0.038936358, 0.016431298, 0.02441355, 0.024293289, 0.041215535, 0.022966582, 0.033159822, 0.016112888, 0.052201003, 0.1299088, 0.1787354, 0.07099093, 0.08289552, 0.06203714, 0.08110064, 0.08861775, 0.048871893, 0.051816907, 0.05116567, 0.06053202, 0.028548297, 0.028466335, 0.09430963, 0.036059827, 0.027970606, 0.03796784, 0.037223708, 0.02317453, 0.033930816, 0.022227667, 0.035902236, 0.025897857, 0.036892522, 0.02451143, 0.030188221, 0.029218301, 0.022930263, 0.030795684, 0.0344098, 0.018182706, 0.041620176, 0.02692412, 0.025008842, 0.029789552, 0.0376263, 0.024469504, 0.08413584, 0.10491048, 0.0686098, 0.05733734, 0.04591511, 0.052696418, 0.057785224, 0.034139764, 0.08802104, 0.055625208, 0.050296415, 0.068244234, 0.036407605, 0.028248018, 0.040081985, 0.036378562, 0.029473094, 0.057569176, 0.057291716, 0.03307435, 0.04019378, 0.038625084, 0.03926605, 0.032263942, 0.0332803, 0.03422725, 0.026938783, 0.021714333, 0.04092578, 0.025663156, 0.0215796, 0.032037202, 0.024923725, 0.032353695, 0.026710525, 0.020766849, 0.025969576, 0.026494121, 0.07958814, 0.11080461, 0.093198, 0.05155173, 0.07704725, 0.05147502, 0.064766444, 0.064668685, 0.047272492, 0.048363477, 0.049230944, 0.037473835, 0.020909732, 0.052088678, 0.03665748, 0.048008543, 0.04660961, 0.027741546, 0.031438872, 0.028228797, 0.037396777, 0.051882293, 0.028309286, 0.033046395, 0.020749524, 0.020092487, 0.020065023, 0.029274222, 0.026784277, 0.020703733, 0.026359564, 0.022776298, 0.023373092, 0.026433967, 0.027804572, 0.025694542, 0.024989158, 0.021748325, 0.018917754, 0.020863412, 0.023725716, 0.03430175, 0.029696897, 0.060833976, 0.068679206, 0.092118286, 0.12141938, 0.20799775, 0.15590434, 0.055693846, 0.03526202, 0.053678863, 0.052825782, 0.05750591, 0.03601308, 0.03410219, 0.043620974, 0.025169319, 0.032468826, 0.037653893, 0.031897064, 0.02350674, 0.021597432, 0.018714739, 0.030772049, 0.0227904, 0.019274376, 0.026936248, 0.016618386, 0.019595949, 0.026092712, 0.028461749, 0.024586346, 0.031228947, 0.025251942, 0.031475015, 0.027745014, 0.018284308, 0.02199276, 0.019431436, 0.016892826, 0.054609217, 0.09608054, 0.10061996, 0.0935087, 0.07109449, 0.11617457, 0.06611794, 0.07883039, 0.065401636, 0.07232668, 0.055386707, 0.043887965, 0.04662807, 0.025228988, 0.032150105, 0.025509244, 0.031228542, 0.03948836, 0.043760154, 0.06544412, 0.042249434, 0.031194696, 0.018513774, 0.024458418, 0.027896529, 0.046699375, 0.025926912, 0.026579555, 0.027205631, 0.026806965, 0.023283806, 0.021024376, 0.019716423, 0.021449441, 0.01858979, 0.02611644, 0.02159754, 0.016955787, 0.058996495, 0.07796099, 0.1290265, 0.069891185, 0.07140971, 0.038853027, 0.121138535, 0.05403055, 0.057078928, 0.047773037, 0.09586769, 0.061030652, 0.031711828, 0.0694347, 0.044677723, 0.04519896, 0.027905239, 0.04730959, 0.030752257, 0.030325206, 0.024754707, 0.026220525, 0.02969203, 0.021840064, 0.03001673, 0.024169872, 0.022911087, 0.028966643, 0.033907693, 0.027613342, 0.023801442, 0.02189634, 0.028138505, 0.019144498, 0.022804948, 0.030217355, 0.027453303, 0.02256361, 0.07266009, 0.07033671, 0.06814394, 0.08059782, 0.12126939, 0.05765074, 0.04892335, 0.04877627, 0.04323195, 0.08044875, 0.040166292, 0.03996553, 0.044575058, 0.059434485, 0.09520229, 0.061864898, 0.045714956, 0.038310103, 0.056987632, 0.037629247, 0.02737606, 0.04065604, 0.02433092, 0.036899358, 0.021575686, 0.027420571, 0.027728649, 0.035047945, 0.018839201, 0.037481662, 0.022389729, 0.025504893, 0.022815958, 0.021679092, 0.027174665, 0.031737026, 0.027341679, 0.023962641, 0.05154077, 0.083829, 0.09223113, 0.1702024, 0.058877897, 0.031701855, 0.049049523, 0.055620626, 0.065439165, 0.06034081, 0.051377274, 0.03042472, 0.042584676, 0.06795622, 0.04880914, 0.039665006, 0.035369206, 0.023466341, 0.015917988, 0.034477748, 0.038195692, 0.02853902, 0.03438313, 0.023020994, 0.027088238, 0.018037897, 0.024362465, 0.023220478, 0.02905743, 0.022110326, 0.025556182, 0.026394706, 0.02426688, 0.023993438, 0.024705075, 0.022710849, 0.026519004, 0.022468654, 0.020118773, 0.024690732, 0.018030545, 0.027283339, 0.027143933, 0.019239975, 0.041295737, 0.1591883, 0.17943741, 0.08807558, 0.067239285, 0.056578316, 0.05758522, 0.08510137, 0.101382054, 0.092381336, 0.05720032, 0.051543586, 0.01960462, 0.05163608, 0.04885869, 0.026924565, 0.039654404, 0.03207156, 0.04371687, 0.018727735, 0.028708138, 0.020356184, 0.02238622, 0.035922736, 0.04163194, 0.028801735, 0.029562905, 0.018130338, 0.04047133, 0.022442685, 0.031337272, 0.025603656, 0.014273316, 0.01605405, 0.02332716, 0.021881375, 0.029566284, 0.038221706, 0.06349188, 0.0708604, 0.10848413, 0.093340084, 0.062434744, 0.05472089, 0.08332792, 0.08869756, 0.067009114, 0.04964835, 0.046119813, 0.052347075, 0.057019923, 0.06179785, 0.05981301, 0.020072423, 0.026755305, 0.025333304, 0.026013365, 0.031477015, 0.022492308, 0.027617231, 0.053817365, 0.034689292, 0.0261971, 0.023138242, 0.021610443, 0.029580299, 0.021604588, 0.027524477, 0.018332215, 0.020233555, 0.028662091, 0.01956902, 0.022138944, 0.025043586, 0.023878882, 0.02312798, 0.11617363, 0.13151546, 0.07710674, 0.12227605, 0.04394103, 0.04322541, 0.03877896, 0.040578317, 0.035026837, 0.076597854, 0.06250095, 0.043759573, 0.04588409, 0.07514609, 0.03096806, 0.026311276, 0.027405215, 0.023869842, 0.02027014, 0.034820534, 0.039373178, 0.038809758, 0.026534388, 0.025012422, 0.01811922, 0.015876252, 0.023167172, 0.020367851, 0.016383337, 0.025369082, 0.0384569, 0.023854274, 0.026775269, 0.020098435, 0.0328287, 0.028764289, 0.020469606, 0.02451458, 0.075671576, 0.124752946, 0.07408292, 0.10286759, 0.08712604, 0.058467306, 0.059495192, 0.04993834, 0.079304025, 0.06502878, 0.058187548, 0.05306673, 0.051891536, 0.038687304, 0.04058211, 0.034590237, 0.022513784, 0.048216682, 0.043646798, 0.042031478, 0.028938428, 0.026817936, 0.025220947, 0.029774468, 0.02392555, 0.023023728, 0.017778905, 0.023892902, 0.021277165, 0.021873036, 0.025609277, 0.025880463, 0.038214162, 0.032900095, 0.027021013, 0.03425878, 0.040920563, 0.02219914, 0.04487597, 0.066393085, 0.053779207, 0.16291352, 0.05280912, 0.053529017, 0.0647731, 0.13219242, 0.08405029, 0.049140334, 0.04502582, 0.0367595, 0.029987944, 0.083903536, 0.06517744, 0.029569488, 0.032008354, 0.028823696, 0.034632597, 0.032525647, 0.027886506, 0.027103955, 0.019360082, 0.031433977, 0.035760686, 0.024597064, 0.024998145, 0.015079833, 0.02955633, 0.02815, 0.022438833, 0.030930175, 0.020169562, 0.018708626, 0.02969879, 0.029378029, 0.02706689, 0.031848777, 0.06323872, 0.07610451, 0.06184335, 0.07457939, 0.07639628, 0.048104446, 0.06336174, 0.11700878, 0.048200987, 0.05332293, 0.04221902, 0.042393282, 0.024785405, 0.025028814, 0.041180316, 0.031057, 0.028500058, 0.0461896, 0.031713713, 0.045775425, 0.03306642, 0.024243433, 0.025937174, 0.021525549, 0.034507263, 0.02363921, 0.0313756, 0.02347441, 0.024206707, 0.02505192, 0.027177688, 0.022902781, 0.02914266, 0.024423268, 0.01988039, 0.02454222, 0.04416414, 0.022707654, 0.07486372, 0.0837873, 0.060680013, 0.07402556, 0.071183264, 0.042113934, 0.114285454, 0.04048563, 0.034012496, 0.05613243, 0.038307134, 0.030367553, 0.05479753, 0.046781063, 0.045644626, 0.025349976, 0.052121907, 0.043305863, 0.050031196, 0.027885055, 0.033530124, 0.031339064, 0.01674653, 0.036308628, 0.02494366, 0.024209274, 0.018533198, 0.024636326, 0.030249232, 0.03609979, 0.028014077, 0.028579915, 0.017141346, 0.031532623, 0.018548822, 0.032749403, 0.02806321, 0.020457253, 0.053908993, 0.05751664, 0.08776721, 0.1001626, 0.07535521, 0.07978751, 0.08770412, 0.058231305, 0.09860108, 0.06235903, 0.032672934, 0.03197504, 0.036882274, 0.032357197, 0.032359973, 0.041693427, 0.025240764, 0.046617877, 0.037771333, 0.030981807, 0.031708397, 0.02393268, 0.022878442, 0.028870918, 0.01883889, 0.019030692, 0.03096332, 0.029920574, 0.025441999, 0.03028286, 0.0284339, 0.021374488, 0.025585828, 0.019032702, 0.024981193, 0.022266006, 0.030714704, 0.021469064, 0.041889615, 0.068971515, 0.10788915, 0.070179805, 0.07939517, 0.07120233, 0.07076765, 0.05883953, 0.048795182, 0.0850792, 0.033031598, 0.072437346, 0.060670294, 0.040027406, 0.028420722, 0.027109466, 0.02060879, 0.01907512, 0.021763628, 0.032862756, 0.019958295, 0.0304024, 0.032493804, 0.019369097, 0.020056259, 0.018872006, 0.034187406, 0.020875752, 0.021357885, 0.016606854, 0.018125111, 0.019276557, 0.024149915, 0.03147851, 0.027664479, 0.022780677, 0.019132933, 0.015194041, 0.09033788, 0.11503158, 0.2558409, 0.13759318, 0.06497774, 0.098002896, 0.091467895, 0.07626947, 0.07149052, 0.0910854, 0.05315733, 0.031995934, 0.06666402, 0.0358729, 0.044809777, 0.060964786, 0.042805385, 0.027770909, 0.029873526, 0.048960216, 0.02848395, 0.03471609, 0.030283533, 0.03581588, 0.019561471, 0.031057958, 0.026339544, 0.02806586, 0.030085858, 0.034253906, 0.023845496, 0.039166294, 0.03598739, 0.025212284, 0.021808472, 0.019377636, 0.022623334, 0.022011628, 0.017502908, 0.016675098, 0.03439215, 0.018498382, 0.026067024, 0.07950809, 0.20155066, 0.12898885, 0.13912328, 0.08325503, 0.04285279, 0.04944495, 0.07235727, 0.08629578, 0.06144223, 0.058734782, 0.043514356, 0.04557055, 0.027407138, 0.030847672, 0.037613805, 0.022519825, 0.029209083, 0.035084102, 0.04505908, 0.029742172, 0.052367512, 0.023604624, 0.02098539, 0.037237376, 0.04933261, 0.03455798, 0.02233828, 0.026075633, 0.03191495, 0.029942488, 0.027015833, 0.028495992, 0.028544432, 0.024363527, 0.022236986, 0.02645422, 0.028294811, 0.060964584, 0.1988913, 0.15309696, 0.1087338, 0.097968236, 0.09754484, 0.036880612, 0.085650116, 0.06586168, 0.07521129, 0.051029388, 0.04065372, 0.034765664, 0.046546176, 0.034773536, 0.054745976, 0.032203842, 0.033685654, 0.03790555, 0.027716354, 0.034128334, 0.031357363, 0.032148402, 0.03670638, 0.021019502, 0.022106517, 0.04013607, 0.023488808, 0.027196972, 0.02321458, 0.022022923, 0.024087563, 0.03627181, 0.022820998, 0.037689, 0.0269989, 0.01812964, 0.029647313, 0.08987553, 0.06438714, 0.052060258, 0.054499436, 0.056552578, 0.04485901, 0.052966345, 0.108531475, 0.05836452, 0.097816415, 0.03973372, 0.025718486, 0.040330824, 0.046898555, 0.03436662, 0.0377332, 0.036378812, 0.027625749, 0.038856894, 0.033492256, 0.039988607, 0.039882172, 0.02656381, 0.029189454, 0.026340699, 0.020127632, 0.031109264, 0.014674954, 0.022973992, 0.029339047, 0.022111272, 0.019123852, 0.02146062, 0.024169859, 0.026047595, 0.034147188, 0.0214825, 0.034554154, 0.053707454, 0.073156714, 0.08542667, 0.09153408, 0.1034728, 0.040166337, 0.03950271, 0.074748866, 0.044340085, 0.035392698, 0.041291278, 0.05084447, 0.038903236, 0.029375149, 0.03326293, 0.044196676, 0.040046167, 0.029205972, 0.020796247, 0.018896773, 0.040649947, 0.026123544, 0.029862747, 0.03813359, 0.023229012, 0.029758155, 0.020185255, 0.020723272, 0.02975312, 0.02288865, 0.020235976, 0.023967322, 0.02997073, 0.027326185, 0.020315792, 0.027489897, 0.038933188, 0.0207743, 0.03478945, 0.104057945, 0.09046956, 0.07503118, 0.12562075, 0.1140156, 0.15750827, 0.09859588, 0.06504538, 0.14664876, 0.082078286, 0.04372334, 0.036299057, 0.036304384, 0.044261128, 0.04230005, 0.040307935, 0.032856077, 0.05535467, 0.04766612, 0.019047929, 0.03054992, 0.028803948, 0.027157407, 0.023974612, 0.029890506, 0.03436203, 0.024010038, 0.029929006, 0.036311984, 0.029025165, 0.021266503, 0.023201382, 0.022886606, 0.028227903, 0.028418835, 0.020888234, 0.021845842, 0.025512489, 0.021588292, 0.020463394, 0.030811545, 0.024961622, 0.021587107, 0.041137364, 0.13837326, 0.1734207, 0.07672339, 0.04943159, 0.045232903, 0.059174106, 0.042124864, 0.07672318, 0.06365458, 0.062461376, 0.056981564, 0.06496534, 0.035613567, 0.028301403, 0.02978748, 0.031873867, 0.028659657, 0.059222452, 0.027767979, 0.026982678, 0.02748981, 0.046285868, 0.023906633, 0.035542767, 0.020370007, 0.02559721, 0.025715256, 0.02697202, 0.03027921, 0.023887625, 0.03107975, 0.017805899, 0.02378441, 0.027928, 0.0142990025, 0.015548532, 0.042812098, 0.064948566, 0.055271663, 0.11648701, 0.13695239, 0.05765908, 0.04269936, 0.05432949, 0.048402395, 0.06070338, 0.04812111, 0.04963546, 0.043060597, 0.044145197, 0.03579691, 0.048923053, 0.033780977, 0.03267099, 0.023520492, 0.028365823, 0.033841737, 0.028218348, 0.02297275, 0.032294452, 0.018372126, 0.035195395, 0.028326178, 0.027763711, 0.026723206, 0.027155602, 0.019261315, 0.018170504, 0.018748078, 0.021879558, 0.025542805, 0.026227955, 0.019045588, 0.023219287, 0.02203931, 0.06078554, 0.08736156, 0.1032947, 0.07451894, 0.06397183, 0.07481234, 0.042179555, 0.08069937, 0.066239454, 0.040702764, 0.028119579, 0.026544552, 0.023050165, 0.024525842, 0.025447661, 0.032787487, 0.030201219, 0.046711035, 0.046116337, 0.040343698, 0.029847499, 0.023457712, 0.027467016, 0.02182111, 0.020640386, 0.021827025, 0.025247615, 0.022238756, 0.026436718, 0.027113551, 0.020301625, 0.041209746, 0.01987469, 0.026283791, 0.040833298, 0.025771711, 0.024884896, 0.03124354, 0.070512824, 0.18058485, 0.14330907, 0.06071526, 0.084423326, 0.046122007, 0.053354684, 0.07992999, 0.04703501, 0.048370324, 0.03954767, 0.04647981, 0.042661417, 0.029755885, 0.044373006, 0.026520418, 0.029749641, 0.03514425, 0.025670137, 0.03237522, 0.02476477, 0.03139384, 0.019197388, 0.02382744, 0.019300481, 0.022926077, 0.034401882, 0.025731841, 0.020009434, 0.023241088, 0.021260971, 0.020509148, 0.015107682, 0.02509465, 0.0269947, 0.02612925, 0.022028442, 0.019380337, 0.12140659, 0.15109344, 0.06781236, 0.08691827, 0.06675365, 0.046430144, 0.11763617, 0.056120705, 0.05405438, 0.04389702, 0.020468311, 0.04658392, 0.025924964, 0.051389683, 0.0734935, 0.03352807, 0.033896826, 0.035276804, 0.040715087, 0.025627755, 0.032478202, 0.025540277, 0.026780793, 0.038800947, 0.026593244, 0.024849212, 0.03299762, 0.021459192, 0.019538099, 0.029036935, 0.028194904, 0.028954927, 0.021541892, 0.025011003, 0.027118469, 0.023905486, 0.029398162, 0.024871446, 0.02074096, 0.01736938, 0.01923474, 0.020592399, 0.027188614, 0.14120503, 0.088156246, 0.07765373, 0.1133562, 0.066090964, 0.07707085, 0.0575249, 0.04987318, 0.06906222, 0.07894892, 0.044323817, 0.028134191, 0.039816137, 0.040713903, 0.027000226, 0.03953046, 0.052695952, 0.023283836, 0.02116943, 0.025467724, 0.022407444, 0.03611307, 0.027668504, 0.025925472, 0.024495505, 0.031500906, 0.01804601, 0.022381485, 0.02744797, 0.02539589, 0.027030597, 0.03021619, 0.026470518, 0.024219885, 0.022559213, 0.023474084, 0.020177422, 0.02342739, 0.15035579, 0.19874163, 0.11019826, 0.10307568, 0.11073144, 0.13846521, 0.07803466, 0.09699822, 0.050533265, 0.053114146, 0.057330497, 0.0626776, 0.04281378, 0.03457253, 0.03551775, 0.039359365, 0.03174724, 0.032960895, 0.02899439, 0.028079674, 0.046631128, 0.032763112, 0.033663977, 0.021737635, 0.021809062, 0.029471334, 0.023162846, 0.027711626, 0.025198642, 0.017793007, 0.03713163, 0.01887169, 0.030116027, 0.022590697, 0.025203295, 0.02749659, 0.022016343, 0.017051348, 0.083979286, 0.10779401, 0.070060134, 0.08951895, 0.06279795, 0.04312907, 0.03460005, 0.10923184, 0.07530347, 0.06428494, 0.029382633, 0.04728397, 0.045290194, 0.04692908, 0.042825047, 0.051545203, 0.03530291, 0.030675737, 0.017960913, 0.020991908, 0.02279604, 0.065128274, 0.039553672, 0.028961731, 0.024433771, 0.037328243, 0.024227628, 0.026536869, 0.024940876, 0.035249937, 0.023091013, 0.021580506, 0.026366701, 0.026064971, 0.028332874, 0.01597787, 0.03139682, 0.025717145, 0.052267216, 0.10439892, 0.11339156, 0.14077786, 0.063856214, 0.047611546, 0.03729479, 0.04690612, 0.033821393, 0.042017132, 0.043013323, 0.037550002, 0.02854186, 0.026279882, 0.024830617, 0.035759784, 0.031582173, 0.037989713, 0.03590532, 0.028399142, 0.03130417, 0.024753565, 0.029337892, 0.022975927, 0.02428549, 0.027455756, 0.017073192, 0.023029175, 0.017236974, 0.03057307, 0.013766935, 0.018556358, 0.0126868095, 0.02872325, 0.020818984, 0.036794215, 0.04185633, 0.02310731, 0.06210492, 0.07438426, 0.06877253, 0.07552703, 0.08241411, 0.058194995, 0.063236654, 0.047061384, 0.061605264, 0.070232615, 0.06967515, 0.03130042, 0.058234397, 0.032244064, 0.033905692, 0.028982447, 0.038487785, 0.025942668, 0.025294498, 0.025944376, 0.044782992, 0.025172973, 0.03814224, 0.028339542, 0.030821964, 0.038573235, 0.023028487, 0.02108473, 0.03241647, 0.028708449, 0.023673354, 0.020601276, 0.016896645, 0.023131821, 0.023744037, 0.020498795, 0.0219975, 0.020120183, 0.027504265, 0.017944217, 0.025253817, 0.02009433, 0.020030972, 0.023713822, 0.08519425, 0.092947, 0.11251869, 0.07562677, 0.06236602, 0.054755777, 0.061389986, 0.072331086, 0.13832185, 0.06191912, 0.11818468, 0.09926214, 0.057323895, 0.04242981, 0.054696452, 0.03544998, 0.036831133, 0.039378732, 0.037797853, 0.037264206, 0.03514952, 0.03349198, 0.03338238, 0.040330883, 0.031868964, 0.04052077, 0.056764163, 0.031279996, 0.03263083, 0.037560787, 0.022397636, 0.016514724, 0.028038574, 0.030415438, 0.02371859, 0.018092012, 0.02707824, 0.022653034, 0.22681291, 0.16806072, 0.13121602, 0.09125998, 0.06424734, 0.046747994, 0.041387726, 0.066266745, 0.06422542, 0.07988835, 0.10154458, 0.053933132, 0.07674576, 0.051646214, 0.048840057, 0.034448657, 0.031901907, 0.030612277, 0.03482357, 0.030737625, 0.044798166, 0.031191321, 0.030128801, 0.03461607, 0.021890432, 0.03270794, 0.028191313, 0.017423095, 0.027166147, 0.040936664, 0.028820122, 0.030409079, 0.042193305, 0.032675892, 0.026161447, 0.028193438, 0.022512916, 0.027765911, 0.09213243, 0.062312476, 0.055231433, 0.1136089, 0.04070664, 0.05890285, 0.035710964, 0.039936934, 0.045856446, 0.055281557, 0.053232145, 0.08480188, 0.055743087, 0.033202887, 0.044854198, 0.029812116, 0.0375019, 0.034254305, 0.045672167, 0.02706972, 0.034245223, 0.02598352, 0.03785576, 0.034347653, 0.037095297, 0.026109163, 0.030349707, 0.02294738, 0.025270304, 0.026092932, 0.020832649, 0.027494853, 0.018564388, 0.025005037, 0.02528436, 0.029261447, 0.023974713, 0.024941584, 0.27203265, 0.17771174, 0.13562262, 0.07268466, 0.07493001, 0.05000935, 0.05646518, 0.052886065, 0.044159524, 0.048390955, 0.058176078, 0.032688692, 0.030127507, 0.051283497, 0.038305927, 0.039490473, 0.030828202, 0.043804344, 0.041574717, 0.04172125, 0.031523295, 0.03067315, 0.028513238, 0.025100807, 0.03315915, 0.01873277, 0.034758557, 0.02470014, 0.044266075, 0.026317643, 0.019625511, 0.028180763, 0.025701586, 0.02125645, 0.020242961, 0.022836193, 0.025933038, 0.019439762, 0.08577679, 0.08048274, 0.056701466, 0.14401624, 0.091324426, 0.045910675, 0.04628208, 0.032788847, 0.04434624, 0.05799651, 0.025495859, 0.03560455, 0.043616865, 0.040523857, 0.05821701, 0.039071526, 0.031156912, 0.038843405, 0.029815592, 0.026298355, 0.036563188, 0.02266969, 0.039146647, 0.026844693, 0.023356898, 0.021081582, 0.0306838, 0.024110673, 0.024922635, 0.021530284, 0.022519726, 0.02163942, 0.023696544, 0.03208072, 0.029222213, 0.03475999, 0.01794817, 0.026614701, 0.017585622, 0.022778794, 0.029558305, 0.02027909, 0.01827769, 0.04112819, 0.07496012, 0.075816154, 0.17506981, 0.17969827, 0.07779562, 0.04860974, 0.064361066, 0.048957147, 0.058413092, 0.039673045, 0.045635615, 0.02993388, 0.028897429, 0.03702327, 0.06829803, 0.030881973, 0.025707273, 0.027932897, 0.025100235, 0.032868683, 0.021361826, 0.024191072, 0.02333436, 0.04232769, 0.027214464, 0.019500557, 0.01887691, 0.026235169, 0.016668763, 0.02230185, 0.024943708, 0.021483224, 0.027326176, 0.031739775, 0.021111518, 0.026011808, 0.021446126, 0.05986708, 0.0745974, 0.08533571, 0.0917659, 0.110108115, 0.08443352, 0.08525804, 0.111666165, 0.08622135, 0.06893482, 0.077654, 0.039063673, 0.039418526, 0.04803587, 0.041344274, 0.029953117, 0.03749968, 0.04081019, 0.04351598, 0.029370932, 0.024260113, 0.02681579, 0.029348984, 0.024561616, 0.03260294, 0.018772116, 0.030208591, 0.029587146, 0.02179966, 0.037050273, 0.019823318, 0.028736962, 0.0236094, 0.02416774, 0.022910444, 0.03869797, 0.01641595, 0.019088274, 0.10280763, 0.045378797, 0.06888626, 0.18996654, 0.11160729, 0.06599762, 0.061391424, 0.066952996, 0.051295925, 0.041138146, 0.036258448, 0.03751777, 0.034287047, 0.05409655, 0.031132372, 0.040507756, 0.037463143, 0.03017558, 0.03757319, 0.057225436, 0.04052211, 0.02936686, 0.023667054, 0.042196188, 0.050198983, 0.031806286, 0.02734629, 0.0293093, 0.031449173, 0.01993212, 0.044163633, 0.030538773, 0.02772638, 0.020728199, 0.023435554, 0.0308392, 0.027058624, 0.021087304, 0.061530583, 0.070935585, 0.07867355, 0.07976806, 0.1339943, 0.05818365, 0.0432462, 0.074656546, 0.038493935, 0.04830521, 0.052422732, 0.034449153, 0.03386286, 0.024771394, 0.037080836, 0.06828888, 0.043562304, 0.041820116, 0.033024598, 0.034811888, 0.028898817, 0.030543534, 0.03738734, 0.03166793, 0.02446248, 0.028612453, 0.015431264, 0.019724552, 0.022348536, 0.027866498, 0.026466953, 0.023090167, 0.020683005, 0.022690736, 0.024910105, 0.02346088, 0.02527578, 0.024266534, 0.16514142, 0.10975159, 0.071026586, 0.09529514, 0.15499724, 0.05976017, 0.07026154, 0.06422609, 0.06291216, 0.044575647, 0.03685299, 0.033326562, 0.04510008, 0.041868865, 0.05491836, 0.034119252, 0.032682538, 0.03767473, 0.034249287, 0.040658884, 0.029140573, 0.03728019, 0.032390155, 0.031513233, 0.039405435, 0.027645914, 0.028892921, 0.025983684, 0.025702028, 0.021853479, 0.024136733, 0.01986218, 0.02644515, 0.02187796, 0.019532809, 0.021102253, 0.030282164, 0.03201103, 0.028224694, 0.01500908, 0.019386804, 0.04580092, 0.017278826, 0.018946575, 0.037234813, 0.068474576, 0.03933471, 0.097569875, 0.04646367, 0.05471809, 0.06425841, 0.06910706, 0.043674104, 0.059970234, 0.040400933, 0.06389165, 0.047182374, 0.03393725, 0.0545589, 0.05391586, 0.05397661, 0.04894752, 0.02173776, 0.027978677, 0.03625588, 0.021449147, 0.024257844, 0.022427967, 0.025377717, 0.024456523, 0.024387699, 0.023361892, 0.030174091, 0.023406325, 0.019003613, 0.021189854, 0.020849949, 0.019803114, 0.021272406, 0.03514181, 0.021314973, 0.021015173, 0.047982153, 0.08217135, 0.10532871, 0.092189774, 0.057746466, 0.05632184, 0.10273008, 0.054200932, 0.029265309, 0.046798233, 0.041099645, 0.038470536, 0.02783796, 0.029380614, 0.044312164, 0.034257174, 0.020534962, 0.05141551, 0.024956936, 0.03312738, 0.02403067, 0.028133085, 0.019711848, 0.022001758, 0.018043967, 0.03364569, 0.039794043, 0.02163221, 0.03275823, 0.022135602, 0.028178588, 0.026149374, 0.024171954, 0.02318162, 0.02291421, 0.029436175, 0.019681461, 0.023021447, 0.061372373, 0.091689244, 0.13575675, 0.057469495, 0.0497402, 0.116519324, 0.059343245, 0.079611, 0.13572152, 0.053167116, 0.0438007, 0.09026897, 0.030509626, 0.049633324, 0.03567077, 0.034832414, 0.027853683, 0.02588469, 0.031607095, 0.03724452, 0.04436959, 0.029406602, 0.034436375, 0.024908442, 0.032257233, 0.033835586, 0.025940401, 0.023877397, 0.02042797, 0.022534681, 0.022325289, 0.016205672, 0.02272372, 0.017038409, 0.028880322, 0.023303345, 0.022695854, 0.020004734, 0.04934056, 0.095975034, 0.07965767, 0.059476182, 0.055885516, 0.0860077, 0.07793712, 0.04733776, 0.06342842, 0.12685522, 0.047697168, 0.04196057, 0.055114973, 0.036145292, 0.05565827, 0.048146795, 0.032818764, 0.03807387, 0.038622238, 0.026114454, 0.036367312, 0.027654318, 0.014447687, 0.035625607, 0.033605967, 0.043585863, 0.023469813, 0.02270441, 0.02303662, 0.022443859, 0.021873122, 0.025882028, 0.031764064, 0.031584684, 0.018255576, 0.025007322, 0.01806225, 0.016641308, 0.04499742, 0.048689943, 0.124694645, 0.1869151, 0.054117605, 0.053383462, 0.046518024, 0.0374615, 0.047254544, 0.069108345, 0.04461418, 0.049716644, 0.04087347, 0.042154904, 0.03157193, 0.03198335, 0.023528673, 0.028100366, 0.036778476, 0.048324965, 0.031882774, 0.032331217, 0.02958351, 0.047472823, 0.030282805, 0.030716777, 0.022909634, 0.019848686, 0.024305291, 0.037558544, 0.026916243, 0.034490503, 0.027190125, 0.02189144, 0.032502204, 0.016419938, 0.022846034, 0.027235523, 0.026045509, 0.02560383, 0.019279446, 0.023270695, 0.022518154, 0.03810413, 0.0502054, 0.12809291, 0.13752627, 0.12559697, 0.085875295, 0.07505537, 0.06529949, 0.059574787, 0.03651239, 0.033148676, 0.05105887, 0.031285502, 0.041029245, 0.041486125, 0.030881433, 0.035850104, 0.024116948, 0.032068998, 0.033753388, 0.029683145, 0.021081066, 0.030535817, 0.04644659, 0.029150147, 0.023630476, 0.02389675, 0.019292073, 0.019818246, 0.018490879, 0.02133678, 0.03160802, 0.01914134, 0.025065174, 0.037373334, 0.021299563, 0.01856711, 0.018171242, 0.05258593, 0.12202367, 0.06851757, 0.07700173, 0.08434559, 0.06831382, 0.06368218, 0.03718718, 0.044920873, 0.03904675, 0.060989518, 0.03200412, 0.03824126, 0.037917245, 0.029269407, 0.046028413, 0.059534486, 0.029789833, 0.03137131, 0.0332189, 0.028820511, 0.030845514, 0.028751489, 0.024449643, 0.021450423, 0.026655413, 0.021424852, 0.022471305, 0.031986736, 0.031440623, 0.023055056, 0.028062005, 0.028213203, 0.01710265, 0.031486876, 0.017719045, 0.03256583, 0.025253976, 0.07376569, 0.109335005, 0.09681547, 0.0636655, 0.13573614, 0.12537451, 0.051698703, 0.039255004, 0.047678806, 0.057688348, 0.034130793, 0.04271964, 0.04549383, 0.07527099, 0.05736328, 0.059320945, 0.030084591, 0.02652957, 0.03408301, 0.03132578, 0.0171803, 0.03616373, 0.03280216, 0.026285855, 0.03738067, 0.030231105, 0.024514807, 0.03524016, 0.046840813, 0.021960828, 0.035500266, 0.01826409, 0.018252892, 0.020633176, 0.024841895, 0.025756525, 0.024276309, 0.027440447, 0.0747998, 0.08421337, 0.11682184, 0.07883086, 0.105946444, 0.060086627, 0.03743594, 0.079850264, 0.0811287, 0.05690546, 0.05744704, 0.043522097, 0.04012422, 0.06295476, 0.036253527, 0.040343706, 0.023667952, 0.023538703, 0.019861339, 0.034176473, 0.041051153, 0.021877743, 0.023681873, 0.020131534, 0.025450386, 0.03328516, 0.027362902, 0.0254862, 0.024272187, 0.015680805, 0.022933675, 0.02107731, 0.039214645, 0.022084026, 0.02483488, 0.031509336, 0.018495293, 0.018435089, 0.04186746, 0.21340059, 0.10644529, 0.09478229, 0.10543649, 0.052903935, 0.060961396, 0.046688415, 0.06242587, 0.1227983, 0.037482474, 0.03413726, 0.029645355, 0.039248414, 0.03230073, 0.025812773, 0.0358258, 0.033001825, 0.042386442, 0.04378664, 0.03679351, 0.04191811, 0.034099266, 0.031016272, 0.04026482, 0.031202702, 0.043099705, 0.024756685, 0.022901015, 0.019724185, 0.026497785, 0.025131071, 0.02992748, 0.021051368, 0.024318857, 0.020944988, 0.027982743, 0.020533595, 0.033075918, 0.026499078, 0.021049565, 0.019608429, 0.024661133, 0.026931515, 0.04261473, 0.10476071, 0.06284389, 0.042242445, 0.030419739, 0.058663882, 0.04612573, 0.035485968, 0.039826166, 0.061950006, 0.06200883, 0.074047476, 0.078828774, 0.06464459, 0.03948513, 0.026575211, 0.046059996, 0.04381945, 0.02347547, 0.030849805, 0.026816154, 0.018099964, 0.016638571, 0.02855074, 0.027260656, 0.024131423, 0.029532969, 0.020095147, 0.020625586, 0.032976136, 0.025933987, 0.026292697, 0.057848737, 0.015544142, 0.015702233, 0.024882149, 0.02136299, 0.015629807, 0.06573968, 0.18653259, 0.14650206, 0.106161796, 0.11445309, 0.07763371, 0.08584243, 0.045105543, 0.049016863, 0.07680278, 0.060430214, 0.043961845, 0.034875467, 0.065309085, 0.05471324, 0.04169655, 0.0332235, 0.03992634, 0.043292288, 0.023566728, 0.02914893, 0.02169702, 0.024499081, 0.024818135, 0.027814893, 0.01885824, 0.02240483, 0.014743963, 0.018163517, 0.0353287, 0.027717628, 0.015187186, 0.027785862, 0.03527424, 0.017581208, 0.024867287, 0.026089272, 0.020524409, 0.089900196, 0.10772967, 0.09259061, 0.10139308, 0.11943742, 0.059265412, 0.05921716, 0.05587076, 0.058739763, 0.04342031, 0.04579997, 0.031827815, 0.036809314, 0.03694812, 0.031253785, 0.052242666, 0.029746408, 0.02595422, 0.02741626, 0.022252839, 0.02894762, 0.019420689, 0.028791782, 0.022672899, 0.030134082, 0.020717422, 0.03052106, 0.023182359, 0.0279471, 0.018051745, 0.041961167, 0.02630406, 0.02025954, 0.019920286, 0.021582523, 0.030181132, 0.019481018, 0.016468054, 0.05608663, 0.06868863, 0.075753726, 0.10396046, 0.04526224, 0.12141573, 0.029613828, 0.05147142, 0.07553039, 0.11876449, 0.03926722, 0.027763903, 0.048726942, 0.02756071, 0.02479241, 0.023893189, 0.048162848, 0.06284274, 0.025795018, 0.04669186, 0.026649855, 0.037570238, 0.029374251, 0.04895674, 0.02708789, 0.03741107, 0.027065732, 0.032313723, 0.028197907, 0.025282111, 0.02686972, 0.023968205, 0.021947062, 0.024573036, 0.019727303, 0.0251893, 0.038131963, 0.024353689, 0.100631095, 0.088456325, 0.06308117, 0.06710787, 0.07270244, 0.052013587, 0.04633911, 0.03296892, 0.06963998, 0.041631173, 0.03666671, 0.04179242, 0.035167873, 0.05332445, 0.043864533, 0.033525914, 0.032460283, 0.024045322, 0.02717143, 0.026551077, 0.036147755, 0.023989901, 0.030646829, 0.024517633, 0.03262165, 0.028402008, 0.03253665, 0.023652984, 0.017067486, 0.019514741, 0.02100254, 0.01909973, 0.031425375, 0.02542673, 0.024983345, 0.030027917, 0.022909231, 0.022662655, 0.02094064, 0.023957856, 0.017593468, 0.023703739, 0.017213752, 0.061506737, 0.047574602, 0.045579202, 0.061904956, 0.12302263, 0.07147295, 0.052739453, 0.07322968, 0.048131358, 0.061763763, 0.04451284, 0.03442985, 0.03949472, 0.039298486, 0.035715286, 0.026685756, 0.024825785, 0.028829575, 0.031285692, 0.017181307, 0.017837835, 0.02796549, 0.022117598, 0.027892292, 0.025219498, 0.028275613, 0.024991574, 0.021805042, 0.018456211, 0.026575789, 0.023607938, 0.02464415, 0.025972372, 0.017518057, 0.017482266, 0.033912722, 0.021995429, 0.022177752, 0.06814284, 0.07663762, 0.09508515, 0.109693386, 0.0980584, 0.05333904, 0.057040308, 0.05113987, 0.063406214, 0.05644913, 0.025447905, 0.037571143, 0.03422934, 0.050916646, 0.039961882, 0.039173577, 0.026239954, 0.02487184, 0.026479652, 0.035905488, 0.024559893, 0.04593043, 0.025078844, 0.031582873, 0.02633043, 0.02689269, 0.026603363, 0.02620497, 0.02093852, 0.026563052, 0.019341301, 0.023246815, 0.01880073, 0.035790436, 0.022733994, 0.018789452, 0.017637992, 0.016128605, 0.05576552, 0.24517383, 0.08357102, 0.09839191, 0.08619239, 0.043688733, 0.03672253, 0.07731463, 0.048504483, 0.050703615, 0.034798417, 0.043274783, 0.03278037, 0.037504118, 0.024548676, 0.027824985, 0.029901551, 0.024646237, 0.02415475, 0.025281327, 0.021580333, 0.03338744, 0.027252834, 0.02675218, 0.018907651, 0.027133739, 0.035780042, 0.03159118, 0.021322422, 0.020310277, 0.030076701, 0.027257932, 0.024020912, 0.024466002, 0.032724105, 0.021074664, 0.028526906, 0.020078856, 0.0617385, 0.062551826, 0.065082796, 0.063944004, 0.08450547, 0.041271925, 0.028913196, 0.039416105, 0.058668066, 0.09801532, 0.047185477, 0.04987666, 0.054421313, 0.051106717, 0.033121467, 0.04005736, 0.032144073, 0.03539027, 0.024243277, 0.033517804, 0.028773976, 0.02755971, 0.026340177, 0.029826537, 0.02742969, 0.031640183, 0.035356723, 0.018440519, 0.021238977, 0.03174979, 0.021027638, 0.025582429, 0.023116952, 0.016968682, 0.026494907, 0.029168371, 0.03340401, 0.022005761, 0.052805375, 0.22870739, 0.103649125, 0.19130464, 0.26335827, 0.056810148, 0.049705226, 0.08005029, 0.042253327, 0.06149424, 0.05263754, 0.044012766, 0.05955542, 0.03921079, 0.050057694, 0.061512526, 0.040681314, 0.02569872, 0.031905852, 0.023387216]

Ascent losses:

{53: -0.00094861333, 93: -0.00060414517, 132: -0.0006896636, 171: -0.0006659701, 221: -0.0006197177, 260: -0.0005660251, 299: -0.0006087894, 338: -0.0006917799, 377: -0.0005374739, 422: -0.00073587353, 461: -0.0007249163, 500: -0.0007282309, 539: -0.0005332536, 578: -0.00038981083, 622: -0.0004069858, 661: -0.00063419895, 700: -0.00066995376, 739: -0.00041728522, 778: -0.0006893065, 823: -0.0005277023, 862: -0.00033338307, 901: -0.0004813676, 940: -0.00039796747, 979: -0.0005359435, 1023: -0.0003664008, 1062: -0.00045652917, 1101: -0.00035105748, 1140: -0.00032066036, 1179: -0.00029149826, 1224: -0.0003124132, 1263: -0.00033011066, 1302: -0.00045652266, 1341: -0.0004842891, 1380: -0.0003302221, 1419: -0.00025102196, 1458: -0.00030809434, 1497: -0.00036604764, 1536: -0.00031779148, 1575: -0.0003719515, 1619: -0.00037292074, 1658: -0.0002732936, 1697: -0.00036519163, 1736: -0.00031893078, 1775: -0.00042527405, 1820: -0.00044492818, 1859: -0.000369334, 1898: -0.00028733353, 1937: -0.00028206385, 1976: -0.00022927979, 2020: -0.00024313043, 2059: -0.000337851, 2098: -0.00023347622, 2137: -0.00027237576, 2176: -0.00020308408, 2221: -0.00018475193, 2260: -0.0002383612, 2299: -0.00026267304, 2338: -0.00027169636, 2377: -0.00020495833, 2421: -0.00017837316, 2460: -0.00042256978, 2499: -0.00020828878, 2538: -0.00020704085, 2577: -0.00021714161, 2622: -0.00023755145, 2661: -0.00025289707, 2700: -0.00021674109, 2739: -0.00025549773, 2778: -0.00018982524, 2822: -0.00026966113, 2861: -0.00015610043, 2900: -0.00025051823, 2939: -0.00021607436, 2978: -0.0001780797, 3023: -0.00026745207, 3062: -0.00025231397, 3101: -0.00024923013, 3140: -0.00023489646, 3179: -0.00018668677, 3223: -0.00032498062, 3262: -0.0002587341, 3301: -0.00023642466, 3340: -0.00017457886, 3379: -0.0001920123, 3424: -0.00023666497, 3463: -0.0002675841, 3502: -0.0001846196, 3541: -0.0002063454, 3580: -0.00023051702, 3619: -0.00015089974, 3658: -0.0001144183, 3697: -0.00014879623, 3736: -0.00022206262, 3775: -0.00016544075, 3819: -0.00014853514, 3858: -0.00019224818, 3897: -0.0001964397, 3936: -0.00016545647, 3975: -0.00016459085, 4020: -0.00022536534, 4059: -0.00016720897, 4098: -0.00017635821, 4137: -0.0002261146, 4176: -0.00017788816, 4220: -0.000133233, 4259: -0.00017229402, 4298: -0.00024608627, 4337: -0.00028385315, 4376: -0.00039666015, 4421: -0.00012816313, 4460: -0.00019048025, 4499: -0.00016482415, 4538: -0.00016889548, 4577: -0.0002499941, 4621: -0.00013838185, 4660: -0.00015929577, 4699: -0.00020662259, 4738: -0.00015690512, 4777: -0.00018701398, 4822: -0.00016233888, 4861: -0.00017744569, 4900: -0.00015868244, 4939: -0.00016244415, 4978: -0.00016035182, 5022: -0.00018352982, 5061: -0.00014266786, 5100: -0.00011882037, 5139: -0.00013277924, 5178: -0.00014524732, 5223: -0.00015613601, 5262: -0.00016987315, 5301: -0.0001824392, 5340: -0.00016894245, 5379: -0.00010701833, 5423: -0.00014963272, 5462: -0.00017009165, 5501: -0.00018676861, 5540: -0.00012836423, 5579: -9.3405324e-05, 5624: -0.00014183114, 5663: -0.000142284, 5702: -9.196026e-05, 5741: -0.00012881808, 5780: -0.00014166143, 5819: -0.0001008383, 5858: -0.00012466921, 5897: -0.00014186799, 5936: -0.0001814597, 5975: -0.00013649902, 6019: -0.00025204723, 6058: -0.00015290613, 6097: -0.00021082486, 6136: -0.00014906643, 6175: -0.00012751346, 6220: -0.00010989073, 6259: -0.00011041537, 6298: -0.0001425406, 6337: -0.00012059062, 6376: -9.2759255e-05, 6420: -0.0002320211, 6459: -8.455702e-05, 6498: -0.000104676794, 6537: -0.00015350715, 6576: -0.0001698737, 6621: -0.00019272493, 6660: -0.00018656184, 6699: -0.00016570916, 6738: -0.00013076198, 6777: -0.00011838423, 6821: -0.00013917832, 6860: -9.2631555e-05, 6899: -0.0001325523, 6938: -0.00012277167, 6977: -0.00012058867, 7022: -0.00015818598, 7061: -0.00015191767, 7100: -0.00015070004, 7139: -0.00010345794, 7178: -0.00011723653, 7222: -0.000103374856, 7261: -0.00010876888, 7300: -0.00013513636, 7339: -0.00013470692, 7378: -0.00012442055, 7423: -0.0001489364, 7462: -0.00011346635, 7501: -0.00019306703, 7540: -0.00012055736, 7579: -8.6633234e-05, 7623: -0.00012207584, 7662: -0.00010807178, 7701: -0.00013848988, 7740: -0.00012415503, 7779: -9.81184e-05, 7824: -8.608244e-05, 7863: -0.00011732241, 7902: -0.00013794802, 7941: -0.00010384754, 7980: -0.000114015085, 8019: -0.00015658251, 8058: -0.00010832328, 8097: -0.0001332763, 8136: -0.00010148863, 8175: -0.0001222055, 8219: -0.00010285192, 8258: -0.00020988415, 8297: -0.00011905415, 8336: -0.00012238322, 8375: -0.00012900603, 8420: -0.000142349, 8459: -9.2291666e-05, 8498: -8.108804e-05, 8537: -0.00010225819, 8576: -0.00010015615, 8620: -0.00012797753, 8659: -0.00010935249, 8698: -0.000118788026, 8737: -0.00010058741, 8776: -0.000116256975, 8821: -0.00012792344, 8860: -0.00011686268, 8899: -0.00011438533, 8938: -7.9445184e-05, 8977: -9.1649075e-05, 9021: -0.00013792238, 9060: -0.00016329721, 9099: -9.754454e-05, 9138: -0.00011406477, 9177: -9.746564e-05, 9222: -0.00010898398, 9261: -9.4907045e-05, 9300: -8.4360276e-05, 9339: -7.160888e-05, 9378: -0.00014511311, 9422: -8.5399864e-05, 9461: -0.0001451818, 9500: -8.647515e-05, 9539: -0.0001018077, 9578: -0.00013527185, 9623: -8.508165e-05, 9662: -0.00010762971, 9701: -0.00011251667, 9740: -0.00011525235, 9779: -0.00018365627, 9823: -9.007712e-05, 9862: -0.000110934074, 9901: -0.000109154156, 9940: -0.00011064026, 9979: -9.7991404e-05}
Importing data files...
['mm1d_1.csv', 'mm1d_4.csv', 'mm1d_0.csv', 'mm1d_2.csv', 'mm1d_3.csv', 'mm1d_5.csv']
Splitting data into training and test sets with a ratio of: 0.2
Total number of training samples is 38400
Total number of test samples is 9600
Length of an output spectrum is 300
Generating torch datasets
training using gradient ascent
Starting training process
Doing Evaluation on the model now
This is Epoch 0, training loss 8.83469, validation loss 8.83778
Resetting learning rate to 0.01000
Epoch    14: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 10, training loss 8.83538, validation loss 8.81592
Epoch    25: reducing learning rate of group 0 to 2.5000e-04.
Epoch    40: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 20, training loss 8.83588, validation loss 8.87626
Epoch    51: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 25
Mean train loss for ascent epoch 26: -0.08835051953792572
Mean eval for ascent epoch 26: 8.835051536560059
Doing Evaluation on the model now
This is Epoch 30, training loss 8.82511, validation loss 8.89290
Epoch    64: reducing learning rate of group 0 to 5.0000e-03.
Epoch    75: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 40, training loss 8.82977, validation loss 8.86382
Epoch    86: reducing learning rate of group 0 to 1.2500e-03.
Epoch    97: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 50, training loss 8.82766, validation loss 8.87854
Epoch   108: reducing learning rate of group 0 to 3.1250e-04.
Epoch   119: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 60, training loss 8.83166, validation loss 8.88266
Epoch   130: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 65
Mean train loss for ascent epoch 66: -0.0883583053946495
Mean eval for ascent epoch 66: 8.835830688476562
Doing Evaluation on the model now
This is Epoch 70, training loss 8.82435, validation loss 8.85294
Epoch   143: reducing learning rate of group 0 to 5.0000e-03.
Epoch   154: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 80, training loss 8.82360, validation loss 8.81275
Epoch   165: reducing learning rate of group 0 to 1.2500e-03.
Epoch   176: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 90, training loss 7.94071, validation loss 7.96522
Epoch   187: reducing learning rate of group 0 to 3.1250e-04.
Epoch   198: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 100, training loss 7.78334, validation loss 7.77332
Epoch   209: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 105
Mean train loss for ascent epoch 106: -0.07763563841581345
Mean eval for ascent epoch 106: 7.765506267547607
Doing Evaluation on the model now
This is Epoch 110, training loss 7.65480, validation loss 7.70326
Epoch   220: reducing learning rate of group 0 to 5.0000e-03.
Epoch   231: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 120, training loss 7.57064, validation loss 7.60173
Epoch   242: reducing learning rate of group 0 to 1.2500e-03.
Epoch   253: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 130, training loss 7.56404, validation loss 7.56071
Epoch   264: reducing learning rate of group 0 to 3.1250e-04.
Epoch   275: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 140, training loss 7.56154, validation loss 7.56149
Epoch   286: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 144
Mean train loss for ascent epoch 145: -0.07562971860170364
Mean eval for ascent epoch 145: 7.562178134918213
Epoch   297: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 150, training loss 7.58899, validation loss 7.59797
Epoch   308: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 160, training loss 7.55887, validation loss 7.59125
Epoch   319: reducing learning rate of group 0 to 1.2500e-03.
Epoch   330: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 170, training loss 7.55927, validation loss 7.57335
Epoch   341: reducing learning rate of group 0 to 3.1250e-04.
Epoch   352: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 180, training loss 7.55494, validation loss 7.56193
Epoch   363: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 183
Mean train loss for ascent epoch 184: -0.07561849802732468
Mean eval for ascent epoch 184: 7.560929298400879
Epoch   374: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 190, training loss 7.56522, validation loss 7.57458
Epoch   385: reducing learning rate of group 0 to 2.5000e-03.
Epoch   396: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 200, training loss 7.55282, validation loss 7.53681
Resetting learning rate to 0.01000
Epoch   407: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 210, training loss 7.55822, validation loss 7.58146
Epoch   418: reducing learning rate of group 0 to 2.5000e-04.
Epoch   429: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 220, training loss 7.56737, validation loss 7.57718
Epoch   440: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 222
Mean train loss for ascent epoch 223: -0.07550691813230515
Mean eval for ascent epoch 223: 7.550047397613525
Epoch   451: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 230, training loss 7.56384, validation loss 7.56594
Epoch   462: reducing learning rate of group 0 to 2.5000e-03.
Epoch   473: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 240, training loss 7.55493, validation loss 7.55623
Epoch   484: reducing learning rate of group 0 to 6.2500e-04.
Epoch   495: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 250, training loss 7.54363, validation loss 7.55133
Epoch   506: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 260, training loss 7.55429, validation loss 7.54487
Epoch   517: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 261
Mean train loss for ascent epoch 262: -0.0755455419421196
Mean eval for ascent epoch 262: 7.553197860717773
Epoch   528: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 270, training loss 7.55855, validation loss 7.56252
Epoch   539: reducing learning rate of group 0 to 2.5000e-03.
Epoch   550: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 280, training loss 7.55273, validation loss 7.57435
Epoch   561: reducing learning rate of group 0 to 6.2500e-04.
Epoch   572: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 290, training loss 7.55065, validation loss 7.54824
Epoch   583: reducing learning rate of group 0 to 1.5625e-04.
Epoch   594: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 300
Doing Evaluation on the model now
This is Epoch 300, training loss 7.55657, validation loss 7.57980
Mean train loss for ascent epoch 301: -0.07560768723487854
Mean eval for ascent epoch 301: 7.559790134429932
Epoch   605: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 310, training loss 7.55908, validation loss 7.54751
Epoch   616: reducing learning rate of group 0 to 2.5000e-03.
Epoch   627: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 320, training loss 7.55335, validation loss 7.54688
Epoch   638: reducing learning rate of group 0 to 6.2500e-04.
Epoch   649: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 330, training loss 7.54882, validation loss 7.55420
Epoch   660: reducing learning rate of group 0 to 1.5625e-04.
Epoch   671: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 339
Mean train loss for ascent epoch 340: -0.07553960382938385
Mean eval for ascent epoch 340: 7.553319931030273
Doing Evaluation on the model now
This is Epoch 340, training loss 7.55332, validation loss 7.59446
Epoch   682: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 350, training loss 7.55214, validation loss 7.56741
Epoch   693: reducing learning rate of group 0 to 2.5000e-03.
Epoch   704: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 360, training loss 7.55056, validation loss 7.56538
Epoch   715: reducing learning rate of group 0 to 6.2500e-04.
Epoch   726: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 370, training loss 7.55328, validation loss 7.54489
Epoch   737: reducing learning rate of group 0 to 1.5625e-04.
Epoch   748: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 378
Mean train loss for ascent epoch 379: -0.07549480348825455
Mean eval for ascent epoch 379: 7.549015045166016
Doing Evaluation on the model now
This is Epoch 380, training loss 7.57411, validation loss 7.54500
Epoch   759: reducing learning rate of group 0 to 5.0000e-03.
Epoch   770: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 390, training loss 7.55626, validation loss 7.57145
Epoch   781: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 400, training loss 7.55902, validation loss 7.56580
Epoch   792: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch   803: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 410, training loss 7.55467, validation loss 7.57608
Epoch   814: reducing learning rate of group 0 to 2.5000e-04.
Epoch   825: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 420, training loss 7.55592, validation loss 7.59645
Epoch   836: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 423
Mean train loss for ascent epoch 424: -0.07548600435256958
Mean eval for ascent epoch 424: 7.5480852127075195
Epoch   847: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 430, training loss 7.55910, validation loss 7.60956
Epoch   858: reducing learning rate of group 0 to 2.5000e-03.
Epoch   869: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 440, training loss 7.55109, validation loss 7.61309
Epoch   880: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 450, training loss 7.54729, validation loss 7.55364
Epoch   891: reducing learning rate of group 0 to 3.1250e-04.
Epoch   902: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 460, training loss 7.54711, validation loss 7.58060
Epoch   913: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 462
Mean train loss for ascent epoch 463: -0.07553154230117798
Mean eval for ascent epoch 463: 7.552495956420898
Epoch   924: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 470, training loss 7.56055, validation loss 7.56842
Epoch   935: reducing learning rate of group 0 to 2.5000e-03.
Epoch   946: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 480, training loss 7.55017, validation loss 7.55109
Epoch   957: reducing learning rate of group 0 to 6.2500e-04.
Epoch   968: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 490, training loss 7.55333, validation loss 7.59564
Epoch   979: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 500, training loss 7.54956, validation loss 7.58983
Epoch   990: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 501
Mean train loss for ascent epoch 502: -0.07555966079235077
Mean eval for ascent epoch 502: 7.555159568786621
Epoch  1001: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 510, training loss 7.55760, validation loss 7.59484
Epoch  1012: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1023: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 520, training loss 7.55567, validation loss 7.55449
Epoch  1034: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1045: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 530, training loss 7.54923, validation loss 7.57145
Epoch  1056: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1067: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 540
Doing Evaluation on the model now
This is Epoch 540, training loss 7.55016, validation loss 7.57794
Mean train loss for ascent epoch 541: -0.0755816102027893
Mean eval for ascent epoch 541: 7.5572829246521
Epoch  1078: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 550, training loss 7.55815, validation loss 7.59496
Epoch  1089: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1100: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 560, training loss 7.55996, validation loss 7.56525
Epoch  1111: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1122: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 570, training loss 7.54910, validation loss 7.56306
Epoch  1133: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1144: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 579
Mean train loss for ascent epoch 580: -0.07552186399698257
Mean eval for ascent epoch 580: 7.551712989807129
Doing Evaluation on the model now
This is Epoch 580, training loss 7.55171, validation loss 7.58259
Epoch  1155: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1166: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 590, training loss 7.55707, validation loss 7.55801
Epoch  1177: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 600, training loss 7.54880, validation loss 7.55722
Resetting learning rate to 0.01000
Epoch  1188: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1199: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 610, training loss 7.54791, validation loss 7.57278
Epoch  1210: reducing learning rate of group 0 to 1.2500e-04.
Epoch  1221: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 618
Mean train loss for ascent epoch 619: -0.0754559189081192
Mean eval for ascent epoch 619: 7.5450544357299805
Doing Evaluation on the model now
This is Epoch 620, training loss 7.59480, validation loss 7.68142
Epoch  1232: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1243: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 630, training loss 7.54686, validation loss 7.56201
Epoch  1254: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1265: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 640, training loss 7.56109, validation loss 7.54739
Epoch  1276: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 650, training loss 7.55384, validation loss 7.57705
Epoch  1287: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1298: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 657
Mean train loss for ascent epoch 658: -0.07561755180358887
Mean eval for ascent epoch 658: 7.560633659362793
Doing Evaluation on the model now
This is Epoch 660, training loss 7.63379, validation loss 7.72540
Epoch  1309: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1320: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 670, training loss 7.55542, validation loss 7.56674
Epoch  1331: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1342: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 680, training loss 7.55432, validation loss 7.56460
Epoch  1353: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1364: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 690, training loss 7.55570, validation loss 7.58979
Epoch  1375: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 696
Mean train loss for ascent epoch 697: -0.07555238157510757
Mean eval for ascent epoch 697: 7.554426670074463
Doing Evaluation on the model now
This is Epoch 700, training loss 7.62970, validation loss 7.61187
Epoch  1386: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1397: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 710, training loss 7.55140, validation loss 7.56271
Epoch  1408: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1419: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 720, training loss 7.55888, validation loss 7.57945
Epoch  1430: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1441: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 730, training loss 7.54601, validation loss 7.55307
Epoch  1452: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 735
Mean train loss for ascent epoch 736: -0.07548631727695465
Mean eval for ascent epoch 736: 7.548040390014648
Doing Evaluation on the model now
This is Epoch 740, training loss 7.72130, validation loss 7.60889
Epoch  1463: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1474: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 750, training loss 7.54875, validation loss 7.59746
Epoch  1485: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1496: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 760, training loss 7.55371, validation loss 7.57315
Epoch  1507: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1518: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 770, training loss 7.54873, validation loss 7.58294
Epoch  1529: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 774
Mean train loss for ascent epoch 775: -0.075546495616436
Mean eval for ascent epoch 775: 7.5539231300354
Epoch  1540: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 780, training loss 7.55142, validation loss 7.56342
Epoch  1551: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 790, training loss 7.55158, validation loss 7.59247
Epoch  1562: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1573: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 800, training loss 7.55341, validation loss 7.56301
Resetting learning rate to 0.01000
Epoch  1584: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1595: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 810, training loss 7.55330, validation loss 7.57333
Epoch  1606: reducing learning rate of group 0 to 1.2500e-04.
Epoch  1617: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 818
Mean train loss for ascent epoch 819: -0.07550480961799622
Mean eval for ascent epoch 819: 7.549877166748047
Doing Evaluation on the model now
This is Epoch 820, training loss 7.57591, validation loss 7.59244
Epoch  1628: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1639: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 830, training loss 7.55057, validation loss 7.59330
Epoch  1650: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 840, training loss 7.55610, validation loss 7.56120
Epoch  1661: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1672: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 850, training loss 7.54969, validation loss 7.54750
Epoch  1683: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1694: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 857
Mean train loss for ascent epoch 858: -0.07546328008174896
Mean eval for ascent epoch 858: 7.545350074768066
Doing Evaluation on the model now
This is Epoch 860, training loss 7.59351, validation loss 7.58152
Epoch  1705: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1716: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 870, training loss 7.55573, validation loss 7.60586
Epoch  1727: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1738: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 880, training loss 7.54843, validation loss 7.59892
Epoch  1749: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 890, training loss 7.54716, validation loss 7.57405
Epoch  1760: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1771: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 896
Mean train loss for ascent epoch 897: -0.07551857829093933
Mean eval for ascent epoch 897: 7.5511322021484375
Doing Evaluation on the model now
This is Epoch 900, training loss 7.61979, validation loss 7.57871
Epoch  1782: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1793: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 910, training loss 7.56433, validation loss 7.58201
Epoch  1804: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1815: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 920, training loss 7.55097, validation loss 7.56053
Epoch  1826: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1837: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 930, training loss 7.54971, validation loss 7.57678
Epoch  1848: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 935
Mean train loss for ascent epoch 936: -0.07548464834690094
Mean eval for ascent epoch 936: 7.548227310180664
Doing Evaluation on the model now
This is Epoch 940, training loss 62.21013, validation loss 34.09549
Epoch  1859: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1870: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 950, training loss 0.53193, validation loss 0.24305
Epoch  1881: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1892: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 960, training loss 0.05470, validation loss 0.06572
Epoch  1903: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1914: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 970, training loss 0.03362, validation loss 0.03781
Epoch  1925: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 974
Mean train loss for ascent epoch 975: -0.001075275824405253
Mean eval for ascent epoch 975: 0.03401671722531319
Epoch  1936: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 980, training loss 1.34749, validation loss 3.00125
Epoch  1947: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 990, training loss 0.14528, validation loss 0.31863
Epoch  1958: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1969: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1000, training loss 0.02558, validation loss 0.04193
Resetting learning rate to 0.01000
Epoch  1980: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1991: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1010, training loss 0.02111, validation loss 0.01914
Epoch  2002: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2013: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1019
Mean train loss for ascent epoch 1020: -0.0007663826108910143
Mean eval for ascent epoch 1020: 0.017081016674637794
Doing Evaluation on the model now
This is Epoch 1020, training loss 0.01708, validation loss 0.01624
Epoch  2024: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2035: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1030, training loss 0.14205, validation loss 0.05945
Epoch  2046: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1040, training loss 0.02736, validation loss 0.02232
Epoch  2057: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2068: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1050, training loss 0.01646, validation loss 0.01835
Epoch  2079: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2090: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1058
Mean train loss for ascent epoch 1059: -0.0008564415038563311
Mean eval for ascent epoch 1059: 0.01438910886645317
Doing Evaluation on the model now
This is Epoch 1060, training loss 0.41704, validation loss 2.12142
Epoch  2101: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2112: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1070, training loss 0.21963, validation loss 0.32761
Epoch  2123: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2134: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1080, training loss 0.06495, validation loss 0.05587
Epoch  2145: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1090, training loss 0.02018, validation loss 0.01188
Epoch  2156: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2167: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1097
Mean train loss for ascent epoch 1098: -0.0009949267841875553
Mean eval for ascent epoch 1098: 0.015831025317311287
Doing Evaluation on the model now
This is Epoch 1100, training loss 0.81680, validation loss 0.53285
Epoch  2178: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2189: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1110, training loss 0.03140, validation loss 0.02689
Epoch  2200: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2211: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1120, training loss 0.03827, validation loss 0.02401
Epoch  2222: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2233: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1130, training loss 0.01776, validation loss 0.01162
Epoch  2244: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1136
Mean train loss for ascent epoch 1137: -0.0007068005506880581
Mean eval for ascent epoch 1137: 0.011299975216388702
Doing Evaluation on the model now
This is Epoch 1140, training loss 0.91413, validation loss 0.32495
Epoch  2255: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2266: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1150, training loss 0.08876, validation loss 0.05743
Epoch  2277: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2288: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1160, training loss 0.02593, validation loss 0.01424
Epoch  2299: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2310: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1170, training loss 0.01903, validation loss 0.01167
Epoch  2321: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1175
Mean train loss for ascent epoch 1176: -0.0006203255616128445
Mean eval for ascent epoch 1176: 0.011509778909385204
Doing Evaluation on the model now
This is Epoch 1180, training loss 1.54076, validation loss 1.39129
Epoch  2332: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2343: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1190, training loss 0.22740, validation loss 0.12080
Epoch  2354: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2365: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1200, training loss 0.01081, validation loss 0.01321
Resetting learning rate to 0.01000
Epoch  2376: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2387: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1210, training loss 0.01048, validation loss 0.01425
Epoch  2398: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2409: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1219
Mean train loss for ascent epoch 1220: -0.0006874039536342025
Mean eval for ascent epoch 1220: 0.010855208151042461
Doing Evaluation on the model now
This is Epoch 1220, training loss 0.01086, validation loss 0.01241
Epoch  2420: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1230, training loss 0.34908, validation loss 0.45742
Epoch  2431: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2442: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1240, training loss 0.03377, validation loss 0.02602
Epoch  2453: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2464: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1250, training loss 0.01815, validation loss 0.01687
Epoch  2475: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2486: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1258
Mean train loss for ascent epoch 1259: -0.0007188749732449651
Mean eval for ascent epoch 1259: 0.009533479809761047
Doing Evaluation on the model now
This is Epoch 1260, training loss 0.37925, validation loss 0.30506
Epoch  2497: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2508: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1270, training loss 0.02505, validation loss 0.09190
Epoch  2519: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1280, training loss 0.04332, validation loss 0.01577
Epoch  2530: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2541: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1290, training loss 0.01004, validation loss 0.00903
Epoch  2552: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2563: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1297
Mean train loss for ascent epoch 1298: -0.0003786920278798789
Mean eval for ascent epoch 1298: 0.008773894980549812
Doing Evaluation on the model now
This is Epoch 1300, training loss 0.55759, validation loss 0.57566
Epoch  2574: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2585: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1310, training loss 0.13378, validation loss 0.07166
Epoch  2596: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2607: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1320, training loss 0.02009, validation loss 0.01640
Epoch  2618: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1330, training loss 0.01007, validation loss 0.00853
Epoch  2629: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2640: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1336
Mean train loss for ascent epoch 1337: -0.00046224548714235425
Mean eval for ascent epoch 1337: 0.008921724744141102
Doing Evaluation on the model now
This is Epoch 1340, training loss 0.50168, validation loss 0.36738
Epoch  2651: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2662: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1350, training loss 0.08909, validation loss 0.10172
Epoch  2673: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2684: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1360, training loss 0.01520, validation loss 0.00866
Epoch  2695: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2706: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1370, training loss 0.01337, validation loss 0.01308
Epoch  2717: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1375
Mean train loss for ascent epoch 1376: -0.0005612896638922393
Mean eval for ascent epoch 1376: 0.009934602305293083
Doing Evaluation on the model now
This is Epoch 1380, training loss 0.19269, validation loss 0.22133
Epoch  2728: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2739: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1390, training loss 0.02642, validation loss 0.04276
Epoch  2750: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2761: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1400, training loss 0.01879, validation loss 0.00941
Resetting learning rate to 0.01000
Epoch  2772: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2783: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1410, training loss 0.00796, validation loss 0.00931
Epoch  2794: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2805: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1420
Doing Evaluation on the model now
This is Epoch 1420, training loss 0.00727, validation loss 0.00939
Mean train loss for ascent epoch 1421: -0.0008480573305860162
Mean eval for ascent epoch 1421: 0.010131603106856346
Epoch  2816: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1430, training loss 0.27878, validation loss 0.05529
Epoch  2827: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2838: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1440, training loss 0.04028, validation loss 0.11888
Epoch  2849: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2860: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1450, training loss 0.01073, validation loss 0.01014
Epoch  2871: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2882: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1459
Mean train loss for ascent epoch 1460: -0.0005242415936663747
Mean eval for ascent epoch 1460: 0.009551167488098145
Doing Evaluation on the model now
This is Epoch 1460, training loss 0.00955, validation loss 0.00924
Epoch  2893: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2904: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1470, training loss 0.05230, validation loss 0.09793
Epoch  2915: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1480, training loss 0.01522, validation loss 0.00713
Epoch  2926: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2937: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1490, training loss 0.00718, validation loss 0.00697
Epoch  2948: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2959: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1498
Mean train loss for ascent epoch 1499: -0.0006661411025561392
Mean eval for ascent epoch 1499: 0.009809201583266258
Doing Evaluation on the model now
This is Epoch 1500, training loss 0.33694, validation loss 0.28116
Epoch  2970: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2981: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1510, training loss 0.14533, validation loss 0.11727
Epoch  2992: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3003: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1520, training loss 0.07318, validation loss 0.08055
Epoch  3014: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1530, training loss 0.00868, validation loss 0.00831
Epoch  3025: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3036: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1537
Mean train loss for ascent epoch 1538: -0.0008305174997076392
Mean eval for ascent epoch 1538: 0.008553607389330864
Doing Evaluation on the model now
This is Epoch 1540, training loss 0.35478, validation loss 0.45488
Epoch  3047: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3058: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1550, training loss 0.10674, validation loss 0.03140
Epoch  3069: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3080: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1560, training loss 0.01454, validation loss 0.01096
Epoch  3091: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3102: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1570, training loss 0.00992, validation loss 0.00976
Epoch  3113: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1576
Mean train loss for ascent epoch 1577: -0.0006530804676003754
Mean eval for ascent epoch 1577: 0.008720808662474155
Doing Evaluation on the model now
This is Epoch 1580, training loss 0.98535, validation loss 1.33127
Epoch  3124: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3135: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1590, training loss 0.02279, validation loss 0.04864
Epoch  3146: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3157: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1600, training loss 0.01371, validation loss 0.01292
Resetting learning rate to 0.01000
Epoch  3168: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3179: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1610, training loss 0.01149, validation loss 0.00949
Epoch  3190: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3201: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1620
Doing Evaluation on the model now
This is Epoch 1620, training loss 0.00784, validation loss 0.00747
Mean train loss for ascent epoch 1621: -0.00048398098442703485
Mean eval for ascent epoch 1621: 0.00810986291617155
Epoch  3212: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1630, training loss 0.26046, validation loss 0.13064
Epoch  3223: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3234: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1640, training loss 0.01543, validation loss 0.01505
Epoch  3245: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3256: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1650, training loss 0.01538, validation loss 0.00885
Epoch  3267: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3278: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1659
Mean train loss for ascent epoch 1660: -0.000391008477890864
Mean eval for ascent epoch 1660: 0.00884066428989172
Doing Evaluation on the model now
This is Epoch 1660, training loss 0.00884, validation loss 0.00833
Epoch  3289: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1670, training loss 0.28412, validation loss 0.46059
Epoch  3300: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3311: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1680, training loss 0.02044, validation loss 0.02208
Epoch  3322: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3333: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1690, training loss 0.01121, validation loss 0.00939
Epoch  3344: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3355: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1698
Mean train loss for ascent epoch 1699: -0.0005915550282225013
Mean eval for ascent epoch 1699: 0.00896358210593462
Doing Evaluation on the model now
This is Epoch 1700, training loss 0.14769, validation loss 0.15487
Epoch  3366: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3377: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1710, training loss 0.14446, validation loss 0.05026
Epoch  3388: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1720, training loss 0.01695, validation loss 0.01355
Epoch  3399: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3410: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1730, training loss 0.00847, validation loss 0.00825
Epoch  3421: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3432: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1737
Mean train loss for ascent epoch 1738: -0.00039353311876766384
Mean eval for ascent epoch 1738: 0.007772121112793684
Doing Evaluation on the model now
This is Epoch 1740, training loss 0.63230, validation loss 0.58176
Epoch  3443: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3454: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1750, training loss 0.07918, validation loss 0.08204
Epoch  3465: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3476: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1760, training loss 0.02239, validation loss 0.01699
Epoch  3487: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1770, training loss 0.01270, validation loss 0.01019
Epoch  3498: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3509: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1776
Mean train loss for ascent epoch 1777: -0.00030086131300777197
Mean eval for ascent epoch 1777: 0.0077706025913357735
Doing Evaluation on the model now
This is Epoch 1780, training loss 0.41545, validation loss 0.39660
Epoch  3520: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3531: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1790, training loss 0.15959, validation loss 0.12909
Epoch  3542: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3553: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1800, training loss 0.01750, validation loss 0.02105
Resetting learning rate to 0.01000
Epoch  3564: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3575: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1810, training loss 0.00743, validation loss 0.00732
Epoch  3586: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1820, training loss 0.00715, validation loss 0.00811
Epoch  3597: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1821
Mean train loss for ascent epoch 1822: -0.0006114516290836036
Mean eval for ascent epoch 1822: 0.007004933897405863
Epoch  3608: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1830, training loss 0.11209, validation loss 0.18426
Epoch  3619: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3630: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1840, training loss 0.02970, validation loss 0.01449
Epoch  3641: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3652: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1850, training loss 0.00917, validation loss 0.00776
Epoch  3663: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3674: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1860
Doing Evaluation on the model now
This is Epoch 1860, training loss 0.00912, validation loss 0.01016
Mean train loss for ascent epoch 1861: -0.000623201543930918
Mean eval for ascent epoch 1861: 0.007498774211853743
Epoch  3685: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1870, training loss 0.12609, validation loss 0.23173
Epoch  3696: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3707: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1880, training loss 0.03875, validation loss 0.09151
Epoch  3718: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3729: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1890, training loss 0.01184, validation loss 0.00905
Epoch  3740: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3751: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1899
Mean train loss for ascent epoch 1900: -0.0007022476638667285
Mean eval for ascent epoch 1900: 0.007218081038445234
Doing Evaluation on the model now
This is Epoch 1900, training loss 0.00722, validation loss 0.00630
Epoch  3762: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3773: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1910, training loss 0.24768, validation loss 0.11257
Epoch  3784: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1920, training loss 0.01899, validation loss 0.01621
Epoch  3795: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3806: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1930, training loss 0.00782, validation loss 0.00779
Epoch  3817: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3828: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1938
Mean train loss for ascent epoch 1939: -0.0007081263465806842
Mean eval for ascent epoch 1939: 0.007935087196528912
Doing Evaluation on the model now
This is Epoch 1940, training loss 0.35860, validation loss 0.30142
Epoch  3839: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3850: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1950, training loss 0.04160, validation loss 0.10702
Epoch  3861: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3872: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1960, training loss 0.02689, validation loss 0.08513
Epoch  3883: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1970, training loss 0.00812, validation loss 0.00645
Epoch  3894: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3905: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1977
Mean train loss for ascent epoch 1978: -0.0005594766698777676
Mean eval for ascent epoch 1978: 0.007176645565778017
Doing Evaluation on the model now
This is Epoch 1980, training loss 0.24963, validation loss 0.24409
Epoch  3916: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3927: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1990, training loss 0.06186, validation loss 0.08656
Epoch  3938: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3949: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2000, training loss 0.01366, validation loss 0.03059
Resetting learning rate to 0.01000
Epoch  3960: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3971: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2010, training loss 0.01994, validation loss 0.02567
Epoch  3982: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2020, training loss 0.00796, validation loss 0.00723
Epoch  3993: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2021
Mean train loss for ascent epoch 2022: -0.00032011629082262516
Mean eval for ascent epoch 2022: 0.006906657014042139
Epoch  4004: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2030, training loss 0.33531, validation loss 0.15858
Epoch  4015: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4026: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2040, training loss 0.05469, validation loss 0.03517
Epoch  4037: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4048: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2050, training loss 0.00822, validation loss 0.00745
Epoch  4059: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4070: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2060
Doing Evaluation on the model now
This is Epoch 2060, training loss 0.00738, validation loss 0.00703
Mean train loss for ascent epoch 2061: -0.00035293353721499443
Mean eval for ascent epoch 2061: 0.007381937932223082
Epoch  4081: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2070, training loss 0.14546, validation loss 0.30495
Epoch  4092: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4103: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2080, training loss 0.04319, validation loss 0.07879
Epoch  4114: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4125: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2090, training loss 0.00900, validation loss 0.00676
Epoch  4136: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4147: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2099
Mean train loss for ascent epoch 2100: -0.0005021585966460407
Mean eval for ascent epoch 2100: 0.007749442011117935
Doing Evaluation on the model now
This is Epoch 2100, training loss 0.00775, validation loss 0.00790
Epoch  4158: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2110, training loss 0.16018, validation loss 0.12600
Epoch  4169: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4180: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2120, training loss 0.05324, validation loss 0.02479
Epoch  4191: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4202: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2130, training loss 0.00900, validation loss 0.00883
Epoch  4213: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4224: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2138
Mean train loss for ascent epoch 2139: -0.0004986280691809952
Mean eval for ascent epoch 2139: 0.0081715676933527
Doing Evaluation on the model now
This is Epoch 2140, training loss 0.09923, validation loss 0.07542
Epoch  4235: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4246: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2150, training loss 0.08644, validation loss 0.17313
Epoch  4257: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2160, training loss 0.01273, validation loss 0.01516
Epoch  4268: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4279: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2170, training loss 0.00977, validation loss 0.00776
Epoch  4290: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4301: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2177
Mean train loss for ascent epoch 2178: -0.0007257814868353307
Mean eval for ascent epoch 2178: 0.007408220320940018
Doing Evaluation on the model now
This is Epoch 2180, training loss 0.97808, validation loss 0.39198
Epoch  4312: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4323: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2190, training loss 0.03741, validation loss 0.09687
Epoch  4334: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4345: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2200, training loss 0.03622, validation loss 0.02148
Resetting learning rate to 0.01000
Epoch  4356: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2210, training loss 0.01159, validation loss 0.01166
Epoch  4367: reducing learning rate of group 0 to 2.5000e-04.
Epoch  4378: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2220, training loss 0.00698, validation loss 0.00656
Epoch  4389: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2222
Mean train loss for ascent epoch 2223: -0.000698701711371541
Mean eval for ascent epoch 2223: 0.010452457703649998
Epoch  4400: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2230, training loss 0.07043, validation loss 0.16790
Epoch  4411: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4422: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2240, training loss 0.01927, validation loss 0.01743
Epoch  4433: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4444: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2250, training loss 0.00874, validation loss 0.00936
Epoch  4455: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2260, training loss 0.00728, validation loss 0.00847
Epoch  4466: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2261
Mean train loss for ascent epoch 2262: -0.0007421752670779824
Mean eval for ascent epoch 2262: 0.010512213222682476
Epoch  4477: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2270, training loss 0.11990, validation loss 0.10240
Epoch  4488: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4499: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2280, training loss 0.01249, validation loss 0.01437
Epoch  4510: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4521: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2290, training loss 0.00849, validation loss 0.00828
Epoch  4532: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4543: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2300
Doing Evaluation on the model now
This is Epoch 2300, training loss 0.00653, validation loss 0.00571
Mean train loss for ascent epoch 2301: -0.00042527198093011975
Mean eval for ascent epoch 2301: 0.006063869688659906
Epoch  4554: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2310, training loss 0.06977, validation loss 0.01909
Epoch  4565: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4576: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2320, training loss 0.01757, validation loss 0.02561
Epoch  4587: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4598: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2330, training loss 0.00731, validation loss 0.00641
Epoch  4609: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4620: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2339
Mean train loss for ascent epoch 2340: -0.0006110967951826751
Mean eval for ascent epoch 2340: 0.007065465673804283
Doing Evaluation on the model now
This is Epoch 2340, training loss 0.00707, validation loss 0.00605
Epoch  4631: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4642: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2350, training loss 0.07602, validation loss 0.03630
Epoch  4653: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2360, training loss 0.02026, validation loss 0.01756
Epoch  4664: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4675: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2370, training loss 0.00827, validation loss 0.00668
Epoch  4686: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4697: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2378
Mean train loss for ascent epoch 2379: -0.000680268625728786
Mean eval for ascent epoch 2379: 0.007609961088746786
Doing Evaluation on the model now
This is Epoch 2380, training loss 0.56223, validation loss 0.10960
Epoch  4708: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4719: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2390, training loss 0.09549, validation loss 0.04213
Epoch  4730: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4741: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2400, training loss 0.01737, validation loss 0.01457
Resetting learning rate to 0.01000
Epoch  4752: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2410, training loss 0.00969, validation loss 0.00647
Epoch  4763: reducing learning rate of group 0 to 2.5000e-04.
Epoch  4774: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2420, training loss 0.00584, validation loss 0.00567
Epoch  4785: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2422
Mean train loss for ascent epoch 2423: -0.000605078530497849
Mean eval for ascent epoch 2423: 0.006199072115123272
Epoch  4796: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2430, training loss 0.66036, validation loss 0.03907
Epoch  4807: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4818: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2440, training loss 0.02738, validation loss 0.01401
Epoch  4829: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4840: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2450, training loss 0.00989, validation loss 0.00816
Epoch  4851: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2460, training loss 0.00753, validation loss 0.00737
Epoch  4862: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2461
Mean train loss for ascent epoch 2462: -0.00047401152551174164
Mean eval for ascent epoch 2462: 0.0075834630988538265
Epoch  4873: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2470, training loss 0.24340, validation loss 0.17906
Epoch  4884: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4895: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2480, training loss 0.04006, validation loss 0.01964
Epoch  4906: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4917: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2490, training loss 0.01426, validation loss 0.00790
Epoch  4928: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4939: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2500
Doing Evaluation on the model now
This is Epoch 2500, training loss 0.00610, validation loss 0.00575
Mean train loss for ascent epoch 2501: -0.0007300989236682653
Mean eval for ascent epoch 2501: 0.0069896443746984005
Epoch  4950: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2510, training loss 0.07294, validation loss 0.12675
Epoch  4961: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4972: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2520, training loss 0.04250, validation loss 0.01901
Epoch  4983: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4994: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2530, training loss 0.00881, validation loss 0.00785
Epoch  5005: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5016: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2539
Mean train loss for ascent epoch 2540: -0.0004357587604317814
Mean eval for ascent epoch 2540: 0.0068475957959890366
Doing Evaluation on the model now
This is Epoch 2540, training loss 0.00685, validation loss 0.00651
Epoch  5027: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2550, training loss 0.08587, validation loss 0.15085
Epoch  5038: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5049: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2560, training loss 0.01466, validation loss 0.00868
Epoch  5060: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5071: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2570, training loss 0.00675, validation loss 0.00686
Epoch  5082: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5093: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2578
Mean train loss for ascent epoch 2579: -0.0005418912041932344
Mean eval for ascent epoch 2579: 0.007211270276457071
Doing Evaluation on the model now
This is Epoch 2580, training loss 0.14431, validation loss 0.10632
Epoch  5104: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5115: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2590, training loss 0.10136, validation loss 0.02101
Epoch  5126: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2600, training loss 0.01763, validation loss 0.04153
Epoch  5137: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  5148: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2610, training loss 0.01443, validation loss 0.00958
Epoch  5159: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5170: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2620, training loss 0.00647, validation loss 0.00586
Epoch  5181: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2623
Mean train loss for ascent epoch 2624: -0.00035678979475051165
Mean eval for ascent epoch 2624: 0.006795081775635481
Epoch  5192: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2630, training loss 0.13956, validation loss 0.14559
Epoch  5203: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5214: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2640, training loss 0.03444, validation loss 0.05693
Epoch  5225: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2650, training loss 0.00790, validation loss 0.00861
Epoch  5236: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5247: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2660, training loss 0.00636, validation loss 0.00582
Epoch  5258: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2662
Mean train loss for ascent epoch 2663: -0.0006286960560828447
Mean eval for ascent epoch 2663: 0.006884229835122824
Epoch  5269: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2670, training loss 0.11821, validation loss 0.31938
Epoch  5280: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5291: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2680, training loss 0.02249, validation loss 0.03234
Epoch  5302: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5313: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2690, training loss 0.00957, validation loss 0.00672
Epoch  5324: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2700, training loss 0.00782, validation loss 0.00648
Epoch  5335: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2701
Mean train loss for ascent epoch 2702: -0.00047824138891883194
Mean eval for ascent epoch 2702: 0.006940594408661127
Epoch  5346: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2710, training loss 0.11507, validation loss 0.20690
Epoch  5357: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5368: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2720, training loss 0.01492, validation loss 0.04028
Epoch  5379: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5390: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2730, training loss 0.00767, validation loss 0.00575
Epoch  5401: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5412: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2740
Doing Evaluation on the model now
This is Epoch 2740, training loss 0.00697, validation loss 0.00706
Mean train loss for ascent epoch 2741: -0.0005604731850326061
Mean eval for ascent epoch 2741: 0.006539739202708006
Epoch  5423: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2750, training loss 0.38959, validation loss 0.39048
Epoch  5434: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5445: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2760, training loss 0.01546, validation loss 0.01614
Epoch  5456: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5467: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2770, training loss 0.00655, validation loss 0.00612
Epoch  5478: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5489: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2779
Mean train loss for ascent epoch 2780: -0.0006305087590590119
Mean eval for ascent epoch 2780: 0.006213658954948187
Doing Evaluation on the model now
This is Epoch 2780, training loss 0.00621, validation loss 0.00560
Epoch  5500: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5511: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2790, training loss 0.14234, validation loss 0.05361
Epoch  5522: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2800, training loss 0.01453, validation loss 0.04118
Resetting learning rate to 0.01000
Epoch  5533: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5544: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2810, training loss 0.00651, validation loss 0.00729
Epoch  5555: reducing learning rate of group 0 to 1.2500e-04.
Epoch  5566: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2818
Mean train loss for ascent epoch 2819: -0.0005037374794483185
Mean eval for ascent epoch 2819: 0.006428396329283714
Doing Evaluation on the model now
This is Epoch 2820, training loss 0.43709, validation loss 1.15116
Epoch  5577: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5588: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2830, training loss 0.06049, validation loss 0.02936
Epoch  5599: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5610: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2840, training loss 0.01399, validation loss 0.02426
Epoch  5621: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2850, training loss 0.00567, validation loss 0.00556
Epoch  5632: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5643: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2857
Mean train loss for ascent epoch 2858: -0.00037927288212813437
Mean eval for ascent epoch 2858: 0.00594289880245924
Doing Evaluation on the model now
This is Epoch 2860, training loss 0.68918, validation loss 0.88940
Epoch  5654: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5665: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2870, training loss 0.03198, validation loss 0.02625
Epoch  5676: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5687: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2880, training loss 0.00956, validation loss 0.00762
Epoch  5698: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5709: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2890, training loss 0.00697, validation loss 0.00768
Epoch  5720: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2896
Mean train loss for ascent epoch 2897: -0.0004390518297441304
Mean eval for ascent epoch 2897: 0.005833357106894255
Doing Evaluation on the model now
This is Epoch 2900, training loss 0.19074, validation loss 0.09123
Epoch  5731: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5742: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2910, training loss 0.05168, validation loss 0.07656
Epoch  5753: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5764: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2920, training loss 0.01920, validation loss 0.00975
Epoch  5775: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5786: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2930, training loss 0.00720, validation loss 0.00720
Epoch  5797: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2935
Mean train loss for ascent epoch 2936: -0.0004158262745477259
Mean eval for ascent epoch 2936: 0.006616272497922182
Doing Evaluation on the model now
This is Epoch 2940, training loss 0.99236, validation loss 0.72262
Epoch  5808: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5819: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2950, training loss 0.02059, validation loss 0.02819
Epoch  5830: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5841: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2960, training loss 0.01726, validation loss 0.01510
Epoch  5852: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5863: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2970, training loss 0.00638, validation loss 0.00611
Epoch  5874: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2974
Mean train loss for ascent epoch 2975: -0.00045373025932349265
Mean eval for ascent epoch 2975: 0.007246808614581823
Epoch  5885: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2980, training loss 0.75761, validation loss 0.09037
Epoch  5896: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2990, training loss 0.05713, validation loss 0.13787
Epoch  5907: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5918: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3000, training loss 0.00927, validation loss 0.01157
Resetting learning rate to 0.01000
Epoch  5929: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5940: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3010, training loss 0.00663, validation loss 0.00605
Epoch  5951: reducing learning rate of group 0 to 1.2500e-04.
Epoch  5962: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3018
Mean train loss for ascent epoch 3019: -0.0005580460419878364
Mean eval for ascent epoch 3019: 0.00671054283156991
Doing Evaluation on the model now
This is Epoch 3020, training loss 0.18513, validation loss 0.25558
Epoch  5973: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5984: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3030, training loss 0.02540, validation loss 0.04741
Epoch  5995: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3040, training loss 0.00987, validation loss 0.00739
Epoch  6006: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6017: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3050, training loss 0.00795, validation loss 0.00802
Epoch  6028: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6039: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3057
Mean train loss for ascent epoch 3058: -0.0004208804457448423
Mean eval for ascent epoch 3058: 0.007578045129776001
Doing Evaluation on the model now
This is Epoch 3060, training loss 0.29587, validation loss 0.36934
Epoch  6050: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6061: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3070, training loss 0.07384, validation loss 0.04325
Epoch  6072: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6083: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3080, training loss 0.01244, validation loss 0.01746
Epoch  6094: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3090, training loss 0.00698, validation loss 0.01370
Epoch  6105: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6116: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3096
Mean train loss for ascent epoch 3097: -0.0005948545876890421
Mean eval for ascent epoch 3097: 0.00719877053052187
Doing Evaluation on the model now
This is Epoch 3100, training loss 0.80491, validation loss 1.99658
Epoch  6127: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6138: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3110, training loss 0.02918, validation loss 0.02354
Epoch  6149: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6160: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3120, training loss 0.00726, validation loss 0.00804
Epoch  6171: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6182: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3130, training loss 0.00633, validation loss 0.00555
Epoch  6193: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3135
Mean train loss for ascent epoch 3136: -0.000555954989977181
Mean eval for ascent epoch 3136: 0.006763257551938295
Doing Evaluation on the model now
This is Epoch 3140, training loss 0.51484, validation loss 0.30853
Epoch  6204: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6215: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3150, training loss 0.02123, validation loss 0.02547
Epoch  6226: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6237: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3160, training loss 0.01184, validation loss 0.00685
Epoch  6248: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6259: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3170, training loss 0.00652, validation loss 0.00663
Epoch  6270: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3174
Mean train loss for ascent epoch 3175: -0.0004462445212993771
Mean eval for ascent epoch 3175: 0.007047571707516909
Epoch  6281: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3180, training loss 0.71113, validation loss 0.64471
Epoch  6292: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3190, training loss 0.07782, validation loss 0.03119
Epoch  6303: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6314: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3200, training loss 0.00882, validation loss 0.01351
Resetting learning rate to 0.01000
Epoch  6325: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6336: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3210, training loss 0.00634, validation loss 0.00666
Epoch  6347: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6358: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3219
Mean train loss for ascent epoch 3220: -0.0007188560557551682
Mean eval for ascent epoch 3220: 0.0063475798815488815
Doing Evaluation on the model now
This is Epoch 3220, training loss 0.00635, validation loss 0.00636
Epoch  6369: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6380: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3230, training loss 0.12682, validation loss 0.17184
Epoch  6391: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3240, training loss 0.01687, validation loss 0.01682
Epoch  6402: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6413: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3250, training loss 0.00626, validation loss 0.00657
Epoch  6424: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6435: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3258
Mean train loss for ascent epoch 3259: -0.00039756426122039557
Mean eval for ascent epoch 3259: 0.00543198874220252
Doing Evaluation on the model now
This is Epoch 3260, training loss 0.09758, validation loss 0.25631
Epoch  6446: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6457: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3270, training loss 0.08037, validation loss 0.06039
Epoch  6468: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6479: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3280, training loss 0.02167, validation loss 0.00958
Epoch  6490: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3290, training loss 0.00597, validation loss 0.00625
Epoch  6501: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6512: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3297
Mean train loss for ascent epoch 3298: -0.0006341786938719451
Mean eval for ascent epoch 3298: 0.006920444313436747
Doing Evaluation on the model now
This is Epoch 3300, training loss 0.34217, validation loss 0.62770
Epoch  6523: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6534: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3310, training loss 0.03997, validation loss 0.04722
Epoch  6545: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6556: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3320, training loss 0.01036, validation loss 0.00967
Epoch  6567: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6578: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3330, training loss 0.00564, validation loss 0.00560
Epoch  6589: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3336
Mean train loss for ascent epoch 3337: -0.000714781868737191
Mean eval for ascent epoch 3337: 0.006709573324769735
Doing Evaluation on the model now
This is Epoch 3340, training loss 1.11276, validation loss 1.74210
Epoch  6600: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6611: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3350, training loss 0.05289, validation loss 0.07597
Epoch  6622: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6633: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3360, training loss 0.00666, validation loss 0.00580
Epoch  6644: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6655: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3370, training loss 0.00621, validation loss 0.00554
Epoch  6666: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3375
Mean train loss for ascent epoch 3376: -0.0004745199694298208
Mean eval for ascent epoch 3376: 0.008761297911405563
Doing Evaluation on the model now
This is Epoch 3380, training loss 0.52489, validation loss 1.33711
Epoch  6677: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6688: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3390, training loss 0.07433, validation loss 0.13090
Epoch  6699: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6710: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3400, training loss 0.00849, validation loss 0.00665
Resetting learning rate to 0.01000
Epoch  6721: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6732: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3410, training loss 0.00606, validation loss 0.00612
Epoch  6743: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6754: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3419
Mean train loss for ascent epoch 3420: -0.0005165683687664568
Mean eval for ascent epoch 3420: 0.006156668998301029
Doing Evaluation on the model now
This is Epoch 3420, training loss 0.00616, validation loss 0.00585
Epoch  6765: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3430, training loss 0.14020, validation loss 0.03301
Epoch  6776: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6787: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3440, training loss 0.03200, validation loss 0.00961
Epoch  6798: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6809: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3450, training loss 0.00743, validation loss 0.00661
Epoch  6820: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6831: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3458
Mean train loss for ascent epoch 3459: -0.00043850793736055493
Mean eval for ascent epoch 3459: 0.0061884173192083836
Doing Evaluation on the model now
This is Epoch 3460, training loss 0.09517, validation loss 0.13287
Epoch  6842: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6853: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3470, training loss 0.12467, validation loss 0.03939
Epoch  6864: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3480, training loss 0.01620, validation loss 0.01293
Epoch  6875: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6886: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3490, training loss 0.00692, validation loss 0.00638
Epoch  6897: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6908: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3497
Mean train loss for ascent epoch 3498: -0.00069860415533185
Mean eval for ascent epoch 3498: 0.007124288938939571
Doing Evaluation on the model now
This is Epoch 3500, training loss 0.14351, validation loss 0.11067
Epoch  6919: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6930: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3510, training loss 0.05343, validation loss 0.14806
Epoch  6941: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6952: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3520, training loss 0.01216, validation loss 0.01020
Epoch  6963: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3530, training loss 0.00716, validation loss 0.00682
Epoch  6974: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6985: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3536
Mean train loss for ascent epoch 3537: -0.0005537152173928916
Mean eval for ascent epoch 3537: 0.007822105661034584
Doing Evaluation on the model now
This is Epoch 3540, training loss 0.61811, validation loss 0.11581
Epoch  6996: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7007: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3550, training loss 0.02785, validation loss 0.05212
Epoch  7018: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7029: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3560, training loss 0.00839, validation loss 0.01311
Epoch  7040: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7051: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3570, training loss 0.00767, validation loss 0.00794
Epoch  7062: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3575
Mean train loss for ascent epoch 3576: -0.0005106096505187452
Mean eval for ascent epoch 3576: 0.006501493044197559
Doing Evaluation on the model now
This is Epoch 3580, training loss 0.35834, validation loss 0.36921
Epoch  7073: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7084: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3590, training loss 0.03090, validation loss 0.09104
Epoch  7095: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7106: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3600, training loss 0.00718, validation loss 0.00689
Resetting learning rate to 0.01000
Epoch  7117: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7128: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3610, training loss 0.00620, validation loss 0.00545
Epoch  7139: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7150: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3620
Doing Evaluation on the model now
This is Epoch 3620, training loss 0.00595, validation loss 0.00665
Mean train loss for ascent epoch 3621: -0.00042929730261676013
Mean eval for ascent epoch 3621: 0.006581188179552555
Epoch  7161: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3630, training loss 0.24486, validation loss 0.07059
Epoch  7172: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7183: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3640, training loss 0.02313, validation loss 0.01922
Epoch  7194: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7205: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3650, training loss 0.00602, validation loss 0.00631
Epoch  7216: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7227: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3659
Mean train loss for ascent epoch 3660: -0.0004529344732873142
Mean eval for ascent epoch 3660: 0.007003805134445429
Doing Evaluation on the model now
This is Epoch 3660, training loss 0.00700, validation loss 0.00657
Epoch  7238: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7249: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3670, training loss 0.05147, validation loss 0.08624
Epoch  7260: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3680, training loss 0.01023, validation loss 0.01182
Epoch  7271: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7282: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3690, training loss 0.00821, validation loss 0.00632
Epoch  7293: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7304: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3698
Mean train loss for ascent epoch 3699: -0.0006884376052767038
Mean eval for ascent epoch 3699: 0.006526924204081297
Doing Evaluation on the model now
This is Epoch 3700, training loss 0.23029, validation loss 0.32189
Epoch  7315: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7326: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3710, training loss 0.04919, validation loss 0.03477
Epoch  7337: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7348: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3720, training loss 0.01368, validation loss 0.02440
Epoch  7359: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3730, training loss 0.00886, validation loss 0.00579
Epoch  7370: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7381: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3737
Mean train loss for ascent epoch 3738: -0.0005433046026155353
Mean eval for ascent epoch 3738: 0.006917823106050491
Doing Evaluation on the model now
This is Epoch 3740, training loss 0.43833, validation loss 2.90948
Epoch  7392: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7403: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3750, training loss 0.02421, validation loss 0.01658
Epoch  7414: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7425: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3760, training loss 0.01563, validation loss 0.01140
Epoch  7436: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7447: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3770, training loss 0.00756, validation loss 0.00811
Epoch  7458: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3776
Mean train loss for ascent epoch 3777: -0.00045552392839454114
Mean eval for ascent epoch 3777: 0.007350184489041567
Doing Evaluation on the model now
This is Epoch 3780, training loss 1.01440, validation loss 0.45320
Epoch  7469: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7480: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3790, training loss 0.02998, validation loss 0.01672
Epoch  7491: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7502: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3800, training loss 0.01565, validation loss 0.03205
Resetting learning rate to 0.01000
Epoch  7513: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7524: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3810, training loss 0.01056, validation loss 0.00677
Epoch  7535: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7546: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3820
Doing Evaluation on the model now
This is Epoch 3820, training loss 0.00658, validation loss 0.00618
Mean train loss for ascent epoch 3821: -0.0003483937762212008
Mean eval for ascent epoch 3821: 0.006158575415611267
Epoch  7557: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3830, training loss 0.19876, validation loss 0.11567
Epoch  7568: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7579: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3840, training loss 0.01686, validation loss 0.01069
Epoch  7590: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7601: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3850, training loss 0.00832, validation loss 0.00609
Epoch  7612: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7623: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3859
Mean train loss for ascent epoch 3860: -0.0005524273146875203
Mean eval for ascent epoch 3860: 0.007216019090265036
Doing Evaluation on the model now
This is Epoch 3860, training loss 0.00722, validation loss 0.00597
Epoch  7634: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3870, training loss 0.09214, validation loss 0.05256
Epoch  7645: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7656: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3880, training loss 0.01124, validation loss 0.00834
Epoch  7667: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7678: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3890, training loss 0.00900, validation loss 0.00645
Epoch  7689: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7700: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3898
Mean train loss for ascent epoch 3899: -0.0004923644009977579
Mean eval for ascent epoch 3899: 0.0063703665509819984
Doing Evaluation on the model now
This is Epoch 3900, training loss 0.13042, validation loss 0.19370
Epoch  7711: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7722: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3910, training loss 0.08333, validation loss 0.02663
Epoch  7733: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3920, training loss 0.06015, validation loss 0.05910
Epoch  7744: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7755: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3930, training loss 0.00979, validation loss 0.00974
Epoch  7766: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7777: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3937
Mean train loss for ascent epoch 3938: -0.00040790438652038574
Mean eval for ascent epoch 3938: 0.0055326842702925205
Doing Evaluation on the model now
This is Epoch 3940, training loss 0.55276, validation loss 0.45600
Epoch  7788: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7799: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3950, training loss 0.09867, validation loss 0.03178
Epoch  7810: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7821: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3960, training loss 0.00818, validation loss 0.00931
Epoch  7832: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3970, training loss 0.00699, validation loss 0.00766
Epoch  7843: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7854: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3976
Mean train loss for ascent epoch 3977: -0.0005090679624117911
Mean eval for ascent epoch 3977: 0.006577668245881796
Doing Evaluation on the model now
This is Epoch 3980, training loss 0.61283, validation loss 2.14986
Epoch  7865: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7876: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3990, training loss 0.02800, validation loss 0.02484
Epoch  7887: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7898: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4000, training loss 0.00762, validation loss 0.00674
Resetting learning rate to 0.01000
Epoch  7909: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7920: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4010, training loss 0.00715, validation loss 0.00785
Epoch  7931: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4020, training loss 0.00667, validation loss 0.00602
Epoch  7942: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4021
Mean train loss for ascent epoch 4022: -0.00030677372706122696
Mean eval for ascent epoch 4022: 0.005934099201112986
Epoch  7953: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4030, training loss 0.12064, validation loss 0.09479
Epoch  7964: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7975: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4040, training loss 0.02153, validation loss 0.00909
Epoch  7986: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7997: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4050, training loss 0.00724, validation loss 0.00875
Epoch  8008: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8019: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4060
Doing Evaluation on the model now
This is Epoch 4060, training loss 0.00749, validation loss 0.00532
Mean train loss for ascent epoch 4061: -0.0005799195496365428
Mean eval for ascent epoch 4061: 0.006506828125566244
Epoch  8030: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4070, training loss 0.55729, validation loss 0.13705
Epoch  8041: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8052: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4080, training loss 0.04520, validation loss 0.07974
Epoch  8063: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8074: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4090, training loss 0.00701, validation loss 0.00765
Epoch  8085: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8096: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4099
Mean train loss for ascent epoch 4100: -0.0006772847846150398
Mean eval for ascent epoch 4100: 0.0065428647212684155
Doing Evaluation on the model now
This is Epoch 4100, training loss 0.00654, validation loss 0.00700
Epoch  8107: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8118: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4110, training loss 0.16020, validation loss 0.03629
Epoch  8129: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4120, training loss 0.01633, validation loss 0.04757
Epoch  8140: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8151: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4130, training loss 0.00749, validation loss 0.00909
Epoch  8162: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8173: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4138
Mean train loss for ascent epoch 4139: -0.0005990882636979222
Mean eval for ascent epoch 4139: 0.007319331169128418
Doing Evaluation on the model now
This is Epoch 4140, training loss 0.10806, validation loss 0.23137
Epoch  8184: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8195: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4150, training loss 0.03531, validation loss 0.01955
Epoch  8206: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8217: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4160, training loss 0.01884, validation loss 0.01329
Epoch  8228: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4170, training loss 0.00637, validation loss 0.00595
Epoch  8239: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8250: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4177
Mean train loss for ascent epoch 4178: -0.00048795895418152213
Mean eval for ascent epoch 4178: 0.007595542352646589
Doing Evaluation on the model now
This is Epoch 4180, training loss 0.33486, validation loss 0.57258
Epoch  8261: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8272: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4190, training loss 0.02611, validation loss 0.04141
Epoch  8283: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8294: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4200, training loss 0.03266, validation loss 0.04441
Resetting learning rate to 0.01000
Epoch  8305: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8316: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4210, training loss 0.00851, validation loss 0.00961
Epoch  8327: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4220, training loss 0.00670, validation loss 0.00903
Epoch  8338: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4221
Mean train loss for ascent epoch 4222: -0.0006409180350601673
Mean eval for ascent epoch 4222: 0.007265585940331221
Epoch  8349: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4230, training loss 0.11494, validation loss 0.15597
Epoch  8360: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8371: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4240, training loss 0.03895, validation loss 0.02415
Epoch  8382: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8393: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4250, training loss 0.00794, validation loss 0.00667
Epoch  8404: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8415: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4260
Doing Evaluation on the model now
This is Epoch 4260, training loss 0.00596, validation loss 0.00535
Mean train loss for ascent epoch 4261: -0.0003744654241017997
Mean eval for ascent epoch 4261: 0.005640316754579544
Epoch  8426: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4270, training loss 0.08168, validation loss 0.03691
Epoch  8437: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8448: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4280, training loss 0.01145, validation loss 0.02449
Epoch  8459: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8470: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4290, training loss 0.00741, validation loss 0.00614
Epoch  8481: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8492: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4299
Mean train loss for ascent epoch 4300: -0.0004745297774206847
Mean eval for ascent epoch 4300: 0.005829951260238886
Doing Evaluation on the model now
This is Epoch 4300, training loss 0.00583, validation loss 0.00575
Epoch  8503: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4310, training loss 0.07750, validation loss 0.04812
Epoch  8514: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8525: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4320, training loss 0.02055, validation loss 0.00882
Epoch  8536: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8547: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4330, training loss 0.00664, validation loss 0.00646
Epoch  8558: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8569: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4338
Mean train loss for ascent epoch 4339: -0.0004332523385528475
Mean eval for ascent epoch 4339: 0.0064121452160179615
Doing Evaluation on the model now
This is Epoch 4340, training loss 0.11988, validation loss 0.15745
Epoch  8580: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8591: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4350, training loss 0.03997, validation loss 0.04951
Epoch  8602: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4360, training loss 0.01818, validation loss 0.00869
Epoch  8613: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8624: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4370, training loss 0.00836, validation loss 0.00744
Epoch  8635: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8646: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4377
Mean train loss for ascent epoch 4378: -0.0004896888858638704
Mean eval for ascent epoch 4378: 0.006693510804325342
Doing Evaluation on the model now
This is Epoch 4380, training loss 0.76220, validation loss 2.29108
Epoch  8657: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8668: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4390, training loss 0.03974, validation loss 0.01753
Epoch  8679: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8690: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4400, training loss 0.02942, validation loss 0.00723
Resetting learning rate to 0.01000
Epoch  8701: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4410, training loss 0.00877, validation loss 0.00923
Epoch  8712: reducing learning rate of group 0 to 2.5000e-04.
Epoch  8723: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4420, training loss 0.00613, validation loss 0.00549
Epoch  8734: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4422
Mean train loss for ascent epoch 4423: -0.0005412035388872027
Mean eval for ascent epoch 4423: 0.0071113863959908485
Epoch  8745: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4430, training loss 0.20656, validation loss 0.26984
Epoch  8756: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8767: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4440, training loss 0.01622, validation loss 0.03093
Epoch  8778: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8789: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4450, training loss 0.00998, validation loss 0.00727
Epoch  8800: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4460, training loss 0.00661, validation loss 0.00663
Epoch  8811: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4461
Mean train loss for ascent epoch 4462: -0.00042481583659537137
Mean eval for ascent epoch 4462: 0.0066947429440915585
Epoch  8822: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4470, training loss 0.06965, validation loss 0.07237
Epoch  8833: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8844: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4480, training loss 0.01624, validation loss 0.00825
Epoch  8855: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8866: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4490, training loss 0.00741, validation loss 0.00636
Epoch  8877: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8888: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4500
Doing Evaluation on the model now
This is Epoch 4500, training loss 0.00660, validation loss 0.00955
Mean train loss for ascent epoch 4501: -0.0005972887156531215
Mean eval for ascent epoch 4501: 0.007336089387536049
Epoch  8899: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4510, training loss 0.07304, validation loss 0.05254
Epoch  8910: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8921: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4520, training loss 0.01658, validation loss 0.01710
Epoch  8932: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8943: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4530, training loss 0.00706, validation loss 0.00623
Epoch  8954: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8965: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4539
Mean train loss for ascent epoch 4540: -0.0005941104609519243
Mean eval for ascent epoch 4540: 0.0064956434071063995
Doing Evaluation on the model now
This is Epoch 4540, training loss 0.00650, validation loss 0.00600
Epoch  8976: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8987: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4550, training loss 0.08442, validation loss 0.05457
Epoch  8998: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4560, training loss 0.02297, validation loss 0.01953
Epoch  9009: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9020: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4570, training loss 0.00622, validation loss 0.00647
Epoch  9031: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9042: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4578
Mean train loss for ascent epoch 4579: -0.0005227408255450428
Mean eval for ascent epoch 4579: 0.006241908762603998
Doing Evaluation on the model now
This is Epoch 4580, training loss 0.17059, validation loss 0.21802
Epoch  9053: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9064: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4590, training loss 0.05167, validation loss 0.02552
Epoch  9075: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9086: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4600, training loss 0.01897, validation loss 0.01587
Resetting learning rate to 0.01000
Epoch  9097: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4610, training loss 0.00730, validation loss 0.01295
Epoch  9108: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9119: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4620, training loss 0.00632, validation loss 0.00673
Epoch  9130: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4622
Mean train loss for ascent epoch 4623: -0.0006421318394131958
Mean eval for ascent epoch 4623: 0.008302136324346066
Epoch  9141: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4630, training loss 0.15751, validation loss 0.14944
Epoch  9152: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9163: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4640, training loss 0.01985, validation loss 0.01693
Epoch  9174: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9185: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4650, training loss 0.01071, validation loss 0.00577
Epoch  9196: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4660, training loss 0.01025, validation loss 0.01083
Epoch  9207: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4661
Mean train loss for ascent epoch 4662: -0.0004168650193605572
Mean eval for ascent epoch 4662: 0.008175022900104523
Epoch  9218: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4670, training loss 0.10281, validation loss 0.03803
Epoch  9229: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9240: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4680, training loss 0.01559, validation loss 0.02338
Epoch  9251: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9262: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4690, training loss 0.00865, validation loss 0.00575
Epoch  9273: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9284: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4700
Doing Evaluation on the model now
This is Epoch 4700, training loss 0.00720, validation loss 0.00738
Mean train loss for ascent epoch 4701: -0.00036642205668613315
Mean eval for ascent epoch 4701: 0.005928585305809975
Epoch  9295: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4710, training loss 0.11578, validation loss 0.11560
Epoch  9306: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9317: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4720, training loss 0.01303, validation loss 0.01114
Epoch  9328: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9339: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4730, training loss 0.00760, validation loss 0.00847
Epoch  9350: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9361: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4739
Mean train loss for ascent epoch 4740: -0.0003880219010170549
Mean eval for ascent epoch 4740: 0.006883026100695133
Doing Evaluation on the model now
This is Epoch 4740, training loss 0.00688, validation loss 0.00617
Epoch  9372: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4750, training loss 0.07846, validation loss 0.03322
Epoch  9383: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9394: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4760, training loss 0.02596, validation loss 0.03640
Epoch  9405: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9416: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4770, training loss 0.00698, validation loss 0.00668
Epoch  9427: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9438: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4778
Mean train loss for ascent epoch 4779: -0.0003979321045335382
Mean eval for ascent epoch 4779: 0.0068494961597025394
Doing Evaluation on the model now
This is Epoch 4780, training loss 0.15289, validation loss 0.14442
Epoch  9449: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9460: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4790, training loss 0.12305, validation loss 0.04330
Epoch  9471: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4800, training loss 0.02171, validation loss 0.01390
Epoch  9482: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  9493: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4810, training loss 0.00626, validation loss 0.00669
Epoch  9504: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9515: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4820, training loss 0.00610, validation loss 0.00592
Epoch  9526: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4823
Mean train loss for ascent epoch 4824: -0.0003768946626223624
Mean eval for ascent epoch 4824: 0.006446640007197857
Epoch  9537: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4830, training loss 0.13757, validation loss 0.04041
Epoch  9548: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9559: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4840, training loss 0.03644, validation loss 0.02921
Epoch  9570: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4850, training loss 0.00736, validation loss 0.00838
Epoch  9581: reducing learning rate of group 0 to 3.1250e-04.
Epoch  9592: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4860, training loss 0.00597, validation loss 0.00558
Epoch  9603: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4862
Mean train loss for ascent epoch 4863: -0.00039352235035039485
Mean eval for ascent epoch 4863: 0.005658925976604223
Epoch  9614: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4870, training loss 0.09737, validation loss 0.07979
Epoch  9625: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9636: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4880, training loss 0.01187, validation loss 0.00768
Epoch  9647: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9658: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4890, training loss 0.00924, validation loss 0.00705
Epoch  9669: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4900, training loss 0.00643, validation loss 0.00658
Epoch  9680: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4901
Mean train loss for ascent epoch 4902: -0.00040647602872923017
Mean eval for ascent epoch 4902: 0.005994770210236311
Epoch  9691: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4910, training loss 0.10024, validation loss 0.13555
Epoch  9702: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9713: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4920, training loss 0.01256, validation loss 0.01433
Epoch  9724: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9735: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4930, training loss 0.00830, validation loss 0.00708
Epoch  9746: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9757: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4940
Doing Evaluation on the model now
This is Epoch 4940, training loss 0.00590, validation loss 0.00647
Mean train loss for ascent epoch 4941: -0.0005162900197319686
Mean eval for ascent epoch 4941: 0.005751732271164656
Epoch  9768: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4950, training loss 0.25839, validation loss 0.05367
Epoch  9779: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9790: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4960, training loss 0.01122, validation loss 0.01257
Epoch  9801: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9812: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4970, training loss 0.00541, validation loss 0.00578
Epoch  9823: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9834: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4979
Mean train loss for ascent epoch 4980: -0.0005575206014327705
Mean eval for ascent epoch 4980: 0.005498790182173252
Doing Evaluation on the model now
This is Epoch 4980, training loss 0.00550, validation loss 0.00520
Epoch  9845: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9856: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4990, training loss 0.05823, validation loss 0.15225
Epoch  9867: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5000, training loss 0.02605, validation loss 0.01917
Resetting learning rate to 0.01000
Epoch  9878: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9889: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5010, training loss 0.00715, validation loss 0.00510
Epoch  9900: reducing learning rate of group 0 to 1.2500e-04.
Epoch  9911: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5018
Mean train loss for ascent epoch 5019: -0.00036414741771295667
Mean eval for ascent epoch 5019: 0.005615344736725092
Doing Evaluation on the model now
This is Epoch 5020, training loss 0.15606, validation loss 0.43522
Epoch  9922: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9933: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5030, training loss 0.07941, validation loss 0.02166
Epoch  9944: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9955: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5040, training loss 0.02230, validation loss 0.00924
Epoch  9966: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5050, training loss 0.00573, validation loss 0.00541
Epoch  9977: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9988: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5057
Mean train loss for ascent epoch 5058: -0.000636630633380264
Mean eval for ascent epoch 5058: 0.006653912831097841
Doing Evaluation on the model now
This is Epoch 5060, training loss 1.04693, validation loss 1.16611
Epoch  9999: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10010: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5070, training loss 0.09365, validation loss 0.02211
Epoch 10021: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10032: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5080, training loss 0.01525, validation loss 0.01569
Epoch 10043: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10054: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5090, training loss 0.00714, validation loss 0.01121
Epoch 10065: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5096
Mean train loss for ascent epoch 5097: -0.0004121946112718433
Mean eval for ascent epoch 5097: 0.0056243822909891605
Doing Evaluation on the model now
This is Epoch 5100, training loss 1.07968, validation loss 4.27821
Epoch 10076: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10087: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5110, training loss 0.04953, validation loss 0.05525
Epoch 10098: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10109: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5120, training loss 0.00787, validation loss 0.01004
Epoch 10120: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10131: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5130, training loss 0.00541, validation loss 0.00507
Epoch 10142: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5135
Mean train loss for ascent epoch 5136: -0.000583699147682637
Mean eval for ascent epoch 5136: 0.006208771374076605
Doing Evaluation on the model now
This is Epoch 5140, training loss 0.51063, validation loss 0.59554
Epoch 10153: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10164: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5150, training loss 0.02667, validation loss 0.05748
Epoch 10175: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10186: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5160, training loss 0.01052, validation loss 0.00776
Epoch 10197: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10208: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5170, training loss 0.00616, validation loss 0.00426
Epoch 10219: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5174
Mean train loss for ascent epoch 5175: -0.0002765402605291456
Mean eval for ascent epoch 5175: 0.005639193579554558
Epoch 10230: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5180, training loss 0.31685, validation loss 0.32718
Epoch 10241: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5190, training loss 0.02967, validation loss 0.01976
Epoch 10252: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10263: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5200, training loss 0.00976, validation loss 0.00713
Resetting learning rate to 0.01000
Epoch 10274: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10285: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5210, training loss 0.00655, validation loss 0.00690
Epoch 10296: reducing learning rate of group 0 to 1.2500e-04.
Epoch 10307: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5218
Mean train loss for ascent epoch 5219: -0.00035211158683523536
Mean eval for ascent epoch 5219: 0.00628245435655117
Doing Evaluation on the model now
This is Epoch 5220, training loss 0.18110, validation loss 0.17789
Epoch 10318: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10329: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5230, training loss 0.05050, validation loss 0.02451
Epoch 10340: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5240, training loss 0.01135, validation loss 0.01029
Epoch 10351: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10362: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5250, training loss 0.00680, validation loss 0.00645
Epoch 10373: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10384: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5257
Mean train loss for ascent epoch 5258: -0.0005934028304181993
Mean eval for ascent epoch 5258: 0.006418332923203707
Doing Evaluation on the model now
This is Epoch 5260, training loss 0.30329, validation loss 0.75897
Epoch 10395: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10406: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5270, training loss 0.06124, validation loss 0.03374
Epoch 10417: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10428: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5280, training loss 0.00757, validation loss 0.00583
Epoch 10439: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5290, training loss 0.00567, validation loss 0.00574
Epoch 10450: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10461: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5296
Mean train loss for ascent epoch 5297: -0.0004177250375505537
Mean eval for ascent epoch 5297: 0.006316302809864283
Doing Evaluation on the model now
This is Epoch 5300, training loss 0.88011, validation loss 0.83327
Epoch 10472: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10483: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5310, training loss 0.02550, validation loss 0.06458
Epoch 10494: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10505: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5320, training loss 0.00831, validation loss 0.01088
Epoch 10516: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10527: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5330, training loss 0.00503, validation loss 0.00497
Epoch 10538: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5335
Mean train loss for ascent epoch 5336: -0.00045508076436817646
Mean eval for ascent epoch 5336: 0.005319160409271717
Doing Evaluation on the model now
This is Epoch 5340, training loss 0.66946, validation loss 0.25782
Epoch 10549: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10560: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5350, training loss 0.06247, validation loss 0.10610
Epoch 10571: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10582: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5360, training loss 0.00835, validation loss 0.01102
Epoch 10593: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10604: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5370, training loss 0.00600, validation loss 0.00574
Epoch 10615: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5374
Mean train loss for ascent epoch 5375: -0.0005537356482818723
Mean eval for ascent epoch 5375: 0.006334156263619661
Epoch 10626: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5380, training loss 0.40898, validation loss 1.62771
Epoch 10637: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5390, training loss 0.04013, validation loss 0.02966
Epoch 10648: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10659: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5400, training loss 0.00914, validation loss 0.00629
Resetting learning rate to 0.01000
Epoch 10670: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10681: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5410, training loss 0.00642, validation loss 0.00698
Epoch 10692: reducing learning rate of group 0 to 1.2500e-04.
Epoch 10703: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5419
Mean train loss for ascent epoch 5420: -0.0004123434191569686
Mean eval for ascent epoch 5420: 0.005616789683699608
Doing Evaluation on the model now
This is Epoch 5420, training loss 0.00562, validation loss 0.00528
Epoch 10714: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10725: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5430, training loss 0.10423, validation loss 0.10394
Epoch 10736: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5440, training loss 0.01458, validation loss 0.01487
Epoch 10747: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10758: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5450, training loss 0.00737, validation loss 0.00764
Epoch 10769: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10780: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5458
Mean train loss for ascent epoch 5459: -0.0005181476008147001
Mean eval for ascent epoch 5459: 0.005888820625841618
Doing Evaluation on the model now
This is Epoch 5460, training loss 0.10980, validation loss 0.07203
Epoch 10791: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10802: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5470, training loss 0.04665, validation loss 0.08872
Epoch 10813: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10824: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5480, training loss 0.01765, validation loss 0.01577
Epoch 10835: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5490, training loss 0.00767, validation loss 0.00637
Epoch 10846: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10857: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5497
Mean train loss for ascent epoch 5498: -0.0005951280472800136
Mean eval for ascent epoch 5498: 0.006124270614236593
Doing Evaluation on the model now
This is Epoch 5500, training loss 0.37642, validation loss 0.40353
Epoch 10868: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10879: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5510, training loss 0.03900, validation loss 0.07428
Epoch 10890: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10901: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5520, training loss 0.00966, validation loss 0.00800
Epoch 10912: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10923: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5530, training loss 0.00704, validation loss 0.00741
Epoch 10934: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5536
Mean train loss for ascent epoch 5537: -0.0010082803200930357
Mean eval for ascent epoch 5537: 0.0057606627233326435
Doing Evaluation on the model now
This is Epoch 5540, training loss 0.34089, validation loss 0.67466
Epoch 10945: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10956: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5550, training loss 0.03609, validation loss 0.08649
Epoch 10967: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10978: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5560, training loss 0.00868, validation loss 0.00691
Epoch 10989: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11000: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5570, training loss 0.00542, validation loss 0.00505
Epoch 11011: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5575
Mean train loss for ascent epoch 5576: -0.00041820990736596286
Mean eval for ascent epoch 5576: 0.005220235325396061
Doing Evaluation on the model now
This is Epoch 5580, training loss 0.84771, validation loss 0.34586
Epoch 11022: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11033: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5590, training loss 0.05702, validation loss 0.10574
Epoch 11044: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11055: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5600, training loss 0.00983, validation loss 0.01648
Resetting learning rate to 0.01000
Epoch 11066: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11077: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5610, training loss 0.00605, validation loss 0.00785
Epoch 11088: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11099: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5619
Mean train loss for ascent epoch 5620: -0.00031881523318588734
Mean eval for ascent epoch 5620: 0.005374405533075333
Doing Evaluation on the model now
This is Epoch 5620, training loss 0.00537, validation loss 0.00513
Epoch 11110: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5630, training loss 0.17268, validation loss 0.19653
Epoch 11121: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11132: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5640, training loss 0.01112, validation loss 0.00819
Epoch 11143: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11154: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5650, training loss 0.00504, validation loss 0.00536
Epoch 11165: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11176: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5658
Mean train loss for ascent epoch 5659: -0.00039067797479219735
Mean eval for ascent epoch 5659: 0.005423641297966242
Doing Evaluation on the model now
This is Epoch 5660, training loss 0.20622, validation loss 0.16548
Epoch 11187: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11198: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5670, training loss 0.03340, validation loss 0.06665
Epoch 11209: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5680, training loss 0.03161, validation loss 0.05916
Epoch 11220: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11231: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5690, training loss 0.00571, validation loss 0.00495
Epoch 11242: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11253: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5697
Mean train loss for ascent epoch 5698: -0.0005089573678560555
Mean eval for ascent epoch 5698: 0.00603929627686739
Doing Evaluation on the model now
This is Epoch 5700, training loss 0.43345, validation loss 0.47851
Epoch 11264: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11275: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5710, training loss 0.03605, validation loss 0.03273
Epoch 11286: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11297: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5720, training loss 0.01190, validation loss 0.01582
Epoch 11308: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5730, training loss 0.00730, validation loss 0.00699
Epoch 11319: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11330: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5736
Mean train loss for ascent epoch 5737: -0.0004584448179230094
Mean eval for ascent epoch 5737: 0.0053231543861329556
Doing Evaluation on the model now
This is Epoch 5740, training loss 0.75739, validation loss 0.99557
Epoch 11341: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11352: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5750, training loss 0.02808, validation loss 0.05351
Epoch 11363: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11374: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5760, training loss 0.00852, validation loss 0.01112
Epoch 11385: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11396: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5770, training loss 0.00609, validation loss 0.00647
Epoch 11407: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5775
Mean train loss for ascent epoch 5776: -0.0003296604845672846
Mean eval for ascent epoch 5776: 0.005408723838627338
Doing Evaluation on the model now
This is Epoch 5780, training loss 0.55584, validation loss 0.65704
Epoch 11418: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11429: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5790, training loss 0.07302, validation loss 0.05010
Epoch 11440: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11451: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5800, training loss 0.00687, validation loss 0.00526
Resetting learning rate to 0.01000
Epoch 11462: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11473: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5810, training loss 0.00719, validation loss 0.00608
Epoch 11484: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11495: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5820
Doing Evaluation on the model now
This is Epoch 5820, training loss 0.00531, validation loss 0.00477
Mean train loss for ascent epoch 5821: -0.00039683625800535083
Mean eval for ascent epoch 5821: 0.005990383680909872
Epoch 11506: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5830, training loss 0.13982, validation loss 0.11744
Epoch 11517: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11528: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5840, training loss 0.03978, validation loss 0.01259
Epoch 11539: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11550: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5850, training loss 0.00584, validation loss 0.00557
Epoch 11561: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11572: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5859
Mean train loss for ascent epoch 5860: -0.000490546808578074
Mean eval for ascent epoch 5860: 0.0057645379565656185
Doing Evaluation on the model now
This is Epoch 5860, training loss 0.00576, validation loss 0.00579
Epoch 11583: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11594: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5870, training loss 0.06916, validation loss 0.03797
Epoch 11605: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5880, training loss 0.03425, validation loss 0.02387
Epoch 11616: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11627: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5890, training loss 0.00595, validation loss 0.00649
Epoch 11638: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11649: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5898
Mean train loss for ascent epoch 5899: -0.0004374372656457126
Mean eval for ascent epoch 5899: 0.006026840303093195
Doing Evaluation on the model now
This is Epoch 5900, training loss 0.21656, validation loss 0.21127
Epoch 11660: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11671: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5910, training loss 0.05653, validation loss 0.04243
Epoch 11682: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11693: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5920, training loss 0.01296, validation loss 0.00781
Epoch 11704: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5930, training loss 0.00616, validation loss 0.00620
Epoch 11715: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11726: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5937
Mean train loss for ascent epoch 5938: -0.00031821904121898115
Mean eval for ascent epoch 5938: 0.0056383959017694
Doing Evaluation on the model now
This is Epoch 5940, training loss 0.55026, validation loss 1.33233
Epoch 11737: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11748: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5950, training loss 0.05476, validation loss 0.02991
Epoch 11759: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11770: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5960, training loss 0.01275, validation loss 0.01597
Epoch 11781: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11792: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5970, training loss 0.00594, validation loss 0.00501
Epoch 11803: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5976
Mean train loss for ascent epoch 5977: -0.00033912548678927124
Mean eval for ascent epoch 5977: 0.004880358465015888
Doing Evaluation on the model now
This is Epoch 5980, training loss 0.22299, validation loss 0.33977
Epoch 11814: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11825: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5990, training loss 0.09626, validation loss 0.15155
Epoch 11836: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11847: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6000, training loss 0.00858, validation loss 0.01548
Resetting learning rate to 0.01000
Epoch 11858: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11869: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6010, training loss 0.00728, validation loss 0.00535
Epoch 11880: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11891: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6020
Doing Evaluation on the model now
This is Epoch 6020, training loss 0.00625, validation loss 0.00656
Mean train loss for ascent epoch 6021: -0.00041795556899160147
Mean eval for ascent epoch 6021: 0.005912507884204388
Epoch 11902: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6030, training loss 0.17396, validation loss 0.13342
Epoch 11913: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11924: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6040, training loss 0.01071, validation loss 0.01720
Epoch 11935: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11946: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6050, training loss 0.00914, validation loss 0.00627
Epoch 11957: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11968: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6059
Mean train loss for ascent epoch 6060: -0.0004114423936698586
Mean eval for ascent epoch 6060: 0.005435135681182146
Doing Evaluation on the model now
This is Epoch 6060, training loss 0.00544, validation loss 0.00499
Epoch 11979: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6070, training loss 0.05919, validation loss 0.07076
Epoch 11990: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12001: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6080, training loss 0.01929, validation loss 0.02307
Epoch 12012: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12023: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6090, training loss 0.00698, validation loss 0.00766
Epoch 12034: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12045: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6098
Mean train loss for ascent epoch 6099: -0.0006716535426676273
Mean eval for ascent epoch 6099: 0.005287035834044218
Doing Evaluation on the model now
This is Epoch 6100, training loss 0.14444, validation loss 0.24236
Epoch 12056: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12067: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6110, training loss 0.02095, validation loss 0.01338
Epoch 12078: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6120, training loss 0.01414, validation loss 0.01919
Epoch 12089: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12100: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6130, training loss 0.00746, validation loss 0.01591
Epoch 12111: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12122: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6137
Mean train loss for ascent epoch 6138: -0.000435405207099393
Mean eval for ascent epoch 6138: 0.005590934306383133
Doing Evaluation on the model now
This is Epoch 6140, training loss 0.38624, validation loss 0.64435
Epoch 12133: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12144: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6150, training loss 0.05283, validation loss 0.06059
Epoch 12155: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12166: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6160, training loss 0.01811, validation loss 0.00623
Epoch 12177: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6170, training loss 0.00597, validation loss 0.00541
Epoch 12188: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12199: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6176
Mean train loss for ascent epoch 6177: -0.0004379950405564159
Mean eval for ascent epoch 6177: 0.005417422857135534
Doing Evaluation on the model now
This is Epoch 6180, training loss 5.46097, validation loss 4.28995
Epoch 12210: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12221: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6190, training loss 0.02521, validation loss 0.04254
Epoch 12232: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12243: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6200, training loss 0.00748, validation loss 0.01053
Resetting learning rate to 0.01000
Epoch 12254: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12265: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6210, training loss 0.00679, validation loss 0.00671
Epoch 12276: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6220, training loss 0.00547, validation loss 0.00461
Epoch 12287: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6221
Mean train loss for ascent epoch 6222: -0.0004345937923062593
Mean eval for ascent epoch 6222: 0.00510240625590086
Epoch 12298: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6230, training loss 0.07731, validation loss 0.17033
Epoch 12309: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12320: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6240, training loss 0.01023, validation loss 0.00969
Epoch 12331: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12342: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6250, training loss 0.00624, validation loss 0.00494
Epoch 12353: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12364: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6260
Doing Evaluation on the model now
This is Epoch 6260, training loss 0.00543, validation loss 0.00479
Mean train loss for ascent epoch 6261: -0.00046930747339501977
Mean eval for ascent epoch 6261: 0.005823394283652306
Epoch 12375: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6270, training loss 0.03274, validation loss 0.03334
Epoch 12386: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12397: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6280, training loss 0.01545, validation loss 0.01396
Epoch 12408: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12419: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6290, training loss 0.00595, validation loss 0.00508
Epoch 12430: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12441: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6299
Mean train loss for ascent epoch 6300: -0.00047007427201606333
Mean eval for ascent epoch 6300: 0.005266611929982901
Doing Evaluation on the model now
This is Epoch 6300, training loss 0.00527, validation loss 0.00585
Epoch 12452: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12463: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6310, training loss 0.14325, validation loss 0.08614
Epoch 12474: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6320, training loss 0.01062, validation loss 0.00995
Epoch 12485: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12496: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6330, training loss 0.00577, validation loss 0.00624
Epoch 12507: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12518: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6338
Mean train loss for ascent epoch 6339: -0.0006234936881810427
Mean eval for ascent epoch 6339: 0.005690327379852533
Doing Evaluation on the model now
This is Epoch 6340, training loss 0.12329, validation loss 0.28055
Epoch 12529: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12540: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6350, training loss 0.07542, validation loss 0.02046
Epoch 12551: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12562: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6360, training loss 0.01678, validation loss 0.02788
Epoch 12573: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6370, training loss 0.00569, validation loss 0.00559
Epoch 12584: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12595: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6377
Mean train loss for ascent epoch 6378: -0.00024279716308228672
Mean eval for ascent epoch 6378: 0.004402536898851395
Doing Evaluation on the model now
This is Epoch 6380, training loss 6.84608, validation loss 4.38332
Epoch 12606: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12617: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6390, training loss 0.05964, validation loss 0.05132
Epoch 12628: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12639: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6400, training loss 0.01053, validation loss 0.01121
Resetting learning rate to 0.01000
Epoch 12650: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12661: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6410, training loss 0.01478, validation loss 0.00826
Epoch 12672: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6420, training loss 0.00572, validation loss 0.00505
Epoch 12683: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6421
Mean train loss for ascent epoch 6422: -0.0005045523284934461
Mean eval for ascent epoch 6422: 0.005515191238373518
Epoch 12694: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6430, training loss 0.16653, validation loss 0.16984
Epoch 12705: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12716: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6440, training loss 0.02860, validation loss 0.03530
Epoch 12727: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12738: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6450, training loss 0.00919, validation loss 0.01016
Epoch 12749: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12760: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6460
Doing Evaluation on the model now
This is Epoch 6460, training loss 0.00623, validation loss 0.00711
Mean train loss for ascent epoch 6461: -0.0004133526817895472
Mean eval for ascent epoch 6461: 0.005817685276269913
Epoch 12771: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6470, training loss 0.17002, validation loss 0.13683
Epoch 12782: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12793: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6480, training loss 0.01431, validation loss 0.01005
Epoch 12804: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12815: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6490, training loss 0.00654, validation loss 0.00648
Epoch 12826: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12837: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6499
Mean train loss for ascent epoch 6500: -0.0005412264727056026
Mean eval for ascent epoch 6500: 0.005248967558145523
Doing Evaluation on the model now
This is Epoch 6500, training loss 0.00525, validation loss 0.00507
Epoch 12848: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6510, training loss 0.10072, validation loss 0.06278
Epoch 12859: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12870: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6520, training loss 0.01348, validation loss 0.00952
Epoch 12881: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12892: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6530, training loss 0.00496, validation loss 0.00590
Epoch 12903: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12914: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6538
Mean train loss for ascent epoch 6539: -0.00038662218139506876
Mean eval for ascent epoch 6539: 0.005130188539624214
Doing Evaluation on the model now
This is Epoch 6540, training loss 0.29570, validation loss 0.64299
Epoch 12925: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12936: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6550, training loss 0.06870, validation loss 0.04795
Epoch 12947: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6560, training loss 0.01384, validation loss 0.01314
Epoch 12958: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12969: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6570, training loss 0.00604, validation loss 0.00666
Epoch 12980: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12991: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6577
Mean train loss for ascent epoch 6578: -0.00044308530050329864
Mean eval for ascent epoch 6578: 0.005904472898691893
Doing Evaluation on the model now
This is Epoch 6580, training loss 2.72905, validation loss 1.55963
Epoch 13002: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13013: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6590, training loss 0.06437, validation loss 0.03566
Epoch 13024: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13035: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6600, training loss 0.02752, validation loss 0.01354
Resetting learning rate to 0.01000
Epoch 13046: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 6610, training loss 0.00785, validation loss 0.00693
Epoch 13057: reducing learning rate of group 0 to 2.5000e-04.
Epoch 13068: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6620, training loss 0.00512, validation loss 0.00447
Epoch 13079: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6622
Mean train loss for ascent epoch 6623: -0.00034352490911260247
Mean eval for ascent epoch 6623: 0.005452756304293871
Epoch 13090: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6630, training loss 0.23776, validation loss 0.20720
Epoch 13101: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13112: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6640, training loss 0.01262, validation loss 0.00802
Epoch 13123: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13134: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6650, training loss 0.00841, validation loss 0.00645
Epoch 13145: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6660, training loss 0.00571, validation loss 0.00536
Epoch 13156: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6661
Mean train loss for ascent epoch 6662: -0.0003517364093568176
Mean eval for ascent epoch 6662: 0.006061704363673925
Epoch 13167: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6670, training loss 0.14535, validation loss 0.08782
Epoch 13178: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13189: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6680, training loss 0.01283, validation loss 0.02778
Epoch 13200: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13211: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6690, training loss 0.00707, validation loss 0.00682
Epoch 13222: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13233: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6700
Doing Evaluation on the model now
This is Epoch 6700, training loss 0.00524, validation loss 0.00516
Mean train loss for ascent epoch 6701: -0.00035546126309782267
Mean eval for ascent epoch 6701: 0.0054254294373095036
Epoch 13244: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6710, training loss 0.08970, validation loss 0.09642
Epoch 13255: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13266: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6720, training loss 0.01239, validation loss 0.00943
Epoch 13277: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13288: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6730, training loss 0.00628, validation loss 0.00635
Epoch 13299: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13310: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6739
Mean train loss for ascent epoch 6740: -0.00044033347512595356
Mean eval for ascent epoch 6740: 0.005300856661051512
Doing Evaluation on the model now
This is Epoch 6740, training loss 0.00530, validation loss 0.00481
Epoch 13321: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13332: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6750, training loss 0.08599, validation loss 0.14216
Epoch 13343: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6760, training loss 0.01658, validation loss 0.01536
Epoch 13354: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13365: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6770, training loss 0.00533, validation loss 0.00487
Epoch 13376: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13387: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6778
Mean train loss for ascent epoch 6779: -0.000320067279972136
Mean eval for ascent epoch 6779: 0.005383647512644529
Doing Evaluation on the model now
This is Epoch 6780, training loss 0.13683, validation loss 0.13030
Epoch 13398: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13409: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6790, training loss 0.11362, validation loss 0.06146
Epoch 13420: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13431: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6800, training loss 0.02351, validation loss 0.02263
Resetting learning rate to 0.01000
Epoch 13442: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 6810, training loss 0.00591, validation loss 0.00479
Epoch 13453: reducing learning rate of group 0 to 2.5000e-04.
Epoch 13464: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6820, training loss 0.00511, validation loss 0.00468
Epoch 13475: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6822
Mean train loss for ascent epoch 6823: -0.0004217719833832234
Mean eval for ascent epoch 6823: 0.005149349570274353
Epoch 13486: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6830, training loss 0.13969, validation loss 0.13165
Epoch 13497: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13508: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6840, training loss 0.03062, validation loss 0.01150
Epoch 13519: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13530: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6850, training loss 0.01440, validation loss 0.01174
Epoch 13541: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6860, training loss 0.00618, validation loss 0.00505
Epoch 13552: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6861
Mean train loss for ascent epoch 6862: -0.00042817299254238605
Mean eval for ascent epoch 6862: 0.005595649592578411
Epoch 13563: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6870, training loss 0.10512, validation loss 0.13425
Epoch 13574: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13585: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6880, training loss 0.02346, validation loss 0.01764
Epoch 13596: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13607: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6890, training loss 0.00960, validation loss 0.00722
Epoch 13618: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13629: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6900
Doing Evaluation on the model now
This is Epoch 6900, training loss 0.00572, validation loss 0.00680
Mean train loss for ascent epoch 6901: -0.0003143486683256924
Mean eval for ascent epoch 6901: 0.005247750785201788
Epoch 13640: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6910, training loss 0.11093, validation loss 0.02304
Epoch 13651: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13662: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6920, training loss 0.01319, validation loss 0.01355
Epoch 13673: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13684: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6930, training loss 0.00800, validation loss 0.00720
Epoch 13695: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13706: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6939
Mean train loss for ascent epoch 6940: -0.00039933304651640356
Mean eval for ascent epoch 6940: 0.0063353776931762695
Doing Evaluation on the model now
This is Epoch 6940, training loss 0.00634, validation loss 0.00563
Epoch 13717: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6950, training loss 0.13776, validation loss 0.07048
Epoch 13728: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13739: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6960, training loss 0.02115, validation loss 0.00649
Epoch 13750: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13761: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6970, training loss 0.00697, validation loss 0.00824
Epoch 13772: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13783: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6978
Mean train loss for ascent epoch 6979: -0.0003950496902689338
Mean eval for ascent epoch 6979: 0.006289550103247166
Doing Evaluation on the model now
This is Epoch 6980, training loss 0.37116, validation loss 1.16648
Epoch 13794: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13805: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6990, training loss 0.06981, validation loss 0.07072
Epoch 13816: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7000, training loss 0.01829, validation loss 0.01855
Epoch 13827: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 13838: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7010, training loss 0.00721, validation loss 0.00633
Epoch 13849: reducing learning rate of group 0 to 2.5000e-04.
Epoch 13860: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7020, training loss 0.00588, validation loss 0.00553
Epoch 13871: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7023
Mean train loss for ascent epoch 7024: -0.0004664938314817846
Mean eval for ascent epoch 7024: 0.0053143249824643135
Epoch 13882: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7030, training loss 0.19523, validation loss 0.09748
Epoch 13893: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13904: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7040, training loss 0.03300, validation loss 0.07425
Epoch 13915: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7050, training loss 0.02157, validation loss 0.00680
Epoch 13926: reducing learning rate of group 0 to 3.1250e-04.
Epoch 13937: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7060, training loss 0.00584, validation loss 0.00554
Epoch 13948: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7062
Mean train loss for ascent epoch 7063: -0.00040269907913170755
Mean eval for ascent epoch 7063: 0.0054586539044976234
Epoch 13959: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7070, training loss 0.19621, validation loss 0.15347
Epoch 13970: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13981: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7080, training loss 0.02740, validation loss 0.02774
Epoch 13992: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14003: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7090, training loss 0.00765, validation loss 0.00598
Epoch 14014: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7100, training loss 0.00579, validation loss 0.00540
Epoch 14025: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7101
Mean train loss for ascent epoch 7102: -0.00040689698653295636
Mean eval for ascent epoch 7102: 0.005316725466400385
Epoch 14036: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7110, training loss 0.18612, validation loss 0.18554
Epoch 14047: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14058: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7120, training loss 0.01327, validation loss 0.01504
Epoch 14069: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14080: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7130, training loss 0.00604, validation loss 0.00806
Epoch 14091: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14102: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7140
Doing Evaluation on the model now
This is Epoch 7140, training loss 0.00491, validation loss 0.00431
Mean train loss for ascent epoch 7141: -0.00030607302323915064
Mean eval for ascent epoch 7141: 0.0045707011595368385
Epoch 14113: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7150, training loss 0.23034, validation loss 0.42907
Epoch 14124: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14135: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7160, training loss 0.01511, validation loss 0.01254
Epoch 14146: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14157: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7170, training loss 0.00581, validation loss 0.00548
Epoch 14168: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14179: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7179
Mean train loss for ascent epoch 7180: -0.00037605277611874044
Mean eval for ascent epoch 7180: 0.00588589021936059
Doing Evaluation on the model now
This is Epoch 7180, training loss 0.00589, validation loss 0.00517
Epoch 14190: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14201: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7190, training loss 0.14543, validation loss 0.11771
Epoch 14212: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7200, training loss 0.01803, validation loss 0.03293
Resetting learning rate to 0.01000
Epoch 14223: reducing learning rate of group 0 to 5.0000e-04.
Epoch 14234: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7210, training loss 0.00629, validation loss 0.00554
Epoch 14245: reducing learning rate of group 0 to 1.2500e-04.
Epoch 14256: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7218
Mean train loss for ascent epoch 7219: -0.00045897773816250265
Mean eval for ascent epoch 7219: 0.005744203459471464
Doing Evaluation on the model now
This is Epoch 7220, training loss 0.61355, validation loss 2.23908
Epoch 14267: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14278: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7230, training loss 0.06136, validation loss 0.09784
Epoch 14289: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14300: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7240, training loss 0.01535, validation loss 0.02935
Epoch 14311: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7250, training loss 0.00639, validation loss 0.00614
Epoch 14322: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14333: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7257
Mean train loss for ascent epoch 7258: -0.0004026313254144043
Mean eval for ascent epoch 7258: 0.006055365316569805
Doing Evaluation on the model now
This is Epoch 7260, training loss 0.92627, validation loss 1.10648
Epoch 14344: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14355: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7270, training loss 0.06572, validation loss 0.03924
Epoch 14366: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14377: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7280, training loss 0.01383, validation loss 0.01463
Epoch 14388: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14399: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7290, training loss 0.00655, validation loss 0.00739
Epoch 14410: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7296
Mean train loss for ascent epoch 7297: -0.0002703438512980938
Mean eval for ascent epoch 7297: 0.004696722608059645
Doing Evaluation on the model now
This is Epoch 7300, training loss 0.92055, validation loss 2.72059
Epoch 14421: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14432: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7310, training loss 0.02806, validation loss 0.03869
Epoch 14443: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14454: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7320, training loss 0.00790, validation loss 0.00786
Epoch 14465: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14476: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7330, training loss 0.00500, validation loss 0.00480
Epoch 14487: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7335
Mean train loss for ascent epoch 7336: -0.00030968652572482824
Mean eval for ascent epoch 7336: 0.0048604547046124935
Doing Evaluation on the model now
This is Epoch 7340, training loss 0.97551, validation loss 0.11824
Epoch 14498: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14509: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7350, training loss 0.05692, validation loss 0.04136
Epoch 14520: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14531: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7360, training loss 0.01103, validation loss 0.00816
Epoch 14542: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14553: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7370, training loss 0.00506, validation loss 0.00697
Epoch 14564: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7374
Mean train loss for ascent epoch 7375: -0.0004752578679472208
Mean eval for ascent epoch 7375: 0.005147307179868221
Epoch 14575: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7380, training loss 0.20585, validation loss 0.16234
Epoch 14586: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7390, training loss 0.03723, validation loss 0.04365
Epoch 14597: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14608: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7400, training loss 0.01082, validation loss 0.00701
Resetting learning rate to 0.01000
Epoch 14619: reducing learning rate of group 0 to 5.0000e-04.
Epoch 14630: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7410, training loss 0.00565, validation loss 0.00515
Epoch 14641: reducing learning rate of group 0 to 1.2500e-04.
Epoch 14652: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7418
Mean train loss for ascent epoch 7419: -0.0003711494500748813
Mean eval for ascent epoch 7419: 0.005285077728331089
Doing Evaluation on the model now
This is Epoch 7420, training loss 0.16612, validation loss 0.13867
Epoch 14663: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14674: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7430, training loss 0.02757, validation loss 0.05724
Epoch 14685: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7440, training loss 0.01631, validation loss 0.03666
Epoch 14696: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14707: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7450, training loss 0.00594, validation loss 0.00559
Epoch 14718: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14729: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7457
Mean train loss for ascent epoch 7458: -0.0004353119875304401
Mean eval for ascent epoch 7458: 0.005758239887654781
Doing Evaluation on the model now
This is Epoch 7460, training loss 0.58669, validation loss 1.69357
Epoch 14740: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14751: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7470, training loss 0.05696, validation loss 0.04463
Epoch 14762: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14773: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7480, training loss 0.03335, validation loss 0.01729
Epoch 14784: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7490, training loss 0.00714, validation loss 0.00813
Epoch 14795: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14806: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7496
Mean train loss for ascent epoch 7497: -0.0006790943443775177
Mean eval for ascent epoch 7497: 0.006353919859975576
Doing Evaluation on the model now
This is Epoch 7500, training loss 0.32087, validation loss 0.62521
Epoch 14817: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14828: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7510, training loss 0.02767, validation loss 0.03819
Epoch 14839: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14850: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7520, training loss 0.00851, validation loss 0.00611
Epoch 14861: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14872: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7530, training loss 0.00546, validation loss 0.00652
Epoch 14883: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7535
Mean train loss for ascent epoch 7536: -0.000350581860402599
Mean eval for ascent epoch 7536: 0.004977192729711533
Doing Evaluation on the model now
This is Epoch 7540, training loss 0.86476, validation loss 0.84527
Epoch 14894: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14905: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7550, training loss 0.06359, validation loss 0.10091
Epoch 14916: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14927: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7560, training loss 0.00741, validation loss 0.00718
Epoch 14938: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14949: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7570, training loss 0.00560, validation loss 0.00614
Epoch 14960: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7574
Mean train loss for ascent epoch 7575: -0.0004849115794058889
Mean eval for ascent epoch 7575: 0.005611644592136145
Epoch 14971: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7580, training loss 0.91044, validation loss 1.64687
Epoch 14982: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7590, training loss 0.05484, validation loss 0.03555
Epoch 14993: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15004: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7600, training loss 0.00978, validation loss 0.00604
Resetting learning rate to 0.01000
Epoch 15015: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15026: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7610, training loss 0.00659, validation loss 0.00904
Epoch 15037: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15048: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7619
Mean train loss for ascent epoch 7620: -0.00042729818960651755
Mean eval for ascent epoch 7620: 0.005572805181145668
Doing Evaluation on the model now
This is Epoch 7620, training loss 0.00557, validation loss 0.00539
Epoch 15059: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15070: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7630, training loss 0.43045, validation loss 0.05496
Epoch 15081: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7640, training loss 0.01341, validation loss 0.01686
Epoch 15092: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15103: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7650, training loss 0.00855, validation loss 0.00843
Epoch 15114: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15125: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7658
Mean train loss for ascent epoch 7659: -0.0004848464741371572
Mean eval for ascent epoch 7659: 0.006202090997248888
Doing Evaluation on the model now
This is Epoch 7660, training loss 0.49412, validation loss 1.10606
Epoch 15136: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15147: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7670, training loss 0.04094, validation loss 0.05057
Epoch 15158: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15169: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7680, training loss 0.01535, validation loss 0.01379
Epoch 15180: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7690, training loss 0.00766, validation loss 0.00696
Epoch 15191: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15202: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7697
Mean train loss for ascent epoch 7698: -0.0004050997958984226
Mean eval for ascent epoch 7698: 0.005764639470726252
Doing Evaluation on the model now
This is Epoch 7700, training loss 0.25286, validation loss 0.28080
Epoch 15213: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15224: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7710, training loss 0.03306, validation loss 0.05703
Epoch 15235: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15246: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7720, training loss 0.01323, validation loss 0.00815
Epoch 15257: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15268: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7730, training loss 0.00708, validation loss 0.00629
Epoch 15279: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7736
Mean train loss for ascent epoch 7737: -0.00032155594089999795
Mean eval for ascent epoch 7737: 0.005646605044603348
Doing Evaluation on the model now
This is Epoch 7740, training loss 0.67743, validation loss 0.58211
Epoch 15290: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15301: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7750, training loss 0.05858, validation loss 0.03673
Epoch 15312: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15323: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7760, training loss 0.01183, validation loss 0.01067
Epoch 15334: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15345: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7770, training loss 0.00611, validation loss 0.00983
Epoch 15356: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7775
Mean train loss for ascent epoch 7776: -0.0005983270821161568
Mean eval for ascent epoch 7776: 0.005012201610952616
Doing Evaluation on the model now
This is Epoch 7780, training loss 1.75035, validation loss 1.13510
Epoch 15367: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15378: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7790, training loss 0.05437, validation loss 0.04608
Epoch 15389: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15400: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7800, training loss 0.00932, validation loss 0.00784
Resetting learning rate to 0.01000
Epoch 15411: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15422: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7810, training loss 0.00651, validation loss 0.00675
Epoch 15433: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15444: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7819
Mean train loss for ascent epoch 7820: -0.0003673013998195529
Mean eval for ascent epoch 7820: 0.006206596735864878
Doing Evaluation on the model now
This is Epoch 7820, training loss 0.00621, validation loss 0.00537
Epoch 15455: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7830, training loss 0.12327, validation loss 0.05035
Epoch 15466: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15477: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7840, training loss 0.01852, validation loss 0.01640
Epoch 15488: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15499: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7850, training loss 0.00680, validation loss 0.00570
Epoch 15510: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15521: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7858
Mean train loss for ascent epoch 7859: -0.0004611904441844672
Mean eval for ascent epoch 7859: 0.008370555937290192
Doing Evaluation on the model now
This is Epoch 7860, training loss 0.15869, validation loss 0.13412
Epoch 15532: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15543: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7870, training loss 0.03613, validation loss 0.02424
Epoch 15554: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7880, training loss 0.02517, validation loss 0.02077
Epoch 15565: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15576: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7890, training loss 0.00642, validation loss 0.00861
Epoch 15587: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15598: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7897
Mean train loss for ascent epoch 7898: -0.00040366104803979397
Mean eval for ascent epoch 7898: 0.0069558206014335155
Doing Evaluation on the model now
This is Epoch 7900, training loss 0.57748, validation loss 0.32364
Epoch 15609: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15620: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7910, training loss 0.05395, validation loss 0.02200
Epoch 15631: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15642: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7920, training loss 0.01885, validation loss 0.00878
Epoch 15653: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7930, training loss 0.00567, validation loss 0.00765
Epoch 15664: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15675: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7936
Mean train loss for ascent epoch 7937: -0.00040921091567724943
Mean eval for ascent epoch 7937: 0.006787165999412537
Doing Evaluation on the model now
This is Epoch 7940, training loss 1.88618, validation loss 9.31147
Epoch 15686: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15697: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7950, training loss 0.15769, validation loss 0.34624
Epoch 15708: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15719: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7960, training loss 0.00785, validation loss 0.01399
Epoch 15730: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15741: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7970, training loss 0.00612, validation loss 0.00583
Epoch 15752: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7975
Mean train loss for ascent epoch 7976: -0.0005231728428043425
Mean eval for ascent epoch 7976: 0.006201696116477251
Doing Evaluation on the model now
This is Epoch 7980, training loss 0.57236, validation loss 0.77083
Epoch 15763: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15774: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7990, training loss 0.03016, validation loss 0.02267
Epoch 15785: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15796: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8000, training loss 0.00710, validation loss 0.00870
Resetting learning rate to 0.01000
Epoch 15807: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15818: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8010, training loss 0.00594, validation loss 0.00674
Epoch 15829: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15840: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8020
Doing Evaluation on the model now
This is Epoch 8020, training loss 0.00671, validation loss 0.00488
Mean train loss for ascent epoch 8021: -0.0006113481940701604
Mean eval for ascent epoch 8021: 0.005938474088907242
Epoch 15851: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8030, training loss 0.33492, validation loss 0.25373
Epoch 15862: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15873: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8040, training loss 0.02524, validation loss 0.03087
Epoch 15884: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15895: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8050, training loss 0.00755, validation loss 0.00639
Epoch 15906: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15917: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8059
Mean train loss for ascent epoch 8060: -0.00040504318894818425
Mean eval for ascent epoch 8060: 0.005566060543060303
Doing Evaluation on the model now
This is Epoch 8060, training loss 0.00557, validation loss 0.00524
Epoch 15928: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15939: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8070, training loss 0.11333, validation loss 0.07031
Epoch 15950: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8080, training loss 0.02482, validation loss 0.01562
Epoch 15961: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15972: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8090, training loss 0.00561, validation loss 0.00486
Epoch 15983: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15994: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8098
Mean train loss for ascent epoch 8099: -0.0004593614430632442
Mean eval for ascent epoch 8099: 0.0050306906923651695
Doing Evaluation on the model now
This is Epoch 8100, training loss 0.12946, validation loss 0.11605
Epoch 16005: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16016: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8110, training loss 0.05634, validation loss 0.07530
Epoch 16027: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16038: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8120, training loss 0.01329, validation loss 0.01953
Epoch 16049: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8130, training loss 0.00730, validation loss 0.00529
Epoch 16060: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16071: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8137
Mean train loss for ascent epoch 8138: -0.0005430102464742959
Mean eval for ascent epoch 8138: 0.006411990150809288
Doing Evaluation on the model now
This is Epoch 8140, training loss 1.05400, validation loss 0.92879
Epoch 16082: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16093: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8150, training loss 0.04005, validation loss 0.03458
Epoch 16104: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16115: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8160, training loss 0.01966, validation loss 0.01202
Epoch 16126: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16137: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8170, training loss 0.00550, validation loss 0.00488
Epoch 16148: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8176
Mean train loss for ascent epoch 8177: -0.0005262570339255035
Mean eval for ascent epoch 8177: 0.005028000567108393
Doing Evaluation on the model now
This is Epoch 8180, training loss 0.64528, validation loss 0.76682
Epoch 16159: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16170: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8190, training loss 0.06166, validation loss 0.02390
Epoch 16181: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16192: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8200, training loss 0.00719, validation loss 0.00979
Resetting learning rate to 0.01000
Epoch 16203: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16214: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8210, training loss 0.01414, validation loss 0.00681
Epoch 16225: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16236: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8220
Doing Evaluation on the model now
This is Epoch 8220, training loss 0.00458, validation loss 0.00480
Mean train loss for ascent epoch 8221: -0.00027580675669014454
Mean eval for ascent epoch 8221: 0.004861961118876934
Epoch 16247: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8230, training loss 0.14498, validation loss 0.19561
Epoch 16258: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16269: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8240, training loss 0.02229, validation loss 0.01378
Epoch 16280: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16291: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8250, training loss 0.00556, validation loss 0.00743
Epoch 16302: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16313: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8259
Mean train loss for ascent epoch 8260: -0.00046434017713181674
Mean eval for ascent epoch 8260: 0.005065751262009144
Doing Evaluation on the model now
This is Epoch 8260, training loss 0.00507, validation loss 0.00537
Epoch 16324: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8270, training loss 0.23108, validation loss 0.31071
Epoch 16335: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16346: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8280, training loss 0.02169, validation loss 0.05291
Epoch 16357: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16368: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8290, training loss 0.00770, validation loss 0.01002
Epoch 16379: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16390: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8298
Mean train loss for ascent epoch 8299: -0.0003668033459689468
Mean eval for ascent epoch 8299: 0.005667224992066622
Doing Evaluation on the model now
This is Epoch 8300, training loss 0.60614, validation loss 2.03431
Epoch 16401: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16412: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8310, training loss 0.12198, validation loss 0.09115
Epoch 16423: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8320, training loss 0.02359, validation loss 0.01477
Epoch 16434: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16445: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8330, training loss 0.00772, validation loss 0.00689
Epoch 16456: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16467: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8337
Mean train loss for ascent epoch 8338: -0.000277871877187863
Mean eval for ascent epoch 8338: 0.00515271071344614
Doing Evaluation on the model now
This is Epoch 8340, training loss 0.51357, validation loss 0.63113
Epoch 16478: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16489: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8350, training loss 0.06295, validation loss 0.06621
Epoch 16500: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16511: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8360, training loss 0.00999, validation loss 0.01321
Epoch 16522: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8370, training loss 0.00551, validation loss 0.00577
Epoch 16533: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16544: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8376
Mean train loss for ascent epoch 8377: -0.00040895745041780174
Mean eval for ascent epoch 8377: 0.005601038690656424
Doing Evaluation on the model now
This is Epoch 8380, training loss 4.39353, validation loss 0.94701
Epoch 16555: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16566: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8390, training loss 0.04072, validation loss 0.01958
Epoch 16577: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16588: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8400, training loss 0.01536, validation loss 0.02630
Resetting learning rate to 0.01000
Epoch 16599: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16610: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8410, training loss 0.01089, validation loss 0.00704
Epoch 16621: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8420, training loss 0.00519, validation loss 0.00503
Epoch 16632: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8421
Mean train loss for ascent epoch 8422: -0.0004796514112967998
Mean eval for ascent epoch 8422: 0.007932530716061592
Epoch 16643: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8430, training loss 0.12473, validation loss 0.05195
Epoch 16654: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16665: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8440, training loss 0.02331, validation loss 0.02434
Epoch 16676: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16687: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8450, training loss 0.00624, validation loss 0.00587
Epoch 16698: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16709: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8460
Doing Evaluation on the model now
This is Epoch 8460, training loss 0.00545, validation loss 0.00804
Mean train loss for ascent epoch 8461: -0.0004663389117922634
Mean eval for ascent epoch 8461: 0.004853254184126854
Epoch 16720: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8470, training loss 0.04045, validation loss 0.09850
Epoch 16731: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16742: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8480, training loss 0.01766, validation loss 0.03427
Epoch 16753: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16764: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8490, training loss 0.00514, validation loss 0.00503
Epoch 16775: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16786: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8499
Mean train loss for ascent epoch 8500: -0.0004400917678140104
Mean eval for ascent epoch 8500: 0.0048573799431324005
Doing Evaluation on the model now
This is Epoch 8500, training loss 0.00486, validation loss 0.00540
Epoch 16797: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16808: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8510, training loss 0.09487, validation loss 0.07686
Epoch 16819: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8520, training loss 0.01723, validation loss 0.03191
Epoch 16830: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16841: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8530, training loss 0.00684, validation loss 0.00958
Epoch 16852: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16863: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8538
Mean train loss for ascent epoch 8539: -0.00043774631922133267
Mean eval for ascent epoch 8539: 0.00495928805321455
Doing Evaluation on the model now
This is Epoch 8540, training loss 0.12440, validation loss 0.03870
Epoch 16874: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16885: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8550, training loss 0.04008, validation loss 0.01213
Epoch 16896: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16907: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8560, training loss 0.02059, validation loss 0.04528
Epoch 16918: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8570, training loss 0.00671, validation loss 0.00539
Epoch 16929: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16940: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8577
Mean train loss for ascent epoch 8578: -0.0005220675957389176
Mean eval for ascent epoch 8578: 0.0052342950366437435
Doing Evaluation on the model now
This is Epoch 8580, training loss 4.62969, validation loss 9.74363
Epoch 16951: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16962: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8590, training loss 0.40680, validation loss 0.20594
Epoch 16973: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16984: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8600, training loss 0.00695, validation loss 0.00800
Resetting learning rate to 0.01000
Epoch 16995: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17006: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8610, training loss 0.00610, validation loss 0.00868
Epoch 17017: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8620, training loss 0.00513, validation loss 0.00591
Epoch 17028: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8621
Mean train loss for ascent epoch 8622: -0.00025681807892397046
Mean eval for ascent epoch 8622: 0.004992567468434572
Epoch 17039: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8630, training loss 0.17398, validation loss 0.07858
Epoch 17050: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17061: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8640, training loss 0.01875, validation loss 0.01771
Epoch 17072: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17083: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8650, training loss 0.00605, validation loss 0.00602
Epoch 17094: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17105: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8660
Doing Evaluation on the model now
This is Epoch 8660, training loss 0.00463, validation loss 0.00437
Mean train loss for ascent epoch 8661: -0.0004535835178103298
Mean eval for ascent epoch 8661: 0.004451492335647345
Epoch 17116: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8670, training loss 0.15227, validation loss 0.09683
Epoch 17127: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17138: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8680, training loss 0.01703, validation loss 0.02128
Epoch 17149: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17160: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8690, training loss 0.00539, validation loss 0.00587
Epoch 17171: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17182: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8699
Mean train loss for ascent epoch 8700: -0.00043388092308305204
Mean eval for ascent epoch 8700: 0.004667051136493683
Doing Evaluation on the model now
This is Epoch 8700, training loss 0.00467, validation loss 0.00516
Epoch 17193: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8710, training loss 0.09567, validation loss 0.05279
Epoch 17204: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17215: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8720, training loss 0.01612, validation loss 0.01879
Epoch 17226: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17237: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8730, training loss 0.00613, validation loss 0.00479
Epoch 17248: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17259: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8738
Mean train loss for ascent epoch 8739: -0.0004081282822880894
Mean eval for ascent epoch 8739: 0.004871434066444635
Doing Evaluation on the model now
This is Epoch 8740, training loss 0.10413, validation loss 0.15114
Epoch 17270: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17281: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8750, training loss 0.03184, validation loss 0.02980
Epoch 17292: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8760, training loss 0.02815, validation loss 0.01429
Epoch 17303: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17314: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8770, training loss 0.00656, validation loss 0.00814
Epoch 17325: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17336: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8777
Mean train loss for ascent epoch 8778: -0.0003366936871316284
Mean eval for ascent epoch 8778: 0.004745339974761009
Doing Evaluation on the model now
This is Epoch 8780, training loss 0.37426, validation loss 1.66933
Epoch 17347: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17358: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8790, training loss 0.04915, validation loss 0.02482
Epoch 17369: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17380: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8800, training loss 0.01086, validation loss 0.00758
Resetting learning rate to 0.01000
Epoch 17391: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 8810, training loss 0.00648, validation loss 0.00780
Epoch 17402: reducing learning rate of group 0 to 2.5000e-04.
Epoch 17413: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8820, training loss 0.00477, validation loss 0.00413
Epoch 17424: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8822
Mean train loss for ascent epoch 8823: -0.0004653598298318684
Mean eval for ascent epoch 8823: 0.0046293106861412525
Epoch 17435: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8830, training loss 0.69466, validation loss 0.12712
Epoch 17446: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17457: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8840, training loss 0.02142, validation loss 0.02250
Epoch 17468: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17479: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8850, training loss 0.00725, validation loss 0.00511
Epoch 17490: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8860, training loss 0.00451, validation loss 0.00534
Epoch 17501: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8861
Mean train loss for ascent epoch 8862: -0.0005582631565630436
Mean eval for ascent epoch 8862: 0.004894650541245937
Epoch 17512: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8870, training loss 0.07389, validation loss 0.02596
Epoch 17523: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17534: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8880, training loss 0.02505, validation loss 0.00832
Epoch 17545: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17556: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8890, training loss 0.00682, validation loss 0.00649
Epoch 17567: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17578: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8900
Doing Evaluation on the model now
This is Epoch 8900, training loss 0.00525, validation loss 0.00612
Mean train loss for ascent epoch 8901: -0.00045500494888983667
Mean eval for ascent epoch 8901: 0.005460116546601057
Epoch 17589: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8910, training loss 0.29221, validation loss 0.27100
Epoch 17600: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17611: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8920, training loss 0.02540, validation loss 0.02498
Epoch 17622: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17633: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8930, training loss 0.00671, validation loss 0.00534
Epoch 17644: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17655: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8939
Mean train loss for ascent epoch 8940: -0.0003351207997184247
Mean eval for ascent epoch 8940: 0.004666510969400406
Doing Evaluation on the model now
This is Epoch 8940, training loss 0.00467, validation loss 0.00421
Epoch 17666: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17677: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8950, training loss 0.17723, validation loss 0.18474
Epoch 17688: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8960, training loss 0.04300, validation loss 0.04835
Epoch 17699: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17710: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8970, training loss 0.00744, validation loss 0.00613
Epoch 17721: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17732: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8978
Mean train loss for ascent epoch 8979: -0.0002640407474245876
Mean eval for ascent epoch 8979: 0.005277534946799278
Doing Evaluation on the model now
This is Epoch 8980, training loss 0.51534, validation loss 0.89160
Epoch 17743: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17754: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8990, training loss 0.05702, validation loss 0.14368
Epoch 17765: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17776: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9000, training loss 0.01308, validation loss 0.01238
Resetting learning rate to 0.01000
Epoch 17787: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9010, training loss 0.00692, validation loss 0.00785
Epoch 17798: reducing learning rate of group 0 to 2.5000e-04.
Epoch 17809: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9020, training loss 0.00550, validation loss 0.00498
Epoch 17820: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9022
Mean train loss for ascent epoch 9023: -0.0004218227113597095
Mean eval for ascent epoch 9023: 0.00555077288299799
Epoch 17831: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9030, training loss 0.32448, validation loss 0.09139
Epoch 17842: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17853: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9040, training loss 0.02180, validation loss 0.01602
Epoch 17864: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17875: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9050, training loss 0.00825, validation loss 0.00807
Epoch 17886: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9060, training loss 0.00520, validation loss 0.00479
Epoch 17897: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9061
Mean train loss for ascent epoch 9062: -0.00032834114972501993
Mean eval for ascent epoch 9062: 0.006162167992442846
Epoch 17908: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9070, training loss 0.10603, validation loss 0.06540
Epoch 17919: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17930: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9080, training loss 0.03657, validation loss 0.01492
Epoch 17941: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17952: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9090, training loss 0.00995, validation loss 0.01017
Epoch 17963: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17974: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9100
Doing Evaluation on the model now
This is Epoch 9100, training loss 0.00570, validation loss 0.00574
Mean train loss for ascent epoch 9101: -0.00042731931898742914
Mean eval for ascent epoch 9101: 0.005990145727992058
Epoch 17985: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9110, training loss 0.20992, validation loss 0.10909
Epoch 17996: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18007: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9120, training loss 0.01418, validation loss 0.02326
Epoch 18018: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18029: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9130, training loss 0.00679, validation loss 0.00712
Epoch 18040: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18051: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9139
Mean train loss for ascent epoch 9140: -0.00044611605699174106
Mean eval for ascent epoch 9140: 0.005846913438290358
Doing Evaluation on the model now
This is Epoch 9140, training loss 0.00585, validation loss 0.00512
Epoch 18062: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9150, training loss 0.09992, validation loss 0.13256
Epoch 18073: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18084: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9160, training loss 0.01683, validation loss 0.02060
Epoch 18095: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18106: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9170, training loss 0.00504, validation loss 0.00460
Epoch 18117: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18128: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9178
Mean train loss for ascent epoch 9179: -0.00034464732743799686
Mean eval for ascent epoch 9179: 0.005455668084323406
Doing Evaluation on the model now
This is Epoch 9180, training loss 0.42716, validation loss 1.17545
Epoch 18139: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18150: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9190, training loss 0.04915, validation loss 0.03047
Epoch 18161: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9200, training loss 0.01958, validation loss 0.02196
Epoch 18172: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 18183: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9210, training loss 0.00769, validation loss 0.00550
Epoch 18194: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18205: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9220, training loss 0.00479, validation loss 0.00524
Epoch 18216: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9223
Mean train loss for ascent epoch 9224: -0.00036557772546075284
Mean eval for ascent epoch 9224: 0.005260773468762636
Epoch 18227: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9230, training loss 0.51309, validation loss 0.45714
Epoch 18238: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18249: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9240, training loss 0.02302, validation loss 0.02367
Epoch 18260: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9250, training loss 0.01052, validation loss 0.01489
Epoch 18271: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18282: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9260, training loss 0.00525, validation loss 0.00540
Epoch 18293: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9262
Mean train loss for ascent epoch 9263: -0.0004744259640574455
Mean eval for ascent epoch 9263: 0.004746816121041775
Epoch 18304: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9270, training loss 0.13647, validation loss 0.17671
Epoch 18315: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18326: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9280, training loss 0.02300, validation loss 0.01677
Epoch 18337: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18348: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9290, training loss 0.00690, validation loss 0.00743
Epoch 18359: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9300, training loss 0.00606, validation loss 0.00571
Epoch 18370: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9301
Mean train loss for ascent epoch 9302: -0.0004100632213521749
Mean eval for ascent epoch 9302: 0.005741187836974859
Epoch 18381: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9310, training loss 0.17020, validation loss 0.14505
Epoch 18392: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18403: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9320, training loss 0.01698, validation loss 0.02184
Epoch 18414: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18425: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9330, training loss 0.00553, validation loss 0.00576
Epoch 18436: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18447: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9340
Doing Evaluation on the model now
This is Epoch 9340, training loss 0.00535, validation loss 0.00492
Mean train loss for ascent epoch 9341: -0.00041642217547632754
Mean eval for ascent epoch 9341: 0.005521278828382492
Epoch 18458: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9350, training loss 0.12445, validation loss 0.27114
Epoch 18469: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18480: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9360, training loss 0.01559, validation loss 0.02846
Epoch 18491: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18502: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9370, training loss 0.00722, validation loss 0.01046
Epoch 18513: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18524: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9379
Mean train loss for ascent epoch 9380: -0.00048790019354783
Mean eval for ascent epoch 9380: 0.004606696777045727
Doing Evaluation on the model now
This is Epoch 9380, training loss 0.00461, validation loss 0.00420
Epoch 18535: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18546: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9390, training loss 0.16671, validation loss 0.16353
Epoch 18557: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9400, training loss 0.01718, validation loss 0.02506
Resetting learning rate to 0.01000
Epoch 18568: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18579: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9410, training loss 0.00530, validation loss 0.00502
Epoch 18590: reducing learning rate of group 0 to 1.2500e-04.
Epoch 18601: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9418
Mean train loss for ascent epoch 9419: -0.0004367297515273094
Mean eval for ascent epoch 9419: 0.005346816498786211
Doing Evaluation on the model now
This is Epoch 9420, training loss 0.45553, validation loss 0.22176
Epoch 18612: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18623: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9430, training loss 0.07399, validation loss 0.03663
Epoch 18634: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18645: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9440, training loss 0.01711, validation loss 0.02267
Epoch 18656: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9450, training loss 0.00661, validation loss 0.00736
Epoch 18667: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18678: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9457
Mean train loss for ascent epoch 9458: -0.00037445302587002516
Mean eval for ascent epoch 9458: 0.006314035505056381
Doing Evaluation on the model now
This is Epoch 9460, training loss 6.74517, validation loss 3.67295
Epoch 18689: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18700: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9470, training loss 0.16877, validation loss 0.11724
Epoch 18711: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18722: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9480, training loss 0.01930, validation loss 0.01671
Epoch 18733: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18744: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9490, training loss 0.00671, validation loss 0.00677
Epoch 18755: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9496
Mean train loss for ascent epoch 9497: -0.000202534458367154
Mean eval for ascent epoch 9497: 0.004705449100583792
Doing Evaluation on the model now
This is Epoch 9500, training loss 1.04158, validation loss 0.16191
Epoch 18766: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18777: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9510, training loss 0.08891, validation loss 0.12181
Epoch 18788: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18799: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9520, training loss 0.01148, validation loss 0.00676
Epoch 18810: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18821: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9530, training loss 0.00518, validation loss 0.00467
Epoch 18832: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9535
Mean train loss for ascent epoch 9536: -0.0002900609979405999
Mean eval for ascent epoch 9536: 0.005177271086722612
Doing Evaluation on the model now
This is Epoch 9540, training loss 3.36455, validation loss 0.67535
Epoch 18843: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18854: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9550, training loss 0.05305, validation loss 0.03453
Epoch 18865: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18876: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9560, training loss 0.00834, validation loss 0.00743
Epoch 18887: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18898: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9570, training loss 0.00603, validation loss 0.00623
Epoch 18909: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9574
Mean train loss for ascent epoch 9575: -0.00043768793693743646
Mean eval for ascent epoch 9575: 0.006236574612557888
Epoch 18920: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9580, training loss 0.36802, validation loss 0.06100
Epoch 18931: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9590, training loss 0.32938, validation loss 0.32508
Epoch 18942: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18953: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9600, training loss 0.00901, validation loss 0.00797
Resetting learning rate to 0.01000
Epoch 18964: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18975: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9610, training loss 0.00615, validation loss 0.00530
Epoch 18986: reducing learning rate of group 0 to 1.2500e-04.
Epoch 18997: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9618
Mean train loss for ascent epoch 9619: -0.0003513959818519652
Mean eval for ascent epoch 9619: 0.004441081080585718
Doing Evaluation on the model now
This is Epoch 9620, training loss 0.15081, validation loss 0.17331
Epoch 19008: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19019: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9630, training loss 0.28562, validation loss 0.05794
Epoch 19030: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9640, training loss 0.02089, validation loss 0.03049
Epoch 19041: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19052: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9650, training loss 0.00631, validation loss 0.00649
Epoch 19063: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19074: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9657
Mean train loss for ascent epoch 9658: -0.0002704524958971888
Mean eval for ascent epoch 9658: 0.006267881952226162
Doing Evaluation on the model now
This is Epoch 9660, training loss 5.07871, validation loss 9.36359
Epoch 19085: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19096: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9670, training loss 0.06468, validation loss 0.02485
Epoch 19107: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19118: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9680, training loss 0.01013, validation loss 0.01219
Epoch 19129: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9690, training loss 0.00543, validation loss 0.00624
Epoch 19140: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19151: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9696
Mean train loss for ascent epoch 9697: -0.00040812857332639396
Mean eval for ascent epoch 9697: 0.005800919141620398
Doing Evaluation on the model now
This is Epoch 9700, training loss 1.07472, validation loss 0.25391
Epoch 19162: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19173: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9710, training loss 0.05204, validation loss 0.05593
Epoch 19184: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19195: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9720, training loss 0.01010, validation loss 0.01169
Epoch 19206: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19217: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9730, training loss 0.00648, validation loss 0.00651
Epoch 19228: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9735
Mean train loss for ascent epoch 9736: -0.0005693039856851101
Mean eval for ascent epoch 9736: 0.006155938375741243
Doing Evaluation on the model now
This is Epoch 9740, training loss 1.83739, validation loss 0.52623
Epoch 19239: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19250: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9750, training loss 0.03925, validation loss 0.01488
Epoch 19261: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19272: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9760, training loss 0.00916, validation loss 0.00974
Epoch 19283: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19294: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9770, training loss 0.00547, validation loss 0.00469
Epoch 19305: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9774
Mean train loss for ascent epoch 9775: -0.0004454704176168889
Mean eval for ascent epoch 9775: 0.005497022531926632
Epoch 19316: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9780, training loss 0.59783, validation loss 0.26137
Epoch 19327: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9790, training loss 0.10014, validation loss 0.08669
Epoch 19338: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19349: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9800, training loss 0.02734, validation loss 0.03427
Resetting learning rate to 0.01000
Epoch 19360: reducing learning rate of group 0 to 5.0000e-04.
Epoch 19371: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9810, training loss 0.00641, validation loss 0.00725
Epoch 19382: reducing learning rate of group 0 to 1.2500e-04.
Epoch 19393: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9819
Mean train loss for ascent epoch 9820: -0.0004886295064352453
Mean eval for ascent epoch 9820: 0.006388404872268438
Doing Evaluation on the model now
This is Epoch 9820, training loss 0.00639, validation loss 0.00478
Epoch 19404: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19415: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9830, training loss 0.10860, validation loss 0.14766
Epoch 19426: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9840, training loss 0.02348, validation loss 0.02352
Epoch 19437: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19448: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9850, training loss 0.00672, validation loss 0.00497
Epoch 19459: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19470: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9858
Mean train loss for ascent epoch 9859: -0.000515791354700923
Mean eval for ascent epoch 9859: 0.005012928508222103
Doing Evaluation on the model now
This is Epoch 9860, training loss 0.18751, validation loss 0.18932
Epoch 19481: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19492: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9870, training loss 0.06486, validation loss 0.05460
Epoch 19503: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19514: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9880, training loss 0.02077, validation loss 0.01617
Epoch 19525: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9890, training loss 0.00857, validation loss 0.00588
Epoch 19536: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19547: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9897
Mean train loss for ascent epoch 9898: -0.00044975243508815765
Mean eval for ascent epoch 9898: 0.006463703699409962
Doing Evaluation on the model now
This is Epoch 9900, training loss 1.98339, validation loss 5.59915
Epoch 19558: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19569: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9910, training loss 0.03625, validation loss 0.04305
Epoch 19580: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19591: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9920, training loss 0.01352, validation loss 0.00563
Epoch 19602: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19613: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9930, training loss 0.00571, validation loss 0.00598
Epoch 19624: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9936
Mean train loss for ascent epoch 9937: -0.00031837730784900486
Mean eval for ascent epoch 9937: 0.0043089441023766994
Doing Evaluation on the model now
This is Epoch 9940, training loss 0.20087, validation loss 0.06308
Epoch 19635: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19646: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9950, training loss 0.05312, validation loss 0.02497
Epoch 19657: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19668: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9960, training loss 0.00873, validation loss 0.00975
Epoch 19679: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19690: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9970, training loss 0.00639, validation loss 0.00498
Epoch 19701: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9975
Mean train loss for ascent epoch 9976: -0.00046483305050060153
Mean eval for ascent epoch 9976: 0.004855841863900423
Doing Evaluation on the model now
This is Epoch 9980, training loss 1.98625, validation loss 0.45699
Epoch 19712: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19723: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9990, training loss 0.05806, validation loss 0.10092
Epoch 19734: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19745: reducing learning rate of group 0 to 6.2500e-04.
POS: 
[8.963531, 8.828977, 8.830719, 8.830645, 8.829804, 8.838432, 8.833758, 8.831798, 8.835029, 8.831488, 8.835384, 8.829628, 8.835785, 8.831982, 8.825518, 8.833126, 8.827058, 8.8348875, 8.830801, 8.830674, 8.835881, 8.835258, 8.838337, 8.83833, 8.8352585, 8.827714, 8.833539, 8.840175, 8.834905, 8.825109, 8.833126, 8.83564, 8.839886, 8.834343, 8.834874, 8.830825, 8.830091, 8.82928, 8.830965, 8.829768, 8.8337755, 8.824079, 8.834597, 8.836652, 8.826138, 8.841216, 8.828276, 8.835269, 8.832741, 8.82766, 8.826922, 8.828555, 8.832687, 8.823865, 8.830384, 8.834928, 8.826525, 8.833341, 8.8308735, 8.831657, 8.829575, 8.83181, 8.835486, 8.834094, 8.830661, 8.837138, 8.835518, 8.825548, 8.824352, 8.837004, 8.830496, 8.828289, 8.831908, 8.8322735, 8.8281, 8.835579, 8.833992, 8.828159, 8.822278, 8.816858, 8.758068, 8.668537, 8.502739, 8.343959, 8.202779, 8.103902, 8.0047245, 7.9516263, 7.924531, 7.8892765, 7.868345, 7.8503275, 7.8238053, 7.8154917, 7.8075047, 7.79358, 7.794071, 7.795583, 7.782343, 7.7806487, 7.7721543, 7.774704, 7.770359, 7.7567596, 7.772084, 7.6926436, 7.6287384, 7.5954885, 7.597567, 7.5837493, 7.584267, 7.5775414, 7.569054, 7.572444, 7.574866, 7.5728173, 7.576975, 7.5671506, 7.566046, 7.561488, 7.5686097, 7.5660133, 7.568145, 7.5666337, 7.568788, 7.5714555, 7.5653934, 7.5647244, 7.5679564, 7.5679, 7.5623665, 7.5700364, 7.565885, 7.561387, 7.5654025, 7.567318, 7.5613265, 7.5620584, 7.569768, 7.565335, 7.567438, 7.566443, 7.5669923, 7.568127, 7.5662346, 7.5736027, 7.5692306, 7.563863, 7.558397, 7.5556374, 7.557418, 7.5643806, 7.561749, 7.556446, 7.555968, 7.5619893, 7.555259, 7.559744, 7.558858, 7.5594916, 7.5560985, 7.558034, 7.5550804, 7.556047, 7.550571, 7.5578804, 7.5589094, 7.5522118, 7.5592027, 7.552267, 7.5580516, 7.553651, 7.5575924, 7.558075, 7.5540996, 7.5528297, 7.555556, 7.561264, 7.5603576, 7.5584116, 7.5631294, 7.5634727, 7.568531, 7.56571, 7.5642557, 7.563843, 7.5637336, 7.5564833, 7.5510173, 7.55699, 7.5526114, 7.555172, 7.5569625, 7.55506, 7.5479913, 7.550929, 7.5596843, 7.5501003, 7.5572157, 7.559636, 7.558932, 7.5517364, 7.5589375, 7.5567665, 7.554783, 7.559001, 7.554143, 7.5490546, 7.553914, 7.5527005, 7.559213, 7.5527062, 7.5572877, 7.550528, 7.556884, 7.5687394, 7.549496, 7.549543, 7.556446, 7.555628, 7.5509224, 7.560149, 7.561722, 7.556577, 7.5598674, 7.556278, 7.5537324, 7.5559, 7.558703, 7.558975, 7.554196, 7.551961, 7.5571885, 7.555367, 7.5536356, 7.556839, 7.551994, 7.5524516, 7.5505548, 7.5531936, 7.5449805, 7.556469, 7.549794, 7.5501084, 7.544633, 7.5527835, 7.5497446, 7.5547566, 7.5590796, 7.5481505, 7.5483255, 7.5530524, 7.550966, 7.5495934, 7.555062, 7.555365, 7.5577664, 7.5629497, 7.5575314, 7.5592556, 7.5572696, 7.5541806, 7.55578, 7.5525656, 7.5530267, 7.5608206, 7.562723, 7.552704, 7.551868, 7.5451307, 7.5523305, 7.55949, 7.5530405, 7.5526066, 7.5504293, 7.5596232, 7.5489607, 7.553613, 7.551501, 7.5470405, 7.551166, 7.560946, 7.5548143, 7.5520773, 7.554445, 7.5518413, 7.554442, 7.5484595, 7.54849, 7.5562825, 7.5471745, 7.5563445, 7.5514865, 7.5574646, 7.5655246, 7.560513, 7.563947, 7.559633, 7.5591373, 7.5573792, 7.5564156, 7.551889, 7.5520873, 7.553637, 7.554048, 7.5486693, 7.559115, 7.557677, 7.5520973, 7.555378, 7.555435, 7.557562, 7.553351, 7.5566325, 7.554785, 7.548557, 7.5513444, 7.545485, 7.5572944, 7.549338, 7.548997, 7.5533786, 7.549991, 7.5490227, 7.5500345, 7.553745, 7.547037, 7.5472116, 7.5479608, 7.556021, 7.5517755, 7.5549035, 7.5692916, 7.55594, 7.5602336, 7.555403, 7.555253, 7.5604625, 7.55671, 7.5510736, 7.557367, 7.5516496, 7.5489974, 7.554793, 7.5503364, 7.548714, 7.544758, 7.557845, 7.555684, 7.556724, 7.5529127, 7.5503907, 7.552164, 7.5510488, 7.5484886, 7.5459948, 7.554245, 7.547862, 7.546139, 7.5542073, 7.555672, 7.5540123, 7.5498834, 7.5515456, 7.54754, 7.5515733, 7.545597, 7.5504713, 7.5575624, 7.5457854, 7.5602064, 7.5635037, 7.5624633, 7.5547953, 7.552932, 7.551451, 7.5544877, 7.5527534, 7.550259, 7.553115, 7.55316, 7.5592103, 7.550786, 7.556318, 7.5518174, 7.5465584, 7.5468655, 7.55226, 7.556272, 7.5498624, 7.5582933, 7.554188, 7.5521584, 7.555779, 7.550253, 7.5572133, 7.5519423, 7.54682, 7.5553975, 7.5512342, 7.5550056, 7.5503764, 7.5519166, 7.5575852, 7.5571337, 7.550638, 7.5461965, 7.5504375, 7.5510664, 7.5517344, 7.5565834, 7.554396, 7.5549765, 7.549163, 7.559108, 7.554462, 7.555386, 7.5488024, 7.555416, 7.5537095, 7.5588603, 7.5562134, 7.553605, 7.5528903, 7.552279, 7.554743, 7.552387, 7.554745, 7.5614285, 7.5516677, 7.548413, 7.542739, 7.5550294, 7.5493317, 7.5536385, 7.55261, 7.5520597, 7.5536838, 7.5531054, 7.5482483, 7.54776, 7.550448, 7.549102, 7.549893, 7.555216, 7.5535293, 7.5511217, 7.5464206, 7.5497966, 7.5476027, 7.550363, 7.5555763, 7.562392, 7.5655613, 7.5586114, 7.5631776, 7.5569577, 7.5538883, 7.549375, 7.5484266, 7.55673, 7.557974, 7.5558767, 7.5518174, 7.547713, 7.554861, 7.5512657, 7.5531816, 7.5498953, 7.551018, 7.558389, 7.554698, 7.554737, 7.5608916, 7.5534635, 7.5493913, 7.55059, 7.5441613, 7.5542107, 7.5421166, 7.550457, 7.5559726, 7.5569844, 7.552623, 7.5485125, 7.557442, 7.549704, 7.5543222, 7.550423, 7.551132, 7.562211, 7.5466967, 7.55807, 7.5496874, 7.5550537, 7.5543156, 7.5534797, 7.5530386, 7.5535088, 7.5522203, 7.5551395, 7.552561, 7.5516157, 7.5530314, 7.5509844, 7.5547957, 7.5555077, 7.5562134, 7.550279, 7.55657, 7.557793, 7.5503435, 7.5537014, 7.5489492, 7.5508437, 7.5444617, 7.5464582, 7.5500207, 7.55349, 7.5510592, 7.552483, 7.555558, 7.553306, 7.5509834, 7.5471406, 7.554016, 7.5489655, 7.551169, 7.5645695, 7.557695, 7.5527253, 7.556064, 7.5549936, 7.5496025, 7.55272, 7.555574, 7.555332, 7.558123, 7.556749, 7.5455403, 7.547421, 7.560393, 7.5495534, 7.5472856, 7.552347, 7.548475, 7.560215, 7.5438957, 7.549278, 7.546597, 7.5516977, 7.5523624, 7.5497413, 7.546645, 7.555225, 7.549717, 7.549595, 7.553071, 7.5451927, 7.550787, 7.554441, 7.553997, 7.551773, 7.5505395, 7.5444655, 7.5465593, 7.561526, 7.5574164, 7.5433125, 7.5628896, 7.552645, 7.5604377, 7.5572214, 7.553765, 7.555218, 7.5532284, 7.55076, 7.557382, 7.556143, 7.5527415, 7.555184, 7.5460625, 7.553685, 7.550382, 7.548402, 7.549111, 7.550986, 7.5498013, 7.5513396, 7.5550947, 7.5463843, 7.54659, 7.5502825, 7.549717, 7.551821, 7.5489902, 7.5499997, 7.55138, 7.5499606, 7.5499234, 7.548112, 7.558178, 7.550104, 7.555401, 7.5565114, 7.5572724, 7.5527954, 7.5611324, 7.5511355, 7.550096, 7.556083, 7.5546756, 7.558522, 7.5600634, 7.547632, 7.557471, 7.549591, 7.5579705, 7.5506277, 7.5454326, 7.5492086, 7.5553956, 7.548908, 7.561885, 7.561864, 7.555076, 7.551947, 7.5527487, 7.548368, 7.5517354, 7.548732, 7.5516267, 7.5561233, 7.5525794, 7.554305, 7.548042, 7.5576315, 7.5524364, 7.559265, 7.553267, 7.5576687, 7.547975, 7.5622263, 7.5588746, 7.5561876, 7.5591607, 7.5508986, 7.549974, 7.5522294, 7.555974, 7.553259, 7.5527225, 7.542675, 7.551193, 7.5522904, 7.550717, 7.5557647, 7.5555468, 7.5447907, 7.54635, 7.549043, 7.5527287, 7.556864, 7.5549374, 7.546398, 7.5558248, 7.550968, 7.5484266, 7.5549765, 7.5489945, 7.5555663, 7.550076, 7.5443864, 7.5567203, 7.557157, 7.5552087, 7.559147, 7.5494094, 7.5622582, 7.5516005, 7.562178, 7.5590553, 7.5567265, 7.554886, 7.558858, 7.5505476, 7.549485, 7.552341, 7.54973, 7.558206, 7.557074, 7.55188, 7.550785, 7.5600667, 7.5505595, 7.5517178, 7.548259, 7.549436, 7.554122, 7.5490513, 7.555326, 7.555105, 7.559488, 7.5493116, 7.5491767, 7.5524936, 7.5541005, 7.5579367, 7.5549893, 7.5456133, 7.5540605, 7.550821, 7.5472527, 7.560029, 7.5492744, 7.5560365, 7.554474, 7.5483584, 7.5620494, 7.5598183, 7.55473, 7.5574455, 7.5581527, 7.5560117, 7.5528684, 7.552968, 7.552703, 7.5539904, 7.5508037, 7.553625, 7.5569773, 7.547881, 7.5512767, 7.5563097, 7.542588, 7.553152, 7.5556192, 7.546354, 7.5553927, 7.554427, 7.5524597, 7.5543704, 7.5484347, 7.5551314, 7.5482073, 7.5467, 7.5478983, 7.551586, 7.552272, 7.557928, 7.5545797, 7.5490584, 7.5515046, 7.553197, 7.562264, 7.5498223, 7.554898, 7.557732, 7.5543637, 7.5567465, 7.551843, 7.55004, 7.551678, 7.546044, 7.5513115, 7.553204, 7.555521, 7.5516887, 7.5537767, 7.549766, 7.550366, 7.554425, 7.550023, 7.558161, 7.556583, 7.5515122, 7.5531864, 7.552524, 7.5485168, 7.545391, 7.5538106, 7.547686, 7.553041, 7.55195, 7.5567226, 7.5556936, 7.549839, 7.55234, 7.551373, 7.5455184, 7.553837, 7.558136, 7.5511017, 7.5562944, 7.5541077, 7.5592613, 7.5520654, 7.550896, 7.547563, 7.5637794, 7.555467, 7.551245, 7.5638475, 7.553772, 7.5547523, 7.550502, 7.5549655, 7.5521007, 7.551535, 7.549951, 7.5555763, 7.550737, 7.5541453, 7.5513115, 7.548364, 7.5540524, 7.554338, 7.5473, 7.550495, 7.555534, 7.5615377, 7.5523386, 7.5519114, 7.5509, 7.5508485, 7.551166, 7.5461626, 7.5471635, 7.562255, 7.550342, 7.548825, 7.547813, 7.549987, 7.548956, 7.552736, 7.548711, 7.5537014, 7.5553308, 7.5550265, 7.5515656, 7.552618, 7.55896, 7.5555863, 7.5560865, 7.559302, 7.5517216, 7.54847, 7.555856, 7.553903, 7.5489135, 7.5497856, 7.554348, 7.5536466, 7.5559654, 7.5539155, 7.5482483, 7.5515842, 7.552718, 7.5493026, 7.5523953, 7.5509844, 7.5507627, 7.551109, 7.550791, 7.561888, 7.5517635, 7.550207, 7.5524774, 7.5476885, 7.555884, 7.551509, 7.5546017, 7.554029, 7.553556, 7.547449, 7.5647135, 7.5549927, 7.557533, 7.5572696, 7.5522346, 7.554714, 7.5571733, 7.5468936, 7.5537415, 7.5547366, 7.556441, 7.552728, 7.5605526, 7.554999, 7.546789, 7.549984, 7.5526805, 7.557287, 7.552764, 7.556515, 7.5544786, 7.5532966, 7.5510006, 7.554096, 7.550288, 7.555294, 7.559109, 7.5521145, 7.5474763, 7.550954, 7.5520535, 7.5541124, 7.5506835, 7.5491734, 7.551954, 7.5507545, 7.553703, 7.553766, 7.5689025, 7.5534453, 7.113224, 5.0688853, 1.8949969, 0.6484143, 0.625966, 0.54467565, 0.26675364, 0.28475392, 0.2992157, 0.21213274, 0.46344194, 0.40839013, 0.2325533, 0.19489743, 0.24491353, 0.16714242, 0.24242581, 0.21929634, 0.13763945, 0.16492452, 0.10813836, 0.1381533, 0.13579543, 0.11425985, 0.11757917, 0.14505589, 0.093326725, 0.1563267, 0.10778528, 0.09906632, 0.070977546, 0.10170814, 0.091370754, 0.11903911, 0.11177292, 0.10430227, 0.26663616, 0.5429014, 0.35618073, 0.23297574, 0.23050818, 0.32573354, 0.15414208, 0.18753587, 0.21510774, 0.34430376, 0.14963342, 0.2973052, 0.20681305, 0.1403181, 0.17560619, 0.10700306, 0.15126866, 0.115572706, 0.14552903, 0.10699828, 0.11423085, 0.19437115, 0.114728265, 0.10684833, 0.11222607, 0.16871762, 0.19315274, 0.115135215, 0.08642658, 0.09969952, 0.089345515, 0.08070966, 0.086816244, 0.09128443, 0.13097557, 0.072483316, 0.07961843, 0.06634381, 0.09351199, 0.09156785, 0.09554813, 0.10372886, 0.0789093, 0.115282595, 0.44228393, 0.25881284, 0.3802515, 0.2911788, 0.2769482, 0.2377036, 0.16380535, 0.15911318, 0.114491366, 0.20630877, 0.09760016, 0.10845425, 0.13066934, 0.1334458, 0.12020265, 0.13089545, 0.14804383, 0.22868119, 0.11506108, 0.08718523, 0.107596375, 0.10101045, 0.088652834, 0.09375143, 0.12969996, 0.10767554, 0.088906005, 0.11073224, 0.110139586, 0.074231744, 0.06751993, 0.061589673, 0.09416677, 0.07924658, 0.112342395, 0.07518356, 0.06691693, 0.06738436, 0.25210083, 0.24903089, 0.18824014, 0.40389493, 0.120055385, 0.20998125, 0.15836287, 0.15515101, 0.12565243, 0.24648649, 0.22595282, 0.2862648, 0.09731771, 0.14054371, 0.10375446, 0.13859093, 0.080747865, 0.121708155, 0.10331361, 0.1086093, 0.12023416, 0.0706316, 0.0954272, 0.06008271, 0.12056786, 0.105395295, 0.067615494, 0.07090847, 0.06872142, 0.06780456, 0.09055496, 0.08851668, 0.07121007, 0.06935134, 0.056275614, 0.08712991, 0.10826276, 0.07107835, 0.16772921, 0.1955045, 0.23753189, 0.5718099, 0.22266725, 0.18274613, 0.29085094, 0.18322225, 0.15952848, 0.15206526, 0.09695654, 0.08389082, 0.14377272, 0.20727484, 0.13548197, 0.108136944, 0.14090352, 0.13804813, 0.07069116, 0.086715996, 0.15870233, 0.07253242, 0.081699245, 0.07824503, 0.07334876, 0.05283611, 0.063699275, 0.069503285, 0.08944059, 0.093110375, 0.110483944, 0.044278543, 0.051025253, 0.08175484, 0.0609221, 0.05160793, 0.08480881, 0.0582588, 0.25465286, 0.19258699, 0.25589508, 0.22431414, 0.20012097, 0.10261622, 0.14984727, 0.30911273, 0.20266162, 0.11385146, 0.15072994, 0.17514636, 0.18752228, 0.20083168, 0.12224711, 0.098081656, 0.07119943, 0.07601633, 0.09804799, 0.07282497, 0.056824397, 0.11571459, 0.118959755, 0.06749354, 0.08829334, 0.0783406, 0.10444385, 0.10665578, 0.07101862, 0.06306235, 0.06363259, 0.07122921, 0.09168339, 0.08158152, 0.108048536, 0.07007045, 0.0980144, 0.07719465, 0.23386174, 0.22136144, 0.27259517, 0.25112554, 0.18398404, 0.124166414, 0.17610234, 0.16083749, 0.1118721, 0.10183012, 0.07628662, 0.16407175, 0.13359673, 0.18936706, 0.102771744, 0.10052484, 0.08490871, 0.124429666, 0.10946053, 0.086552575, 0.10190051, 0.072727256, 0.05354894, 0.07845301, 0.06107961, 0.07262462, 0.08450109, 0.056290437, 0.0953617, 0.0788331, 0.05465072, 0.09578518, 0.102661125, 0.08162595, 0.08453425, 0.07826605, 0.0864476, 0.08164206, 0.0479885, 0.07829637, 0.057993818, 0.08314207, 0.11656525, 0.2936592, 0.5037244, 0.2886206, 0.20129567, 0.16358332, 0.15165919, 0.2767726, 0.13393462, 0.057243317, 0.15014093, 0.11505301, 0.3132372, 0.14380538, 0.10598533, 0.124908775, 0.096731395, 0.056819163, 0.111329496, 0.11130422, 0.12623915, 0.08361894, 0.04768464, 0.09633168, 0.087624624, 0.07204582, 0.07456962, 0.09243308, 0.066900335, 0.08892571, 0.06961414, 0.07300082, 0.07459184, 0.05924898, 0.075599775, 0.058374863, 0.06853123, 0.058181673, 0.06499476, 0.2550144, 0.1563797, 0.7513465, 0.2692893, 0.34600958, 0.16044989, 0.1453499, 0.1696334, 0.20929252, 0.11034694, 0.09225819, 0.06694427, 0.11490121, 0.087775156, 0.14100629, 0.10875854, 0.07166448, 0.09574352, 0.101952456, 0.084712416, 0.110563, 0.07188294, 0.055974666, 0.08036867, 0.107768595, 0.07156928, 0.08630836, 0.0799314, 0.0731475, 0.07484899, 0.04297665, 0.05535844, 0.058821786, 0.06294022, 0.05944905, 0.06871325, 0.07425147, 0.05697807, 0.1029342, 0.43328878, 0.18623155, 0.26013628, 0.17700396, 0.12041363, 0.15325901, 0.15196696, 0.17439298, 0.19659664, 0.21448608, 0.19794564, 0.08163138, 0.11564941, 0.087350756, 0.18287456, 0.132448, 0.088307776, 0.09201088, 0.063103974, 0.09397151, 0.10593838, 0.081291854, 0.06209953, 0.057819355, 0.05666955, 0.08136262, 0.058095645, 0.059852682, 0.059989184, 0.06542341, 0.071787156, 0.07651825, 0.06862248, 0.068672, 0.06337907, 0.08338169, 0.064254224, 0.11017146, 0.21227889, 0.23028542, 0.2342924, 0.36710483, 0.32036152, 0.22980544, 0.102347754, 0.13413979, 0.11136824, 0.071931295, 0.08139611, 0.17467375, 0.076231055, 0.08128419, 0.10008894, 0.07867678, 0.09496293, 0.08755079, 0.073928, 0.086407445, 0.05130355, 0.07204619, 0.10750605, 0.08054473, 0.09393813, 0.07601719, 0.06387248, 0.11926769, 0.06599679, 0.069041066, 0.09311714, 0.0644277, 0.06394163, 0.07768883, 0.069711335, 0.07858737, 0.06719953, 0.26682886, 0.23640317, 0.21662328, 0.19126451, 0.17407104, 0.12829363, 0.10909286, 0.089916006, 0.09569707, 0.11816036, 0.10105611, 0.17266165, 0.08122304, 0.09720609, 0.12810607, 0.088350005, 0.046128362, 0.124494955, 0.08802955, 0.07426405, 0.05965059, 0.061317537, 0.075909436, 0.053238437, 0.057805877, 0.05983529, 0.0798237, 0.09557016, 0.08063135, 0.069791384, 0.07743803, 0.057335045, 0.055797826, 0.050551545, 0.07726372, 0.056194257, 0.04282113, 0.054809734, 0.06579803, 0.08421466, 0.044264596, 0.050790466, 0.05946187, 0.041797537, 0.14063434, 0.32158077, 0.23206182, 0.21979287, 0.19833796, 0.097436234, 0.1584272, 0.07157927, 0.24481763, 0.1935515, 0.09634363, 0.08722553, 0.24148309, 0.11068144, 0.10901664, 0.063820794, 0.06664297, 0.11422906, 0.06607021, 0.062333535, 0.06629371, 0.08105276, 0.065608956, 0.07503448, 0.06706436, 0.10605083, 0.086163424, 0.063538566, 0.083396934, 0.083671436, 0.06905244, 0.065750174, 0.057225503, 0.06625651, 0.056911897, 0.068769634, 0.07370033, 0.05745918, 0.17228876, 0.17736423, 0.13873516, 0.109207794, 0.09302082, 0.10247885, 0.12563606, 0.11154674, 0.16291112, 0.0605954, 0.09459914, 0.100171566, 0.07102444, 0.082275435, 0.11958874, 0.09889312, 0.09053349, 0.09283786, 0.07593793, 0.06467632, 0.095960714, 0.121802144, 0.08097852, 0.06984706, 0.05089295, 0.097834766, 0.06084212, 0.09059436, 0.048266042, 0.04963711, 0.055976182, 0.056328863, 0.058628093, 0.05420245, 0.05782266, 0.06437908, 0.05639885, 0.074154094, 0.2974821, 0.20244707, 0.24899669, 0.12480822, 0.09751922, 0.1434652, 0.1949546, 0.18410811, 0.20180894, 0.24603036, 0.15859233, 0.08150622, 0.070704274, 0.13760364, 0.07075405, 0.06307164, 0.0494532, 0.08173789, 0.10657565, 0.1808732, 0.12406723, 0.0710997, 0.06501036, 0.061228074, 0.073882595, 0.08244562, 0.06852705, 0.058766585, 0.0604674, 0.050223123, 0.06338361, 0.06978892, 0.073766276, 0.07617155, 0.05893903, 0.093350425, 0.063666396, 0.07308172, 0.16088584, 0.16921875, 0.1750923, 0.18426779, 0.27956063, 0.14115027, 0.108014986, 0.13703415, 0.19614466, 0.15423895, 0.099953264, 0.15044367, 0.11491114, 0.14551581, 0.1125505, 0.06390697, 0.0853377, 0.090092674, 0.0910364, 0.07216771, 0.11807241, 0.06791528, 0.05395701, 0.10407006, 0.072662234, 0.059617784, 0.059613753, 0.097501785, 0.062604584, 0.03716554, 0.09080403, 0.055730965, 0.10057449, 0.076055, 0.066916466, 0.050366927, 0.06085238, 0.056023132, 0.18965803, 0.3967176, 0.33277714, 0.2376746, 0.18127461, 0.10937139, 0.11531679, 0.14786492, 0.1434183, 0.1582741, 0.08158238, 0.113135636, 0.07298523, 0.089397535, 0.098651305, 0.106312335, 0.059694804, 0.08196541, 0.086443804, 0.098868154, 0.09744146, 0.06719866, 0.06622253, 0.058100652, 0.07326052, 0.08247003, 0.08651839, 0.07335299, 0.07903614, 0.07759164, 0.063482575, 0.07372787, 0.080925085, 0.0762774, 0.060738914, 0.056308176, 0.0610352, 0.06150463, 0.0588771, 0.077772185, 0.07156071, 0.05240761, 0.041404396, 0.15011834, 0.19514714, 0.17753157, 0.24398525, 0.37809077, 0.15123431, 0.122828186, 0.1493296, 0.20156014, 0.21669845, 0.121418774, 0.12211481, 0.13461147, 0.112174526, 0.06915325, 0.06698614, 0.08143181, 0.096254066, 0.05605084, 0.06707416, 0.07690551, 0.08150414, 0.09376428, 0.067552984, 0.08974695, 0.07868277, 0.06566221, 0.059915267, 0.061110396, 0.068078704, 0.09557303, 0.062270492, 0.039929193, 0.075468294, 0.04485897, 0.073186204, 0.07766668, 0.061164152, 0.27871066, 0.29323748, 0.13748781, 0.13594306, 0.15606986, 0.27627447, 0.16217649, 0.14493819, 0.10374552, 0.2103665, 0.13450778, 0.12699585, 0.082243584, 0.1878072, 0.1895577, 0.09913566, 0.09247967, 0.059089888, 0.053988088, 0.07005469, 0.10322476, 0.059781853, 0.13977449, 0.09702905, 0.049343977, 0.0797622, 0.07121625, 0.08556293, 0.07624168, 0.07973352, 0.04868175, 0.059788797, 0.08179632, 0.055372566, 0.050756253, 0.061557047, 0.065067865, 0.06061077, 0.15298232, 0.20768476, 0.21919742, 0.13195561, 0.21650839, 0.15548056, 0.09783686, 0.16176903, 0.24468331, 0.19633469, 0.15397601, 0.057738807, 0.13702714, 0.06859686, 0.09963118, 0.09108514, 0.10399938, 0.093719274, 0.06431234, 0.091879055, 0.07320916, 0.065602906, 0.07474246, 0.09595639, 0.0756841, 0.059480373, 0.064518355, 0.044931572, 0.07301095, 0.08182917, 0.057154447, 0.06884165, 0.05212095, 0.06756923, 0.045236122, 0.059529807, 0.03962104, 0.09850945, 0.13816331, 0.2382612, 0.18579125, 0.17777681, 0.15038474, 0.116313, 0.13065898, 0.12420295, 0.13262579, 0.25577125, 0.13051842, 0.116171286, 0.12265042, 0.07043592, 0.14138739, 0.12108143, 0.0717249, 0.100411914, 0.067849174, 0.12762679, 0.07017451, 0.07985488, 0.082978636, 0.08081675, 0.06683868, 0.104741566, 0.08410741, 0.047118925, 0.056655653, 0.076762274, 0.072718315, 0.078653075, 0.064786315, 0.09535004, 0.052344173, 0.056784097, 0.06872796, 0.07519052, 0.10936929, 0.14973147, 0.22477777, 0.22046617, 0.14942844, 0.31941685, 0.1086132, 0.1132367, 0.12169598, 0.11004295, 0.21378386, 0.18738604, 0.13747822, 0.15345325, 0.12889868, 0.10585875, 0.08605688, 0.04826014, 0.054487158, 0.115812995, 0.042818118, 0.06267925, 0.13179767, 0.092886426, 0.06003268, 0.05626993, 0.063389875, 0.051132895, 0.030631725, 0.0697876, 0.053257026, 0.04557678, 0.055890873, 0.057228077, 0.07642437, 0.07593196, 0.046014495, 0.055563763, 0.062031325, 0.067597225, 0.060688514, 0.0593548, 0.059158914, 0.04856566, 0.16304465, 0.17878847, 0.21764617, 0.27485028, 0.21738574, 0.23224103, 0.14230214, 0.116098545, 0.09998542, 0.19848016, 0.19000232, 0.104865715, 0.061530903, 0.0959711, 0.06449367, 0.08647116, 0.0844843, 0.083008, 0.07823794, 0.07423796, 0.07666044, 0.066113755, 0.080563165, 0.05455618, 0.07086735, 0.08593018, 0.05942723, 0.052621447, 0.06281372, 0.05439289, 0.05445605, 0.050343838, 0.072556056, 0.05530175, 0.06954072, 0.05959551, 0.06314946, 0.051284347, 0.24968027, 0.36409536, 0.18400459, 0.32465458, 0.13224354, 0.11518625, 0.1545382, 0.1218731, 0.100551255, 0.21654133, 0.20123541, 0.10664817, 0.14040458, 0.065119006, 0.051831495, 0.08910122, 0.083912075, 0.0416516, 0.10969446, 0.06966669, 0.06807203, 0.08306823, 0.047957774, 0.04250394, 0.10003446, 0.08191167, 0.078194164, 0.0733516, 0.079756215, 0.054527227, 0.051747598, 0.06775548, 0.06808103, 0.0654643, 0.05383907, 0.071427986, 0.06600601, 0.0485118, 0.100795686, 0.42386878, 0.11465722, 0.2117217, 0.15744643, 0.08212856, 0.105761014, 0.09582517, 0.11434001, 0.26169577, 0.1094132, 0.07249954, 0.12464487, 0.10850312, 0.09031321, 0.095183656, 0.06827461, 0.072330974, 0.069800355, 0.08454815, 0.04955839, 0.055895165, 0.04495876, 0.07382081, 0.06948211, 0.07928951, 0.060554814, 0.076316856, 0.070061244, 0.07799897, 0.056555208, 0.04810037, 0.039288294, 0.058876947, 0.05000766, 0.05871121, 0.06869944, 0.085780814, 0.2906948, 0.17703801, 0.11535621, 0.19126615, 0.12793879, 0.094799355, 0.19710663, 0.12945355, 0.14570105, 0.1305396, 0.09110603, 0.08831313, 0.13110021, 0.07513662, 0.09732777, 0.05663848, 0.064806744, 0.08475682, 0.08818891, 0.05950584, 0.060870122, 0.06800584, 0.09457054, 0.052211337, 0.07994386, 0.0603885, 0.052818336, 0.060930658, 0.092939064, 0.07131163, 0.043659236, 0.09421964, 0.060630653, 0.058918796, 0.08561932, 0.048692487, 0.0563456, 0.067213796, 0.09184149, 0.14168528, 0.1750529, 0.203865, 0.1775924, 0.121169895, 0.08021875, 0.12634107, 0.15783922, 0.4040692, 0.21303433, 0.09186193, 0.055682786, 0.09235716, 0.07973167, 0.08943755, 0.09190978, 0.09822037, 0.046285376, 0.07234421, 0.05252078, 0.047541726, 0.09244673, 0.044884305, 0.061036788, 0.06523809, 0.058178738, 0.058464985, 0.050085712, 0.039366245, 0.0863704, 0.097218014, 0.08260289, 0.06897309, 0.087631404, 0.036744967, 0.05940337, 0.031603348, 0.051334266, 0.074589506, 0.07259494, 0.065292455, 0.07027767, 0.06776785, 0.29856744, 0.2334204, 0.2504003, 0.34112033, 0.13906817, 0.2950282, 0.20332807, 0.1290422, 0.106500514, 0.20356134, 0.11470037, 0.09288759, 0.07275439, 0.09158362, 0.08644483, 0.07390077, 0.122760735, 0.09672, 0.083441556, 0.072369084, 0.052187413, 0.03799836, 0.08603347, 0.065054566, 0.055118963, 0.042981654, 0.057151787, 0.04502019, 0.057998795, 0.083113216, 0.079790585, 0.052693065, 0.06802122, 0.07103539, 0.043102983, 0.0351297, 0.057180017, 0.22444673, 0.160189, 0.19978891, 0.17815775, 0.25555915, 0.11433583, 0.18720278, 0.11744899, 0.18366174, 0.19917452, 0.076267585, 0.062032845, 0.07488703, 0.08686302, 0.06369253, 0.10192753, 0.0931825, 0.07125604, 0.09739771, 0.089818284, 0.08032472, 0.059682578, 0.07153878, 0.07054082, 0.08841028, 0.06834883, 0.050937485, 0.061089147, 0.06898215, 0.08523066, 0.055610567, 0.055554044, 0.053780444, 0.071165845, 0.057942003, 0.05226383, 0.07688889, 0.058283035, 0.2205027, 0.3311574, 0.09462558, 0.27696064, 0.18457444, 0.12301889, 0.09884061, 0.13071297, 0.14110245, 0.18913555, 0.13479356, 0.12531647, 0.06658883, 0.1889555, 0.1148643, 0.0649872, 0.06590884, 0.066323936, 0.144892, 0.10143266, 0.05550282, 0.07652082, 0.100657366, 0.06515367, 0.056262147, 0.074850395, 0.0595402, 0.067213275, 0.061017465, 0.053838104, 0.057238787, 0.061768964, 0.04390889, 0.056473643, 0.05581347, 0.06271672, 0.059983026, 0.03758017, 0.09620302, 0.11771984, 0.21186663, 0.17622681, 0.18265513, 0.109602064, 0.0871233, 0.088091396, 0.14296778, 0.112088695, 0.08954122, 0.08179257, 0.10369766, 0.055640623, 0.06554431, 0.124854654, 0.13229962, 0.060563296, 0.101552464, 0.06216781, 0.070102476, 0.08042901, 0.076091096, 0.043854225, 0.06904638, 0.069060914, 0.046481263, 0.07034725, 0.04284413, 0.059966777, 0.08067891, 0.05829367, 0.07060153, 0.056711424, 0.042950597, 0.04949421, 0.038280357, 0.045092065, 0.31610128, 0.37917393, 0.19289589, 0.11845253, 0.18875846, 0.101787925, 0.25950566, 0.22685686, 0.20294765, 0.13876112, 0.069822624, 0.077990614, 0.08331023, 0.110418536, 0.14085957, 0.061670993, 0.10371976, 0.094971634, 0.055992905, 0.045128666, 0.07477579, 0.072510935, 0.08213678, 0.043057974, 0.055398673, 0.09176173, 0.0797245, 0.038603123, 0.06736651, 0.08592297, 0.08391371, 0.08897049, 0.03332594, 0.0845935, 0.06668733, 0.06724187, 0.06276526, 0.059101086, 0.05139269, 0.060631678, 0.057677224, 0.057227593, 0.04757602, 0.05999151, 0.20884754, 0.16878043, 0.24258508, 0.2416708, 0.14277184, 0.06606609, 0.07804976, 0.36292508, 0.17040557, 0.10739985, 0.20579222, 0.07133686, 0.07740859, 0.06265477, 0.10019009, 0.09038432, 0.0730294, 0.058280926, 0.062599204, 0.061497267, 0.09093354, 0.07587494, 0.054304462, 0.054721862, 0.107133865, 0.091251716, 0.037848998, 0.06328454, 0.0959482, 0.058416028, 0.043501407, 0.05570812, 0.037213553, 0.056170613, 0.06960608, 0.040266797, 0.07042234, 0.04730817, 0.16636574, 0.1606046, 0.06733165, 0.19031276, 0.17184013, 0.11817138, 0.102259286, 0.14997514, 0.124775924, 0.14981577, 0.068246506, 0.048506863, 0.081931, 0.059377413, 0.091868095, 0.08219156, 0.07299178, 0.072979406, 0.060690954, 0.06365027, 0.039846458, 0.07535945, 0.0655778, 0.07104028, 0.050920665, 0.08450909, 0.048657127, 0.055853643, 0.040241994, 0.045188554, 0.07107519, 0.0640097, 0.065515585, 0.0651168, 0.048348438, 0.06897391, 0.048253942, 0.041921776, 0.19955, 0.13426732, 0.2193857, 0.17635691, 0.08476224, 0.1259684, 0.16952167, 0.115594134, 0.13276222, 0.08259911, 0.06501365, 0.077635795, 0.100009136, 0.07222816, 0.08089312, 0.08881992, 0.07113825, 0.074373335, 0.056652226, 0.059009206, 0.053657826, 0.0837021, 0.050461333, 0.058064424, 0.07214078, 0.07068543, 0.046752166, 0.054451328, 0.046450306, 0.0517787, 0.055206925, 0.059833463, 0.061463803, 0.05718634, 0.047293514, 0.04891983, 0.07840443, 0.053625703, 0.16871814, 0.30161282, 0.2564772, 0.2064932, 0.19295676, 0.07298369, 0.17647944, 0.15688813, 0.102346785, 0.1318349, 0.098449595, 0.046160705, 0.06751746, 0.0847766, 0.07418571, 0.06429137, 0.10550475, 0.172123, 0.09651027, 0.08497773, 0.057120442, 0.062467687, 0.078403585, 0.051048864, 0.05916056, 0.061890174, 0.051779937, 0.041199256, 0.06505149, 0.06419239, 0.09311498, 0.06904632, 0.07505754, 0.080699734, 0.06291128, 0.043706965, 0.06660766, 0.056177646, 0.19378328, 0.14138395, 0.09342577, 0.12959939, 0.100873515, 0.08533489, 0.09015532, 0.09314342, 0.11728404, 0.15003377, 0.10577759, 0.08341144, 0.07611232, 0.072471745, 0.093384765, 0.08182734, 0.06601186, 0.05868886, 0.06596105, 0.05031908, 0.048080146, 0.14877176, 0.059455585, 0.09150666, 0.08414467, 0.060886018, 0.04803095, 0.03135016, 0.04775233, 0.08004423, 0.061761335, 0.059932545, 0.05588307, 0.03270733, 0.04949777, 0.056422584, 0.06266565, 0.05603109, 0.07247974, 0.047936212, 0.060214113, 0.045651525, 0.03746292, 0.12415891, 0.14780664, 0.12656088, 0.14880772, 0.16528638, 0.17515449, 0.09643141, 0.113381445, 0.10062929, 0.12596272, 0.06440718, 0.068932116, 0.09568159, 0.12631819, 0.06428672, 0.075325795, 0.089321926, 0.07766949, 0.05275451, 0.0854275, 0.0947632, 0.06323966, 0.046565376, 0.09302187, 0.06831774, 0.10391536, 0.04916742, 0.04831762, 0.06378925, 0.090077855, 0.057889152, 0.05832268, 0.05001257, 0.09000173, 0.05224658, 0.074530706, 0.049828585, 0.04433569, 0.08283129, 0.13711742, 0.18398039, 0.15443653, 0.3482206, 0.115441024, 0.14139622, 0.13395515, 0.16445911, 0.22517996, 0.12645635, 0.20647337, 0.14601979, 0.089861326, 0.089118145, 0.09317113, 0.08795568, 0.08040093, 0.05635766, 0.0853344, 0.061114084, 0.079674795, 0.062650114, 0.056557417, 0.03643736, 0.05314826, 0.063165486, 0.08811018, 0.052274015, 0.04510208, 0.051983584, 0.074172355, 0.05467821, 0.06225432, 0.059000086, 0.049076695, 0.05432115, 0.050347053, 0.14122109, 0.20456287, 0.3887376, 0.1364357, 0.13880736, 0.16834418, 0.07672229, 0.09535712, 0.0739403, 0.13699538, 0.07326452, 0.051875282, 0.063385345, 0.061936516, 0.06396669, 0.094258, 0.055483874, 0.11286792, 0.099455096, 0.13341758, 0.07350302, 0.05762752, 0.0591421, 0.07412811, 0.050135016, 0.07028272, 0.06405935, 0.066485964, 0.066860124, 0.047362834, 0.063281, 0.050431058, 0.05887824, 0.06290955, 0.09543348, 0.062164344, 0.06752574, 0.046830837, 0.1935759, 0.20355223, 0.13629101, 0.28487024, 0.20088702, 0.16699053, 0.17971827, 0.112738624, 0.074106276, 0.114868425, 0.1213089, 0.0885566, 0.059676666, 0.07211835, 0.074570835, 0.14741822, 0.048968032, 0.0817165, 0.0823807, 0.061428916, 0.058663983, 0.055502307, 0.051537126, 0.07933484, 0.073540285, 0.047290318, 0.09201067, 0.05291445, 0.04713761, 0.05154593, 0.04656529, 0.09665012, 0.066051275, 0.0583888, 0.06058029, 0.060682718, 0.05742116, 0.04774718, 0.13718066, 0.16494755, 0.28890726, 0.26880795, 0.13651037, 0.21685374, 0.08283243, 0.20355414, 0.28908116, 0.1145239, 0.15594716, 0.07151591, 0.07250563, 0.11365789, 0.090576135, 0.06872491, 0.07528311, 0.08631608, 0.1270776, 0.07275216, 0.06382151, 0.07160873, 0.052648827, 0.07756778, 0.05033536, 0.08239581, 0.05697355, 0.088207975, 0.08393373, 0.071496055, 0.06482242, 0.06612529, 0.10349653, 0.081419446, 0.043763436, 0.04327679, 0.058323886, 0.06645029, 0.06379759, 0.043650687, 0.04609603, 0.036313288, 0.057707988, 0.05669515, 0.16270542, 0.2580378, 0.11720584, 0.21809849, 0.103489354, 0.12425912, 0.1540431, 0.19910234, 0.2072782, 0.13899948, 0.09394755, 0.07628026, 0.083758004, 0.05841117, 0.06471267, 0.08604292, 0.0638081, 0.058111887, 0.06541123, 0.054184273, 0.09795162, 0.061693795, 0.04770039, 0.036574185, 0.06326283, 0.048653174, 0.065996945, 0.04679842, 0.043424297, 0.08920426, 0.08912891, 0.072182074, 0.042288754, 0.05154734, 0.055835146, 0.05324005, 0.06365365, 0.08111903, 0.23260498, 0.16310395, 0.13831423, 0.2512947, 0.09726354, 0.11620611, 0.12821428, 0.23523225, 0.15747125, 0.16125111, 0.074430294, 0.08244726, 0.06658265, 0.06816109, 0.09938357, 0.08438939, 0.09049446, 0.069725215, 0.07604938, 0.09507453, 0.07347508, 0.07736649, 0.064545676, 0.049167614, 0.06568913, 0.04761764, 0.0691776, 0.045353156, 0.044732034, 0.06110846, 0.053701784, 0.050816514, 0.050280362, 0.060493488, 0.041108143, 0.071474254, 0.08072772, 0.054479994, 0.16523738, 0.19235466, 0.1585786, 0.16994119, 0.11283398, 0.1601496, 0.15009257, 0.12720287, 0.117972925, 0.080480784, 0.08716512, 0.07556067, 0.04164875, 0.05392332, 0.052009035, 0.09212355, 0.06975822, 0.04849687, 0.103213646, 0.11948776, 0.06603715, 0.056508083, 0.054686435, 0.06641996, 0.055098984, 0.06897444, 0.06454495, 0.047364723, 0.07534469, 0.09761769, 0.05397993, 0.06904551, 0.056382135, 0.057398494, 0.06952365, 0.053205434, 0.04244416, 0.070696175, 0.12429153, 0.18660145, 0.106132984, 0.16070163, 0.11445654, 0.09580989, 0.12609923, 0.12025354, 0.0975538, 0.09566008, 0.0704568, 0.07640775, 0.09235525, 0.11635581, 0.1379732, 0.087528914, 0.10480469, 0.085572265, 0.08784483, 0.053579286, 0.06350612, 0.08094076, 0.081812896, 0.041530505, 0.07244407, 0.06206823, 0.08110411, 0.046379786, 0.06294011, 0.046010714, 0.062230587, 0.06516575, 0.055597756, 0.03603761, 0.045140445, 0.047088772, 0.052884866, 0.056840036, 0.08158891, 0.1954857, 0.19481425, 0.116133474, 0.0871999, 0.120089546, 0.090449795, 0.12533931, 0.06039998, 0.14016774, 0.11240891, 0.06978915, 0.106455065, 0.092061885, 0.10662104, 0.083235435, 0.11993476, 0.093887314, 0.060993414, 0.046555456, 0.058492605, 0.05235228, 0.051762052, 0.083403066, 0.043343253, 0.0586904, 0.06440006, 0.065547824, 0.054988466, 0.039460264, 0.07516335, 0.050510198, 0.081150055, 0.09006813, 0.050133403, 0.05131806, 0.058616262, 0.03277492, 0.28190577, 0.21644413, 0.14271145, 0.16872478, 0.26333812, 0.13825746, 0.09009521, 0.10490406, 0.08235181, 0.100089245, 0.07987732, 0.07849134, 0.10019744, 0.0879673, 0.08244227, 0.08529338, 0.043115705, 0.07391396, 0.08806374, 0.058485553, 0.06701979, 0.046239328, 0.044371027, 0.065577105, 0.07539282, 0.058672417, 0.08066795, 0.050428066, 0.05687878, 0.039217222, 0.04963781, 0.04643493, 0.057816535, 0.07785553, 0.042680096, 0.060862333, 0.039800316, 0.053026594, 0.1289727, 0.21391682, 0.31079522, 0.1561055, 0.12478656, 0.110614225, 0.20896143, 0.090212286, 0.107282855, 0.119688034, 0.092960484, 0.114690214, 0.070976116, 0.05245103, 0.059955742, 0.073971935, 0.11164079, 0.13266067, 0.0694506, 0.09207147, 0.07184837, 0.06989932, 0.047874466, 0.05637903, 0.091050275, 0.045683794, 0.055212982, 0.064132646, 0.08183078, 0.04794256, 0.041360337, 0.057203118, 0.04562601, 0.076555595, 0.047755163, 0.046618212, 0.040677756, 0.059483197, 0.14237066, 0.1585854, 0.17981112, 0.117969275, 0.08891199, 0.09364853, 0.07467897, 0.12788264, 0.27094686, 0.14212222, 0.06789274, 0.07981105, 0.083568655, 0.085047625, 0.122170836, 0.05937669, 0.067843415, 0.07644414, 0.054892804, 0.05799388, 0.06846609, 0.072477125, 0.068629086, 0.0674055, 0.04604277, 0.05005944, 0.10278073, 0.054598946, 0.059183232, 0.04212919, 0.053526852, 0.05821163, 0.0536055, 0.047308236, 0.05071951, 0.05141619, 0.06995782, 0.07540468, 0.16201209, 0.20293626, 0.24317713, 0.09826842, 0.12117047, 0.10832618, 0.08462992, 0.097169496, 0.084196776, 0.16174993, 0.068551525, 0.06124676, 0.08889675, 0.059809376, 0.057817314, 0.08310127, 0.07232648, 0.055428717, 0.1525495, 0.085793674, 0.10956285, 0.06751368, 0.053318933, 0.051647738, 0.06025771, 0.08373044, 0.06501175, 0.09035924, 0.052464515, 0.0587152, 0.044040002, 0.061214246, 0.058098864, 0.04568716, 0.052525993, 0.058990348, 0.075022295, 0.054709304, 0.10237986, 0.17997341, 0.21335272, 0.1470706, 0.11127523, 0.06262352, 0.12725474, 0.10792933, 0.18307285, 0.11489171, 0.07452301, 0.10143526, 0.08632761, 0.08544149, 0.09214124, 0.0904332, 0.07487434, 0.10287964, 0.101610295, 0.070774436, 0.055971384, 0.061997157, 0.05848345, 0.05010123, 0.06254204, 0.07263986, 0.061022688, 0.0617121, 0.08633882, 0.07252462, 0.03509987, 0.060664378, 0.073451206, 0.056346625, 0.055640623, 0.053201508, 0.064718574, 0.067283936, 0.0636431, 0.03694325, 0.050781976, 0.036505513, 0.04825883, 0.13978572, 0.25410083, 0.3542862, 0.32077837, 0.19058031, 0.14444277, 0.19176717, 0.09633721, 0.10451059, 0.05942945, 0.06656319, 0.06366199, 0.10246107, 0.15234418, 0.14571542, 0.09326507, 0.08097067, 0.08009813, 0.09849288, 0.07064084, 0.058958422, 0.061725754, 0.08853144, 0.07107652, 0.08209185, 0.09966095, 0.06647337, 0.05667458, 0.053890515, 0.07110959, 0.06139193, 0.06571, 0.06365363, 0.06389228, 0.067403525, 0.08572861, 0.061680805, 0.06074656, 0.11228702, 0.19426715, 0.1276298, 0.1845034, 0.10769131, 0.15786676, 0.15981889, 0.10511146, 0.1388538, 0.14412192, 0.08321311, 0.064334095, 0.05556453, 0.07521089, 0.04422908, 0.081635185, 0.09040116, 0.06349359, 0.039630845, 0.08169371, 0.058022775, 0.0585847, 0.059941724, 0.04258465, 0.06343579, 0.055168163, 0.04628921, 0.054089464, 0.060355958, 0.089039914, 0.05128572, 0.06698084, 0.06595241, 0.056085836, 0.05148096, 0.05145665, 0.07496307, 0.04112921, 0.10207086, 0.14508773, 0.31231332, 0.4415071, 0.2363034, 0.09930486, 0.06887177, 0.063903116, 0.12994452, 0.14484806, 0.103409946, 0.099705, 0.06255784, 0.08562984, 0.122500606, 0.11061485, 0.055426408, 0.03895146, 0.06379092, 0.076206334, 0.043454558, 0.04083446, 0.0730451, 0.0421827, 0.07322122, 0.075216405, 0.054477215, 0.03941082, 0.052766267, 0.049975563, 0.06645406, 0.048218373, 0.081864335, 0.043396052, 0.065957285, 0.041556932, 0.054884426, 0.05237806, 0.12587005, 0.13798697, 0.15285712, 0.117959335, 0.10609966, 0.13512933, 0.15006925, 0.123720735, 0.042738006, 0.08172527, 0.12265974, 0.079214215, 0.057356283, 0.070065975, 0.109431304, 0.1011853, 0.08117828, 0.09449043, 0.06604909, 0.046646375, 0.057198048, 0.05738752, 0.087994784, 0.041397188, 0.0792644, 0.08901713, 0.06315341, 0.06718644, 0.070083916, 0.051486343, 0.06729982, 0.0439565, 0.072652325, 0.06235343, 0.07668204, 0.06305939, 0.0463884, 0.05851821, 0.23516703, 0.32948276, 0.18995114, 0.24948747, 0.2282703, 0.10000246, 0.09288261, 0.13470471, 0.068122216, 0.07713369, 0.12583028, 0.08694928, 0.08311237, 0.14089528, 0.08253689, 0.081429124, 0.059117574, 0.07318334, 0.06236137, 0.084009625, 0.06891304, 0.047639087, 0.04458525, 0.064674735, 0.06481803, 0.07689597, 0.05555869, 0.06355086, 0.043763522, 0.05436472, 0.04699676, 0.046126615, 0.055630196, 0.0661763, 0.05818998, 0.05254151, 0.037763447, 0.03201982, 0.055248726, 0.0512443, 0.054670066, 0.05279297, 0.051699907, 0.053671252, 0.13075875, 0.25296944, 0.14270274, 0.17340972, 0.09347542, 0.10323105, 0.11023077, 0.14730948, 0.24231532, 0.12189243, 0.10408607, 0.08963256, 0.07451818, 0.08563697, 0.103912495, 0.045657985, 0.08389856, 0.06429421, 0.06479534, 0.07650265, 0.0536697, 0.05643468, 0.055461336, 0.0637873, 0.06166757, 0.085516356, 0.065135196, 0.059524655, 0.055014465, 0.05921197, 0.051140517, 0.03714849, 0.04958275, 0.032218426, 0.061297674, 0.049929287, 0.04946674, 0.04579674, 0.08666585, 0.12582065, 0.18948896, 0.17899434, 0.11526293, 0.08391274, 0.1528726, 0.16058125, 0.17862326, 0.08990489, 0.09533879, 0.0652335, 0.09106636, 0.09077863, 0.09183377, 0.09162364, 0.06277615, 0.044931144, 0.07318748, 0.07725161, 0.07527616, 0.033529285, 0.06480982, 0.035943087, 0.077793024, 0.08731767, 0.05354931, 0.066598736, 0.055231363, 0.071330406, 0.042674687, 0.056130785, 0.04961551, 0.085048854, 0.049511988, 0.047747973, 0.035372976, 0.04858451, 0.12093864, 0.17210397, 0.24011298, 0.115683444, 0.09437981, 0.10416067, 0.12720239, 0.12636243, 0.04314542, 0.14010449, 0.07828591, 0.07272205, 0.0738777, 0.05964501, 0.14396074, 0.10417303, 0.06911616, 0.05760718, 0.076709986, 0.060603574, 0.07954265, 0.071299724, 0.06979862, 0.05410141, 0.050788812, 0.054892827, 0.062737264, 0.036376104, 0.06342598, 0.040359076, 0.043266412, 0.059330437, 0.04345199, 0.047636945, 0.07139099, 0.06400312, 0.060593147, 0.04483854, 0.174639, 0.18882924, 0.37246147, 0.21737355, 0.102923654, 0.062643096, 0.15575251, 0.18778692, 0.1321615, 0.09123827, 0.078072846, 0.09559574, 0.076515846, 0.09177333, 0.099832065, 0.06678277, 0.04481579, 0.05348784, 0.06370307, 0.06969593, 0.06642546, 0.054574985, 0.03976582, 0.073735766, 0.06773866, 0.060258873, 0.043423418, 0.036512855, 0.061065353, 0.06149402, 0.055139232, 0.050083686, 0.05019692, 0.052405562, 0.042830482, 0.056009978, 0.07207771, 0.10790014, 0.16624375, 0.12862793, 0.15164752, 0.17710112, 0.1455469, 0.08426617, 0.1518646, 0.06524794, 0.1045651, 0.10257682, 0.074496485, 0.11152201, 0.07927365, 0.14686944, 0.118114986, 0.10368409, 0.07979448, 0.05867781, 0.062408034, 0.043768883, 0.053328425, 0.11100747, 0.06760976, 0.049582113, 0.0747084, 0.086472034, 0.057750978, 0.060796592, 0.06532731, 0.046141636, 0.042196512, 0.056821465, 0.061469097, 0.04025724, 0.060572825, 0.06580298, 0.038210988, 0.03567694, 0.052122902, 0.037318934, 0.050593402, 0.052445885, 0.045814622, 0.10022946, 0.102495946, 0.15645312, 0.10697646, 0.22110271, 0.116972096, 0.1280163, 0.09910445, 0.08763859, 0.124413066, 0.058208115, 0.0908509, 0.07383587, 0.10216324, 0.14929579, 0.09011008, 0.0808519, 0.077729486, 0.042654306, 0.08606063, 0.06553847, 0.07361595, 0.0528891, 0.04518269, 0.07025496, 0.07504018, 0.08856381, 0.053568434, 0.05262027, 0.06575644, 0.040893853, 0.06559008, 0.041317794, 0.051665157, 0.052232716, 0.04876451, 0.054872304, 0.042042483, 0.07723775, 0.17495984, 0.11588669, 0.17776535, 0.16439876, 0.1566263, 0.14825435, 0.11168482, 0.07908604, 0.101677716, 0.14469177, 0.082734086, 0.09098513, 0.09298655, 0.097867444, 0.05768092, 0.07855302, 0.062283944, 0.050049856, 0.060609322, 0.062927514, 0.047269948, 0.07254354, 0.04947507, 0.06418483, 0.07674269, 0.061184324, 0.0536216, 0.079640485, 0.056165997, 0.078878686, 0.07141924, 0.049075715, 0.05089093, 0.049033716, 0.085405126, 0.034218803, 0.05019954, 0.074955404, 0.13968605, 0.30774286, 0.16729504, 0.120236374, 0.11458166, 0.18711312, 0.12283272, 0.07892842, 0.13495421, 0.08602039, 0.098078236, 0.12873876, 0.07992156, 0.057500135, 0.053056322, 0.043365322, 0.097912334, 0.06249259, 0.08331536, 0.06457747, 0.07016016, 0.065882385, 0.057670493, 0.063383184, 0.05713718, 0.06583249, 0.07794024, 0.045724474, 0.083019525, 0.060416523, 0.063599385, 0.059399605, 0.061564934, 0.03779373, 0.05221984, 0.054632884, 0.04863448, 0.0927489, 0.18258256, 0.101744436, 0.13108958, 0.11659822, 0.17409255, 0.1887333, 0.076563634, 0.07694577, 0.076087154, 0.09044572, 0.08274293, 0.06743531, 0.08790959, 0.078048825, 0.06526038, 0.079540595, 0.058850482, 0.059859898, 0.07185133, 0.05581268, 0.046072733, 0.04958672, 0.056932468, 0.043168414, 0.057647374, 0.07558483, 0.069814436, 0.049651336, 0.058233425, 0.05639546, 0.04721442, 0.04743479, 0.063265234, 0.06176057, 0.05423685, 0.040570807, 0.03395892, 0.11837988, 0.1148592, 0.14250672, 0.16043635, 0.15416937, 0.044654336, 0.056382068, 0.07038572, 0.1067654, 0.097761035, 0.07264308, 0.05452789, 0.068723775, 0.04746237, 0.07478526, 0.058320034, 0.06568755, 0.062413592, 0.083553694, 0.089332916, 0.04949465, 0.057057094, 0.06026615, 0.05805275, 0.050851464, 0.059337642, 0.08904294, 0.04129945, 0.06954166, 0.067002, 0.055723976, 0.042773593, 0.060647845, 0.03520794, 0.060204167, 0.051839147, 0.045325845, 0.06636441, 0.06736944, 0.060528804, 0.04002019, 0.05770783, 0.05145762, 0.048781916, 0.153878, 0.21090621, 0.17378457, 0.1924261, 0.17913353, 0.08800194, 0.10158322, 0.08626151, 0.10998093, 0.11655107, 0.06784318, 0.06776042, 0.07154363, 0.059811298, 0.13483772, 0.0795275, 0.103243046, 0.07032829, 0.07215168, 0.06616402, 0.08459765, 0.04838394, 0.04008548, 0.063621104, 0.04441322, 0.07172502, 0.07600027, 0.041647147, 0.03230837, 0.07867538, 0.08513905, 0.0650236, 0.05600271, 0.067807145, 0.062347412, 0.05974683, 0.05035306, 0.041037317, 0.16196239, 0.2530914, 0.101968676, 0.12087704, 0.14841904, 0.121385574, 0.15850191, 0.15653618, 0.07436635, 0.07445206, 0.114195526, 0.08880918, 0.11598901, 0.06906758, 0.08507325, 0.11207454, 0.059376515, 0.09093001, 0.053455997, 0.03594299, 0.07572184, 0.08260292, 0.10002173, 0.08075257, 0.06904173, 0.043832015, 0.059208144, 0.039971977, 0.052464318, 0.05559945, 0.05606932, 0.052558333, 0.0526676, 0.05196302, 0.043221228, 0.04672997, 0.04757269, 0.06440892, 0.16644171, 0.08696685, 0.11092182, 0.13580927, 0.13233832, 0.09210222, 0.053273153, 0.09544883, 0.12429679, 0.1365344, 0.059968065, 0.06178613, 0.0637586, 0.052676585, 0.07329311, 0.05348104, 0.059889644, 0.07091069, 0.07316869, 0.07805272, 0.052926272, 0.065883204, 0.046168324, 0.06537169, 0.07671613, 0.04612459, 0.05327509, 0.05343132, 0.051421292, 0.061765164, 0.04479207, 0.06509017, 0.059013378, 0.05049821, 0.04717927, 0.06331453, 0.0618619, 0.08314361, 0.092026524, 0.17100973, 0.14381956, 0.07572565, 0.12594412, 0.05284967, 0.15362494, 0.07035889, 0.10432916, 0.17018007, 0.07902608, 0.062616974, 0.061123658, 0.06671544, 0.100587934, 0.057885032, 0.059590984, 0.09443063, 0.08650238, 0.06477452, 0.065048665, 0.06857247, 0.046109945, 0.062169086, 0.07611691, 0.06151446, 0.07098566, 0.060613044, 0.034125343, 0.06481127, 0.048452098, 0.06072252, 0.028784871, 0.050534695, 0.047819946, 0.048318047, 0.071947135, 0.064228006, 0.06287691, 0.14232156, 0.12865253, 0.18275881, 0.102402955, 0.13374169, 0.083763316, 0.0701113, 0.14100571, 0.21845745, 0.11419047, 0.0634326, 0.06277126, 0.101964965, 0.076307766, 0.06553569, 0.06713257, 0.07149188, 0.084568754, 0.051564544, 0.08403059, 0.067192376, 0.061934087, 0.051933043, 0.044104975, 0.051854547, 0.052758604, 0.061918806, 0.057231013, 0.041184634, 0.061061528, 0.06442861, 0.0544386, 0.056035556, 0.05200538, 0.06267928, 0.070719264, 0.055097487, 0.0568827, 0.0607162, 0.064497, 0.07651443, 0.042674612, 0.07976831, 0.12795977, 0.12440172, 0.18322341, 0.12904117, 0.17010918, 0.25390854, 0.10563419, 0.15174371, 0.16582626, 0.09477302, 0.12236146, 0.093302116, 0.096472934, 0.05245716, 0.059605986, 0.07922515, 0.086894326, 0.053976517, 0.11087473, 0.07610581, 0.08516376, 0.038877483, 0.054855615, 0.06538325, 0.057350498, 0.07436162, 0.05371686, 0.040043894, 0.052444667, 0.04625221, 0.051988434, 0.04122109, 0.053722527, 0.048049342, 0.0600814, 0.050652944, 0.05946901, 0.11396935, 0.11696517, 0.15977518, 0.13364972, 0.17369886, 0.09876787, 0.060558263, 0.14448789, 0.14555983, 0.17458604, 0.106900916, 0.13159859, 0.1382278, 0.055158038, 0.0706383, 0.09135701, 0.052530997, 0.07233087, 0.049903657, 0.04835787, 0.06762224, 0.070914246, 0.062019154, 0.055306006, 0.04121003, 0.04796643, 0.0637928, 0.058773674, 0.067173354, 0.04709609, 0.05460013, 0.05800104, 0.056628793, 0.06553557, 0.060394533, 0.044925533, 0.0681556, 0.033069864, 0.09720657, 0.07545645, 0.08377376, 0.081303604, 0.17491637, 0.1475013, 0.14916085, 0.11392599, 0.083109416, 0.20303139, 0.0932297, 0.067436256, 0.09288786, 0.106911235, 0.10649735, 0.06093401, 0.05699619, 0.04243921, 0.037652895, 0.076072805, 0.056126833, 0.04961566, 0.05576684, 0.05283842, 0.039922506, 0.06654748, 0.05299327, 0.037477802, 0.03478809, 0.07537892, 0.061919004, 0.06878439, 0.068197235, 0.06237216, 0.058275416, 0.046197455, 0.041792158, 0.034285963, 0.15619159, 0.28600514, 0.15925422, 0.27372488, 0.1864802, 0.15187466, 0.13114536, 0.13038388, 0.119543254, 0.103554025, 0.060201634, 0.06470142, 0.065621115, 0.06266004, 0.07177575, 0.084417515, 0.080103524, 0.056031298, 0.04456107, 0.0512995, 0.05248947, 0.044607546, 0.051679794, 0.062038988, 0.03777546, 0.0538501, 0.07116133, 0.07510357, 0.06604897, 0.055257935, 0.04848812, 0.054227937, 0.050188947, 0.050310284, 0.053647984, 0.045933288, 0.06651614, 0.04693723, 0.20616475, 0.24267772, 0.23309077, 0.1547596, 0.23788232, 0.08796147, 0.091301665, 0.097656004, 0.089836344, 0.07600913, 0.08958083, 0.07824597, 0.058912307, 0.08808535, 0.05481486, 0.06309145, 0.070050456, 0.05629169, 0.052543588, 0.07154531, 0.06633929, 0.072018005, 0.051206943, 0.044412702, 0.06675615, 0.054568697, 0.056447756, 0.06438099, 0.075936176, 0.06561614, 0.05810346, 0.06315418, 0.074435115, 0.044396657, 0.042157084, 0.055401225, 0.058600187, 0.05215517, 0.039261613, 0.041907873, 0.047358576, 0.040151726, 0.06802231, 0.047436047, 0.09909152, 0.28401926, 0.13024966, 0.20557638, 0.12616827, 0.123542055, 0.09049112, 0.115080245, 0.14971574, 0.12118945, 0.06075804, 0.06987876, 0.04896166, 0.09044901, 0.075676896, 0.05960615, 0.04691121, 0.069199614, 0.06200658, 0.09054413, 0.061500262, 0.070369266, 0.04332112, 0.058488432, 0.064093255, 0.04958421, 0.048470568, 0.053110573, 0.03550574, 0.06766866, 0.047127903, 0.039211858, 0.06709621, 0.06858189, 0.044123348, 0.055773523, 0.060662974, 0.04483639, 0.13242607, 0.1355275, 0.13983408, 0.12258148, 0.20275135, 0.15931402, 0.16948499, 0.10481699, 0.09122547, 0.08932665, 0.11112498, 0.09145718, 0.068701245, 0.14090128, 0.1295471, 0.07260077, 0.06418751, 0.09539796, 0.12459203, 0.10786876, 0.07792016, 0.06362716, 0.06616065, 0.064760745, 0.062519014, 0.071237415, 0.05043132, 0.050296724, 0.05345165, 0.0477586, 0.05220561, 0.062953524, 0.05555552, 0.06251833, 0.04688878, 0.03949033, 0.055295497, 0.03436099, 0.07842078, 0.11168355, 0.16150439, 0.11113813, 0.13163221, 0.10600028, 0.09219203, 0.10392354, 0.07452253, 0.1416727, 0.079015814, 0.06271552, 0.04144072, 0.10382621, 0.06299978, 0.05766212, 0.07620008, 0.058304574, 0.05188469, 0.08506262, 0.06702578, 0.04526988, 0.05202606, 0.041656245, 0.051310472, 0.059838403, 0.04444167, 0.0848546, 0.07195505, 0.049366537, 0.050186742, 0.06873556, 0.07668439, 0.077399656, 0.044006556, 0.063412756, 0.042457707, 0.0732187, 0.09295609, 0.24262165, 0.3011674, 0.17326924, 0.11037847, 0.099417746, 0.09674674, 0.113762766, 0.12090467, 0.09172399, 0.06199426, 0.09702033, 0.09986873, 0.071002625, 0.057124265, 0.06719436, 0.04985375, 0.047898848, 0.041028988, 0.068097, 0.05505858, 0.0809998, 0.052954327, 0.08158082, 0.06270212, 0.06721012, 0.05861794, 0.07210564, 0.049818225, 0.048037883, 0.06030619, 0.06353309, 0.074782096, 0.04065475, 0.054036718, 0.071171924, 0.045203783, 0.049189594, 0.12621579, 0.19445889, 0.28884172, 0.22236453, 0.106037065, 0.08837745, 0.11859806, 0.1374938, 0.17949152, 0.15473746, 0.0692585, 0.05023375, 0.1080353, 0.06650659, 0.09087671, 0.08894222, 0.069169946, 0.097741164, 0.064683266, 0.058298837, 0.07245965, 0.11948074, 0.06449974, 0.08323661, 0.053089626, 0.055062935, 0.046120986, 0.04460639, 0.05544052, 0.0706666, 0.039168544, 0.061895873, 0.04797404, 0.058761213, 0.050117187, 0.04893174, 0.044989515, 0.06591346, 0.048067577, 0.09874802, 0.040054277, 0.06793888, 0.04119965, 0.1322604, 0.15692692, 0.08536512, 0.180383, 0.15782733, 0.09894627, 0.05668511, 0.13024585, 0.08580453, 0.14448154, 0.09949905, 0.12771533, 0.13513844, 0.072863065, 0.078917325, 0.061876014, 0.071173094, 0.09491198, 0.079086795, 0.067660205, 0.054871753, 0.067200825, 0.070701376, 0.0560588, 0.061810922, 0.056803703, 0.069250196, 0.050307322, 0.053600486, 0.046804227, 0.054982845, 0.047665786, 0.05333157, 0.043772686, 0.04586956, 0.06369012, 0.060978312, 0.034420658, 0.0797262, 0.099511676, 0.16004318, 0.14299521, 0.15803179, 0.12853642, 0.10606905, 0.11573788, 0.12576067, 0.06706724, 0.06837957, 0.07105236, 0.06423818, 0.07682827, 0.080098, 0.08397083, 0.048487294, 0.07509279, 0.03928537, 0.062289927, 0.060692318, 0.074876964, 0.05629469, 0.03288755, 0.054454137, 0.05897027, 0.052239887, 0.069438316, 0.033145703, 0.049175855, 0.054618083, 0.04026442, 0.052365255, 0.051331375, 0.05914559, 0.045415465, 0.051960934, 0.049040172, 0.119257875, 0.076399215, 0.13467035, 0.2108862, 0.31481436, 0.09531068, 0.12667987, 0.08209411, 0.10836845, 0.08047429, 0.10423319, 0.07234106, 0.094601505, 0.05467677, 0.07276347, 0.07373208, 0.05895299, 0.07027617, 0.05621328, 0.07379972, 0.040539503, 0.04759285, 0.05243427, 0.06345777, 0.056222465, 0.067320295, 0.053634405, 0.040898632, 0.084051535, 0.0595346, 0.066143095, 0.054195378, 0.06002711, 0.05492142, 0.037517413, 0.06840995, 0.06449527, 0.052025866, 0.058235537, 0.18357036, 0.15213162, 0.2650012, 0.22339888, 0.11753946, 0.07193565, 0.08217673, 0.105740935, 0.07929926, 0.087829925, 0.05724447, 0.06013305, 0.05437997, 0.063656956, 0.07313622, 0.08264836, 0.097219415, 0.036759105, 0.05132611, 0.065888956, 0.081944495, 0.04031761, 0.047973163, 0.046094764, 0.05339552, 0.0794405, 0.08388439, 0.066524245, 0.06361489, 0.04956405, 0.053640805, 0.038987536, 0.037272044, 0.03539841, 0.04642518, 0.035164427, 0.042823162, 0.07084378, 0.21847832, 0.16041665, 0.13382097, 0.13102582, 0.11211147, 0.117191054, 0.10814796, 0.07798165, 0.112743154, 0.084242985, 0.084425345, 0.07037457, 0.057246476, 0.06421351, 0.058929574, 0.093657695, 0.06760004, 0.06196675, 0.08270887, 0.08643621, 0.06065408, 0.05493893, 0.040288024, 0.053667814, 0.041280538, 0.049093083, 0.04933043, 0.074212134, 0.080090396, 0.06909325, 0.08127516, 0.061681464, 0.062959924, 0.05409877, 0.049444336, 0.03140693, 0.04474255, 0.047300786, 0.038569372, 0.064234816, 0.033768307, 0.05405441, 0.04645834, 0.11710411, 0.3431077, 0.22583991, 0.22763634, 0.10778233, 0.14123379, 0.09658405, 0.0955237, 0.16309345, 0.09854625, 0.0801883, 0.053096652, 0.0718866, 0.08405431, 0.053438853, 0.0529243, 0.06496746, 0.09655529, 0.06886945, 0.05969197, 0.031935923, 0.05183326, 0.05368827, 0.040809292, 0.065975845, 0.06508035, 0.041491278, 0.050518245, 0.0814479, 0.035983533, 0.042352717, 0.06586322, 0.030456895, 0.037459645, 0.051034242, 0.0606946, 0.037004862, 0.065193124, 0.15399195, 0.1385718, 0.1361353, 0.13059123, 0.08123424, 0.110958174, 0.07928639, 0.10599214, 0.12743627, 0.099062346, 0.07693567, 0.09402562, 0.07246459, 0.06330125, 0.08822293, 0.04593888, 0.05133918, 0.05134933, 0.07402516, 0.044563904, 0.073825456, 0.10782319, 0.08053856, 0.06159466, 0.058775157, 0.06703574, 0.08336375, 0.036383346, 0.03701056, 0.043532073, 0.06955203, 0.049363643, 0.03513438, 0.07043338, 0.05539871, 0.059016164, 0.06524159, 0.05501849, 0.09654122, 0.1684779, 0.16489597, 0.13243735, 0.079862535, 0.14318296, 0.1791227, 0.13846372, 0.06457684, 0.08350849, 0.06484138, 0.061706323, 0.09296626, 0.10927021, 0.08809631, 0.08135798, 0.051186025, 0.051748365, 0.0596418, 0.04451409, 0.055883598, 0.0684717, 0.038031925, 0.03840705, 0.06351121, 0.06147162, 0.06666751, 0.060291152, 0.047846865, 0.041452277, 0.054718338, 0.04362606, 0.06313643, 0.047415607, 0.06732252, 0.052515317, 0.06637748, 0.054530192, 0.1503591, 0.15150726, 0.123267666, 0.15005857, 0.1286023, 0.05699896, 0.07084229, 0.09339088, 0.1048576, 0.09648221, 0.0602581, 0.12830824, 0.096870326, 0.09184475, 0.09454983, 0.08988318, 0.05039369, 0.060671944, 0.070849866, 0.050465725, 0.04670976, 0.051816158, 0.059072524, 0.071478106, 0.046983168, 0.04664789, 0.044412542, 0.043993723, 0.06573615, 0.032104805, 0.06652753, 0.09612594, 0.039150793, 0.046402887, 0.041941185, 0.06832394, 0.037425254, 0.04879229, 0.09599186, 0.19877188, 0.26798424, 0.33295467, 0.16861396, 0.06394082, 0.053892072, 0.07792392, 0.11575616, 0.120127276, 0.08886347, 0.12739645, 0.08903041, 0.07759828, 0.055926792, 0.048014227, 0.06368306, 0.054893218, 0.06483854, 0.056944657, 0.060524132, 0.048692685, 0.07342671, 0.060810033, 0.051715523, 0.05799002, 0.03236276, 0.049401794, 0.0640046, 0.054292567, 0.046111718, 0.054354988, 0.052199993, 0.055949043, 0.070303604, 0.055937666, 0.06152785, 0.059942648, 0.054320876, 0.065595336, 0.046528835, 0.06795311, 0.04144403, 0.09287668, 0.13287167, 0.13278678, 0.15284717, 0.126686, 0.103822365, 0.101116106, 0.18802992, 0.14117698, 0.0867995, 0.08502638, 0.08939901, 0.093240835, 0.073715985, 0.06532566, 0.049502086, 0.054716706, 0.056131493, 0.058020443, 0.051026277, 0.06950198, 0.07130532, 0.06838038, 0.039051358, 0.07501247, 0.06448835, 0.039639592, 0.046196483, 0.045366853, 0.045505192, 0.05436205, 0.058214832, 0.054252535, 0.05399086, 0.053407036, 0.057098363, 0.077002905, 0.048535418, 0.25447014, 0.18455003, 0.16696091, 0.08815444, 0.13556503, 0.079550445, 0.12569551, 0.14183621, 0.11053232, 0.077626646, 0.08681654, 0.04357657, 0.052626777, 0.08452865, 0.06887701, 0.10781405, 0.06927303, 0.058247063, 0.056751966, 0.072908096, 0.08256683, 0.048408017, 0.04689728, 0.044176973, 0.048022296, 0.03905183, 0.055247657, 0.051137783, 0.049734186, 0.06084661, 0.034691785, 0.04874303, 0.051608156, 0.054559726, 0.034709737, 0.0519996, 0.04409315, 0.053818356, 0.1089912, 0.16489159, 0.11371536, 0.12469013, 0.23766708, 0.13235107, 0.09323767, 0.17107886, 0.10737447, 0.088052854, 0.058952376, 0.08987506, 0.056764722, 0.07889613, 0.083193436, 0.062992476, 0.06789681, 0.079084955, 0.060939487, 0.05140364, 0.07294949, 0.06827374, 0.04714524, 0.054618247, 0.037733667, 0.05108133, 0.04931689, 0.05600637, 0.053306945, 0.092099026, 0.06857413, 0.05778686, 0.04187197, 0.057133324, 0.049679898, 0.044121455, 0.067191295, 0.063487545, 0.117217764, 0.15101314, 0.16273192, 0.22341146, 0.16488911, 0.11487839, 0.1488034, 0.12060794, 0.095709346, 0.077059165, 0.05082473, 0.07744298, 0.05366419, 0.080361694, 0.08187613, 0.06600616, 0.09689413, 0.06816781, 0.093985535, 0.07718867, 0.06093856, 0.073994815, 0.047249243, 0.087645605, 0.083633825, 0.05037812, 0.039433092, 0.037161943, 0.037363064, 0.06331327, 0.039449256, 0.043099534, 0.051421605, 0.067398906, 0.048655294, 0.045688044, 0.059620775, 0.045925774, 0.10484529, 0.10848027, 0.1559646, 0.2088438, 0.16600919, 0.08982535, 0.1034725, 0.07119136, 0.1377933, 0.10585066, 0.12776884, 0.10084647, 0.050459936, 0.07375225, 0.08836285, 0.09205859, 0.07027171, 0.044266596, 0.06584636, 0.045727987, 0.06560177, 0.0613681, 0.0574955, 0.054423, 0.0536878, 0.06392488, 0.044525705, 0.05108541, 0.030784497, 0.038470674, 0.052028827, 0.06264421, 0.033939525, 0.06708552, 0.06253693, 0.0467615, 0.059258293, 0.044349115, 0.05125774, 0.04602947, 0.05069634, 0.059331242, 0.052266877, 0.048993696, 0.08873792, 0.10781647, 0.1932219, 0.11044068, 0.11481546, 0.078927316, 0.101953566, 0.08732467, 0.0666481, 0.10744486, 0.051592793, 0.11085296, 0.07723931, 0.05699717, 0.09156368, 0.06518179, 0.055018187, 0.06854121, 0.05816809, 0.072274856, 0.07058782, 0.04812162, 0.057641223, 0.06844325, 0.045692023, 0.044147912, 0.03520856, 0.074018024, 0.05142414, 0.079647124, 0.035287987, 0.042019892, 0.06441683, 0.056680623, 0.050983626, 0.046357512, 0.041874934, 0.043322098, 0.15132797, 0.10765915, 0.16313191, 0.12877111, 0.088684164, 0.102554895, 0.13477583, 0.13433433, 0.12741871, 0.09579011, 0.135658, 0.07618098, 0.07251048, 0.090510316, 0.07949284, 0.071082264, 0.061644245, 0.07458774, 0.06054903, 0.058414698, 0.04868973, 0.059702285, 0.061541595, 0.071572855, 0.04441467, 0.06966752, 0.059984952, 0.09597657, 0.04711919, 0.05758504, 0.04965832, 0.05066922, 0.06876149, 0.040089842, 0.04889964, 0.0675258, 0.05186834, 0.024921736, 0.11574528, 0.14677681, 0.094985165, 0.1000359, 0.10647795, 0.08947387, 0.08938347, 0.11725895, 0.07860956, 0.09685973, 0.08869896, 0.06289399, 0.05312438, 0.047337294, 0.06970833, 0.080185555, 0.047177184, 0.052225996, 0.053810522, 0.049050815, 0.06185541, 0.048016712, 0.06426577, 0.062151838, 0.07840179, 0.036838625, 0.06032263, 0.0592136, 0.04519677, 0.04775895, 0.032598544, 0.042862233, 0.049634464, 0.042723224, 0.05613381, 0.060879525, 0.03911638, 0.060608972, 0.13436516, 0.13997239, 0.12830102, 0.20494604, 0.10537972, 0.06906763, 0.08275014, 0.13325498, 0.115048945, 0.10341859, 0.14285886, 0.11798368, 0.080440864, 0.053610932, 0.0654123, 0.08625429, 0.059985474, 0.063792385, 0.043499112, 0.0522891, 0.053671572, 0.05866335, 0.047332816, 0.059347767, 0.062942356, 0.04820298, 0.0402972, 0.050866485, 0.043929677, 0.04989353, 0.037122104, 0.04491627, 0.051430747, 0.028842308, 0.06254983, 0.03701145, 0.044738214, 0.05927801, 0.11935705, 0.11183035, 0.08591872, 0.16041888, 0.12728441, 0.09925958, 0.09954971, 0.07586865, 0.107187845, 0.060165405, 0.07084857, 0.054552197, 0.07814932, 0.052548807, 0.06215299, 0.060365733, 0.049637347, 0.06826284, 0.07408466, 0.07931071, 0.059678555, 0.06151045, 0.029803405, 0.048043936, 0.041219987, 0.04126345, 0.05253609, 0.041830324, 0.040005755, 0.061570827, 0.05247945, 0.047165677, 0.042160507, 0.064835764, 0.053631168, 0.03491299, 0.058202937, 0.03978118, 0.10292122, 0.16076668, 0.13818611, 0.15528482, 0.094745226, 0.07616263, 0.07983113, 0.048976243, 0.09723905, 0.14821637, 0.10094902, 0.07770131, 0.053324863, 0.06690333, 0.07017275, 0.05395584, 0.057916835, 0.040154707, 0.05987612, 0.06688612, 0.053206984, 0.053173736, 0.057109468, 0.07187734, 0.05397295, 0.0646079, 0.06506922, 0.055626344, 0.044884156, 0.031191358, 0.047073133, 0.041535646, 0.07202398, 0.06216496, 0.051215142, 0.056732737, 0.038367834, 0.053313605, 0.11306354, 0.1015057, 0.13351142, 0.114188306, 0.077227995, 0.11037075, 0.0837145, 0.078203045, 0.092520975, 0.070819594, 0.10628838, 0.092741765, 0.07446583, 0.09143581, 0.10058339, 0.08391425, 0.060979635, 0.070185184, 0.057585414, 0.10151083, 0.09977139, 0.07763022, 0.06358039, 0.033769436, 0.055510312, 0.051205177, 0.055593006, 0.05592781, 0.05408324, 0.044442073, 0.035546504, 0.059573743, 0.07400937, 0.05720664, 0.03164664, 0.045749594, 0.042791758, 0.03850425, 0.18945977, 0.12999043, 0.116124615, 0.118047036, 0.081402995, 0.11246911, 0.16332254, 0.07738342, 0.08371744, 0.08101799, 0.08238886, 0.09084926, 0.07457952, 0.06921185, 0.05784147, 0.048382595, 0.045146834, 0.060290903, 0.07312588, 0.04369328, 0.036573898, 0.05650691, 0.051009957, 0.0412297, 0.04936303, 0.038059447, 0.06604567, 0.048160896, 0.05092031, 0.03571706, 0.05731665, 0.032828808, 0.042152595, 0.037535444, 0.038214337, 0.044513177, 0.05174913, 0.044042233, 0.06436887, 0.110895395, 0.09783788, 0.18036625, 0.081207626, 0.08549291, 0.15237945, 0.11560389, 0.06102624, 0.101691864, 0.04772532, 0.095065676, 0.09520742, 0.05731107, 0.09667698, 0.05321125, 0.047169954, 0.040116984, 0.05010511, 0.056668565, 0.06349189, 0.03524493, 0.055348206, 0.047499575, 0.049782645, 0.034340665, 0.070347324, 0.04533227, 0.055241786, 0.05253715, 0.037621927, 0.044673707, 0.029929211, 0.036897633, 0.0662019, 0.056533575, 0.03868968, 0.024684878, 0.099971235, 0.10243484, 0.10135011, 0.08751475, 0.08530498, 0.09734451, 0.12437791, 0.069865495, 0.0932215, 0.10556001, 0.054215543, 0.06504073, 0.0913028, 0.077905, 0.074251436, 0.07372267, 0.037577555, 0.053856205, 0.05541866, 0.062490374, 0.058710016, 0.03658132, 0.04652451, 0.04444091, 0.052668836, 0.059177965, 0.065039836, 0.04085798, 0.058866117, 0.049327355, 0.05137584, 0.06325242, 0.03215143, 0.056838136, 0.045386285, 0.044029642, 0.040503588, 0.05101754, 0.056538846, 0.04609223, 0.03463311, 0.04744026, 0.035953157, 0.10241046, 0.096339196, 0.07687678, 0.08139196, 0.12157875, 0.25228015, 0.1922901, 0.087468944, 0.0791744, 0.17474394, 0.12338622, 0.047186416, 0.07405345, 0.10078854, 0.07875227, 0.08044298, 0.068903185, 0.06035769, 0.04842886, 0.052815363, 0.04300086, 0.043008484, 0.046864778, 0.037879936, 0.0376682, 0.07283272, 0.039251313, 0.06376143, 0.052764002, 0.04147002, 0.05109806, 0.04582204, 0.058772556, 0.03726351, 0.04889962, 0.050038088, 0.05010154, 0.04852037, 0.09652454, 0.102629274, 0.19292328, 0.19116563, 0.08492776, 0.13625683, 0.18935363, 0.09870801, 0.13914663, 0.08144719, 0.058765046, 0.075554214, 0.09430766, 0.06315022, 0.08593526, 0.12735294, 0.078422956, 0.038203813, 0.086128615, 0.061158445, 0.044375114, 0.049235355, 0.042591702, 0.048241977, 0.0526011, 0.047115386, 0.05732839, 0.05810447, 0.030385755, 0.037986487, 0.0356107, 0.05504156, 0.031550564, 0.063340396, 0.04641452, 0.044328094, 0.056050826, 0.044214852, 0.1265941, 0.25749308, 0.23281498, 0.11239195, 0.12201104, 0.15520601, 0.09099802, 0.103430726, 0.08041214, 0.100697584, 0.064005114, 0.067064315, 0.069249846, 0.070496894, 0.0610551, 0.04660801, 0.051853586, 0.0682149, 0.1021172, 0.05311845, 0.032127652, 0.038838692, 0.04968116, 0.039251953, 0.051717598, 0.058087587, 0.06461395, 0.05372494, 0.075078376, 0.049098574, 0.03549754, 0.04358514, 0.021146676, 0.06720098, 0.04392864, 0.037537985, 0.03564323, 0.039303955, 0.18533368, 0.34378514, 0.18574773, 0.1304026, 0.17528522, 0.0671739, 0.075815104, 0.11614497, 0.15747875, 0.059963234, 0.088987, 0.08566218, 0.058788106, 0.11324864, 0.10267732, 0.06342875, 0.05557987, 0.05868998, 0.062027905, 0.06433706, 0.05097227, 0.040852014, 0.049733885, 0.032827374, 0.06379327, 0.055759594, 0.05369301, 0.03336663, 0.031173801, 0.041938256, 0.064529724, 0.05047827, 0.040697638, 0.04157081, 0.04406459, 0.062722385, 0.047361445, 0.035667412, 0.14580937, 0.11914512, 0.23248596, 0.2868265, 0.16764049, 0.07326938, 0.08319161, 0.07071142, 0.08953097, 0.1162469, 0.083973765, 0.06842543, 0.107885584, 0.084934525, 0.08544334, 0.057178948, 0.06248298, 0.06162979, 0.045912772, 0.05244218, 0.06867149, 0.071145, 0.05576431, 0.03508474, 0.038369615, 0.057008293, 0.05747076, 0.045559455, 0.067282535, 0.06367062, 0.0623947, 0.03832675, 0.037859987, 0.07061647, 0.05642955, 0.048173063, 0.03969588, 0.051350493, 0.03386957, 0.047318503, 0.047469042, 0.050546546, 0.047656786, 0.031080747, 0.12015621, 0.09434033, 0.12393019, 0.1602022, 0.04406936, 0.065307096, 0.07402046, 0.12847254, 0.06316404, 0.053920727, 0.103005506, 0.092411585, 0.060777556, 0.07090422, 0.05247663, 0.055591136, 0.07739394, 0.061425604, 0.061678316, 0.054902278, 0.05989504, 0.045614887, 0.084265895, 0.041108184, 0.07251424, 0.040679567, 0.051509574, 0.05159434, 0.06098084, 0.06771403, 0.051227458, 0.044704385, 0.03461359, 0.043118123, 0.03229179, 0.045294702, 0.046312932, 0.042648654, 0.11284962, 0.15559645, 0.21528457, 0.10514777, 0.06012491, 0.11263545, 0.082605444, 0.09778713, 0.08698211, 0.06873388, 0.08453274, 0.08475364, 0.08150298, 0.080482535, 0.055017482, 0.061460108, 0.046696957, 0.04591625, 0.069024205, 0.03865813, 0.044372734, 0.072786406, 0.044480402, 0.06507451, 0.056251857, 0.050817773, 0.05000632, 0.04553633, 0.045384195, 0.059713032, 0.045606606, 0.04482383, 0.044827342, 0.04614691, 0.04057802, 0.03832917, 0.05839102, 0.05510903, 0.085614145, 0.110539936, 0.08983124, 0.13096346, 0.10534457, 0.113378905, 0.083804734, 0.086476825, 0.13767922, 0.087932155, 0.037873298, 0.057262026, 0.08015702, 0.07685086, 0.08136279, 0.07148879, 0.053772558, 0.07269688, 0.057659086, 0.049444247, 0.055249974, 0.05771395, 0.058252603, 0.055493418, 0.046499528, 0.0664349, 0.030865127, 0.040590882, 0.047698747, 0.06607876, 0.066799134, 0.058426525, 0.047870804, 0.052916456, 0.047296617, 0.0619891, 0.032221638, 0.03312474, 0.12041389, 0.105160415, 0.22300605, 0.18003374, 0.12557888, 0.108831994, 0.1320708, 0.10915371, 0.120593496, 0.10050224, 0.047276855, 0.075138934, 0.057355136, 0.055019606, 0.043644384, 0.04534374, 0.074938275, 0.0371454, 0.054365296, 0.057380702, 0.06816438, 0.058947913, 0.05675282, 0.055856, 0.032976195, 0.037219815, 0.06123818, 0.060137738, 0.06783048, 0.03905525, 0.03964566, 0.046494186, 0.043989785, 0.028218463, 0.04013393, 0.04106432, 0.03811918, 0.048116434, 0.1751892, 0.14569373, 0.18766007, 0.117088854, 0.06828256, 0.12314582, 0.14666215, 0.103710346, 0.08907363, 0.0974734, 0.07479698, 0.07187794, 0.06903177, 0.07761615, 0.063307524, 0.05775985, 0.058991507, 0.05760117, 0.08798821, 0.04249357, 0.060576137, 0.05439488, 0.06368598, 0.050473437, 0.08153889, 0.070583485, 0.055622146, 0.034226593, 0.036390986, 0.049306072, 0.047883607, 0.05517649, 0.045830622, 0.038462024, 0.041009583, 0.044949066, 0.027407957, 0.05774178, 0.053200033, 0.056647576, 0.041754324, 0.06442222, 0.03942131, 0.10356393, 0.19584711, 0.24159043, 0.29867324, 0.163741, 0.15031143, 0.10004779, 0.119438864, 0.043008827, 0.075275235, 0.086125076, 0.052431155, 0.057363328, 0.057335924, 0.061077055, 0.0699249, 0.038307548, 0.05400367, 0.043025482, 0.05501175, 0.05872601, 0.03395971, 0.040782694, 0.08121854, 0.044023942, 0.047357865, 0.041514885, 0.033660874, 0.048795745, 0.037660655, 0.040862516, 0.043885972, 0.028452108, 0.042070318, 0.0551369, 0.047624853, 0.05265854, 0.026052753, 0.117142424, 0.095204525, 0.09391865, 0.11311339, 0.15686662, 0.19347249, 0.10741514, 0.096883275, 0.08963304, 0.05540116, 0.056254197, 0.08169395, 0.06972935, 0.04865097, 0.094652295, 0.062211286, 0.057795614, 0.09205908, 0.05792849, 0.036381744, 0.08120682, 0.08389131, 0.057576757, 0.091570504, 0.054077964, 0.03257024, 0.04063595, 0.03979874, 0.04848465, 0.040444832, 0.04620372, 0.05228814, 0.026931744, 0.046664264, 0.046051282, 0.048740905, 0.039881557, 0.045427855, 0.18541294, 0.14567077, 0.12389651, 0.15997969, 0.12363003, 0.07295033, 0.10108507, 0.08579093, 0.049841534, 0.09517877, 0.08971446, 0.07126415, 0.07313806, 0.14206447, 0.072524615, 0.03845821, 0.034028564, 0.06077683, 0.046297085, 0.046871703, 0.056249756, 0.030555788, 0.046842124, 0.04817441, 0.053789854, 0.044212043, 0.047255944, 0.054981653, 0.05158251, 0.03225283, 0.09198564, 0.053773347, 0.045763317, 0.04858116, 0.075268924, 0.04115465, 0.04859734, 0.031259365, 0.09412591, 0.124444775, 0.13097936, 0.17599694, 0.18984607, 0.07479608, 0.111192554, 0.06244578, 0.10816046, 0.07922502, 0.10131532, 0.10856177, 0.058133394, 0.07401278, 0.06724839, 0.06808152, 0.069955744, 0.047022115, 0.06796545, 0.06826509, 0.04570642, 0.047198858, 0.081624106, 0.0532843, 0.047771122, 0.05770798, 0.055143565, 0.06281579, 0.060326345, 0.060432937, 0.04149039, 0.046946865, 0.039964136, 0.034781013, 0.0391726, 0.04943119, 0.04802244, 0.029342467, 0.15394375, 0.11185652, 0.09797137, 0.1407831, 0.11840336, 0.23589894, 0.12356674, 0.050718524, 0.10489417, 0.07290534, 0.09565983, 0.08110037, 0.083298944, 0.055536997, 0.060447086, 0.05063221, 0.06387833, 0.049630214, 0.059762157, 0.04952224, 0.04396999, 0.04043503, 0.074971415, 0.032424938, 0.04604724, 0.07758992, 0.04966217, 0.043398235, 0.068517864, 0.047204763, 0.072715074, 0.037952468, 0.043363374, 0.06419665, 0.043385286, 0.05254476, 0.056159228, 0.0518268, 0.050027683, 0.04659667, 0.042469744, 0.049641322, 0.034756485, 0.03718676, 0.10849793, 0.20739424, 0.19714035, 0.11082473, 0.07709668, 0.054443352, 0.10031376, 0.07799463, 0.11375589, 0.043870937, 0.06988308, 0.09656055, 0.066687725, 0.05514318, 0.09264065, 0.0687739, 0.037885614, 0.070746474, 0.08381855, 0.05512905, 0.042445317, 0.048298817, 0.04080866, 0.08983662, 0.033137217, 0.03533107, 0.0342601, 0.04290377, 0.033369392, 0.052445184, 0.045732494, 0.038153734, 0.031807072, 0.033560153, 0.028651256, 0.05572563, 0.040344734, 0.042006623, 0.13277884, 0.19633593, 0.17760615, 0.11188649, 0.123868115, 0.13705546, 0.08727406, 0.08764401, 0.112152725, 0.06963555, 0.06075892, 0.06435751, 0.06985896, 0.09948417, 0.045740925, 0.06928656, 0.042567622, 0.0583148, 0.06998352, 0.084795594, 0.058396507, 0.043260913, 0.061082114, 0.046211068, 0.05750242, 0.062656894, 0.053871483, 0.061659988, 0.031696737, 0.033996295, 0.059480667, 0.040028322, 0.041990582, 0.046102297, 0.055674065, 0.03129985, 0.0594243, 0.050449755, 0.1639536, 0.112359725, 0.13415845, 0.1386984, 0.09001634, 0.07245797, 0.069074996, 0.115487225, 0.10492385, 0.10419227, 0.075003006, 0.068468764, 0.06803049, 0.0687172, 0.12731381, 0.07121656, 0.05149383, 0.046165895, 0.047423694, 0.05990033, 0.05502079, 0.06975068, 0.053659916, 0.038250003, 0.05783635, 0.050688647, 0.039751023, 0.060830843, 0.073674396, 0.045041397, 0.043591965, 0.059239794, 0.04935364, 0.04525742, 0.04815836, 0.054305773, 0.046356525, 0.039770603, 0.12898785, 0.1299777, 0.09912294, 0.1543223, 0.09219578, 0.07470627, 0.056825686, 0.080025084, 0.071871184, 0.047061015, 0.070456855, 0.08819078, 0.08270385, 0.051190518, 0.07371838, 0.0597693, 0.07197026, 0.08301037, 0.065518096, 0.076078534, 0.053260658, 0.06786773, 0.04432309, 0.04212339, 0.043983046, 0.04964646, 0.041487776, 0.0599597, 0.04857269, 0.04666874, 0.056855608, 0.026399676, 0.030333973, 0.054595184, 0.03320022, 0.03168707, 0.033911284, 0.042468935, 0.09346779, 0.08409073, 0.11394703, 0.2076927, 0.05335653, 0.111324735, 0.045915514, 0.104935855, 0.08018198, 0.13187967, 0.10168296, 0.07868333, 0.10414181, 0.0565542, 0.05366717, 0.048221104, 0.05236769, 0.062877454, 0.03990562, 0.058621664, 0.063838795, 0.056832056, 0.060749248, 0.046799213, 0.044898562, 0.06767323, 0.052417457, 0.043408018, 0.06718982, 0.040697966, 0.049073137, 0.07402599, 0.04641924, 0.03766935, 0.051570047, 0.04437759, 0.046934377, 0.044857565, 0.06936856, 0.029639276, 0.057118252, 0.04866425, 0.036843967, 0.0843922, 0.12882544, 0.09479203, 0.09770481, 0.1854165, 0.11205391, 0.059927903, 0.060110122, 0.11377881, 0.0753195, 0.044199932, 0.08223151, 0.11556373, 0.061471056, 0.054986138, 0.045029074, 0.05341022, 0.057537686, 0.051327933, 0.056916364, 0.07835416, 0.08675005, 0.039674174, 0.059762526, 0.06781669, 0.0482807, 0.046755604, 0.046361033, 0.037306618, 0.054670736, 0.03515952, 0.039129227, 0.040424634, 0.030730028, 0.04748615, 0.049093965, 0.057900358, 0.04911111, 0.2262841, 0.13258792, 0.10257399, 0.10888543, 0.098286234, 0.14019914, 0.07854485, 0.08853089, 0.11559135, 0.11368506, 0.04877276, 0.04520533, 0.08647178, 0.10927222, 0.057969343, 0.05173349, 0.04533579, 0.05947819, 0.08070596, 0.0681091, 0.060875002, 0.05856952, 0.031943887, 0.03824885, 0.055953033, 0.055682465, 0.08556276, 0.03632923, 0.04675246, 0.05932908, 0.063327, 0.045214683, 0.046019882, 0.03192082, 0.055040408, 0.044109058, 0.0360149, 0.026279714, 0.06404, 0.1458776, 0.10375758, 0.12508936, 0.2050296, 0.13451946, 0.097800866, 0.12486806, 0.08615435, 0.06381577, 0.050049335, 0.063423954, 0.039506115, 0.07408827, 0.0782121, 0.07219624, 0.044455703, 0.052781772, 0.08326437, 0.04545213, 0.052832704, 0.03663076, 0.043291796, 0.060678802, 0.046459984, 0.063643694, 0.065171845, 0.036228552, 0.023329703, 0.04478577, 0.07328585, 0.04779384, 0.04637658, 0.04795306, 0.062115543, 0.045348953, 0.045256875, 0.05032064, 0.13054629, 0.14108065, 0.114176765, 0.0954892, 0.2713439, 0.097880095, 0.12362838, 0.08779882, 0.12961493, 0.09697279, 0.079617016, 0.056443088, 0.072154135, 0.06524361, 0.091470055, 0.065494545, 0.06815547, 0.05595957, 0.048280157, 0.04783723, 0.049087524, 0.059678584, 0.02836026, 0.07657633, 0.0565759, 0.051511057, 0.043162946, 0.036992144, 0.044510726, 0.0714287, 0.04362563, 0.034453597, 0.037144613, 0.043925278, 0.04016288, 0.042626347, 0.033430766, 0.037904855, 0.18203506, 0.24113922, 0.17188898, 0.2986337, 0.12161847, 0.16551113, 0.15682445, 0.10337704, 0.123745754, 0.05544415, 0.046324547, 0.08865011, 0.06867219, 0.050393865, 0.060311865, 0.089551754, 0.068980366, 0.050913684, 0.0442785, 0.043593843, 0.02596112, 0.05218206, 0.038803484, 0.0647746, 0.07291696, 0.08204597, 0.052319914, 0.068237245, 0.03571066, 0.049454663, 0.04391864, 0.0692745, 0.044657305, 0.039823297, 0.02629796, 0.05097564, 0.029753905, 0.034362722, 0.039684538, 0.043109685, 0.047865853, 0.046165638, 0.03834625, 0.06282411, 0.0959069, 0.105568595, 0.13853283, 0.18733822, 0.07138735, 0.06770227, 0.086922005, 0.06999419, 0.09578845, 0.11430901, 0.07526243, 0.048758503, 0.057202246, 0.06154308, 0.042807147, 0.06944104, 0.0579174, 0.044348393, 0.06737902, 0.03494062, 0.05656734, 0.03598274, 0.058855165, 0.050033264, 0.03037533, 0.059464674, 0.04461893, 0.042436548, 0.03217711, 0.04783993, 0.029884692, 0.040767144, 0.060297877, 0.053476702, 0.039876845, 0.038098037, 0.055568345, 0.06842213, 0.11119232, 0.21590729, 0.18832906, 0.07801004, 0.08328071, 0.10509545, 0.21401262, 0.08120566, 0.050879717, 0.06859806, 0.04333952, 0.055050243, 0.07868532, 0.05019219, 0.05091127, 0.05993843, 0.07017174, 0.0748201, 0.05594352, 0.04696855, 0.058916677, 0.04418583, 0.042126533, 0.0476984, 0.061694887, 0.06062548, 0.0588404, 0.046152093, 0.026719382, 0.049729876, 0.07648402, 0.053739835, 0.07231154, 0.049960762, 0.04433728, 0.028897787, 0.03308509, 0.048970312, 0.14717205, 0.16482697, 0.21347369, 0.18560779, 0.07334535, 0.05004181, 0.122166984, 0.12930992, 0.081289016, 0.05646267, 0.0872252, 0.10298377, 0.10959004, 0.10245455, 0.07776131, 0.053365577, 0.05799771, 0.052517503, 0.05940044, 0.045742013, 0.06327586, 0.08051085, 0.053860426, 0.048245896, 0.056316778, 0.061679814, 0.03234341, 0.070814185, 0.0519238, 0.040787887, 0.043890093, 0.06023857, 0.044358604, 0.049782526, 0.034799542, 0.06288438, 0.05022837, 0.037583813, 0.074077174, 0.092565514, 0.09893924, 0.15892957, 0.15593359, 0.101994954, 0.064246, 0.1140986, 0.055448424, 0.1361262, 0.067368746, 0.038024932, 0.058854505, 0.06657521, 0.07076419, 0.04487979, 0.05216618, 0.08919104, 0.06238885, 0.04595356, 0.04888005, 0.04969719, 0.05868754, 0.04942344, 0.048602995, 0.040092144, 0.034301974, 0.03452951, 0.02883443, 0.038171757, 0.046528738, 0.048369605, 0.046275586, 0.025223507, 0.047937065, 0.03330307, 0.03891894, 0.04224116, 0.14160776, 0.09548498, 0.19036485, 0.12693818, 0.11906022, 0.07858055, 0.06445696, 0.07997303, 0.12637995, 0.08837002, 0.060178977, 0.1018337, 0.08894553, 0.07716902, 0.09642553, 0.06129864, 0.063782334, 0.04292038, 0.057787895, 0.05357955, 0.06494145, 0.043422632, 0.03365569, 0.06568989, 0.06199407, 0.058097217, 0.030951276, 0.0408476, 0.06321188, 0.0401904, 0.053517368, 0.05239601, 0.030246878, 0.050251413, 0.07291546, 0.038257457, 0.036863606, 0.04561599, 0.038128167, 0.03746672, 0.03845791, 0.04402404, 0.039462756, 0.07444141, 0.1769328, 0.13401788, 0.11184804, 0.14222679, 0.10789022, 0.07886203, 0.10815824, 0.11406642, 0.09924767, 0.08522889, 0.1025826, 0.050287396, 0.07557429, 0.08771915, 0.07270612, 0.047133867, 0.063553415, 0.060568977, 0.039634835, 0.053548694, 0.04029057, 0.047755823, 0.054733247, 0.03835102, 0.050353028, 0.037486233, 0.074664555, 0.053674053, 0.04213889, 0.047342613, 0.034327902, 0.042965937, 0.029369973, 0.07652864, 0.05353644, 0.04562866, 0.049383476, 0.17086102, 0.1318852, 0.16004369, 0.08911021, 0.13232537, 0.058692098, 0.07816363, 0.08875514, 0.11789874, 0.061941423, 0.03638553, 0.04108785, 0.03655194, 0.07129977, 0.07401937, 0.14244264, 0.087670095, 0.0500876, 0.036501165, 0.058571585, 0.06272999, 0.04327479, 0.057955693, 0.043558832, 0.061321616, 0.042353526, 0.032999024, 0.045365494, 0.06430489, 0.06821012, 0.06259177, 0.059974607, 0.049517084, 0.052022837, 0.053274244, 0.046524826, 0.037136592, 0.049131405, 0.10223489, 0.16746941, 0.2217875, 0.13564517, 0.10257027, 0.065393746, 0.07528335, 0.12011561, 0.14000656, 0.057858117, 0.06186898, 0.04772101, 0.06671723, 0.108082235, 0.060792346, 0.046180185, 0.037257005, 0.06733348, 0.049025465, 0.05433819, 0.040529575, 0.036463235, 0.031903774, 0.049823895, 0.037196543, 0.057098344, 0.038107745, 0.03705173, 0.03590481, 0.027888978, 0.0657841, 0.028925108, 0.050905716, 0.04303073, 0.05614422, 0.052142486, 0.049942333, 0.049086064, 0.08801008, 0.1896183, 0.49238124, 0.17043804, 0.16606605, 0.06390278, 0.0986401, 0.07844806, 0.07025925, 0.12534241, 0.06990664, 0.073635645, 0.10986159, 0.07773541, 0.044481505, 0.059655618, 0.06410492, 0.043144446, 0.07635862, 0.05958645, 0.035407387, 0.05539758, 0.07078671, 0.058622085, 0.053866755, 0.05691667, 0.064453945, 0.05054307, 0.040575486, 0.039075572, 0.050342664, 0.050402243, 0.06360819, 0.03510452, 0.043791533, 0.03770702, 0.059611868, 0.033717226, 0.2335187, 0.20833853, 0.13779175, 0.11782041, 0.14882348, 0.07261406, 0.12324172, 0.089825, 0.06570854, 0.07712092, 0.06397586, 0.057199866, 0.056040406, 0.08116998, 0.060964346, 0.074397616, 0.04973789, 0.057953183, 0.047393948, 0.064134076, 0.050278794, 0.0558148, 0.06694677, 0.054562073, 0.055867184, 0.032829378, 0.050586194, 0.036106244, 0.068638906, 0.035651002, 0.053454828, 0.042072468, 0.046075728, 0.05505612, 0.0427589, 0.03861887, 0.038503353, 0.03871623, 0.038489718, 0.038523804, 0.036176972, 0.052950565, 0.046966046, 0.040149, 0.15449178, 0.13071004, 0.116338156, 0.10332035, 0.10401468, 0.13990265, 0.08637506, 0.10657643, 0.066115186, 0.11742572, 0.1834648, 0.08768192, 0.08106357, 0.051836357, 0.05108431, 0.06339301, 0.04549973, 0.056919347, 0.045008525, 0.08318766, 0.1175381, 0.08219948, 0.057423383, 0.05531509, 0.042963367, 0.043540306, 0.04977369, 0.033697963, 0.048258156, 0.04681265, 0.054352496, 0.051583074, 0.0315781, 0.033121616, 0.03946118, 0.037903477, 0.043686755, 0.034005716, 0.115090095, 0.109650575, 0.10073656, 0.15509665, 0.12671816, 0.12932992, 0.064775206, 0.0617512, 0.07176579, 0.14615043, 0.10938114, 0.04476832, 0.052345317, 0.043943416, 0.07254573, 0.067583606, 0.0524258, 0.057524808, 0.04808586, 0.04053548, 0.05321442, 0.06030344, 0.06543695, 0.040522587, 0.032695334, 0.032747693, 0.06902722, 0.038885206, 0.046968777, 0.038249627, 0.060959045, 0.046852145, 0.037644453, 0.0298213, 0.035197496, 0.035062198, 0.048997533, 0.03609712, 0.08257574, 0.10893712, 0.14453362, 0.19782433, 0.09767382, 0.08183222, 0.084603265, 0.08232237, 0.061011422, 0.07560459, 0.06631326, 0.06701721, 0.06833682, 0.093584076, 0.068624556, 0.05215765, 0.06739492, 0.044030875, 0.042939592, 0.087011136, 0.058597077, 0.06151778, 0.054665655, 0.046282694, 0.0409973, 0.073384784, 0.0293861, 0.051146314, 0.031962268, 0.057149492, 0.03748126, 0.033349168, 0.053016055, 0.053105224, 0.042161155, 0.055010088, 0.040271875, 0.03415884, 0.09287065, 0.10293685, 0.112417586, 0.08348248, 0.13156702, 0.08834858, 0.07829148, 0.064133756, 0.06319962, 0.1271965, 0.0670938, 0.053170316, 0.0702011, 0.05435365, 0.061780874, 0.07483753, 0.08551592, 0.0547235, 0.06662365, 0.04280313, 0.0449866, 0.054554656, 0.031738292, 0.03688339, 0.038198303, 0.038630094, 0.048464205, 0.054692425, 0.037499502, 0.052910298, 0.05185413, 0.039286807, 0.048940808, 0.07062176, 0.04221211, 0.032071706, 0.066064104, 0.051709898, 0.10901125, 0.11767963, 0.1307287, 0.15586424, 0.14265443, 0.10923586, 0.09398693, 0.09708482, 0.062947124, 0.2230733, 0.09638629, 0.11124631, 0.094947554, 0.05369741, 0.043079723, 0.051752497, 0.036310147, 0.05231412, 0.041090343, 0.042418607, 0.078609765, 0.047259003, 0.054034263, 0.047646075, 0.03772952, 0.058257233, 0.049302205, 0.034771275, 0.04305776, 0.025325919, 0.04561409, 0.037485152, 0.032829616, 0.065573, 0.027124172, 0.069128215, 0.044197883, 0.034088567, 0.033433493, 0.050236516, 0.030791286, 0.034444083, 0.050204974, 0.08651643, 0.24617647, 0.26249915, 0.11382644, 0.18298662, 0.08954114, 0.09873001, 0.09044128, 0.055268455, 0.089615114, 0.077355064, 0.049075466, 0.060227722, 0.06713, 0.10054391, 0.06776065, 0.04472832, 0.06528184, 0.065517254, 0.034661368, 0.06035601, 0.048341796, 0.054194525, 0.041800715, 0.043203544, 0.060722344, 0.061915718, 0.05315206, 0.06077504, 0.04685257, 0.034912206, 0.05317774, 0.03635001, 0.04901619, 0.04640342, 0.043467764, 0.040974665, 0.0528139, 0.13677227, 0.14241672, 0.10401194, 0.13767795, 0.10935531, 0.086859845, 0.06420312, 0.09566857, 0.07188346, 0.06561806, 0.058618773, 0.070893764, 0.06389695, 0.07504926, 0.056158055, 0.06398402, 0.06532142, 0.061843585, 0.04929693, 0.049833022, 0.048190866, 0.045712195, 0.05428512, 0.0553856, 0.059263136, 0.09546354, 0.06942404, 0.044866547, 0.05736583, 0.03010444, 0.042355567, 0.047463108, 0.062915966, 0.042793956, 0.036013797, 0.055086944, 0.038463615, 0.040966056, 0.09417633, 0.1759135, 0.14230023, 0.09042631, 0.13678758, 0.16884874, 0.074899785, 0.11947165, 0.083055995, 0.08832132, 0.08007139, 0.092163466, 0.04600104, 0.07574533, 0.095397875, 0.068604276, 0.05372646, 0.038302884, 0.048707265, 0.06916623, 0.038582973, 0.062174708, 0.042420253, 0.0593139, 0.06373129, 0.057559263, 0.05481562, 0.07151395, 0.062254265, 0.028905002, 0.035581738, 0.034957398, 0.050553255, 0.04709469, 0.04594662, 0.02913149, 0.091789626, 0.044939004, 0.06806758, 0.11114699, 0.113912456, 0.23241095, 0.15476826, 0.07285464, 0.07098282, 0.07025592, 0.1060456, 0.08687241, 0.04065822, 0.047779977, 0.036649466, 0.054381873, 0.06859381, 0.09168108, 0.05464988, 0.08028183, 0.06007241, 0.06250375, 0.06696612, 0.060313188, 0.046492286, 0.047122914, 0.03313161, 0.028513774, 0.050818134, 0.057274114, 0.043986168, 0.043303672, 0.064058706, 0.036023818, 0.033761024, 0.057445563, 0.025684396, 0.037389107, 0.03868342, 0.044783212, 0.07883783, 0.12253235, 0.11364656, 0.107283026, 0.07025268, 0.10757686, 0.08181849, 0.07085165, 0.17137864, 0.13574131, 0.08175798, 0.06357962, 0.069806546, 0.07555729, 0.051212743, 0.0417502, 0.05822686, 0.051436093, 0.046235472, 0.05225659, 0.06121406, 0.040498823, 0.066475816, 0.07287165, 0.05387144, 0.07201691, 0.05282293, 0.050096896, 0.046904027, 0.052799456, 0.04756678, 0.03397094, 0.035215277, 0.059648525, 0.036472257, 0.037143618, 0.05596899, 0.057918604, 0.035020206, 0.054954767, 0.043143157, 0.030600056, 0.04252106, 0.051667634, 0.1824873, 0.1293424, 0.13770175, 0.098392926, 0.07274395, 0.087683775, 0.06260866, 0.068276264, 0.06830289, 0.06331926, 0.07233775, 0.068560846, 0.035551433, 0.053879667, 0.07403848, 0.06515505, 0.06071646, 0.045843292, 0.08983597, 0.11026106, 0.042935356, 0.04397735, 0.047183096, 0.054765474, 0.08219916, 0.06738661, 0.05954427, 0.028827501, 0.06614808, 0.041996557, 0.035193633, 0.035735987, 0.0427769, 0.070510134, 0.062189233, 0.045539007, 0.08115767, 0.044454318, 0.07551653, 0.08590742, 0.07761107, 0.09556375, 0.083047055, 0.07307251, 0.19827561, 0.15021339, 0.06699594, 0.084886044, 0.06929678, 0.07467352, 0.09337148, 0.05529641, 0.087484986, 0.07520716, 0.08628631, 0.055128004, 0.0762806, 0.05879438, 0.052373853, 0.051693603, 0.047621757, 0.04000594, 0.034783375, 0.03067837, 0.043470375, 0.044632107, 0.035444167, 0.04288756, 0.03527435, 0.049636193, 0.047819216, 0.04396714, 0.058908664, 0.045175456, 0.04018653, 0.037244853, 0.0889512, 0.22466218, 0.13988778, 0.16127521, 0.08174905, 0.060753953, 0.076986566, 0.12923874, 0.08250103, 0.072533354, 0.053627096, 0.059098527, 0.04672129, 0.04520532, 0.046911065, 0.045172203, 0.03830113, 0.028474664, 0.03661292, 0.044715118, 0.06977506, 0.04725275, 0.04196063, 0.04269933, 0.0277072, 0.059210707, 0.038734835, 0.049485527, 0.055971697, 0.036392566, 0.039224442, 0.03801347, 0.03418313, 0.043872222, 0.044069994, 0.049781512, 0.031624127, 0.039088555, 0.11045121, 0.23535454, 0.10450234, 0.16219416, 0.088190176, 0.11227709, 0.07444536, 0.104312874, 0.13380492, 0.11785841, 0.05413933, 0.10009418, 0.063235156, 0.057386033, 0.06568919, 0.050565436, 0.0738248, 0.04333197, 0.053010892, 0.061197042, 0.06388167, 0.059171695, 0.03456049, 0.045921575, 0.047937796, 0.05402155, 0.04229712, 0.04639286, 0.045470342, 0.043760844, 0.043890703, 0.049200308, 0.033716332, 0.034901235, 0.052049164, 0.04091519, 0.06541752, 0.05337994, 0.051060148, 0.07727796, 0.10864641, 0.12134848, 0.096062675, 0.079351574, 0.09912302, 0.08778912, 0.08564632, 0.10538902, 0.10502537, 0.07458701, 0.052125454, 0.05940207, 0.06332467, 0.05845975, 0.06869463, 0.03542301, 0.042471305, 0.07645039, 0.067978084, 0.04877601, 0.04298078, 0.03972079, 0.04333053, 0.07569929, 0.0402482, 0.061754722, 0.05356561, 0.059755236, 0.059134394, 0.046331484, 0.06293736, 0.04422648, 0.03549817, 0.04835841, 0.08103524, 0.038227353, 0.1599505, 0.15301175, 0.28250742, 0.059777368, 0.045979287, 0.09259659, 0.10973787, 0.12473104, 0.2178405, 0.10502098, 0.059093345, 0.07428031, 0.07350743, 0.087737724, 0.060195383, 0.053043164, 0.079924054, 0.080874145, 0.053656314, 0.04305453, 0.040363226, 0.028309587, 0.048552603, 0.051332664, 0.045601066, 0.048246015, 0.052286413, 0.032475565, 0.044092298, 0.07780965, 0.0553312, 0.04228222, 0.043500878, 0.03650549, 0.04308306, 0.05341297, 0.051561218, 0.04374499, 0.18365689, 0.14058997, 0.08396166, 0.096276544, 0.10978207, 0.07692101, 0.07269565, 0.09020999, 0.07993536, 0.079235464, 0.0975056, 0.06915814, 0.039767902, 0.05337301, 0.047782123, 0.046619505, 0.03283311, 0.035257045, 0.039260425, 0.06044142, 0.0674553, 0.060951434, 0.041022513, 0.039077044, 0.03630523, 0.030744659, 0.031278547, 0.051558554, 0.045840673, 0.0643059, 0.04728636, 0.04412402, 0.049591452, 0.0385127, 0.04139927, 0.042050257, 0.04858029, 0.033751145, 0.11216351, 0.11212991, 0.15228994, 0.19862375, 0.19322604, 0.09197024, 0.13379018, 0.082416944, 0.087137915, 0.08876145, 0.08953936, 0.03635563, 0.05731898, 0.038597252, 0.060700014, 0.06119707, 0.068715595, 0.036044855, 0.05087702, 0.025497777, 0.070930995, 0.039107982, 0.0577167, 0.042088997, 0.038836822, 0.037084855, 0.04715313, 0.04299014, 0.03432288, 0.03480349, 0.03941791, 0.050549887, 0.040992122, 0.03247814, 0.04521954, 0.046858937, 0.04038667, 0.04209441, 0.07438726, 0.086921975, 0.090811655, 0.14962286, 0.15152013, 0.105739176, 0.059918236, 0.08606321, 0.05792166, 0.064333595, 0.053123355, 0.06695735, 0.06703613, 0.083593905, 0.05844369, 0.050508846, 0.06290622, 0.044625822, 0.10143803, 0.0995903, 0.067830026, 0.061817896, 0.04986112, 0.04073204, 0.058296967, 0.035530772, 0.029332688, 0.050062265, 0.03148449, 0.0368196, 0.036259476, 0.029820941, 0.033357207, 0.046584483, 0.033977497, 0.04566148, 0.052298713, 0.047170907, 0.11126581, 0.21903728, 0.13421461, 0.17445064, 0.09567088, 0.06428425, 0.09118172, 0.078827605, 0.05924117, 0.06306122, 0.09622908, 0.073221974, 0.09750452, 0.057768904, 0.05546424, 0.03670111, 0.084757626, 0.05429042, 0.06691142, 0.069228545, 0.07739065, 0.058489133, 0.062110927, 0.044166736, 0.049854554, 0.04673092, 0.053428136, 0.050765622, 0.048440192, 0.053393602, 0.05058921, 0.026455553, 0.040708706, 0.050833814, 0.02813816, 0.052922927, 0.020513954, 0.051535606, 0.0650095, 0.043262046, 0.035107583, 0.041843038, 0.03765551, 0.0793013, 0.13379005, 0.094265535, 0.13933553, 0.12287674, 0.13376337, 0.07969708, 0.113992274, 0.092888795, 0.091368794, 0.056906234, 0.072898604, 0.070146814, 0.07839606, 0.068742916, 0.06859057, 0.056351423, 0.049112767, 0.04851955, 0.053361986, 0.06353228, 0.058870066, 0.050825924, 0.03913474, 0.05539776, 0.03690096, 0.041390155, 0.041161105, 0.04212924, 0.04644121, 0.045897294, 0.044121597, 0.055595692, 0.04154068, 0.05280358, 0.07758155, 0.0606648, 0.05060357, 0.13639945, 0.14882973, 0.15799165, 0.1117697, 0.07251086, 0.039456405, 0.06796147, 0.11466241, 0.069300935, 0.10009515, 0.07750125, 0.05235495, 0.10919129, 0.08781329, 0.08038809, 0.08674254, 0.046808254, 0.03267286, 0.04121807, 0.09560181, 0.063635856, 0.07919294, 0.04871728, 0.037021488, 0.030639097, 0.043240275, 0.041451693, 0.06623095, 0.042123556, 0.06122114, 0.04348043, 0.06072611, 0.024831088, 0.039207157, 0.026317533, 0.04565588, 0.035351094, 0.058676627, 0.058313973, 0.11772404, 0.13418314, 0.09433287, 0.12260162, 0.104776084, 0.08353024, 0.07606779, 0.07201955, 0.077102885, 0.05701306, 0.038772654, 0.034567427, 0.046509515, 0.0446114, 0.06459895, 0.026928786, 0.061610892, 0.047275912, 0.06262502, 0.056296237, 0.037068892, 0.05365344, 0.042741373, 0.036525805, 0.04456902, 0.047767412, 0.034752518, 0.05139044, 0.04207155, 0.027787846, 0.046297412, 0.03612888, 0.06044935, 0.03757083, 0.048346985, 0.028699595, 0.031651314, 0.1240777, 0.08989977, 0.15913518, 0.18721561, 0.08324123, 0.08110822, 0.05512155, 0.08731171, 0.05664753, 0.06199325, 0.060013175, 0.06115663, 0.0700588, 0.08150628, 0.05739175, 0.06134952, 0.06950433, 0.057504848, 0.061060604, 0.061286084, 0.05016342, 0.038047716, 0.034049153, 0.0485959, 0.047316574, 0.048676122, 0.059917863, 0.069592595, 0.039965097, 0.046624474, 0.037812825, 0.04194007, 0.0660383, 0.026604129, 0.054569785, 0.071103506, 0.040275205, 0.055686317, 0.07637113, 0.11763058, 0.14609389, 0.20146048, 0.1402322, 0.088244915, 0.074785866, 0.15349928, 0.086210296, 0.09507897, 0.06675639, 0.05197515, 0.05495165, 0.083703935, 0.06722698, 0.05949331, 0.06355137, 0.055930898, 0.060426462, 0.051971506, 0.062178724, 0.034759823, 0.039877247, 0.051635165, 0.04441727, 0.06371031, 0.050370153, 0.032345936, 0.04552866, 0.059815, 0.07463226, 0.054056086, 0.05768033, 0.040743016, 0.053850744, 0.03634847, 0.035959743, 0.052428447, 0.046723116, 0.04120967, 0.05378284, 0.03636289, 0.054395564, 0.042683773, 0.13089028, 0.12382678, 0.17460524, 0.12993832, 0.12039951, 0.061640907, 0.077509075, 0.04540059, 0.096644856, 0.08707998, 0.074382156, 0.0664618, 0.03946031, 0.06316848, 0.053902395, 0.058156297, 0.056264218, 0.042583838, 0.07302833, 0.035750337, 0.038448714, 0.04546855, 0.0525168, 0.045405075, 0.059713565, 0.055071693, 0.05295468, 0.04210948, 0.060505025, 0.04873941, 0.053601015, 0.045518033, 0.05511235, 0.034056187, 0.023256078, 0.03633373, 0.04127727, 0.028822789, 0.15039669, 0.103821, 0.07218048, 0.11096054, 0.0876768, 0.07060263, 0.07686871, 0.0985797, 0.08459865, 0.07786745, 0.06860009, 0.060191814, 0.09912768, 0.05897211, 0.14765802, 0.06033834, 0.05265712, 0.047330875, 0.04369588, 0.05582162, 0.06144319, 0.054929882, 0.04748529, 0.070852056, 0.053964026, 0.06561258, 0.048572756, 0.04345614, 0.041522164, 0.043599393, 0.037420325, 0.028990563, 0.07263755, 0.03245508, 0.0430901, 0.035151795, 0.03308671, 0.032940295, 0.06947345, 0.15321733, 0.16123584, 0.36383048, 0.12800829, 0.08192708, 0.12983003, 0.055731744, 0.05471422, 0.051255833, 0.048706844, 0.047291495, 0.055427928, 0.047094967, 0.09135712, 0.08346874, 0.08256359, 0.052156188, 0.05017016, 0.06910594, 0.048527967, 0.04032203, 0.050093252, 0.053985607, 0.054954115, 0.037379894, 0.061412316, 0.046481468, 0.036816295, 0.04287953, 0.05467118, 0.061988723, 0.031537376, 0.049066693, 0.05207729, 0.03031102, 0.047335073, 0.030341184, 0.14194177, 0.110549174, 0.18489006, 0.12571244, 0.07253955, 0.052590374, 0.12518503, 0.13615176, 0.10019867, 0.07071347, 0.06394614, 0.07225496, 0.05615548, 0.06154113, 0.057938606, 0.06885038, 0.036113113, 0.05085721, 0.064944826, 0.058285695, 0.057380363, 0.054377876, 0.041635778, 0.06528994, 0.051107798, 0.08016177, 0.061108783, 0.042115103, 0.027352557, 0.026151722, 0.032004207, 0.05166325, 0.043481816, 0.02795715, 0.048810117, 0.043780543, 0.031370316, 0.04010575, 0.10851811, 0.12357851, 0.23375933, 0.09331968, 0.0785427, 0.098619536, 0.102232926, 0.10808926, 0.098010324, 0.09102555, 0.0596819, 0.06818042, 0.06196027, 0.11097877, 0.09575059, 0.0702486, 0.047255754, 0.047317863, 0.046112724, 0.039837856, 0.06909489, 0.04558354, 0.04119871, 0.03714326, 0.06871976, 0.05499221, 0.07660586, 0.03758458, 0.03952381, 0.049585987, 0.04682796, 0.028303746, 0.04247856, 0.04008164, 0.04736651, 0.039214045, 0.0553818, 0.035169076, 0.054213956, 0.049868625, 0.090562046, 0.049355354, 0.040260524, 0.071674034, 0.15433292, 0.1265047, 0.15735942, 0.075318225, 0.058707044, 0.08879965, 0.077801205, 0.058627322, 0.07408496, 0.038689744, 0.06392923, 0.045402054, 0.041283105, 0.046267755, 0.04980624, 0.04801767, 0.040015087, 0.047414232, 0.042408478, 0.037739143, 0.0425497, 0.048101176, 0.03291757, 0.04509503, 0.032185, 0.04290079, 0.025965733, 0.06348417, 0.03372856, 0.107934, 0.052674137, 0.028625567, 0.05710403, 0.037365694, 0.042927276, 0.03862246, 0.06255312, 0.053636085, 0.10469824, 0.12762313, 0.09508439, 0.16659272, 0.07697414, 0.08425987, 0.043112855, 0.07771442, 0.036606096, 0.05488412, 0.04821871, 0.0312955, 0.0822323, 0.046647586, 0.114695385, 0.066373855, 0.043462954, 0.05539086, 0.056152795, 0.04716289, 0.045016225, 0.042611152, 0.0583315, 0.04139035, 0.034932666, 0.041544043, 0.059022143, 0.036860347, 0.050399233, 0.051411286, 0.044175595, 0.02943258, 0.0317861, 0.050041586, 0.031073555, 0.03418885, 0.041687816, 0.0696793, 0.14162657, 0.14768441, 0.13778871, 0.095447525, 0.09675221, 0.14799759, 0.101366885, 0.06866933, 0.05920481, 0.061376546, 0.07767808, 0.058381714, 0.0641515, 0.06795615, 0.04108763, 0.09009735, 0.040178806, 0.046354346, 0.063961506, 0.06249591, 0.050439667, 0.03906958, 0.0630497, 0.060850427, 0.06664433, 0.04761711, 0.045711, 0.050682757, 0.050951254, 0.030140841, 0.04202477, 0.041439228, 0.044727553, 0.063515164, 0.047177613, 0.061310757, 0.04747138, 0.06695662, 0.12914547, 0.2152015, 0.16962905, 0.12653324, 0.0507913, 0.05704138, 0.07994101, 0.09391496, 0.09172307, 0.07591715, 0.058724165, 0.06764845, 0.047529507, 0.05634628, 0.07199, 0.062382862, 0.057484627, 0.04314527, 0.040139217, 0.052147903, 0.04180166, 0.052010775, 0.07171788, 0.05396113, 0.037562463, 0.052122086, 0.07366329, 0.028638626, 0.055033434, 0.049160305, 0.04243118, 0.065882474, 0.046837255, 0.03952791, 0.024565376, 0.04801221, 0.042882424, 0.13257988, 0.13756245, 0.13296768, 0.14951685, 0.10984525, 0.0770143, 0.056109592, 0.1076558, 0.10252895, 0.07272064, 0.08025316, 0.056394678, 0.07409439, 0.057869818, 0.03882002, 0.05263614, 0.06217244, 0.050845064, 0.05570967, 0.06117264, 0.06329282, 0.06943402, 0.04825121, 0.048138283, 0.05379799, 0.031386383, 0.055210214, 0.03649591, 0.043395396, 0.041562654, 0.04071867, 0.07854929, 0.041269086, 0.037477247, 0.043746397, 0.04968522, 0.03549498, 0.03798734, 0.04884147, 0.04448427, 0.033294298, 0.057763778, 0.046716314, 0.047544193, 0.064706765, 0.12019504, 0.13800235, 0.13521102, 0.13407144, 0.11252422, 0.1030178, 0.27752998, 0.092253916, 0.084769204, 0.05715173, 0.07586569, 0.07796795, 0.061201498, 0.06588811, 0.043969605, 0.060616758, 0.053799447, 0.056343682, 0.059918355, 0.036291108, 0.07119568, 0.07230515, 0.04159684, 0.037459113, 0.045102905, 0.03554864, 0.047860324, 0.033982303, 0.048934452, 0.050921094, 0.030958276, 0.05645066, 0.06485548, 0.040254205, 0.040590834, 0.034352317, 0.036180746, 0.08176608, 0.24999757, 0.15032153, 0.06550273, 0.060607407, 0.057223916, 0.09529955, 0.1594458, 0.16619535, 0.08489513, 0.12492554, 0.07393378, 0.052041404, 0.07368881, 0.060808826, 0.061401624, 0.055134516, 0.074001014, 0.03848535, 0.06709048, 0.04153419, 0.045826975, 0.057634503, 0.05563197, 0.044206236, 0.04662967, 0.03730301, 0.043205533, 0.037376415, 0.043552324, 0.04852881, 0.042815734, 0.03632981, 0.03634587, 0.03867604, 0.03624228, 0.03646787, 0.028130252, 0.099174045, 0.12621754, 0.08609346, 0.085184395, 0.06631144, 0.10768579, 0.08163952, 0.1371618, 0.17872353, 0.081504695, 0.063589476, 0.08038237, 0.10182182, 0.06979143, 0.099365145, 0.07600017, 0.053677138, 0.07634439, 0.06560392, 0.04363221, 0.03851574, 0.053902086, 0.026584515, 0.048403196, 0.03513472, 0.074360706, 0.054605328, 0.03580629, 0.04685254, 0.03937528, 0.044576876, 0.046680998, 0.037707366, 0.047768574, 0.055525348, 0.04375223, 0.037431467, 0.03962174, 0.07439272, 0.16628297, 0.09622417, 0.121519625, 0.07644384, 0.062428877, 0.06380861, 0.12790683, 0.11534859, 0.06022027, 0.0845773, 0.052519888, 0.0701432, 0.06058594, 0.059368223, 0.05044629, 0.045667287, 0.05018673, 0.054322693, 0.06427187, 0.04904308, 0.05382725, 0.03169921, 0.047415033, 0.048048686, 0.044901192, 0.0340559, 0.041220114, 0.032692827, 0.030365972, 0.04534167, 0.024582269, 0.035425495, 0.025008058, 0.026472274, 0.03841387, 0.033357292, 0.03716018, 0.107103296, 0.06701364, 0.066540666, 0.10622007, 0.086387075, 0.1068305, 0.07197763, 0.10101523, 0.06212911, 0.07229197, 0.12166554, 0.09892463, 0.06020844, 0.058542207, 0.089734346, 0.0304973, 0.07748562, 0.08868804, 0.08849867, 0.034614593, 0.048517045, 0.026867518, 0.029422127, 0.03879357, 0.039871953, 0.073034756, 0.042790856, 0.07121787, 0.03461418, 0.04972447, 0.042072527, 0.04590371, 0.05355141, 0.059506766, 0.039513115, 0.03640729, 0.03184786, 0.046565574, 0.03486742, 0.033346068, 0.04012064, 0.025434854, 0.041231602, 0.07830987, 0.06699508, 0.120451115, 0.10422634, 0.13720404, 0.085853994, 0.11583169, 0.08963532, 0.08837844, 0.12443249, 0.0763229, 0.05266073, 0.03511544, 0.07353204, 0.058104884, 0.04182608, 0.06652641, 0.060832117, 0.04131364, 0.040979017, 0.04735065, 0.055010714, 0.038073193, 0.025843536, 0.05707869, 0.047399748, 0.052541845, 0.030152183, 0.04067145, 0.0377964, 0.034627784, 0.04330511, 0.05268646, 0.029819163, 0.037352555, 0.03538683, 0.043480333, 0.04829529, 0.09214894, 0.13312526, 0.16374747, 0.13706306, 0.10400155, 0.14692025, 0.11136376, 0.076318614, 0.085944, 0.16319352, 0.06590501, 0.09994362, 0.07255609, 0.06015751, 0.07287481, 0.05407169, 0.06488968, 0.043626495, 0.044298425, 0.04786777, 0.05012482, 0.0392605, 0.06733175, 0.06475937, 0.041910186, 0.03450958, 0.041147, 0.033265013, 0.051055614, 0.05831505, 0.054683685, 0.057400502, 0.04706307, 0.048025094, 0.025516555, 0.032916225, 0.033179607, 0.03171877, 0.1291063, 0.13476002, 0.1467721, 0.11753863, 0.10982493, 0.08787376, 0.08476345, 0.12270949, 0.07500373, 0.110067666, 0.064511485, 0.048020616, 0.06113634, 0.06472434, 0.04318658, 0.04708322, 0.07071406, 0.04597547, 0.04672673, 0.055772636, 0.042410597, 0.034947067, 0.031452414, 0.027992373, 0.053783923, 0.043312658, 0.042428825, 0.05948386, 0.057618454, 0.048096433, 0.05142128, 0.066065215, 0.030814871, 0.03969652, 0.026950125, 0.054512016, 0.065579474, 0.039301157, 0.10317129, 0.087677464, 0.2062989, 0.17387214, 0.1152601, 0.07849372, 0.060203303, 0.10506697, 0.08090811, 0.07059135, 0.05152823, 0.08488992, 0.08024002, 0.06804209, 0.0604295, 0.04366038, 0.07859563, 0.06621764, 0.07217851, 0.053547576, 0.052375082, 0.036465477, 0.047172, 0.05427322, 0.042079963, 0.04784395, 0.04155372, 0.035755042, 0.041403044, 0.041950833, 0.043457367, 0.04068191, 0.040526234, 0.034549385, 0.0369036, 0.042275447, 0.031095464, 0.02724512, 0.06495477, 0.12099066, 0.14043455, 0.20419134, 0.10867185, 0.13273537, 0.10382178, 0.13364081, 0.078362085, 0.10690293, 0.12528299, 0.09138556, 0.058415707, 0.04557846, 0.057459053, 0.02767502, 0.056017328, 0.043869298, 0.05113394, 0.060927454, 0.031629212, 0.04402423, 0.07229904, 0.0490023, 0.08958065, 0.05178995, 0.028234398, 0.038115714, 0.063757785, 0.036297172, 0.053394236, 0.053714626, 0.06158104, 0.033392783, 0.049277887, 0.045439612, 0.048454154, 0.04906769, 0.032560766, 0.044626728, 0.05478395, 0.04487626, 0.037723225, 0.062459175, 0.06440599, 0.093422204, 0.112571605, 0.09634672, 0.088151135, 0.05084309, 0.11246955, 0.05511745, 0.05372813, 0.059712693, 0.0626356, 0.03189497, 0.100798845, 0.05101273, 0.057378795, 0.074855275, 0.05514122, 0.05050712, 0.04933654, 0.03568444, 0.037784453, 0.038515337, 0.054398466, 0.04179935, 0.05395682, 0.029538192, 0.0431503, 0.042977434, 0.049156744, 0.041574683, 0.045800976, 0.054842427, 0.042656425, 0.036722254, 0.03217807, 0.061071917, 0.028046047, 0.05198558, 0.108250014, 0.121691376, 0.20200662, 0.18353365, 0.1338135, 0.065589726, 0.07523972, 0.072820164, 0.07478543, 0.08363784, 0.07568458, 0.054807913, 0.03675578, 0.04410049, 0.05884313, 0.046514902, 0.064058945, 0.06664246, 0.034797393, 0.040690422, 0.051236622, 0.04368198, 0.059102453, 0.04778618, 0.05570655, 0.040382225, 0.06063278, 0.04803529, 0.02430231, 0.046935637, 0.047024738, 0.036067054, 0.02964513, 0.04476685, 0.02161136, 0.033163898, 0.03854677, 0.039941262, 0.13021573, 0.15775351, 0.12281984, 0.12649071, 0.107859276, 0.0843845, 0.08564019, 0.073038146, 0.058340613, 0.075977385, 0.06376239, 0.0820666, 0.06773995, 0.04558107, 0.11309255, 0.048759576, 0.042553257, 0.050781097, 0.02924602, 0.06446717, 0.039215542, 0.06411923, 0.038230933, 0.05871285, 0.060288146, 0.04375559, 0.02916189, 0.037849613, 0.036932614, 0.0492344, 0.042921275, 0.04591177, 0.03532679, 0.037721757, 0.034953333, 0.041097417, 0.033609927, 0.037409674, 0.07621254, 0.11665205, 0.10432626, 0.24164069, 0.14543243, 0.06418083, 0.08616949, 0.06955244, 0.1099665, 0.06899367, 0.04000038, 0.043288518, 0.03151473, 0.05844701, 0.064478606, 0.05420152, 0.04355186, 0.04646798, 0.049898364, 0.043199807, 0.049568668, 0.04086345, 0.051279735, 0.031772297, 0.056022607, 0.044673774, 0.04871046, 0.07463577, 0.029787188, 0.0392092, 0.02049438, 0.030346591, 0.03014505, 0.046997618, 0.028003179, 0.03940029, 0.052104723, 0.046916593, 0.21212097, 0.2703707, 0.13065456, 0.19562148, 0.09853238, 0.06744121, 0.08813446, 0.0913892, 0.09806398, 0.08359013, 0.11909757, 0.076921, 0.071486026, 0.04665475, 0.0532613, 0.043324713, 0.049450334, 0.05551975, 0.05627622, 0.040847953, 0.05016673, 0.04231546, 0.054229792, 0.046073843, 0.044288933, 0.05471872, 0.03865873, 0.048071593, 0.051612783, 0.03948463, 0.028986147, 0.035718113, 0.04218943, 0.035941377, 0.039094627, 0.046763267, 0.04906832, 0.047455497, 0.031511728, 0.036278363, 0.030578032, 0.036110695, 0.06401931, 0.13541245, 0.18701024, 0.087660596, 0.072856985, 0.10751437, 0.20927656, 0.1341108, 0.09962704, 0.06400697, 0.118184306, 0.10223927, 0.08757066, 0.075895905, 0.03744874, 0.056853503, 0.061661255, 0.060920466, 0.05170398, 0.038140003, 0.05137415, 0.04212685, 0.055116065, 0.046189442, 0.029201563, 0.031375527, 0.042266034, 0.045294024, 0.041444045, 0.042394973, 0.045638513, 0.029883102, 0.04314505, 0.036274273, 0.02915084, 0.02666694, 0.03796143, 0.04054903, 0.028270528, 0.12547119, 0.2566735, 0.17088223, 0.18114714, 0.11247198, 0.07926434, 0.12909211, 0.07908962, 0.101405896, 0.07976414, 0.07987996, 0.047586225, 0.05201899, 0.055511493, 0.06869316, 0.059958138, 0.0392596, 0.04394293, 0.041853994, 0.045525305, 0.04693446, 0.03737499, 0.06435019, 0.042860627, 0.022280421, 0.0375061, 0.032146875, 0.049967166, 0.052477017, 0.067300096, 0.03416997, 0.036162626, 0.02821523, 0.043239553, 0.019610994, 0.025504494, 0.047273144, 0.05060026, 0.0871139, 0.10807962, 0.09023333, 0.19996978, 0.23850055, 0.08551872, 0.057762336, 0.05939703, 0.058598947, 0.088528104, 0.060065653, 0.06395376, 0.06852903, 0.07107463, 0.0814021, 0.05696293, 0.03853877, 0.051266927, 0.04248746, 0.058629557, 0.052177712, 0.06976531, 0.047871463, 0.031333897, 0.026933689, 0.04894067, 0.030282542, 0.033789586, 0.03221326, 0.046954066, 0.033234894, 0.044139925, 0.03645107, 0.029808301, 0.039320532, 0.06387238, 0.036084622, 0.031722635, 0.101316184, 0.112075366, 0.09333643, 0.100489706, 0.10297147, 0.06849298, 0.09508397, 0.05731338, 0.09626249, 0.06999859, 0.038924154, 0.033277847, 0.047660343, 0.09186415, 0.04829222, 0.063210696, 0.04732561, 0.057875413, 0.040298134, 0.05480171, 0.06692763, 0.042845115, 0.035521228, 0.043095328, 0.053184044, 0.029689495, 0.034773797, 0.04654089, 0.040949482, 0.056761157, 0.03231318, 0.029830195, 0.032923356, 0.028465616, 0.039031, 0.017998055, 0.03408787, 0.03595797, 0.060713492, 0.09541415, 0.12121199, 0.17825697, 0.20796235, 0.06989631, 0.04390181, 0.049155, 0.07356356, 0.101512186, 0.06402236, 0.053070564, 0.07988372, 0.07284799, 0.064217016, 0.08846728, 0.05109825, 0.054488372, 0.03915118, 0.039848194, 0.02763384, 0.03138114, 0.03440707, 0.042030364, 0.0317502, 0.030902153, 0.040871054, 0.05180467, 0.038011327, 0.041748207, 0.036862154, 0.044602204, 0.045573976, 0.028870074, 0.064036466, 0.042302907, 0.043348443, 0.025741419, 0.023741143, 0.05862152, 0.04581228, 0.02670493, 0.028905872, 0.03324561, 0.124923155, 0.12573026, 0.18297876, 0.23044026, 0.112694204, 0.083013095, 0.08052898, 0.09004807, 0.095782496, 0.061727688, 0.05678234, 0.04579378, 0.06874254, 0.07082341, 0.06028089, 0.04646725, 0.060679372, 0.047299985, 0.04343592, 0.048637617, 0.03584311, 0.020942114, 0.032637212, 0.044943962, 0.049116433, 0.047206618, 0.03980307, 0.045516204, 0.030602083, 0.037459034, 0.050583996, 0.0521634, 0.027933294, 0.03149348, 0.023748387, 0.028688932, 0.032981932, 0.040523794, 0.10298289, 0.120836936, 0.09339713, 0.09787788, 0.066430636, 0.043620612, 0.08871926, 0.0873673, 0.086541235, 0.06985378, 0.047171146, 0.05141425, 0.051925056, 0.04389869, 0.041465312, 0.0584013, 0.089293376, 0.051198263, 0.050097805, 0.03862911, 0.055218503, 0.05499748, 0.05840187, 0.0619653, 0.049632985, 0.036701705, 0.04944991, 0.048162352, 0.052020665, 0.031326294, 0.03780721, 0.043752108, 0.050235856, 0.036228035, 0.034858156, 0.03862647, 0.040343054, 0.03943378, 0.061774004, 0.10399693, 0.065448344, 0.071578704, 0.068767756, 0.071021505, 0.0997491, 0.09194038, 0.10072926, 0.09478105, 0.026230955, 0.09294365, 0.09323339, 0.05694434, 0.08077104, 0.04859407, 0.044420045, 0.04901556, 0.043517504, 0.04765192, 0.02670421, 0.04493629, 0.06215795, 0.03375923, 0.02214815, 0.036762375, 0.04671358, 0.06357676, 0.03640945, 0.034929406, 0.06644234, 0.041516878, 0.041458048, 0.028611485, 0.03773187, 0.050140504, 0.031753566, 0.029368693, 0.19409728, 0.1494341, 0.13752238, 0.1519355, 0.09099785, 0.08810059, 0.060483474, 0.07877809, 0.080876626, 0.071366824, 0.04544235, 0.04555481, 0.053121705, 0.062632985, 0.116499685, 0.06536519, 0.045346737, 0.061975963, 0.044527944, 0.075994365, 0.06768072, 0.07120726, 0.062626354, 0.038173787, 0.044226497, 0.041573774, 0.046458665, 0.04205072, 0.034109518, 0.049937434, 0.03192867, 0.030012002, 0.0323203, 0.044496275, 0.037823383, 0.056366526, 0.0549045, 0.03469434, 0.10033309, 0.09043381, 0.058602653, 0.07684185, 0.10628419, 0.0867709, 0.10531455, 0.08713212, 0.09084351, 0.07117267, 0.076492295, 0.04465671, 0.05549596, 0.05617522, 0.04272603, 0.06748559, 0.04459437, 0.040620137, 0.04010281, 0.051038206, 0.066835, 0.049148656, 0.05441111, 0.04965929, 0.04452786, 0.056264438, 0.053458765, 0.03864144, 0.04482697, 0.039094254, 0.050980497, 0.038903818, 0.057454467, 0.028014816, 0.03625066, 0.034879804, 0.035733297, 0.022790113, 0.04960308, 0.04902602, 0.045452792, 0.07461024, 0.060382713, 0.17901285, 0.13407981, 0.07373991, 0.12665129, 0.1311834, 0.1083471, 0.097248934, 0.11368843, 0.11096519, 0.10348967, 0.074973084, 0.06741247, 0.062316034, 0.08144395, 0.06983917, 0.051237747, 0.0441145, 0.065286964, 0.047311235, 0.061002493, 0.042346686, 0.04549473, 0.052465748, 0.04942061, 0.043435395, 0.045159217, 0.039042946, 0.038632087, 0.03759868, 0.029275917, 0.04195844, 0.038315915, 0.06420474, 0.031654052, 0.05100958, 0.02956529, 0.038611185, 0.04042739, 0.12869802, 0.10377253, 0.19169518, 0.13251348, 0.15665407, 0.055681735, 0.05488627, 0.079375245, 0.05358886, 0.10543693, 0.10093619, 0.074715406, 0.08832561, 0.05893994, 0.054937877, 0.0730434, 0.06700498, 0.06405937, 0.04680848, 0.06586713, 0.08773384, 0.051383313, 0.0412995, 0.04572433, 0.043708302, 0.036166757, 0.055606008, 0.059064195, 0.043073762, 0.034310408, 0.03325404, 0.053761933, 0.03943408, 0.04753722, 0.033298764, 0.033104345, 0.04661032, 0.04225694, 0.06355428, 0.18594076, 0.18563357, 0.092759065, 0.14111735, 0.14023541, 0.1172817, 0.14556961, 0.11550025, 0.10235449, 0.068294175, 0.0982455, 0.066042356, 0.087569, 0.038929608, 0.05309248, 0.053345058, 0.038825203, 0.049881786, 0.042005762, 0.044696193, 0.052652746, 0.03618293, 0.03957651, 0.052532814, 0.057062935, 0.04560134, 0.045829833, 0.04714802, 0.04090612, 0.042048406, 0.040085852, 0.04116415, 0.051995553, 0.050948378, 0.04496466, 0.050325345, 0.056896698, 0.08574503, 0.06330297, 0.06428411, 0.15716827, 0.21077362, 0.08069693, 0.07589444, 0.07339095, 0.050673403, 0.06452714, 0.06641719, 0.04528929, 0.0763253, 0.04345752, 0.07022074, 0.06338134, 0.056571677, 0.044237155, 0.04130275, 0.0475942, 0.05259503, 0.03811797, 0.0422512, 0.034885805, 0.034852207, 0.033376828, 0.050426662, 0.053977158, 0.040270958, 0.029044366, 0.03010228, 0.056233194, 0.051252753, 0.040373567, 0.05306543, 0.066625684, 0.04600959, 0.04037572, 0.06908375, 0.082384944, 0.082877524, 0.14269005, 0.09713543, 0.06981401, 0.07772086, 0.080370344, 0.102286264, 0.05509414, 0.05214662, 0.07904401, 0.041352462, 0.05139024, 0.06994248, 0.052749168, 0.052119546, 0.039654028, 0.043263454, 0.032776725, 0.061193537, 0.06771858, 0.03767875, 0.028484827, 0.04113252, 0.07560816, 0.052472547, 0.034616683, 0.04839489, 0.033849616, 0.037747752, 0.064331606, 0.065696515, 0.044295922, 0.049587823, 0.027177017, 0.064230554, 0.058694124, 0.032884862, 0.03023127, 0.037344802, 0.038015854, 0.039735828, 0.047395036, 0.036295246, 0.19665328, 0.17483118, 0.09734484, 0.10379603, 0.077968925, 0.07004819, 0.09118479, 0.05633984, 0.08282551, 0.07165311, 0.06313632, 0.082423575, 0.050124157, 0.039406423, 0.06622656, 0.060842264, 0.04841434, 0.055235952, 0.049625594, 0.028348064, 0.028040422, 0.02745081, 0.061829735, 0.040684603, 0.049538895, 0.037005216, 0.035385646, 0.039114963, 0.04203395, 0.03135391, 0.03456379, 0.046607524, 0.03230282, 0.05068822, 0.04825448, 0.026197664, 0.05264006, 0.07782909, 0.110622354, 0.12649581, 0.16393833, 0.06920751, 0.133559, 0.09909158, 0.08997734, 0.081314996, 0.12479868, 0.07336418, 0.056334432, 0.046305157, 0.07936424, 0.04479556, 0.051336624, 0.05366485, 0.07510257, 0.053046897, 0.02562581, 0.042942725, 0.032552857, 0.03974112, 0.045330863, 0.04240188, 0.04099186, 0.05002309, 0.051066574, 0.034893867, 0.043928176, 0.054642428, 0.035462286, 0.05806889, 0.04900121, 0.037969135, 0.06621551, 0.060078915, 0.037533116, 0.20682828, 0.095931016, 0.13378243, 0.06986351, 0.0701811, 0.07667869, 0.08751418, 0.07293131, 0.052135617, 0.09452323, 0.035226375, 0.047685653, 0.04586693, 0.090097345, 0.04068131, 0.047588494, 0.057364915, 0.04812282, 0.045763444, 0.04143764, 0.051563434, 0.04788901, 0.05350267, 0.064156145, 0.064091705, 0.03212677, 0.05746803, 0.041137435, 0.0467808, 0.060679957, 0.04197089, 0.042711522, 0.032311734, 0.04799887, 0.03542736, 0.038877018, 0.045453597, 0.04054239, 0.10423825, 0.070406504, 0.106704384, 0.11934521, 0.15015317, 0.086364724, 0.046891056, 0.03940296, 0.068667665, 0.074553, 0.06625973, 0.057150897, 0.058653243, 0.045583203, 0.0403572, 0.03635416, 0.067627214, 0.04933514, 0.044161685, 0.061533477, 0.05010407, 0.059628624, 0.06833662, 0.04600429, 0.03973782, 0.034839362, 0.040383548, 0.037416887, 0.062253587, 0.034563333, 0.037812907, 0.032547407, 0.03784065, 0.04251772, 0.046746053, 0.032979053, 0.044552162, 0.03868744, 0.12182965, 0.1313818, 0.14678364, 0.06441289, 0.03846319, 0.12422164, 0.07951752, 0.081481874, 0.10436581, 0.11517435, 0.12091495, 0.07915128, 0.055993024, 0.0777619, 0.0810209, 0.064282395, 0.048521064, 0.052847173, 0.045608774, 0.045699507, 0.040176213, 0.0273777, 0.056663074, 0.03132686, 0.04668369, 0.04106462, 0.053784564, 0.031045318, 0.060004603, 0.04288568, 0.038573686, 0.027878987, 0.035639353, 0.03446817, 0.025917958, 0.036381144, 0.03801764, 0.049819253, 0.14036326, 0.08503632, 0.13319917, 0.16439849, 0.1997413, 0.0942066, 0.09696098, 0.1028403, 0.06024614, 0.07694089, 0.048463993, 0.049577463, 0.073845364, 0.069973, 0.06798511, 0.043019485, 0.043761987, 0.04672784, 0.038213022, 0.070434615, 0.044029567, 0.07690042, 0.056628253, 0.043893036, 0.05210542, 0.036706097, 0.034356464, 0.033205908, 0.033110082, 0.05677294, 0.033642504, 0.02823168, 0.047993816, 0.047117908, 0.037645556, 0.03814158, 0.05092816, 0.05653867, 0.16267605, 0.15067244, 0.08284021, 0.14904621, 0.06494266, 0.05049734, 0.06713444, 0.07971819, 0.072826505, 0.07399669, 0.049876295, 0.1173004, 0.103050694, 0.044782747, 0.05269333, 0.054336384, 0.1067022, 0.036827117, 0.058867857, 0.04420655, 0.047531918, 0.058155976, 0.05812608, 0.030228531, 0.039098524, 0.03358848, 0.040066704, 0.04296244, 0.061474636, 0.06713709, 0.046476442, 0.051646993, 0.03515551, 0.041413225, 0.04963843, 0.052077316, 0.03209401, 0.03903122, 0.099715635, 0.12566881, 0.06332869, 0.04104137, 0.07957415, 0.06473092, 0.044044804, 0.07845315, 0.09545462, 0.086091794, 0.07553103, 0.07668454, 0.049062956, 0.07158015, 0.06366435, 0.05408861, 0.040727384, 0.034627758, 0.025445888, 0.05750682, 0.068131536, 0.045994755, 0.030727841, 0.034342367, 0.051512443, 0.062192503, 0.044453736, 0.03547399, 0.0580997, 0.046901334, 0.038399085, 0.03344249, 0.036266454, 0.036978588, 0.039121553, 0.04320066, 0.058309373, 0.03983472, 0.07786747, 0.23745565, 0.15246674, 0.082445964, 0.057460476, 0.10500302, 0.07736317, 0.10416277, 0.044609252, 0.04270961, 0.0440418, 0.051219646, 0.05303526, 0.061468143, 0.042958077, 0.060153164, 0.060519367, 0.057424113, 0.034647174, 0.061946493, 0.063779816, 0.05348718, 0.038505796, 0.036002357, 0.030041868, 0.03735987, 0.0375366, 0.033297062, 0.048552457, 0.04182368, 0.03263528, 0.064522736, 0.06578791, 0.05453183, 0.0469761, 0.057182513, 0.048640642, 0.031442452, 0.13525225, 0.14226298, 0.10433147, 0.0630981, 0.077487476, 0.09666906, 0.040115252, 0.08287612, 0.086336315, 0.08687754, 0.09246837, 0.0894209, 0.042183932, 0.04116391, 0.09611718, 0.055752415, 0.029068649, 0.04689009, 0.036844518, 0.062001266, 0.046835262, 0.037691668, 0.04761645, 0.037552908, 0.027818551, 0.037720192, 0.03467478, 0.04187929, 0.022641864, 0.031041352, 0.06250837, 0.049616456, 0.04135775, 0.052974965, 0.024673223, 0.038065147, 0.032383464, 0.044248287, 0.032476626, 0.032996174, 0.050730083, 0.037072074, 0.032563817, 0.08400825, 0.08522112, 0.07506107, 0.06908246, 0.08768908, 0.079008855, 0.07405203, 0.060370453, 0.088638544, 0.05482678, 0.060501676, 0.0743254, 0.044166796, 0.08231387, 0.10609394, 0.071831286, 0.07198133, 0.037459243, 0.0513961, 0.06309841, 0.031589948, 0.056436714, 0.040679865, 0.036251087, 0.03150401, 0.03444684, 0.072936796, 0.055011712, 0.03719746, 0.06798294, 0.058317523, 0.03736442, 0.038631737, 0.052156713, 0.04655196, 0.0452242, 0.0549534, 0.047603328, 0.16746494, 0.15565652, 0.12556617, 0.094319366, 0.12924199, 0.111920215, 0.10464233, 0.12848024, 0.091032006, 0.08044985, 0.057214808, 0.08349925, 0.06494636, 0.05383585, 0.089325495, 0.059066277, 0.048367377, 0.0627007, 0.050854586, 0.041442405, 0.040163465, 0.0384397, 0.057861704, 0.06548186, 0.041592572, 0.05255871, 0.05901563, 0.03815923, 0.025370639, 0.041123517, 0.034861933, 0.055390745, 0.05453174, 0.051620495, 0.03873421, 0.038799394, 0.03677261, 0.050760467, 0.07889132, 0.13974945, 0.057910737, 0.13692765, 0.2235245, 0.06100356, 0.15159926, 0.1793982, 0.1527918, 0.047412068, 0.128546, 0.08043333, 0.055877835, 0.062126536, 0.07118447, 0.06129041, 0.062960915, 0.041216683, 0.030859543, 0.04133016, 0.0493721, 0.03779075, 0.06803651, 0.035961237, 0.047610965, 0.02950861, 0.06405193, 0.03574213, 0.031629603, 0.057270933, 0.04489949, 0.04110913, 0.052337497, 0.063809894, 0.043745928, 0.031057922, 0.029889569, 0.036273792, 0.12895921, 0.10759097, 0.20806909, 0.21272266, 0.12214481, 0.07603285, 0.06937394, 0.078572765, 0.05109021, 0.053813137, 0.066880524, 0.0749372, 0.051972173, 0.037504405, 0.049220942, 0.050179455, 0.052670073, 0.08372849, 0.04717062, 0.073038116, 0.039980676, 0.041374028, 0.03622709, 0.04547272, 0.030116273, 0.04010031, 0.03510965, 0.045056105, 0.030354101, 0.03482462, 0.046898205, 0.042611852, 0.03837702, 0.06876041, 0.034819864, 0.044301156, 0.049560122, 0.04088792, 0.085842036, 0.070043676, 0.10534053, 0.09637595, 0.13763906, 0.09046877, 0.069205284, 0.071647406, 0.11210719, 0.06717728, 0.055153094, 0.071132824, 0.0635278, 0.06401453, 0.08697017, 0.070092976, 0.05899452, 0.048089646, 0.059244264, 0.09107007, 0.07438759, 0.07606868, 0.07417196, 0.06569524, 0.057285298, 0.050291702, 0.050068364, 0.051294517, 0.043053202, 0.035846308, 0.026697984, 0.037837476, 0.044054836, 0.030192403, 0.05847888, 0.046455473, 0.05198005, 0.033477653, 0.055962116, 0.031174902, 0.03337331, 0.047257695, 0.060842983, 0.033749413, 0.14171717, 0.14157887, 0.11027652, 0.09621843, 0.058484748, 0.10995544, 0.123706006, 0.093598895, 0.052944247, 0.053457253, 0.06541312, 0.079307534, 0.07811135, 0.06157655, 0.04545074, 0.03867623, 0.044917442, 0.076579876, 0.07487823, 0.040759042, 0.06616072, 0.053296715, 0.055506192, 0.061547846, 0.052632462, 0.02215211, 0.032677043, 0.020894155, 0.041338116, 0.057987716, 0.029300896, 0.03918889, 0.05107439, 0.045973554, 0.04076921, 0.034562152, 0.038847547, 0.035148643, 0.053264517, 0.09142759, 0.11708898, 0.10627428, 0.07052887, 0.097953595, 0.06801047, 0.111257754, 0.11480684, 0.0912595, 0.052282773, 0.039644837, 0.047032557, 0.067201145, 0.06399343, 0.08946956, 0.035359573, 0.055067953, 0.050665032, 0.05199925, 0.043533653, 0.03794272, 0.04108609, 0.0387775, 0.05270313, 0.047953047, 0.04778478, 0.03006319, 0.03584867, 0.068707466, 0.04740351, 0.042931706, 0.029693797, 0.03356029, 0.03351022, 0.039194673, 0.030542368, 0.03403294, 0.12658806, 0.07752779, 0.13844886, 0.10902557, 0.07874318, 0.09711001, 0.079125844, 0.09650143, 0.06714523, 0.088850036, 0.07059998, 0.05039119, 0.03780687, 0.08497589, 0.069479175, 0.04881991, 0.035854496, 0.053758074, 0.053598687, 0.08964565, 0.047166217, 0.03546658, 0.03705533, 0.052678917, 0.057705116, 0.049477547, 0.02997166, 0.044683922, 0.026100997, 0.048260957, 0.022072965, 0.030120144, 0.035833444, 0.027722254, 0.030541016, 0.025800236, 0.02955396, 0.024039866, 0.10797541, 0.09048535, 0.08772559, 0.08462168, 0.05594216, 0.07676573, 0.07458782, 0.09240043, 0.100864954, 0.0825813, 0.042032935, 0.07529154, 0.047045607, 0.05886462, 0.062052514, 0.033769324, 0.037470847, 0.05195612, 0.07976921, 0.09237752, 0.06096188, 0.041985735, 0.05295663, 0.04700085, 0.059515532, 0.03566997, 0.043968946, 0.034387387, 0.033910025, 0.031378757, 0.06852161, 0.043980956, 0.039913796, 0.027646733, 0.027165087, 0.02814535, 0.060447894, 0.034048993, 0.07264953, 0.11318873, 0.11795866, 0.0856341, 0.1226319, 0.09190736, 0.089847356, 0.13316102, 0.07367947, 0.12711997, 0.07377452, 0.06133484, 0.10032818, 0.06833879, 0.057788968, 0.056643892, 0.043682743, 0.03333813, 0.058794744, 0.05761045, 0.03655115, 0.048120856, 0.04186337]

Ascent losses:

{26: -0.08835052, 66: -0.088358305, 106: -0.07763564, 145: -0.07562972, 184: -0.0756185, 223: -0.07550692, 262: -0.07554554, 301: -0.07560769, 340: -0.075539604, 379: -0.0754948, 424: -0.075486004, 463: -0.07553154, 502: -0.07555966, 541: -0.07558161, 580: -0.075521864, 619: -0.07545592, 658: -0.07561755, 697: -0.07555238, 736: -0.07548632, 775: -0.075546496, 819: -0.07550481, 858: -0.07546328, 897: -0.07551858, 936: -0.07548465, 975: -0.0010752758, 1020: -0.0007663826, 1059: -0.0008564415, 1098: -0.0009949268, 1137: -0.00070680055, 1176: -0.00062032556, 1220: -0.00068740395, 1259: -0.000718875, 1298: -0.00037869203, 1337: -0.0004622455, 1376: -0.00056128966, 1421: -0.00084805733, 1460: -0.0005242416, 1499: -0.0006661411, 1538: -0.0008305175, 1577: -0.00065308047, 1621: -0.00048398098, 1660: -0.00039100848, 1699: -0.000591555, 1738: -0.00039353312, 1777: -0.0003008613, 1822: -0.0006114516, 1861: -0.00062320154, 1900: -0.00070224766, 1939: -0.00070812635, 1978: -0.00055947667, 2022: -0.0003201163, 2061: -0.00035293354, 2100: -0.0005021586, 2139: -0.00049862807, 2178: -0.0007257815, 2223: -0.0006987017, 2262: -0.00074217527, 2301: -0.00042527198, 2340: -0.0006110968, 2379: -0.0006802686, 2423: -0.00060507853, 2462: -0.00047401153, 2501: -0.0007300989, 2540: -0.00043575876, 2579: -0.0005418912, 2624: -0.0003567898, 2663: -0.00062869606, 2702: -0.0004782414, 2741: -0.0005604732, 2780: -0.00063050876, 2819: -0.0005037375, 2858: -0.00037927288, 2897: -0.00043905183, 2936: -0.00041582627, 2975: -0.00045373026, 3019: -0.00055804604, 3058: -0.00042088045, 3097: -0.0005948546, 3136: -0.000555955, 3175: -0.00044624452, 3220: -0.00071885606, 3259: -0.00039756426, 3298: -0.0006341787, 3337: -0.00071478187, 3376: -0.00047451997, 3420: -0.00051656837, 3459: -0.00043850794, 3498: -0.00069860416, 3537: -0.0005537152, 3576: -0.00051060965, 3621: -0.0004292973, 3660: -0.00045293447, 3699: -0.0006884376, 3738: -0.0005433046, 3777: -0.00045552393, 3821: -0.00034839378, 3860: -0.0005524273, 3899: -0.0004923644, 3938: -0.0004079044, 3977: -0.00050906796, 4022: -0.00030677373, 4061: -0.00057991955, 4100: -0.0006772848, 4139: -0.00059908826, 4178: -0.00048795895, 4222: -0.00064091804, 4261: -0.00037446542, 4300: -0.00047452978, 4339: -0.00043325234, 4378: -0.0004896889, 4423: -0.00054120354, 4462: -0.00042481584, 4501: -0.0005972887, 4540: -0.00059411046, 4579: -0.0005227408, 4623: -0.00064213184, 4662: -0.00041686502, 4701: -0.00036642206, 4740: -0.0003880219, 4779: -0.0003979321, 4824: -0.00037689466, 4863: -0.00039352235, 4902: -0.00040647603, 4941: -0.00051629, 4980: -0.0005575206, 5019: -0.00036414742, 5058: -0.00063663063, 5097: -0.0004121946, 5136: -0.00058369915, 5175: -0.00027654026, 5219: -0.0003521116, 5258: -0.00059340283, 5297: -0.00041772504, 5336: -0.00045508076, 5375: -0.00055373565, 5420: -0.00041234342, 5459: -0.0005181476, 5498: -0.00059512805, 5537: -0.0010082803, 5576: -0.0004182099, 5620: -0.00031881523, 5659: -0.00039067797, 5698: -0.00050895737, 5737: -0.00045844482, 5776: -0.00032966048, 5821: -0.00039683626, 5860: -0.0004905468, 5899: -0.00043743727, 5938: -0.00031821904, 5977: -0.0003391255, 6021: -0.00041795557, 6060: -0.0004114424, 6099: -0.00067165354, 6138: -0.0004354052, 6177: -0.00043799504, 6222: -0.0004345938, 6261: -0.00046930747, 6300: -0.00047007427, 6339: -0.0006234937, 6378: -0.00024279716, 6422: -0.0005045523, 6461: -0.00041335268, 6500: -0.0005412265, 6539: -0.00038662218, 6578: -0.0004430853, 6623: -0.0003435249, 6662: -0.0003517364, 6701: -0.00035546126, 6740: -0.00044033348, 6779: -0.00032006728, 6823: -0.00042177198, 6862: -0.000428173, 6901: -0.00031434867, 6940: -0.00039933305, 6979: -0.0003950497, 7024: -0.00046649383, 7063: -0.00040269908, 7102: -0.000406897, 7141: -0.00030607302, 7180: -0.00037605278, 7219: -0.00045897774, 7258: -0.00040263133, 7297: -0.00027034385, 7336: -0.00030968653, 7375: -0.00047525787, 7419: -0.00037114945, 7458: -0.000435312, 7497: -0.00067909434, 7536: -0.00035058186, 7575: -0.00048491158, 7620: -0.0004272982, 7659: -0.00048484647, 7698: -0.0004050998, 7737: -0.00032155594, 7776: -0.0005983271, 7820: -0.0003673014, 7859: -0.00046119044, 7898: -0.00040366105, 7937: -0.00040921092, 7976: -0.00052317284, 8021: -0.0006113482, 8060: -0.0004050432, 8099: -0.00045936144, 8138: -0.00054301025, 8177: -0.00052625703, 8221: -0.00027580676, 8260: -0.00046434018, 8299: -0.00036680335, 8338: -0.00027787188, 8377: -0.00040895745, 8422: -0.0004796514, 8461: -0.0004663389, 8500: -0.00044009177, 8539: -0.00043774632, 8578: -0.0005220676, 8622: -0.00025681808, 8661: -0.00045358352, 8700: -0.00043388092, 8739: -0.00040812828, 8778: -0.0003366937, 8823: -0.00046535983, 8862: -0.00055826316, 8901: -0.00045500495, 8940: -0.0003351208, 8979: -0.00026404075, 9023: -0.0004218227, 9062: -0.00032834115, 9101: -0.00042731932, 9140: -0.00044611606, 9179: -0.00034464733, 9224: -0.00036557773, 9263: -0.00047442596, 9302: -0.00041006322, 9341: -0.00041642218, 9380: -0.0004879002, 9419: -0.00043672975, 9458: -0.00037445303, 9497: -0.00020253446, 9536: -0.000290061, 9575: -0.00043768794, 9619: -0.00035139598, 9658: -0.0002704525, 9697: -0.00040812857, 9736: -0.000569304, 9775: -0.00044547042, 9820: -0.0004886295, 9859: -0.00051579135, 9898: -0.00044975244, 9937: -0.0003183773, 9976: -0.00046483305}
Importing data files...
['mm1d_1.csv', 'mm1d_4.csv', 'mm1d_0.csv', 'mm1d_2.csv', 'mm1d_3.csv', 'mm1d_5.csv']
Splitting data into training and test sets with a ratio of: 0.2
Total number of training samples is 38400
Total number of test samples is 9600
Length of an output spectrum is 300
Generating torch datasets
training using gradient ascent
Starting training process
Doing Evaluation on the model now
This is Epoch 0, training loss 8.82039, validation loss 8.86095
Resetting learning rate to 0.01000
Epoch    14: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 10, training loss 8.82840, validation loss 8.88844
Epoch    28: reducing learning rate of group 0 to 2.5000e-04.
Epoch    39: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 20, training loss 8.83391, validation loss 8.88553
Epoch    50: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 25
Mean train loss for ascent epoch 26: -0.4419128894805908
Mean eval for ascent epoch 26: 8.838257789611816
Doing Evaluation on the model now
This is Epoch 30, training loss 8.83645, validation loss 8.84824
Epoch    64: reducing learning rate of group 0 to 5.0000e-03.
Epoch    75: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 40, training loss 8.83391, validation loss 8.86324
Epoch    86: reducing learning rate of group 0 to 1.2500e-03.
Epoch    97: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 50, training loss 3.21560, validation loss 3.02850
Epoch   108: reducing learning rate of group 0 to 3.1250e-04.
Epoch   119: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 60, training loss 0.16483, validation loss 0.14250
Epoch   130: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 65
Mean train loss for ascent epoch 66: -0.018962273374199867
Mean eval for ascent epoch 66: 0.3761151432991028
Doing Evaluation on the model now
This is Epoch 70, training loss 1.08777, validation loss 1.83081
Epoch   141: reducing learning rate of group 0 to 5.0000e-03.
Epoch   152: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 80, training loss 0.09851, validation loss 0.06486
Epoch   163: reducing learning rate of group 0 to 1.2500e-03.
Epoch   174: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 90, training loss 0.03875, validation loss 0.02813
Epoch   185: reducing learning rate of group 0 to 3.1250e-04.
Epoch   196: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 100, training loss 0.02355, validation loss 0.02739
Epoch   207: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 104
Mean train loss for ascent epoch 105: -0.008222019299864769
Mean eval for ascent epoch 105: 0.024198921397328377
Epoch   218: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 110, training loss 1.11759, validation loss 0.32360
Epoch   229: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 120, training loss 0.11407, validation loss 0.11162
Epoch   240: reducing learning rate of group 0 to 1.2500e-03.
Epoch   251: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 130, training loss 0.04951, validation loss 0.04399
Epoch   262: reducing learning rate of group 0 to 3.1250e-04.
Epoch   273: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 140, training loss 0.02017, validation loss 0.01923
Epoch   284: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 143
Mean train loss for ascent epoch 144: -0.005968905054032803
Mean eval for ascent epoch 144: 0.021770451217889786
Epoch   295: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 150, training loss 0.16295, validation loss 0.07566
Epoch   306: reducing learning rate of group 0 to 2.5000e-03.
Epoch   317: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 160, training loss 0.15224, validation loss 0.29708
Epoch   328: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 170, training loss 0.06098, validation loss 0.03249
Epoch   339: reducing learning rate of group 0 to 3.1250e-04.
Epoch   350: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 180, training loss 0.02791, validation loss 0.02168
Epoch   361: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 182
Mean train loss for ascent epoch 183: -0.0056813606061041355
Mean eval for ascent epoch 183: 0.022311711683869362
Epoch   372: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 190, training loss 0.20473, validation loss 0.84990
Epoch   383: reducing learning rate of group 0 to 2.5000e-03.
Epoch   394: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 200, training loss 0.05405, validation loss 0.04972
Resetting learning rate to 0.01000
Epoch   405: reducing learning rate of group 0 to 5.0000e-04.
Epoch   416: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 210, training loss 0.01813, validation loss 0.02172
Epoch   427: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 220, training loss 0.01827, validation loss 0.03255
Epoch   438: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 221
Mean train loss for ascent epoch 222: -0.005532088689506054
Mean eval for ascent epoch 222: 0.018423551693558693
Epoch   449: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 230, training loss 0.15592, validation loss 0.46648
Epoch   460: reducing learning rate of group 0 to 2.5000e-03.
Epoch   471: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 240, training loss 0.05847, validation loss 0.06514
Epoch   482: reducing learning rate of group 0 to 6.2500e-04.
Epoch   493: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 250, training loss 0.03857, validation loss 0.04135
Epoch   504: reducing learning rate of group 0 to 1.5625e-04.
Epoch   515: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 260
Doing Evaluation on the model now
This is Epoch 260, training loss 0.01602, validation loss 0.01616
Mean train loss for ascent epoch 261: -0.004447691608220339
Mean eval for ascent epoch 261: 0.01884397864341736
Epoch   526: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 270, training loss 0.26980, validation loss 0.63574
Epoch   537: reducing learning rate of group 0 to 2.5000e-03.
Epoch   548: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 280, training loss 0.04745, validation loss 0.05041
Epoch   559: reducing learning rate of group 0 to 6.2500e-04.
Epoch   570: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 290, training loss 0.02002, validation loss 0.01479
Epoch   581: reducing learning rate of group 0 to 1.5625e-04.
Epoch   592: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 299
Mean train loss for ascent epoch 300: -0.005676337517797947
Mean eval for ascent epoch 300: 0.017165442928671837
Doing Evaluation on the model now
This is Epoch 300, training loss 0.01717, validation loss 0.01516
Epoch   603: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 310, training loss 0.17165, validation loss 0.14372
Epoch   614: reducing learning rate of group 0 to 2.5000e-03.
Epoch   625: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 320, training loss 0.04426, validation loss 0.06974
Epoch   636: reducing learning rate of group 0 to 6.2500e-04.
Epoch   647: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 330, training loss 0.02306, validation loss 0.01696
Epoch   658: reducing learning rate of group 0 to 1.5625e-04.
Epoch   669: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 338
Mean train loss for ascent epoch 339: -0.004026309587061405
Mean eval for ascent epoch 339: 0.014560644514858723
Doing Evaluation on the model now
This is Epoch 340, training loss 0.26032, validation loss 0.16894
Epoch   680: reducing learning rate of group 0 to 5.0000e-03.
Epoch   691: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 350, training loss 0.09818, validation loss 0.09200
Epoch   702: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 360, training loss 0.04207, validation loss 0.01643
Epoch   713: reducing learning rate of group 0 to 6.2500e-04.
Epoch   724: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 370, training loss 0.01387, validation loss 0.01201
Epoch   735: reducing learning rate of group 0 to 1.5625e-04.
Epoch   746: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 377
Mean train loss for ascent epoch 378: -0.004754942376166582
Mean eval for ascent epoch 378: 0.013618501834571362
Doing Evaluation on the model now
This is Epoch 380, training loss 0.70359, validation loss 0.26624
Epoch   757: reducing learning rate of group 0 to 5.0000e-03.
Epoch   768: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 390, training loss 0.10191, validation loss 0.04676
Epoch   779: reducing learning rate of group 0 to 1.2500e-03.
Epoch   790: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 400, training loss 0.03534, validation loss 0.02969
Resetting learning rate to 0.01000
Epoch   801: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 410, training loss 0.01809, validation loss 0.01401
Epoch   812: reducing learning rate of group 0 to 2.5000e-04.
Epoch   823: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 420, training loss 0.01296, validation loss 0.01262
Epoch   834: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 422
Mean train loss for ascent epoch 423: -0.004533042199909687
Mean eval for ascent epoch 423: 0.01397076714783907
Epoch   845: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 430, training loss 0.09900, validation loss 0.11087
Epoch   856: reducing learning rate of group 0 to 2.5000e-03.
Epoch   867: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 440, training loss 0.04450, validation loss 0.02201
Epoch   878: reducing learning rate of group 0 to 6.2500e-04.
Epoch   889: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 450, training loss 0.02057, validation loss 0.01447
Epoch   900: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 460, training loss 0.01091, validation loss 0.01058
Epoch   911: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 461
Mean train loss for ascent epoch 462: -0.004541659262031317
Mean eval for ascent epoch 462: 0.011186787858605385
Epoch   922: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 470, training loss 0.34730, validation loss 0.73367
Epoch   933: reducing learning rate of group 0 to 2.5000e-03.
Epoch   944: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 480, training loss 0.05168, validation loss 0.02612
Epoch   955: reducing learning rate of group 0 to 6.2500e-04.
Epoch   966: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 490, training loss 0.01614, validation loss 0.01813
Epoch   977: reducing learning rate of group 0 to 1.5625e-04.
Epoch   988: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 500
Doing Evaluation on the model now
This is Epoch 500, training loss 0.01390, validation loss 0.01796
Mean train loss for ascent epoch 501: -0.003664506133645773
Mean eval for ascent epoch 501: 0.014120947569608688
Epoch   999: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 510, training loss 0.20739, validation loss 0.10011
Epoch  1010: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1021: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 520, training loss 0.02216, validation loss 0.02070
Epoch  1032: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1043: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 530, training loss 0.01719, validation loss 0.01412
Epoch  1054: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1065: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 539
Mean train loss for ascent epoch 540: -0.004708666354417801
Mean eval for ascent epoch 540: 0.01461674552410841
Doing Evaluation on the model now
This is Epoch 540, training loss 0.01462, validation loss 0.01172
Epoch  1076: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1087: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 550, training loss 0.38154, validation loss 0.12101
Epoch  1098: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 560, training loss 0.05998, validation loss 0.02617
Epoch  1109: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1120: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 570, training loss 0.01512, validation loss 0.01481
Epoch  1131: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1142: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 578
Mean train loss for ascent epoch 579: -0.005218213889747858
Mean eval for ascent epoch 579: 0.013445320539176464
Doing Evaluation on the model now
This is Epoch 580, training loss 0.23794, validation loss 0.15380
Epoch  1153: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1164: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 590, training loss 0.08865, validation loss 0.08410
Epoch  1175: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1186: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 600, training loss 0.04880, validation loss 0.02255
Resetting learning rate to 0.01000
Epoch  1197: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 610, training loss 0.01643, validation loss 0.02580
Epoch  1208: reducing learning rate of group 0 to 2.5000e-04.
Epoch  1219: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 620, training loss 0.01151, validation loss 0.01034
Epoch  1230: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 622
Mean train loss for ascent epoch 623: -0.00465252622961998
Mean eval for ascent epoch 623: 0.011677458882331848
Epoch  1241: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 630, training loss 0.08608, validation loss 0.11386
Epoch  1252: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1263: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 640, training loss 0.03708, validation loss 0.02690
Epoch  1274: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1285: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 650, training loss 0.02016, validation loss 0.02261
Epoch  1296: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 660, training loss 0.01247, validation loss 0.01230
Epoch  1307: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 661
Mean train loss for ascent epoch 662: -0.004028137307614088
Mean eval for ascent epoch 662: 0.0125217130407691
Epoch  1318: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 670, training loss 0.15934, validation loss 0.15557
Epoch  1329: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1340: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 680, training loss 0.02530, validation loss 0.01923
Epoch  1351: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1362: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 690, training loss 0.01898, validation loss 0.01543
Epoch  1373: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1384: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 700
Doing Evaluation on the model now
This is Epoch 700, training loss 0.01236, validation loss 0.01469
Mean train loss for ascent epoch 701: -0.0035484079271554947
Mean eval for ascent epoch 701: 0.014195460826158524
Epoch  1395: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 710, training loss 0.11472, validation loss 0.22372
Epoch  1406: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1417: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 720, training loss 0.05798, validation loss 0.07306
Epoch  1428: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1439: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 730, training loss 0.01222, validation loss 0.01144
Epoch  1450: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1461: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 739
Mean train loss for ascent epoch 740: -0.004232197534292936
Mean eval for ascent epoch 740: 0.012709304690361023
Doing Evaluation on the model now
This is Epoch 740, training loss 0.01271, validation loss 0.01239
Epoch  1472: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 750, training loss 0.10202, validation loss 0.08729
Epoch  1483: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1494: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 760, training loss 0.03438, validation loss 0.02503
Epoch  1505: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1516: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 770, training loss 0.01308, validation loss 0.01362
Epoch  1527: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1538: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 778
Mean train loss for ascent epoch 779: -0.005239027086645365
Mean eval for ascent epoch 779: 0.013152972795069218
Doing Evaluation on the model now
This is Epoch 780, training loss 0.31547, validation loss 0.23752
Epoch  1549: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1560: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 790, training loss 0.12585, validation loss 0.04704
Epoch  1571: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 800, training loss 0.02947, validation loss 0.02667
Epoch  1582: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  1593: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 810, training loss 0.01473, validation loss 0.01143
Epoch  1604: reducing learning rate of group 0 to 2.5000e-04.
Epoch  1615: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 820, training loss 0.01136, validation loss 0.01224
Epoch  1626: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 823
Mean train loss for ascent epoch 824: -0.0036929340567439795
Mean eval for ascent epoch 824: 0.010677664540708065
Epoch  1637: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 830, training loss 0.08177, validation loss 0.08755
Epoch  1648: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1659: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 840, training loss 0.03128, validation loss 0.02260
Epoch  1670: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 850, training loss 0.01779, validation loss 0.02649
Epoch  1681: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1692: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 860, training loss 0.01043, validation loss 0.00935
Epoch  1703: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 862
Mean train loss for ascent epoch 863: -0.003151800949126482
Mean eval for ascent epoch 863: 0.011013838462531567
Epoch  1714: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 870, training loss 0.21238, validation loss 0.27347
Epoch  1725: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1736: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 880, training loss 0.03023, validation loss 0.01411
Epoch  1747: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1758: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 890, training loss 0.01422, validation loss 0.01360
Epoch  1769: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 900, training loss 0.01068, validation loss 0.01099
Epoch  1780: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 901
Mean train loss for ascent epoch 902: -0.002512693405151367
Mean eval for ascent epoch 902: 0.010283383540809155
Epoch  1791: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 910, training loss 0.08468, validation loss 0.14036
Epoch  1802: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1813: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 920, training loss 0.03986, validation loss 0.03025
Epoch  1824: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1835: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 930, training loss 0.01171, validation loss 0.01360
Epoch  1846: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1857: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 940
Doing Evaluation on the model now
This is Epoch 940, training loss 0.01018, validation loss 0.01305
Mean train loss for ascent epoch 941: -0.0034617444034665823
Mean eval for ascent epoch 941: 0.010881173424422741
Epoch  1868: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 950, training loss 0.06577, validation loss 0.09379
Epoch  1879: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1890: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 960, training loss 0.04623, validation loss 0.02268
Epoch  1901: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1912: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 970, training loss 0.01385, validation loss 0.01303
Epoch  1923: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1934: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 979
Mean train loss for ascent epoch 980: -0.003966071177273989
Mean eval for ascent epoch 980: 0.008815155364573002
Doing Evaluation on the model now
This is Epoch 980, training loss 0.00882, validation loss 0.00874
Epoch  1945: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1956: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 990, training loss 0.14608, validation loss 0.05044
Epoch  1967: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1000, training loss 0.03519, validation loss 0.01511
Resetting learning rate to 0.01000
Epoch  1978: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1989: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1010, training loss 0.01035, validation loss 0.01008
Epoch  2000: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2011: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1018
Mean train loss for ascent epoch 1019: -0.005205454770475626
Mean eval for ascent epoch 1019: 0.01082055363804102
Doing Evaluation on the model now
This is Epoch 1020, training loss 0.24052, validation loss 0.28762
Epoch  2022: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2033: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1030, training loss 0.04294, validation loss 0.03010
Epoch  2044: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2055: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1040, training loss 0.02215, validation loss 0.01206
Epoch  2066: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1050, training loss 0.01166, validation loss 0.01146
Epoch  2077: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2088: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1057
Mean train loss for ascent epoch 1058: -0.002811085432767868
Mean eval for ascent epoch 1058: 0.01026926003396511
Doing Evaluation on the model now
This is Epoch 1060, training loss 0.68938, validation loss 1.01524
Epoch  2099: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2110: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1070, training loss 0.06117, validation loss 0.05090
Epoch  2121: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2132: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1080, training loss 0.03566, validation loss 0.02986
Epoch  2143: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2154: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1090, training loss 0.01192, validation loss 0.01059
Epoch  2165: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1096
Mean train loss for ascent epoch 1097: -0.003522112499922514
Mean eval for ascent epoch 1097: 0.009649615734815598
Doing Evaluation on the model now
This is Epoch 1100, training loss 0.27464, validation loss 0.14894
Epoch  2176: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2187: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1110, training loss 0.04722, validation loss 0.03724
Epoch  2198: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2209: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1120, training loss 0.01682, validation loss 0.01555
Epoch  2220: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2231: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1130, training loss 0.00992, validation loss 0.00914
Epoch  2242: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1135
Mean train loss for ascent epoch 1136: -0.002732127672061324
Mean eval for ascent epoch 1136: 0.008532681502401829
Doing Evaluation on the model now
This is Epoch 1140, training loss 0.49308, validation loss 0.32660
Epoch  2253: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2264: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1150, training loss 0.04501, validation loss 0.05241
Epoch  2275: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2286: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1160, training loss 0.01529, validation loss 0.03174
Epoch  2297: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2308: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1170, training loss 0.00780, validation loss 0.00926
Epoch  2319: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1174
Mean train loss for ascent epoch 1175: -0.002772730775177479
Mean eval for ascent epoch 1175: 0.007592989597469568
Epoch  2330: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1180, training loss 0.14808, validation loss 0.14949
Epoch  2341: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1190, training loss 0.04083, validation loss 0.03131
Epoch  2352: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2363: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1200, training loss 0.01714, validation loss 0.01115
Resetting learning rate to 0.01000
Epoch  2374: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2385: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1210, training loss 0.01076, validation loss 0.01063
Epoch  2396: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2407: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1218
Mean train loss for ascent epoch 1219: -0.0031142050866037607
Mean eval for ascent epoch 1219: 0.00849767867475748
Doing Evaluation on the model now
This is Epoch 1220, training loss 0.18013, validation loss 0.35655
Epoch  2418: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2429: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1230, training loss 0.05104, validation loss 0.01474
Epoch  2440: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1240, training loss 0.01744, validation loss 0.03773
Epoch  2451: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2462: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1250, training loss 0.01086, validation loss 0.00953
Epoch  2473: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2484: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1257
Mean train loss for ascent epoch 1258: -0.003404557704925537
Mean eval for ascent epoch 1258: 0.007800807245075703
Doing Evaluation on the model now
This is Epoch 1260, training loss 0.65099, validation loss 0.80889
Epoch  2495: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2506: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1270, training loss 0.03630, validation loss 0.06059
Epoch  2517: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2528: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1280, training loss 0.01786, validation loss 0.01664
Epoch  2539: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1290, training loss 0.00840, validation loss 0.00881
Epoch  2550: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2561: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1296
Mean train loss for ascent epoch 1297: -0.0029572765342891216
Mean eval for ascent epoch 1297: 0.00819601584225893
Doing Evaluation on the model now
This is Epoch 1300, training loss 0.18391, validation loss 0.05888
Epoch  2572: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2583: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1310, training loss 0.08781, validation loss 0.09199
Epoch  2594: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2605: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1320, training loss 0.01804, validation loss 0.01598
Epoch  2616: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2627: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1330, training loss 0.00856, validation loss 0.00738
Epoch  2638: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1335
Mean train loss for ascent epoch 1336: -0.0020815511234104633
Mean eval for ascent epoch 1336: 0.006841557566076517
Doing Evaluation on the model now
This is Epoch 1340, training loss 0.59699, validation loss 0.69878
Epoch  2649: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2660: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1350, training loss 0.04451, validation loss 0.03226
Epoch  2671: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2682: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1360, training loss 0.01591, validation loss 0.01866
Epoch  2693: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2704: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1370, training loss 0.00844, validation loss 0.01677
Epoch  2715: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1374
Mean train loss for ascent epoch 1375: -0.002004576614126563
Mean eval for ascent epoch 1375: 0.007972411811351776
Epoch  2726: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1380, training loss 0.38617, validation loss 0.21832
Epoch  2737: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1390, training loss 0.06405, validation loss 0.08827
Epoch  2748: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2759: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1400, training loss 0.02028, validation loss 0.01265
Resetting learning rate to 0.01000
Epoch  2770: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2781: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1410, training loss 0.00762, validation loss 0.00951
Epoch  2792: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2803: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1419
Mean train loss for ascent epoch 1420: -0.0025675727520138025
Mean eval for ascent epoch 1420: 0.007116100285202265
Doing Evaluation on the model now
This is Epoch 1420, training loss 0.00712, validation loss 0.00674
Epoch  2814: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2825: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1430, training loss 0.04897, validation loss 0.02976
Epoch  2836: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1440, training loss 0.02105, validation loss 0.03342
Epoch  2847: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2858: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1450, training loss 0.01047, validation loss 0.00807
Epoch  2869: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2880: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1458
Mean train loss for ascent epoch 1459: -0.0025857698637992144
Mean eval for ascent epoch 1459: 0.00786937028169632
Doing Evaluation on the model now
This is Epoch 1460, training loss 0.06326, validation loss 0.06379
Epoch  2891: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2902: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1470, training loss 0.02667, validation loss 0.02132
Epoch  2913: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2924: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1480, training loss 0.02248, validation loss 0.04229
Epoch  2935: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1490, training loss 0.01015, validation loss 0.00710
Epoch  2946: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2957: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1497
Mean train loss for ascent epoch 1498: -0.002064292784780264
Mean eval for ascent epoch 1498: 0.007515764329582453
Doing Evaluation on the model now
This is Epoch 1500, training loss 0.16512, validation loss 0.20898
Epoch  2968: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2979: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1510, training loss 0.06022, validation loss 0.09906
Epoch  2990: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3001: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1520, training loss 0.01895, validation loss 0.02083
Epoch  3012: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3023: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1530, training loss 0.00913, validation loss 0.00801
Epoch  3034: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1536
Mean train loss for ascent epoch 1537: -0.003367096884176135
Mean eval for ascent epoch 1537: 0.007354799658060074
Doing Evaluation on the model now
This is Epoch 1540, training loss 0.19631, validation loss 0.42392
Epoch  3045: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3056: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1550, training loss 0.04796, validation loss 0.06115
Epoch  3067: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3078: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1560, training loss 0.01354, validation loss 0.01197
Epoch  3089: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3100: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1570, training loss 0.00686, validation loss 0.00676
Epoch  3111: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1575
Mean train loss for ascent epoch 1576: -0.0018877143738791347
Mean eval for ascent epoch 1576: 0.007685147691518068
Doing Evaluation on the model now
This is Epoch 1580, training loss 0.27609, validation loss 0.26005
Epoch  3122: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3133: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1590, training loss 0.05399, validation loss 0.11308
Epoch  3144: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3155: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1600, training loss 0.01073, validation loss 0.01027
Resetting learning rate to 0.01000
Epoch  3166: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3177: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1610, training loss 0.01385, validation loss 0.01025
Epoch  3188: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3199: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1619
Mean train loss for ascent epoch 1620: -0.002202417701482773
Mean eval for ascent epoch 1620: 0.007701846305280924
Doing Evaluation on the model now
This is Epoch 1620, training loss 0.00770, validation loss 0.00622
Epoch  3210: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1630, training loss 0.10861, validation loss 0.14372
Epoch  3221: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3232: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1640, training loss 0.01941, validation loss 0.03217
Epoch  3243: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3254: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1650, training loss 0.00657, validation loss 0.00671
Epoch  3265: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3276: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1658
Mean train loss for ascent epoch 1659: -0.0016016728477552533
Mean eval for ascent epoch 1659: 0.006646487861871719
Doing Evaluation on the model now
This is Epoch 1660, training loss 0.12899, validation loss 0.23237
Epoch  3287: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3298: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1670, training loss 0.03361, validation loss 0.03277
Epoch  3309: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1680, training loss 0.01962, validation loss 0.02068
Epoch  3320: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3331: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1690, training loss 0.00806, validation loss 0.00746
Epoch  3342: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3353: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1697
Mean train loss for ascent epoch 1698: -0.0030265087261795998
Mean eval for ascent epoch 1698: 0.008733757771551609
Doing Evaluation on the model now
This is Epoch 1700, training loss 0.09957, validation loss 0.20261
Epoch  3364: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3375: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1710, training loss 0.02491, validation loss 0.04125
Epoch  3386: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3397: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1720, training loss 0.01415, validation loss 0.01097
Epoch  3408: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1730, training loss 0.00869, validation loss 0.01202
Epoch  3419: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3430: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1736
Mean train loss for ascent epoch 1737: -0.002095235977321863
Mean eval for ascent epoch 1737: 0.009301219135522842
Doing Evaluation on the model now
This is Epoch 1740, training loss 0.45517, validation loss 0.44340
Epoch  3441: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3452: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1750, training loss 0.05733, validation loss 0.03059
Epoch  3463: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3474: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1760, training loss 0.01335, validation loss 0.01116
Epoch  3485: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3496: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1770, training loss 0.00763, validation loss 0.00853
Epoch  3507: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1775
Mean train loss for ascent epoch 1776: -0.003630982479080558
Mean eval for ascent epoch 1776: 0.00773259112611413
Doing Evaluation on the model now
This is Epoch 1780, training loss 0.21053, validation loss 0.12407
Epoch  3518: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3529: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1790, training loss 0.03826, validation loss 0.02810
Epoch  3540: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3551: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1800, training loss 0.01130, validation loss 0.01012
Resetting learning rate to 0.01000
Epoch  3562: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3573: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1810, training loss 0.00691, validation loss 0.00677
Epoch  3584: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3595: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1820
Doing Evaluation on the model now
This is Epoch 1820, training loss 0.00700, validation loss 0.00667
Mean train loss for ascent epoch 1821: -0.002823852701112628
Mean eval for ascent epoch 1821: 0.007618431933224201
Epoch  3606: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1830, training loss 0.05511, validation loss 0.02654
Epoch  3617: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3628: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1840, training loss 0.01378, validation loss 0.02741
Epoch  3639: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3650: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1850, training loss 0.00865, validation loss 0.00752
Epoch  3661: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3672: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1859
Mean train loss for ascent epoch 1860: -0.0018238560296595097
Mean eval for ascent epoch 1860: 0.00637830188497901
Doing Evaluation on the model now
This is Epoch 1860, training loss 0.00638, validation loss 0.00631
Epoch  3683: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3694: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1870, training loss 0.05228, validation loss 0.10685
Epoch  3705: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1880, training loss 0.01610, validation loss 0.05831
Epoch  3716: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3727: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1890, training loss 0.00694, validation loss 0.00760
Epoch  3738: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3749: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1898
Mean train loss for ascent epoch 1899: -0.0017772618448361754
Mean eval for ascent epoch 1899: 0.006621289998292923
Doing Evaluation on the model now
This is Epoch 1900, training loss 0.21129, validation loss 0.80036
Epoch  3760: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3771: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1910, training loss 0.04053, validation loss 0.02709
Epoch  3782: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3793: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1920, training loss 0.01649, validation loss 0.01069
Epoch  3804: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1930, training loss 0.01097, validation loss 0.01144
Epoch  3815: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3826: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1937
Mean train loss for ascent epoch 1938: -0.0020599619019776583
Mean eval for ascent epoch 1938: 0.006405949592590332
Doing Evaluation on the model now
This is Epoch 1940, training loss 0.12188, validation loss 0.15523
Epoch  3837: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3848: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1950, training loss 0.06298, validation loss 0.01286
Epoch  3859: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3870: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1960, training loss 0.00845, validation loss 0.01002
Epoch  3881: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3892: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1970, training loss 0.00675, validation loss 0.00941
Epoch  3903: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1976
Mean train loss for ascent epoch 1977: -0.0020330355037003756
Mean eval for ascent epoch 1977: 0.006526487413793802
Doing Evaluation on the model now
This is Epoch 1980, training loss 0.29419, validation loss 0.67138
Epoch  3914: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3925: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1990, training loss 0.03456, validation loss 0.03239
Epoch  3936: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3947: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2000, training loss 0.01813, validation loss 0.01675
Resetting learning rate to 0.01000
Epoch  3958: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3969: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2010, training loss 0.00769, validation loss 0.00701
Epoch  3980: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3991: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2020
Doing Evaluation on the model now
This is Epoch 2020, training loss 0.00558, validation loss 0.00525
Mean train loss for ascent epoch 2021: -0.0016876221634447575
Mean eval for ascent epoch 2021: 0.005694589577615261
Epoch  4002: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2030, training loss 0.15483, validation loss 0.19001
Epoch  4013: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4024: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2040, training loss 0.02083, validation loss 0.01400
Epoch  4035: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4046: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2050, training loss 0.00959, validation loss 0.00730
Epoch  4057: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4068: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2059
Mean train loss for ascent epoch 2060: -0.0014774278970435262
Mean eval for ascent epoch 2060: 0.0058638290502130985
Doing Evaluation on the model now
This is Epoch 2060, training loss 0.00586, validation loss 0.00561
Epoch  4079: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2070, training loss 0.03829, validation loss 0.06939
Epoch  4090: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4101: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2080, training loss 0.02386, validation loss 0.03227
Epoch  4112: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4123: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2090, training loss 0.00888, validation loss 0.00724
Epoch  4134: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4145: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2098
Mean train loss for ascent epoch 2099: -0.0017701713368296623
Mean eval for ascent epoch 2099: 0.007158663123846054
Doing Evaluation on the model now
This is Epoch 2100, training loss 0.08160, validation loss 0.08228
Epoch  4156: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4167: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2110, training loss 0.02827, validation loss 0.05692
Epoch  4178: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2120, training loss 0.02259, validation loss 0.04207
Epoch  4189: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4200: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2130, training loss 0.00665, validation loss 0.00765
Epoch  4211: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4222: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2137
Mean train loss for ascent epoch 2138: -0.0024985801428556442
Mean eval for ascent epoch 2138: 0.005482863634824753
Doing Evaluation on the model now
This is Epoch 2140, training loss 0.08812, validation loss 0.03395
Epoch  4233: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4244: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2150, training loss 0.01916, validation loss 0.01690
Epoch  4255: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4266: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2160, training loss 0.01123, validation loss 0.01205
Epoch  4277: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2170, training loss 0.00547, validation loss 0.00566
Epoch  4288: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4299: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2176
Mean train loss for ascent epoch 2177: -0.0014759545447304845
Mean eval for ascent epoch 2177: 0.005993479397147894
Doing Evaluation on the model now
This is Epoch 2180, training loss 0.28040, validation loss 0.51681
Epoch  4310: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4321: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2190, training loss 0.03604, validation loss 0.01749
Epoch  4332: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4343: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2200, training loss 0.01059, validation loss 0.00976
Resetting learning rate to 0.01000
Epoch  4354: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4365: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2210, training loss 0.00651, validation loss 0.00565
Epoch  4376: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2220, training loss 0.00545, validation loss 0.00518
Epoch  4387: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2221
Mean train loss for ascent epoch 2222: -0.001949705765582621
Mean eval for ascent epoch 2222: 0.005348443519324064
Epoch  4398: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2230, training loss 0.04203, validation loss 0.01213
Epoch  4409: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4420: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2240, training loss 0.01819, validation loss 0.01565
Epoch  4431: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4442: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2250, training loss 0.00625, validation loss 0.00732
Epoch  4453: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4464: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2260
Doing Evaluation on the model now
This is Epoch 2260, training loss 0.00573, validation loss 0.00527
Mean train loss for ascent epoch 2261: -0.0014681537868455052
Mean eval for ascent epoch 2261: 0.005195388104766607
Epoch  4475: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2270, training loss 0.06282, validation loss 0.03312
Epoch  4486: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4497: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2280, training loss 0.02440, validation loss 0.02192
Epoch  4508: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4519: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2290, training loss 0.00678, validation loss 0.00657
Epoch  4530: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4541: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2299
Mean train loss for ascent epoch 2300: -0.00145727617200464
Mean eval for ascent epoch 2300: 0.00521196611225605
Doing Evaluation on the model now
This is Epoch 2300, training loss 0.00521, validation loss 0.00475
Epoch  4552: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4563: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2310, training loss 0.08136, validation loss 0.03170
Epoch  4574: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2320, training loss 0.01169, validation loss 0.01191
Epoch  4585: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4596: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2330, training loss 0.00847, validation loss 0.00941
Epoch  4607: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4618: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2338
Mean train loss for ascent epoch 2339: -0.002087968634441495
Mean eval for ascent epoch 2339: 0.005482928361743689
Doing Evaluation on the model now
This is Epoch 2340, training loss 0.12203, validation loss 0.56825
Epoch  4629: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4640: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2350, training loss 0.03990, validation loss 0.03133
Epoch  4651: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4662: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2360, training loss 0.01580, validation loss 0.02518
Epoch  4673: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2370, training loss 0.00880, validation loss 0.00832
Epoch  4684: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4695: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2377
Mean train loss for ascent epoch 2378: -0.0016739595448598266
Mean eval for ascent epoch 2378: 0.005466129165142775
Doing Evaluation on the model now
This is Epoch 2380, training loss 0.36830, validation loss 0.47136
Epoch  4706: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4717: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2390, training loss 0.04203, validation loss 0.16544
Epoch  4728: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4739: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2400, training loss 0.01522, validation loss 0.01993
Resetting learning rate to 0.01000
Epoch  4750: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4761: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2410, training loss 0.00734, validation loss 0.00611
Epoch  4772: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2420, training loss 0.00495, validation loss 0.00519
Epoch  4783: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2421
Mean train loss for ascent epoch 2422: -0.0019233223283663392
Mean eval for ascent epoch 2422: 0.006430256180465221
Epoch  4794: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2430, training loss 0.08160, validation loss 0.04027
Epoch  4805: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4816: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2440, training loss 0.01728, validation loss 0.02088
Epoch  4827: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4838: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2450, training loss 0.00659, validation loss 0.00603
Epoch  4849: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4860: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2460
Doing Evaluation on the model now
This is Epoch 2460, training loss 0.00533, validation loss 0.00493
Mean train loss for ascent epoch 2461: -0.0015870773931965232
Mean eval for ascent epoch 2461: 0.005245080683380365
Epoch  4871: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2470, training loss 0.06719, validation loss 0.02281
Epoch  4882: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4893: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2480, training loss 0.01377, validation loss 0.01984
Epoch  4904: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4915: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2490, training loss 0.00521, validation loss 0.00477
Epoch  4926: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4937: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2499
Mean train loss for ascent epoch 2500: -0.0017200661823153496
Mean eval for ascent epoch 2500: 0.005678923334926367
Doing Evaluation on the model now
This is Epoch 2500, training loss 0.00568, validation loss 0.00610
Epoch  4948: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2510, training loss 0.03155, validation loss 0.02791
Epoch  4959: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4970: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2520, training loss 0.02079, validation loss 0.01182
Epoch  4981: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4992: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2530, training loss 0.00921, validation loss 0.01093
Epoch  5003: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5014: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2538
Mean train loss for ascent epoch 2539: -0.002218716312199831
Mean eval for ascent epoch 2539: 0.00640130415558815
Doing Evaluation on the model now
This is Epoch 2540, training loss 0.09140, validation loss 0.06252
Epoch  5025: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5036: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2550, training loss 0.05666, validation loss 0.05479
Epoch  5047: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2560, training loss 0.01492, validation loss 0.00849
Epoch  5058: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5069: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2570, training loss 0.00544, validation loss 0.00538
Epoch  5080: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5091: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2577
Mean train loss for ascent epoch 2578: -0.0015626465901732445
Mean eval for ascent epoch 2578: 0.0057662129402160645
Doing Evaluation on the model now
This is Epoch 2580, training loss 0.19682, validation loss 0.17315
Epoch  5102: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5113: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2590, training loss 0.02765, validation loss 0.02634
Epoch  5124: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5135: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2600, training loss 0.01033, validation loss 0.00683
Resetting learning rate to 0.01000
Epoch  5146: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2610, training loss 0.00583, validation loss 0.00715
Epoch  5157: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5168: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2620, training loss 0.00593, validation loss 0.00583
Epoch  5179: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2622
Mean train loss for ascent epoch 2623: -0.0016868211096152663
Mean eval for ascent epoch 2623: 0.0061262138187885284
Epoch  5190: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2630, training loss 0.08501, validation loss 0.11119
Epoch  5201: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5212: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2640, training loss 0.01099, validation loss 0.01703
Epoch  5223: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5234: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2650, training loss 0.00526, validation loss 0.00522
Epoch  5245: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2660, training loss 0.00514, validation loss 0.00603
Epoch  5256: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2661
Mean train loss for ascent epoch 2662: -0.0012330503668636084
Mean eval for ascent epoch 2662: 0.005043590907007456
Epoch  5267: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2670, training loss 0.06693, validation loss 0.08224
Epoch  5278: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5289: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2680, training loss 0.02748, validation loss 0.00990
Epoch  5300: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5311: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2690, training loss 0.00578, validation loss 0.00584
Epoch  5322: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5333: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2700
Doing Evaluation on the model now
This is Epoch 2700, training loss 0.00479, validation loss 0.00474
Mean train loss for ascent epoch 2701: -0.0012923207832500339
Mean eval for ascent epoch 2701: 0.004784068558365107
Epoch  5344: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2710, training loss 0.07195, validation loss 0.05731
Epoch  5355: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5366: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2720, training loss 0.01790, validation loss 0.01626
Epoch  5377: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5388: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2730, training loss 0.00552, validation loss 0.00517
Epoch  5399: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5410: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2739
Mean train loss for ascent epoch 2740: -0.0014051924226805568
Mean eval for ascent epoch 2740: 0.00576765788719058
Doing Evaluation on the model now
This is Epoch 2740, training loss 0.00577, validation loss 0.00453
Epoch  5421: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5432: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2750, training loss 0.07078, validation loss 0.03192
Epoch  5443: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2760, training loss 0.01281, validation loss 0.01253
Epoch  5454: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5465: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2770, training loss 0.00716, validation loss 0.00670
Epoch  5476: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5487: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2778
Mean train loss for ascent epoch 2779: -0.001606037956662476
Mean eval for ascent epoch 2779: 0.0052908905781805515
Doing Evaluation on the model now
This is Epoch 2780, training loss 0.07951, validation loss 0.17571
Epoch  5498: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5509: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2790, training loss 0.06321, validation loss 0.05653
Epoch  5520: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5531: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2800, training loss 0.02420, validation loss 0.01307
Resetting learning rate to 0.01000
Epoch  5542: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2810, training loss 0.00836, validation loss 0.00647
Epoch  5553: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5564: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2820, training loss 0.00589, validation loss 0.00689
Epoch  5575: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2822
Mean train loss for ascent epoch 2823: -0.002061867853626609
Mean eval for ascent epoch 2823: 0.005222551058977842
Epoch  5586: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2830, training loss 0.06588, validation loss 0.05186
Epoch  5597: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5608: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2840, training loss 0.01415, validation loss 0.01455
Epoch  5619: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5630: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2850, training loss 0.01002, validation loss 0.00952
Epoch  5641: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2860, training loss 0.00478, validation loss 0.00509
Epoch  5652: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2861
Mean train loss for ascent epoch 2862: -0.0013820899184793234
Mean eval for ascent epoch 2862: 0.00496360519900918
Epoch  5663: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2870, training loss 0.07855, validation loss 0.03440
Epoch  5674: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5685: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2880, training loss 0.01951, validation loss 0.02243
Epoch  5696: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5707: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2890, training loss 0.00598, validation loss 0.00446
Epoch  5718: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5729: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2900
Doing Evaluation on the model now
This is Epoch 2900, training loss 0.00538, validation loss 0.00586
Mean train loss for ascent epoch 2901: -0.0020818207412958145
Mean eval for ascent epoch 2901: 0.005479197483509779
Epoch  5740: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2910, training loss 0.07408, validation loss 0.01934
Epoch  5751: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5762: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2920, training loss 0.03014, validation loss 0.03197
Epoch  5773: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5784: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2930, training loss 0.00593, validation loss 0.00623
Epoch  5795: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5806: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2939
Mean train loss for ascent epoch 2940: -0.0015374330105260015
Mean eval for ascent epoch 2940: 0.00614943215623498
Doing Evaluation on the model now
This is Epoch 2940, training loss 0.00615, validation loss 0.00527
Epoch  5817: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2950, training loss 0.08663, validation loss 0.09639
Epoch  5828: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5839: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2960, training loss 0.01517, validation loss 0.02078
Epoch  5850: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5861: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2970, training loss 0.00613, validation loss 0.00714
Epoch  5872: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5883: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2978
Mean train loss for ascent epoch 2979: -0.0016423193737864494
Mean eval for ascent epoch 2979: 0.005026092752814293
Doing Evaluation on the model now
This is Epoch 2980, training loss 0.09650, validation loss 0.10532
Epoch  5894: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5905: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2990, training loss 0.05867, validation loss 0.05062
Epoch  5916: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3000, training loss 0.01996, validation loss 0.02602
Epoch  5927: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  5938: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 3010, training loss 0.00865, validation loss 0.00501
Epoch  5949: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5960: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3020, training loss 0.00508, validation loss 0.00472
Epoch  5971: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3023
Mean train loss for ascent epoch 3024: -0.001052756211720407
Mean eval for ascent epoch 3024: 0.0047785998322069645
Epoch  5982: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3030, training loss 0.19326, validation loss 0.38465
Epoch  5993: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6004: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3040, training loss 0.01700, validation loss 0.01683
Epoch  6015: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3050, training loss 0.00818, validation loss 0.00868
Epoch  6026: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6037: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3060, training loss 0.00492, validation loss 0.00639
Epoch  6048: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3062
Mean train loss for ascent epoch 3063: -0.002061825944110751
Mean eval for ascent epoch 3063: 0.005492802709341049
Epoch  6059: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3070, training loss 0.11323, validation loss 0.04318
Epoch  6070: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6081: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3080, training loss 0.01905, validation loss 0.01049
Epoch  6092: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6103: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3090, training loss 0.00804, validation loss 0.00547
Epoch  6114: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3100, training loss 0.00460, validation loss 0.00475
Epoch  6125: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3101
Mean train loss for ascent epoch 3102: -0.0014422923559322953
Mean eval for ascent epoch 3102: 0.004654183983802795
Epoch  6136: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3110, training loss 0.14576, validation loss 0.18974
Epoch  6147: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6158: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3120, training loss 0.01833, validation loss 0.03083
Epoch  6169: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6180: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3130, training loss 0.00449, validation loss 0.00565
Epoch  6191: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6202: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3140
Doing Evaluation on the model now
This is Epoch 3140, training loss 0.00438, validation loss 0.00433
Mean train loss for ascent epoch 3141: -0.0010559955844655633
Mean eval for ascent epoch 3141: 0.004717569798231125
Epoch  6213: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3150, training loss 0.10656, validation loss 0.13414
Epoch  6224: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6235: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3160, training loss 0.03231, validation loss 0.04529
Epoch  6246: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6257: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3170, training loss 0.00664, validation loss 0.00601
Epoch  6268: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6279: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3179
Mean train loss for ascent epoch 3180: -0.0011854707263410091
Mean eval for ascent epoch 3180: 0.004467439837753773
Doing Evaluation on the model now
This is Epoch 3180, training loss 0.00447, validation loss 0.00432
Epoch  6290: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6301: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3190, training loss 0.14972, validation loss 0.12630
Epoch  6312: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3200, training loss 0.03210, validation loss 0.02591
Resetting learning rate to 0.01000
Epoch  6323: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6334: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3210, training loss 0.00512, validation loss 0.00451
Epoch  6345: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6356: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3218
Mean train loss for ascent epoch 3219: -0.0012061730958521366
Mean eval for ascent epoch 3219: 0.004524532705545425
Doing Evaluation on the model now
This is Epoch 3220, training loss 0.14040, validation loss 0.10100
Epoch  6367: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6378: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3230, training loss 0.07934, validation loss 0.04551
Epoch  6389: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6400: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3240, training loss 0.03495, validation loss 0.00843
Epoch  6411: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3250, training loss 0.00576, validation loss 0.00641
Epoch  6422: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6433: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3257
Mean train loss for ascent epoch 3258: -0.0010927238035947084
Mean eval for ascent epoch 3258: 0.004560356494039297
Doing Evaluation on the model now
This is Epoch 3260, training loss 0.76842, validation loss 1.20593
Epoch  6444: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6455: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3270, training loss 0.05095, validation loss 0.08161
Epoch  6466: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6477: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3280, training loss 0.01277, validation loss 0.01031
Epoch  6488: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6499: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3290, training loss 0.00651, validation loss 0.00716
Epoch  6510: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3296
Mean train loss for ascent epoch 3297: -0.0011152111692354083
Mean eval for ascent epoch 3297: 0.00547097297385335
Doing Evaluation on the model now
This is Epoch 3300, training loss 0.39827, validation loss 0.32059
Epoch  6521: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6532: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3310, training loss 0.06398, validation loss 0.08919
Epoch  6543: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6554: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3320, training loss 0.00952, validation loss 0.01233
Epoch  6565: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6576: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3330, training loss 0.00551, validation loss 0.00610
Epoch  6587: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3335
Mean train loss for ascent epoch 3336: -0.0014363910304382443
Mean eval for ascent epoch 3336: 0.004857898689806461
Doing Evaluation on the model now
This is Epoch 3340, training loss 1.11407, validation loss 0.34544
Epoch  6598: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6609: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3350, training loss 0.04769, validation loss 0.03101
Epoch  6620: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6631: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3360, training loss 0.01039, validation loss 0.01428
Epoch  6642: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6653: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3370, training loss 0.00511, validation loss 0.00417
Epoch  6664: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3374
Mean train loss for ascent epoch 3375: -0.0016935489838942885
Mean eval for ascent epoch 3375: 0.004749380052089691
Epoch  6675: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3380, training loss 0.10474, validation loss 0.07152
Epoch  6686: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3390, training loss 0.05404, validation loss 0.06007
Epoch  6697: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6708: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3400, training loss 0.01377, validation loss 0.02157
Resetting learning rate to 0.01000
Epoch  6719: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6730: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3410, training loss 0.00494, validation loss 0.00514
Epoch  6741: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6752: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3418
Mean train loss for ascent epoch 3419: -0.0010912787402048707
Mean eval for ascent epoch 3419: 0.004282683599740267
Doing Evaluation on the model now
This is Epoch 3420, training loss 0.21174, validation loss 0.49570
Epoch  6763: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6774: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3430, training loss 0.08559, validation loss 0.08543
Epoch  6785: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3440, training loss 0.02583, validation loss 0.02236
Epoch  6796: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6807: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3450, training loss 0.00642, validation loss 0.00621
Epoch  6818: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6829: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3457
Mean train loss for ascent epoch 3458: -0.0010244508739560843
Mean eval for ascent epoch 3458: 0.004029002506285906
Doing Evaluation on the model now
This is Epoch 3460, training loss 1.39791, validation loss 0.90432
Epoch  6840: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6851: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3470, training loss 0.07022, validation loss 0.04629
Epoch  6862: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6873: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3480, training loss 0.01718, validation loss 0.01725
Epoch  6884: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3490, training loss 0.00626, validation loss 0.00665
Epoch  6895: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6906: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3496
Mean train loss for ascent epoch 3497: -0.001616362133063376
Mean eval for ascent epoch 3497: 0.004929909482598305
Doing Evaluation on the model now
This is Epoch 3500, training loss 0.07965, validation loss 0.09897
Epoch  6917: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6928: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3510, training loss 0.03584, validation loss 0.02081
Epoch  6939: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6950: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3520, training loss 0.01394, validation loss 0.01575
Epoch  6961: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6972: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3530, training loss 0.00552, validation loss 0.00471
Epoch  6983: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3535
Mean train loss for ascent epoch 3536: -0.001394186052493751
Mean eval for ascent epoch 3536: 0.004855489823967218
Doing Evaluation on the model now
This is Epoch 3540, training loss 0.23380, validation loss 0.04581
Epoch  6994: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7005: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3550, training loss 0.07270, validation loss 0.02909
Epoch  7016: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7027: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3560, training loss 0.00708, validation loss 0.00974
Epoch  7038: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7049: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3570, training loss 0.00463, validation loss 0.00386
Epoch  7060: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3574
Mean train loss for ascent epoch 3575: -0.0016158598009496927
Mean eval for ascent epoch 3575: 0.004090599250048399
Epoch  7071: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3580, training loss 0.74977, validation loss 0.68189
Epoch  7082: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3590, training loss 0.08392, validation loss 0.11083
Epoch  7093: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7104: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3600, training loss 0.01019, validation loss 0.01298
Resetting learning rate to 0.01000
Epoch  7115: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7126: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3610, training loss 0.00471, validation loss 0.00421
Epoch  7137: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7148: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3619
Mean train loss for ascent epoch 3620: -0.0009014639654196799
Mean eval for ascent epoch 3620: 0.0036339869257062674
Doing Evaluation on the model now
This is Epoch 3620, training loss 0.00363, validation loss 0.00463
Epoch  7159: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7170: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3630, training loss 0.06472, validation loss 0.11131
Epoch  7181: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3640, training loss 0.03018, validation loss 0.03987
Epoch  7192: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7203: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3650, training loss 0.00550, validation loss 0.00475
Epoch  7214: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7225: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3658
Mean train loss for ascent epoch 3659: -0.0009587607346475124
Mean eval for ascent epoch 3659: 0.004685326479375362
Doing Evaluation on the model now
This is Epoch 3660, training loss 0.23885, validation loss 0.39540
Epoch  7236: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7247: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3670, training loss 0.05683, validation loss 0.04938
Epoch  7258: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7269: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3680, training loss 0.02564, validation loss 0.01550
Epoch  7280: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3690, training loss 0.00637, validation loss 0.00661
Epoch  7291: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7302: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3697
Mean train loss for ascent epoch 3698: -0.0009918499272316694
Mean eval for ascent epoch 3698: 0.003834649221971631
Doing Evaluation on the model now
This is Epoch 3700, training loss 1.23863, validation loss 1.03753
Epoch  7313: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7324: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3710, training loss 0.05999, validation loss 0.06846
Epoch  7335: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7346: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3720, training loss 0.01670, validation loss 0.02214
Epoch  7357: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7368: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3730, training loss 0.00611, validation loss 0.00566
Epoch  7379: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3736
Mean train loss for ascent epoch 3737: -0.001250711502507329
Mean eval for ascent epoch 3737: 0.004330499563366175
Doing Evaluation on the model now
This is Epoch 3740, training loss 1.23089, validation loss 1.27279
Epoch  7390: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7401: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3750, training loss 0.09802, validation loss 0.10571
Epoch  7412: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7423: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3760, training loss 0.01300, validation loss 0.01011
Epoch  7434: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7445: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3770, training loss 0.00512, validation loss 0.00377
Epoch  7456: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3775
Mean train loss for ascent epoch 3776: -0.001955283572897315
Mean eval for ascent epoch 3776: 0.004635073244571686
Doing Evaluation on the model now
This is Epoch 3780, training loss 0.24723, validation loss 0.21392
Epoch  7467: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7478: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3790, training loss 0.04851, validation loss 0.03041
Epoch  7489: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7500: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3800, training loss 0.01123, validation loss 0.01539
Resetting learning rate to 0.01000
Epoch  7511: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7522: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3810, training loss 0.00481, validation loss 0.00592
Epoch  7533: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7544: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3819
Mean train loss for ascent epoch 3820: -0.0011860900558531284
Mean eval for ascent epoch 3820: 0.0039057668764144182
Doing Evaluation on the model now
This is Epoch 3820, training loss 0.00391, validation loss 0.00302
Epoch  7555: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3830, training loss 0.13933, validation loss 0.15833
Epoch  7566: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7577: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3840, training loss 0.02167, validation loss 0.02509
Epoch  7588: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7599: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3850, training loss 0.00522, validation loss 0.00594
Epoch  7610: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7621: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3858
Mean train loss for ascent epoch 3859: -0.0019985444378107786
Mean eval for ascent epoch 3859: 0.004530542995780706
Doing Evaluation on the model now
This is Epoch 3860, training loss 0.22216, validation loss 0.27341
Epoch  7632: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7643: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3870, training loss 0.06736, validation loss 0.08240
Epoch  7654: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3880, training loss 0.01687, validation loss 0.01123
Epoch  7665: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7676: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3890, training loss 0.00426, validation loss 0.00467
Epoch  7687: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7698: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3897
Mean train loss for ascent epoch 3898: -0.0011103354627266526
Mean eval for ascent epoch 3898: 0.004406391177326441
Doing Evaluation on the model now
This is Epoch 3900, training loss 0.82235, validation loss 0.81748
Epoch  7709: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7720: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3910, training loss 0.06793, validation loss 0.07106
Epoch  7731: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7742: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3920, training loss 0.01182, validation loss 0.00819
Epoch  7753: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3930, training loss 0.00504, validation loss 0.00437
Epoch  7764: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7775: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3936
Mean train loss for ascent epoch 3937: -0.001126418705098331
Mean eval for ascent epoch 3937: 0.005456467624753714
Doing Evaluation on the model now
This is Epoch 3940, training loss 0.60992, validation loss 0.67720
Epoch  7786: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7797: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3950, training loss 0.03997, validation loss 0.05890
Epoch  7808: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7819: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3960, training loss 0.00739, validation loss 0.00865
Epoch  7830: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7841: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3970, training loss 0.00431, validation loss 0.00515
Epoch  7852: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3975
Mean train loss for ascent epoch 3976: -0.0016575645422562957
Mean eval for ascent epoch 3976: 0.0051103574223816395
Doing Evaluation on the model now
This is Epoch 3980, training loss 0.17461, validation loss 0.45408
Epoch  7863: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7874: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3990, training loss 0.06705, validation loss 0.05249
Epoch  7885: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7896: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4000, training loss 0.00823, validation loss 0.01186
Resetting learning rate to 0.01000
Epoch  7907: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7918: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4010, training loss 0.00478, validation loss 0.00503
Epoch  7929: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7940: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4020
Doing Evaluation on the model now
This is Epoch 4020, training loss 0.00469, validation loss 0.00586
Mean train loss for ascent epoch 4021: -0.0008736272575333714
Mean eval for ascent epoch 4021: 0.004621565341949463
Epoch  7951: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4030, training loss 0.04874, validation loss 0.04726
Epoch  7962: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7973: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4040, training loss 0.01551, validation loss 0.01507
Epoch  7984: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7995: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4050, training loss 0.00634, validation loss 0.00559
Epoch  8006: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8017: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4059
Mean train loss for ascent epoch 4060: -0.00107625185046345
Mean eval for ascent epoch 4060: 0.004071681760251522
Doing Evaluation on the model now
This is Epoch 4060, training loss 0.00407, validation loss 0.00370
Epoch  8028: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8039: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4070, training loss 0.05822, validation loss 0.08681
Epoch  8050: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4080, training loss 0.02826, validation loss 0.02430
Epoch  8061: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8072: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4090, training loss 0.00559, validation loss 0.00514
Epoch  8083: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8094: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4098
Mean train loss for ascent epoch 4099: -0.0009399112313985825
Mean eval for ascent epoch 4099: 0.003977689892053604
Doing Evaluation on the model now
This is Epoch 4100, training loss 0.19103, validation loss 0.44314
Epoch  8105: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8116: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4110, training loss 0.04581, validation loss 0.05412
Epoch  8127: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8138: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4120, training loss 0.01318, validation loss 0.01850
Epoch  8149: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4130, training loss 0.00576, validation loss 0.00471
Epoch  8160: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8171: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4137
Mean train loss for ascent epoch 4138: -0.0010992955649271607
Mean eval for ascent epoch 4138: 0.004062482621520758
Doing Evaluation on the model now
This is Epoch 4140, training loss 0.45307, validation loss 0.94410
Epoch  8182: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8193: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4150, training loss 0.02486, validation loss 0.03756
Epoch  8204: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8215: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4160, training loss 0.00776, validation loss 0.01278
Epoch  8226: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8237: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4170, training loss 0.00450, validation loss 0.00561
Epoch  8248: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4176
Mean train loss for ascent epoch 4177: -0.0014218317810446024
Mean eval for ascent epoch 4177: 0.003693106584250927
Doing Evaluation on the model now
This is Epoch 4180, training loss 0.55875, validation loss 0.42240
Epoch  8259: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8270: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4190, training loss 0.03449, validation loss 0.07478
Epoch  8281: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8292: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4200, training loss 0.00896, validation loss 0.00750
Resetting learning rate to 0.01000
Epoch  8303: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8314: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4210, training loss 0.00599, validation loss 0.00439
Epoch  8325: reducing learning rate of group 0 to 1.2500e-04.
Epoch  8336: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4220
Doing Evaluation on the model now
This is Epoch 4220, training loss 0.00438, validation loss 0.00412
Mean train loss for ascent epoch 4221: -0.0013683069264516234
Mean eval for ascent epoch 4221: 0.0038623150903731585
Epoch  8347: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4230, training loss 0.06154, validation loss 0.11992
Epoch  8358: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8369: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4240, training loss 0.02415, validation loss 0.03080
Epoch  8380: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8391: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4250, training loss 0.00571, validation loss 0.00480
Epoch  8402: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8413: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4259
Mean train loss for ascent epoch 4260: -0.0013275801902636886
Mean eval for ascent epoch 4260: 0.0041796057485044
Doing Evaluation on the model now
This is Epoch 4260, training loss 0.00418, validation loss 0.00440
Epoch  8424: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4270, training loss 0.11345, validation loss 0.02265
Epoch  8435: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8446: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4280, training loss 0.02679, validation loss 0.01827
Epoch  8457: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8468: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4290, training loss 0.00534, validation loss 0.00723
Epoch  8479: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8490: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4298
Mean train loss for ascent epoch 4299: -0.00192433618940413
Mean eval for ascent epoch 4299: 0.006411077920347452
Doing Evaluation on the model now
This is Epoch 4300, training loss 0.12235, validation loss 0.15695
Epoch  8501: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8512: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4310, training loss 0.05994, validation loss 0.09230
Epoch  8523: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4320, training loss 0.02062, validation loss 0.01862
Epoch  8534: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8545: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4330, training loss 0.00621, validation loss 0.00647
Epoch  8556: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8567: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4337
Mean train loss for ascent epoch 4338: -0.0015067775966599584
Mean eval for ascent epoch 4338: 0.004679771140217781
Doing Evaluation on the model now
This is Epoch 4340, training loss 0.48690, validation loss 1.04108
Epoch  8578: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8589: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4350, training loss 0.03844, validation loss 0.02600
Epoch  8600: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8611: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4360, training loss 0.01644, validation loss 0.01826
Epoch  8622: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4370, training loss 0.00700, validation loss 0.00509
Epoch  8633: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8644: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4376
Mean train loss for ascent epoch 4377: -0.0019954033195972443
Mean eval for ascent epoch 4377: 0.00472524156793952
Doing Evaluation on the model now
This is Epoch 4380, training loss 0.63592, validation loss 0.63183
Epoch  8655: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8666: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4390, training loss 0.07476, validation loss 0.02955
Epoch  8677: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8688: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4400, training loss 0.00841, validation loss 0.01204
Resetting learning rate to 0.01000
Epoch  8699: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8710: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4410, training loss 0.00601, validation loss 0.00519
Epoch  8721: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4420, training loss 0.00473, validation loss 0.00430
Epoch  8732: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4421
Mean train loss for ascent epoch 4422: -0.0012939471052959561
Mean eval for ascent epoch 4422: 0.005114749073982239
Epoch  8743: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4430, training loss 0.05125, validation loss 0.05520
Epoch  8754: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8765: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4440, training loss 0.01686, validation loss 0.01025
Epoch  8776: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8787: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4450, training loss 0.00640, validation loss 0.00447
Epoch  8798: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8809: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4460
Doing Evaluation on the model now
This is Epoch 4460, training loss 0.00446, validation loss 0.00446
Mean train loss for ascent epoch 4461: -0.0014061707770451903
Mean eval for ascent epoch 4461: 0.004662711173295975
Epoch  8820: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4470, training loss 0.03538, validation loss 0.03584
Epoch  8831: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8842: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4480, training loss 0.02114, validation loss 0.02179
Epoch  8853: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8864: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4490, training loss 0.00458, validation loss 0.00566
Epoch  8875: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8886: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4499
Mean train loss for ascent epoch 4500: -0.0012012263759970665
Mean eval for ascent epoch 4500: 0.004212380852550268
Doing Evaluation on the model now
This is Epoch 4500, training loss 0.00421, validation loss 0.00422
Epoch  8897: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8908: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4510, training loss 0.10430, validation loss 0.16721
Epoch  8919: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4520, training loss 0.02341, validation loss 0.01966
Epoch  8930: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8941: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4530, training loss 0.00547, validation loss 0.00577
Epoch  8952: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8963: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4538
Mean train loss for ascent epoch 4539: -0.000987724051810801
Mean eval for ascent epoch 4539: 0.00473373755812645
Doing Evaluation on the model now
This is Epoch 4540, training loss 0.15131, validation loss 0.38839
Epoch  8974: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8985: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4550, training loss 0.05505, validation loss 0.05398
Epoch  8996: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9007: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4560, training loss 0.03808, validation loss 0.03909
Epoch  9018: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4570, training loss 0.00621, validation loss 0.00478
Epoch  9029: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9040: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4577
Mean train loss for ascent epoch 4578: -0.0011951227206736803
Mean eval for ascent epoch 4578: 0.004993791691958904
Doing Evaluation on the model now
This is Epoch 4580, training loss 0.30409, validation loss 0.64573
Epoch  9051: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9062: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4590, training loss 0.05564, validation loss 0.02472
Epoch  9073: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9084: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4600, training loss 0.01813, validation loss 0.01373
Resetting learning rate to 0.01000
Epoch  9095: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9106: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4610, training loss 0.00762, validation loss 0.00531
Epoch  9117: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4620, training loss 0.00452, validation loss 0.00395
Epoch  9128: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4621
Mean train loss for ascent epoch 4622: -0.0014962689019739628
Mean eval for ascent epoch 4622: 0.004499432630836964
Epoch  9139: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4630, training loss 0.09858, validation loss 0.21318
Epoch  9150: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9161: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4640, training loss 0.03418, validation loss 0.02611
Epoch  9172: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9183: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4650, training loss 0.00769, validation loss 0.00512
Epoch  9194: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9205: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4660
Doing Evaluation on the model now
This is Epoch 4660, training loss 0.00482, validation loss 0.00517
Mean train loss for ascent epoch 4661: -0.0013795554405078292
Mean eval for ascent epoch 4661: 0.00441755773499608
Epoch  9216: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4670, training loss 0.16819, validation loss 0.08686
Epoch  9227: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9238: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4680, training loss 0.02317, validation loss 0.01625
Epoch  9249: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9260: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4690, training loss 0.00648, validation loss 0.00933
Epoch  9271: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9282: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4699
Mean train loss for ascent epoch 4700: -0.002012329874560237
Mean eval for ascent epoch 4700: 0.0052120801992714405
Doing Evaluation on the model now
This is Epoch 4700, training loss 0.00521, validation loss 0.00396
Epoch  9293: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4710, training loss 0.21727, validation loss 0.15989
Epoch  9304: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9315: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4720, training loss 0.03309, validation loss 0.04954
Epoch  9326: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9337: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4730, training loss 0.00646, validation loss 0.00550
Epoch  9348: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9359: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4738
Mean train loss for ascent epoch 4739: -0.0007618395029567182
Mean eval for ascent epoch 4739: 0.004991712514311075
Doing Evaluation on the model now
This is Epoch 4740, training loss 0.29209, validation loss 0.11919
Epoch  9370: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9381: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4750, training loss 0.04650, validation loss 0.01370
Epoch  9392: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4760, training loss 0.02213, validation loss 0.03764
Epoch  9403: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9414: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4770, training loss 0.00610, validation loss 0.00507
Epoch  9425: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9436: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4777
Mean train loss for ascent epoch 4778: -0.0008425751002505422
Mean eval for ascent epoch 4778: 0.0036320784129202366
Doing Evaluation on the model now
This is Epoch 4780, training loss 0.65931, validation loss 0.44000
Epoch  9447: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9458: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4790, training loss 0.06298, validation loss 0.05551
Epoch  9469: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9480: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4800, training loss 0.01426, validation loss 0.01112
Resetting learning rate to 0.01000
Epoch  9491: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4810, training loss 0.00975, validation loss 0.01040
Epoch  9502: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9513: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4820, training loss 0.00466, validation loss 0.00448
Epoch  9524: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4822
Mean train loss for ascent epoch 4823: -0.0013572587631642818
Mean eval for ascent epoch 4823: 0.00481399754062295
Epoch  9535: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4830, training loss 0.08893, validation loss 0.15433
Epoch  9546: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9557: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4840, training loss 0.02941, validation loss 0.03811
Epoch  9568: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9579: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4850, training loss 0.01032, validation loss 0.01001
Epoch  9590: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4860, training loss 0.00501, validation loss 0.00427
Epoch  9601: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4861
Mean train loss for ascent epoch 4862: -0.0011604594765231013
Mean eval for ascent epoch 4862: 0.004699226934462786
Epoch  9612: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4870, training loss 0.06140, validation loss 0.04110
Epoch  9623: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9634: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4880, training loss 0.02628, validation loss 0.02283
Epoch  9645: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9656: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4890, training loss 0.00697, validation loss 0.00687
Epoch  9667: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9678: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4900
Doing Evaluation on the model now
This is Epoch 4900, training loss 0.00495, validation loss 0.00437
Mean train loss for ascent epoch 4901: -0.0013298154808580875
Mean eval for ascent epoch 4901: 0.005531812086701393
Epoch  9689: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4910, training loss 0.06007, validation loss 0.08721
Epoch  9700: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9711: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4920, training loss 0.02072, validation loss 0.02901
Epoch  9722: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9733: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4930, training loss 0.00850, validation loss 0.00784
Epoch  9744: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9755: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4939
Mean train loss for ascent epoch 4940: -0.0010680800769478083
Mean eval for ascent epoch 4940: 0.004688909277319908
Doing Evaluation on the model now
This is Epoch 4940, training loss 0.00469, validation loss 0.00516
Epoch  9766: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9777: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4950, training loss 0.09854, validation loss 0.10632
Epoch  9788: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4960, training loss 0.01902, validation loss 0.03307
Epoch  9799: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9810: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4970, training loss 0.00557, validation loss 0.00748
Epoch  9821: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9832: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4978
Mean train loss for ascent epoch 4979: -0.0010364489862695336
Mean eval for ascent epoch 4979: 0.004925074987113476
Doing Evaluation on the model now
This is Epoch 4980, training loss 0.18499, validation loss 0.31353
Epoch  9843: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9854: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4990, training loss 0.01833, validation loss 0.02834
Epoch  9865: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9876: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5000, training loss 0.01851, validation loss 0.01953
Resetting learning rate to 0.01000
Epoch  9887: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5010, training loss 0.00974, validation loss 0.00613
Epoch  9898: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9909: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5020, training loss 0.00468, validation loss 0.00408
Epoch  9920: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5022
Mean train loss for ascent epoch 5023: -0.0014913086779415607
Mean eval for ascent epoch 5023: 0.004931203089654446
Epoch  9931: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5030, training loss 0.10612, validation loss 0.05063
Epoch  9942: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9953: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5040, training loss 0.01822, validation loss 0.00659
Epoch  9964: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9975: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5050, training loss 0.00834, validation loss 0.00951
Epoch  9986: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5060, training loss 0.00466, validation loss 0.00437
Epoch  9997: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5061
Mean train loss for ascent epoch 5062: -0.0017654342809692025
Mean eval for ascent epoch 5062: 0.005227621644735336
Epoch 10008: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5070, training loss 0.07676, validation loss 0.15458
Epoch 10019: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10030: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5080, training loss 0.02677, validation loss 0.03158
Epoch 10041: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10052: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5090, training loss 0.00848, validation loss 0.00776
Epoch 10063: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10074: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5100
Doing Evaluation on the model now
This is Epoch 5100, training loss 0.00562, validation loss 0.00534
Mean train loss for ascent epoch 5101: -0.0014619819121435285
Mean eval for ascent epoch 5101: 0.004919074010103941
Epoch 10085: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5110, training loss 0.18407, validation loss 0.18461
Epoch 10096: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10107: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5120, training loss 0.02030, validation loss 0.01292
Epoch 10118: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10129: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5130, training loss 0.00612, validation loss 0.00479
Epoch 10140: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10151: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5139
Mean train loss for ascent epoch 5140: -0.0014499974204227328
Mean eval for ascent epoch 5140: 0.005737252999097109
Doing Evaluation on the model now
This is Epoch 5140, training loss 0.00574, validation loss 0.00539
Epoch 10162: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5150, training loss 0.18526, validation loss 0.13516
Epoch 10173: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10184: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5160, training loss 0.03882, validation loss 0.02448
Epoch 10195: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10206: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5170, training loss 0.00694, validation loss 0.00617
Epoch 10217: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10228: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5178
Mean train loss for ascent epoch 5179: -0.001234175986610353
Mean eval for ascent epoch 5179: 0.004812185652554035
Doing Evaluation on the model now
This is Epoch 5180, training loss 0.28927, validation loss 0.68500
Epoch 10239: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10250: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5190, training loss 0.07279, validation loss 0.08631
Epoch 10261: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5200, training loss 0.02859, validation loss 0.02745
Epoch 10272: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 10283: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5210, training loss 0.00924, validation loss 0.00721
Epoch 10294: reducing learning rate of group 0 to 2.5000e-04.
Epoch 10305: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5220, training loss 0.00471, validation loss 0.00458
Epoch 10316: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5223
Mean train loss for ascent epoch 5224: -0.000985549995675683
Mean eval for ascent epoch 5224: 0.005618670955300331
Epoch 10327: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5230, training loss 0.10520, validation loss 0.06186
Epoch 10338: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10349: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5240, training loss 0.02401, validation loss 0.03129
Epoch 10360: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5250, training loss 0.00775, validation loss 0.00979
Epoch 10371: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10382: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5260, training loss 0.00535, validation loss 0.00554
Epoch 10393: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5262
Mean train loss for ascent epoch 5263: -0.001186294131912291
Mean eval for ascent epoch 5263: 0.005592642817646265
Epoch 10404: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5270, training loss 0.05573, validation loss 0.06136
Epoch 10415: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10426: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5280, training loss 0.01305, validation loss 0.01435
Epoch 10437: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10448: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5290, training loss 0.00817, validation loss 0.00519
Epoch 10459: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5300, training loss 0.00497, validation loss 0.00457
Epoch 10470: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5301
Mean train loss for ascent epoch 5302: -0.0011122666765004396
Mean eval for ascent epoch 5302: 0.004960553254932165
Epoch 10481: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5310, training loss 0.14176, validation loss 0.06136
Epoch 10492: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10503: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5320, training loss 0.02546, validation loss 0.02991
Epoch 10514: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10525: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5330, training loss 0.00667, validation loss 0.00678
Epoch 10536: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10547: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5340
Doing Evaluation on the model now
This is Epoch 5340, training loss 0.00473, validation loss 0.00435
Mean train loss for ascent epoch 5341: -0.0015819530235603452
Mean eval for ascent epoch 5341: 0.004499298520386219
Epoch 10558: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5350, training loss 0.11428, validation loss 0.01665
Epoch 10569: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10580: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5360, training loss 0.03110, validation loss 0.02779
Epoch 10591: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10602: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5370, training loss 0.00573, validation loss 0.00607
Epoch 10613: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10624: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5379
Mean train loss for ascent epoch 5380: -0.0012092537945136428
Mean eval for ascent epoch 5380: 0.004793828818947077
Doing Evaluation on the model now
This is Epoch 5380, training loss 0.00479, validation loss 0.00500
Epoch 10635: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10646: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5390, training loss 0.04025, validation loss 0.03421
Epoch 10657: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5400, training loss 0.02039, validation loss 0.03087
Resetting learning rate to 0.01000
Epoch 10668: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10679: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5410, training loss 0.00564, validation loss 0.00584
Epoch 10690: reducing learning rate of group 0 to 1.2500e-04.
Epoch 10701: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5418
Mean train loss for ascent epoch 5419: -0.0011549056507647038
Mean eval for ascent epoch 5419: 0.005553820636123419
Doing Evaluation on the model now
This is Epoch 5420, training loss 0.18282, validation loss 0.52315
Epoch 10712: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10723: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5430, training loss 0.03953, validation loss 0.05312
Epoch 10734: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10745: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5440, training loss 0.02730, validation loss 0.02512
Epoch 10756: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5450, training loss 0.00520, validation loss 0.00650
Epoch 10767: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10778: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5457
Mean train loss for ascent epoch 5458: -0.0016804137267172337
Mean eval for ascent epoch 5458: 0.004699956625699997
Doing Evaluation on the model now
This is Epoch 5460, training loss 0.52629, validation loss 0.10488
Epoch 10789: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10800: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5470, training loss 0.05214, validation loss 0.07194
Epoch 10811: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10822: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5480, training loss 0.01324, validation loss 0.00431
Epoch 10833: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10844: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5490, training loss 0.00747, validation loss 0.00797
Epoch 10855: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5496
Mean train loss for ascent epoch 5497: -0.000989918364211917
Mean eval for ascent epoch 5497: 0.004547500517219305
Doing Evaluation on the model now
This is Epoch 5500, training loss 1.45742, validation loss 1.55428
Epoch 10866: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10877: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5510, training loss 0.03803, validation loss 0.01779
Epoch 10888: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10899: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5520, training loss 0.01207, validation loss 0.00528
Epoch 10910: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10921: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5530, training loss 0.00699, validation loss 0.00846
Epoch 10932: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5535
Mean train loss for ascent epoch 5536: -0.0010541854426264763
Mean eval for ascent epoch 5536: 0.005035306327044964
Doing Evaluation on the model now
This is Epoch 5540, training loss 0.24269, validation loss 0.13957
Epoch 10943: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10954: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5550, training loss 0.08120, validation loss 0.07174
Epoch 10965: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10976: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5560, training loss 0.00708, validation loss 0.00913
Epoch 10987: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10998: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5570, training loss 0.00518, validation loss 0.00545
Epoch 11009: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5574
Mean train loss for ascent epoch 5575: -0.0009858300909399986
Mean eval for ascent epoch 5575: 0.00496621523052454
Epoch 11020: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5580, training loss 0.15461, validation loss 0.17568
Epoch 11031: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5590, training loss 0.08094, validation loss 0.01316
Epoch 11042: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11053: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5600, training loss 0.01295, validation loss 0.01831
Resetting learning rate to 0.01000
Epoch 11064: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11075: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5610, training loss 0.00596, validation loss 0.00511
Epoch 11086: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11097: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5618
Mean train loss for ascent epoch 5619: -0.0015200633788481355
Mean eval for ascent epoch 5619: 0.006143709644675255
Doing Evaluation on the model now
This is Epoch 5620, training loss 0.17153, validation loss 0.19995
Epoch 11108: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11119: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5630, training loss 0.03067, validation loss 0.02588
Epoch 11130: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5640, training loss 0.01656, validation loss 0.02456
Epoch 11141: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11152: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5650, training loss 0.00574, validation loss 0.00487
Epoch 11163: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11174: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5657
Mean train loss for ascent epoch 5658: -0.0011338569456711411
Mean eval for ascent epoch 5658: 0.004798999056220055
Doing Evaluation on the model now
This is Epoch 5660, training loss 0.39676, validation loss 0.57724
Epoch 11185: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11196: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5670, training loss 0.04518, validation loss 0.04401
Epoch 11207: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11218: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5680, training loss 0.01406, validation loss 0.01176
Epoch 11229: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5690, training loss 0.00601, validation loss 0.00656
Epoch 11240: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11251: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5696
Mean train loss for ascent epoch 5697: -0.001133275218307972
Mean eval for ascent epoch 5697: 0.005037456750869751
Doing Evaluation on the model now
This is Epoch 5700, training loss 0.51217, validation loss 0.41966
Epoch 11262: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11273: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5710, training loss 0.04800, validation loss 0.06583
Epoch 11284: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11295: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5720, training loss 0.00786, validation loss 0.00876
Epoch 11306: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11317: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5730, training loss 0.00481, validation loss 0.00424
Epoch 11328: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5735
Mean train loss for ascent epoch 5736: -0.0015401151031255722
Mean eval for ascent epoch 5736: 0.004966390319168568
Doing Evaluation on the model now
This is Epoch 5740, training loss 0.09843, validation loss 0.07207
Epoch 11339: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11350: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5750, training loss 0.06754, validation loss 0.07161
Epoch 11361: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11372: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5760, training loss 0.01276, validation loss 0.02043
Epoch 11383: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11394: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5770, training loss 0.00496, validation loss 0.00615
Epoch 11405: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5774
Mean train loss for ascent epoch 5775: -0.0013425011420622468
Mean eval for ascent epoch 5775: 0.004936226177960634
Epoch 11416: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5780, training loss 0.17919, validation loss 0.24154
Epoch 11427: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5790, training loss 0.03284, validation loss 0.03545
Epoch 11438: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11449: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5800, training loss 0.00996, validation loss 0.00595
Resetting learning rate to 0.01000
Epoch 11460: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11471: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5810, training loss 0.00428, validation loss 0.00503
Epoch 11482: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11493: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5819
Mean train loss for ascent epoch 5820: -0.0008338613552041352
Mean eval for ascent epoch 5820: 0.003665196243673563
Doing Evaluation on the model now
This is Epoch 5820, training loss 0.00367, validation loss 0.00341
Epoch 11504: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11515: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5830, training loss 0.08608, validation loss 0.03958
Epoch 11526: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5840, training loss 0.01948, validation loss 0.01450
Epoch 11537: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11548: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5850, training loss 0.00522, validation loss 0.00552
Epoch 11559: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11570: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5858
Mean train loss for ascent epoch 5859: -0.0006905639893375337
Mean eval for ascent epoch 5859: 0.0047931489534676075
Doing Evaluation on the model now
This is Epoch 5860, training loss 0.39142, validation loss 0.33250
Epoch 11581: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11592: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5870, training loss 0.04128, validation loss 0.05583
Epoch 11603: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11614: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5880, training loss 0.02073, validation loss 0.02805
Epoch 11625: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5890, training loss 0.00604, validation loss 0.00677
Epoch 11636: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11647: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5897
Mean train loss for ascent epoch 5898: -0.0015792703488841653
Mean eval for ascent epoch 5898: 0.005759159568697214
Doing Evaluation on the model now
This is Epoch 5900, training loss 0.28939, validation loss 0.09443
Epoch 11658: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11669: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5910, training loss 0.04213, validation loss 0.04174
Epoch 11680: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11691: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5920, training loss 0.00982, validation loss 0.00880
Epoch 11702: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11713: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5930, training loss 0.00708, validation loss 0.00487
Epoch 11724: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5936
Mean train loss for ascent epoch 5937: -0.0012564618373289704
Mean eval for ascent epoch 5937: 0.004646837245672941
Doing Evaluation on the model now
This is Epoch 5940, training loss 1.07370, validation loss 0.57033
Epoch 11735: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11746: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5950, training loss 0.05658, validation loss 0.08133
Epoch 11757: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11768: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5960, training loss 0.01166, validation loss 0.01100
Epoch 11779: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11790: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5970, training loss 0.00587, validation loss 0.00577
Epoch 11801: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5975
Mean train loss for ascent epoch 5976: -0.0009256107150577009
Mean eval for ascent epoch 5976: 0.004714219365268946
Doing Evaluation on the model now
This is Epoch 5980, training loss 1.39956, validation loss 1.10904
Epoch 11812: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11823: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5990, training loss 0.03615, validation loss 0.03723
Epoch 11834: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11845: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6000, training loss 0.00794, validation loss 0.00883
Resetting learning rate to 0.01000
Epoch 11856: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11867: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6010, training loss 0.00510, validation loss 0.00505
Epoch 11878: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11889: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6019
Mean train loss for ascent epoch 6020: -0.0009874387178570032
Mean eval for ascent epoch 6020: 0.004326419439166784
Doing Evaluation on the model now
This is Epoch 6020, training loss 0.00433, validation loss 0.00449
Epoch 11900: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6030, training loss 0.06970, validation loss 0.08878
Epoch 11911: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11922: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6040, training loss 0.02440, validation loss 0.02529
Epoch 11933: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11944: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6050, training loss 0.00574, validation loss 0.00526
Epoch 11955: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11966: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6058
Mean train loss for ascent epoch 6059: -0.0012397948885336518
Mean eval for ascent epoch 6059: 0.005028561223298311
Doing Evaluation on the model now
This is Epoch 6060, training loss 0.14438, validation loss 0.36037
Epoch 11977: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11988: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6070, training loss 0.05166, validation loss 0.03838
Epoch 11999: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6080, training loss 0.03058, validation loss 0.01576
Epoch 12010: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12021: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6090, training loss 0.00623, validation loss 0.00559
Epoch 12032: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12043: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6097
Mean train loss for ascent epoch 6098: -0.0015398641116917133
Mean eval for ascent epoch 6098: 0.004948103334754705
Doing Evaluation on the model now
This is Epoch 6100, training loss 0.88339, validation loss 1.01054
Epoch 12054: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12065: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6110, training loss 0.05907, validation loss 0.03664
Epoch 12076: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12087: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6120, training loss 0.01271, validation loss 0.01426
Epoch 12098: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6130, training loss 0.00661, validation loss 0.00898
Epoch 12109: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12120: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6136
Mean train loss for ascent epoch 6137: -0.0009490011143498123
Mean eval for ascent epoch 6137: 0.0039060821291059256
Doing Evaluation on the model now
This is Epoch 6140, training loss 0.54050, validation loss 0.33617
Epoch 12131: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12142: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6150, training loss 0.05696, validation loss 0.06009
Epoch 12153: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12164: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6160, training loss 0.00853, validation loss 0.00603
Epoch 12175: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12186: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6170, training loss 0.00417, validation loss 0.00310
Epoch 12197: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6175
Mean train loss for ascent epoch 6176: -0.000992371584288776
Mean eval for ascent epoch 6176: 0.0036277822218835354
Doing Evaluation on the model now
This is Epoch 6180, training loss 0.59464, validation loss 0.69138
Epoch 12208: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12219: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6190, training loss 0.05793, validation loss 0.05240
Epoch 12230: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12241: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6200, training loss 0.00961, validation loss 0.00651
Resetting learning rate to 0.01000
Epoch 12252: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12263: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6210, training loss 0.00385, validation loss 0.00462
Epoch 12274: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12285: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6220
Doing Evaluation on the model now
This is Epoch 6220, training loss 0.00331, validation loss 0.00292
Mean train loss for ascent epoch 6221: -0.0009441729635000229
Mean eval for ascent epoch 6221: 0.003220254322513938
Epoch 12296: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6230, training loss 0.05003, validation loss 0.11591
Epoch 12307: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12318: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6240, training loss 0.02328, validation loss 0.03913
Epoch 12329: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12340: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6250, training loss 0.00584, validation loss 0.00365
Epoch 12351: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12362: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6259
Mean train loss for ascent epoch 6260: -0.0008094505756162107
Mean eval for ascent epoch 6260: 0.002864964073523879
Doing Evaluation on the model now
This is Epoch 6260, training loss 0.00286, validation loss 0.00337
Epoch 12373: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12384: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6270, training loss 0.11685, validation loss 0.20963
Epoch 12395: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6280, training loss 0.02916, validation loss 0.02993
Epoch 12406: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12417: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6290, training loss 0.00549, validation loss 0.00600
Epoch 12428: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12439: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6298
Mean train loss for ascent epoch 6299: -0.0011072366032749414
Mean eval for ascent epoch 6299: 0.0037075087893754244
Doing Evaluation on the model now
This is Epoch 6300, training loss 0.17894, validation loss 0.27425
Epoch 12450: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12461: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6310, training loss 0.03847, validation loss 0.04311
Epoch 12472: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12483: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6320, training loss 0.01766, validation loss 0.00692
Epoch 12494: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6330, training loss 0.00502, validation loss 0.00527
Epoch 12505: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12516: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6337
Mean train loss for ascent epoch 6338: -0.0008005650597624481
Mean eval for ascent epoch 6338: 0.003306259401142597
Doing Evaluation on the model now
This is Epoch 6340, training loss 1.10337, validation loss 0.98627
Epoch 12527: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12538: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6350, training loss 0.03301, validation loss 0.01375
Epoch 12549: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12560: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6360, training loss 0.01091, validation loss 0.01207
Epoch 12571: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12582: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6370, training loss 0.00376, validation loss 0.00387
Epoch 12593: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6376
Mean train loss for ascent epoch 6377: -0.0010525492252781987
Mean eval for ascent epoch 6377: 0.003065377939492464
Doing Evaluation on the model now
This is Epoch 6380, training loss 0.39416, validation loss 0.10605
Epoch 12604: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12615: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6390, training loss 0.01596, validation loss 0.03605
Epoch 12626: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12637: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6400, training loss 0.01055, validation loss 0.00673
Resetting learning rate to 0.01000
Epoch 12648: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12659: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6410, training loss 0.00615, validation loss 0.00510
Epoch 12670: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12681: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6420
Doing Evaluation on the model now
This is Epoch 6420, training loss 0.00330, validation loss 0.00331
Mean train loss for ascent epoch 6421: -0.001211928785778582
Mean eval for ascent epoch 6421: 0.003490575822070241
Epoch 12692: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6430, training loss 0.06145, validation loss 0.07928
Epoch 12703: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12714: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6440, training loss 0.01715, validation loss 0.02641
Epoch 12725: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12736: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6450, training loss 0.00401, validation loss 0.00410
Epoch 12747: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12758: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6459
Mean train loss for ascent epoch 6460: -0.0004974704352207482
Mean eval for ascent epoch 6460: 0.002918653655797243
Doing Evaluation on the model now
This is Epoch 6460, training loss 0.00292, validation loss 0.00237
Epoch 12769: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6470, training loss 0.09281, validation loss 0.04365
Epoch 12780: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12791: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6480, training loss 0.02583, validation loss 0.01862
Epoch 12802: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12813: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6490, training loss 0.00475, validation loss 0.00405
Epoch 12824: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12835: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6498
Mean train loss for ascent epoch 6499: -0.0006349305622279644
Mean eval for ascent epoch 6499: 0.0035960227251052856
Doing Evaluation on the model now
This is Epoch 6500, training loss 0.21980, validation loss 0.29148
Epoch 12846: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12857: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6510, training loss 0.09024, validation loss 0.02634
Epoch 12868: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6520, training loss 0.01959, validation loss 0.01872
Epoch 12879: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12890: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6530, training loss 0.00428, validation loss 0.00453
Epoch 12901: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12912: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6537
Mean train loss for ascent epoch 6538: -0.0006254261825233698
Mean eval for ascent epoch 6538: 0.0031922373455017805
Doing Evaluation on the model now
This is Epoch 6540, training loss 0.60537, validation loss 0.56993
Epoch 12923: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12934: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6550, training loss 0.06043, validation loss 0.04757
Epoch 12945: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12956: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6560, training loss 0.01172, validation loss 0.01046
Epoch 12967: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6570, training loss 0.00398, validation loss 0.00411
Epoch 12978: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12989: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6576
Mean train loss for ascent epoch 6577: -0.000873795070219785
Mean eval for ascent epoch 6577: 0.0034493859857320786
Doing Evaluation on the model now
This is Epoch 6580, training loss 0.32663, validation loss 0.31838
Epoch 13000: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13011: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6590, training loss 0.03047, validation loss 0.04215
Epoch 13022: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13033: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6600, training loss 0.01184, validation loss 0.00957
Resetting learning rate to 0.01000
Epoch 13044: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13055: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6610, training loss 0.00475, validation loss 0.00431
Epoch 13066: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6620, training loss 0.00313, validation loss 0.00364
Epoch 13077: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6621
Mean train loss for ascent epoch 6622: -0.0007229534676298499
Mean eval for ascent epoch 6622: 0.003061806783080101
Epoch 13088: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6630, training loss 0.12027, validation loss 0.07787
Epoch 13099: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13110: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6640, training loss 0.01784, validation loss 0.02831
Epoch 13121: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13132: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6650, training loss 0.00445, validation loss 0.00382
Epoch 13143: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13154: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6660
Doing Evaluation on the model now
This is Epoch 6660, training loss 0.00330, validation loss 0.00306
Mean train loss for ascent epoch 6661: -0.0012956972932443023
Mean eval for ascent epoch 6661: 0.003348196391016245
Epoch 13165: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6670, training loss 0.08422, validation loss 0.11369
Epoch 13176: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13187: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6680, training loss 0.02026, validation loss 0.01422
Epoch 13198: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13209: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6690, training loss 0.00548, validation loss 0.00428
Epoch 13220: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13231: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6699
Mean train loss for ascent epoch 6700: -0.0006665151449851692
Mean eval for ascent epoch 6700: 0.0032587090972810984
Doing Evaluation on the model now
This is Epoch 6700, training loss 0.00326, validation loss 0.00282
Epoch 13242: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13253: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6710, training loss 0.05247, validation loss 0.07730
Epoch 13264: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6720, training loss 0.02445, validation loss 0.01967
Epoch 13275: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13286: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6730, training loss 0.00304, validation loss 0.00336
Epoch 13297: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13308: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6738
Mean train loss for ascent epoch 6739: -0.0008742677164264023
Mean eval for ascent epoch 6739: 0.003242654725909233
Doing Evaluation on the model now
This is Epoch 6740, training loss 0.14806, validation loss 0.60048
Epoch 13319: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13330: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6750, training loss 0.08376, validation loss 0.06658
Epoch 13341: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13352: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6760, training loss 0.02456, validation loss 0.02413
Epoch 13363: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6770, training loss 0.00560, validation loss 0.00505
Epoch 13374: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13385: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6777
Mean train loss for ascent epoch 6778: -0.0016200769459828734
Mean eval for ascent epoch 6778: 0.0056513831950724125
Doing Evaluation on the model now
This is Epoch 6780, training loss 0.14389, validation loss 0.09551
Epoch 13396: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13407: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6790, training loss 0.03835, validation loss 0.06604
Epoch 13418: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13429: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6800, training loss 0.01894, validation loss 0.00755
Resetting learning rate to 0.01000
Epoch 13440: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13451: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6810, training loss 0.00739, validation loss 0.00910
Epoch 13462: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6820, training loss 0.00484, validation loss 0.00478
Epoch 13473: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6821
Mean train loss for ascent epoch 6822: -0.001077346969395876
Mean eval for ascent epoch 6822: 0.005092786159366369
Epoch 13484: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6830, training loss 0.11931, validation loss 0.17321
Epoch 13495: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13506: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6840, training loss 0.02404, validation loss 0.02714
Epoch 13517: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13528: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6850, training loss 0.00843, validation loss 0.00780
Epoch 13539: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13550: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6860
Doing Evaluation on the model now
This is Epoch 6860, training loss 0.00536, validation loss 0.00820
Mean train loss for ascent epoch 6861: -0.0010573058389127254
Mean eval for ascent epoch 6861: 0.008323922753334045
Epoch 13561: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6870, training loss 0.12172, validation loss 0.13252
Epoch 13572: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13583: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6880, training loss 0.02853, validation loss 0.03171
Epoch 13594: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13605: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6890, training loss 0.00712, validation loss 0.00607
Epoch 13616: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13627: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6899
Mean train loss for ascent epoch 6900: -0.00116433622315526
Mean eval for ascent epoch 6900: 0.005141410976648331
Doing Evaluation on the model now
This is Epoch 6900, training loss 0.00514, validation loss 0.00550
Epoch 13638: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6910, training loss 0.16626, validation loss 0.18745
Epoch 13649: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13660: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6920, training loss 0.02994, validation loss 0.04111
Epoch 13671: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13682: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6930, training loss 0.00552, validation loss 0.00440
Epoch 13693: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13704: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6938
Mean train loss for ascent epoch 6939: -0.0012591996928676963
Mean eval for ascent epoch 6939: 0.005203606560826302
Doing Evaluation on the model now
This is Epoch 6940, training loss 0.28426, validation loss 0.18270
Epoch 13715: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13726: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6950, training loss 0.06392, validation loss 0.04879
Epoch 13737: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6960, training loss 0.01981, validation loss 0.01799
Epoch 13748: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13759: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6970, training loss 0.00676, validation loss 0.00565
Epoch 13770: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13781: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6977
Mean train loss for ascent epoch 6978: -0.0013651286717504263
Mean eval for ascent epoch 6978: 0.0053006745874881744
Doing Evaluation on the model now
This is Epoch 6980, training loss 0.57215, validation loss 0.63255
Epoch 13792: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13803: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6990, training loss 0.12927, validation loss 0.12572
Epoch 13814: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13825: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7000, training loss 0.01897, validation loss 0.01533
Resetting learning rate to 0.01000
Epoch 13836: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7010, training loss 0.00654, validation loss 0.00658
Epoch 13847: reducing learning rate of group 0 to 2.5000e-04.
Epoch 13858: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7020, training loss 0.00433, validation loss 0.00418
Epoch 13869: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7022
Mean train loss for ascent epoch 7023: -0.0009809430921450257
Mean eval for ascent epoch 7023: 0.0037109486293047667
Epoch 13880: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7030, training loss 0.23050, validation loss 0.20059
Epoch 13891: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13902: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7040, training loss 0.02192, validation loss 0.03368
Epoch 13913: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13924: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7050, training loss 0.00585, validation loss 0.00755
Epoch 13935: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7060, training loss 0.00432, validation loss 0.00503
Epoch 13946: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7061
Mean train loss for ascent epoch 7062: -0.0009206539252772927
Mean eval for ascent epoch 7062: 0.003675062907859683
Epoch 13957: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7070, training loss 0.08218, validation loss 0.08175
Epoch 13968: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13979: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7080, training loss 0.02684, validation loss 0.01955
Epoch 13990: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14001: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7090, training loss 0.01019, validation loss 0.00848
Epoch 14012: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14023: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7100
Doing Evaluation on the model now
This is Epoch 7100, training loss 0.00439, validation loss 0.00487
Mean train loss for ascent epoch 7101: -0.000780000351369381
Mean eval for ascent epoch 7101: 0.004316685255616903
Epoch 14034: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7110, training loss 0.04195, validation loss 0.04237
Epoch 14045: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14056: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7120, training loss 0.02058, validation loss 0.01719
Epoch 14067: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14078: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7130, training loss 0.00478, validation loss 0.00450
Epoch 14089: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14100: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7139
Mean train loss for ascent epoch 7140: -0.0009758317610248923
Mean eval for ascent epoch 7140: 0.003640921786427498
Doing Evaluation on the model now
This is Epoch 7140, training loss 0.00364, validation loss 0.00307
Epoch 14111: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14122: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7150, training loss 0.12869, validation loss 0.09372
Epoch 14133: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7160, training loss 0.01833, validation loss 0.02294
Epoch 14144: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14155: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7170, training loss 0.00490, validation loss 0.00418
Epoch 14166: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14177: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7178
Mean train loss for ascent epoch 7179: -0.001197664998471737
Mean eval for ascent epoch 7179: 0.0032110230531543493
Doing Evaluation on the model now
This is Epoch 7180, training loss 0.37808, validation loss 1.20699
Epoch 14188: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14199: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7190, training loss 0.05865, validation loss 0.03332
Epoch 14210: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14221: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7200, training loss 0.01420, validation loss 0.01262
Resetting learning rate to 0.01000
Epoch 14232: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7210, training loss 0.00680, validation loss 0.00716
Epoch 14243: reducing learning rate of group 0 to 2.5000e-04.
Epoch 14254: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7220, training loss 0.00326, validation loss 0.00348
Epoch 14265: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7222
Mean train loss for ascent epoch 7223: -0.0006244590622372925
Mean eval for ascent epoch 7223: 0.003150742966681719
Epoch 14276: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7230, training loss 0.08569, validation loss 0.07975
Epoch 14287: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14298: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7240, training loss 0.02005, validation loss 0.01306
Epoch 14309: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14320: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7250, training loss 0.00637, validation loss 0.00857
Epoch 14331: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7260, training loss 0.00355, validation loss 0.00312
Epoch 14342: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7261
Mean train loss for ascent epoch 7262: -0.0009419124107807875
Mean eval for ascent epoch 7262: 0.002931197639554739
Epoch 14353: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7270, training loss 0.12185, validation loss 0.15201
Epoch 14364: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14375: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7280, training loss 0.01835, validation loss 0.01468
Epoch 14386: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14397: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7290, training loss 0.00592, validation loss 0.00521
Epoch 14408: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14419: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7300
Doing Evaluation on the model now
This is Epoch 7300, training loss 0.00329, validation loss 0.00322
Mean train loss for ascent epoch 7301: -0.0007274115341715515
Mean eval for ascent epoch 7301: 0.003085980424657464
Epoch 14430: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7310, training loss 0.14733, validation loss 0.08257
Epoch 14441: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14452: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7320, training loss 0.02705, validation loss 0.03271
Epoch 14463: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14474: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7330, training loss 0.00308, validation loss 0.00344
Epoch 14485: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14496: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7339
Mean train loss for ascent epoch 7340: -0.0008265853393822908
Mean eval for ascent epoch 7340: 0.003446251852437854
Doing Evaluation on the model now
This is Epoch 7340, training loss 0.00345, validation loss 0.00520
Epoch 14507: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7350, training loss 0.05958, validation loss 0.14736
Epoch 14518: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14529: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7360, training loss 0.01777, validation loss 0.01853
Epoch 14540: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14551: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7370, training loss 0.00408, validation loss 0.00555
Epoch 14562: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14573: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7378
Mean train loss for ascent epoch 7379: -0.0005452125333249569
Mean eval for ascent epoch 7379: 0.002726634033024311
Doing Evaluation on the model now
This is Epoch 7380, training loss 0.32852, validation loss 0.45073
Epoch 14584: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14595: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7390, training loss 0.03658, validation loss 0.01613
Epoch 14606: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7400, training loss 0.01630, validation loss 0.01806
Epoch 14617: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 14628: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7410, training loss 0.00632, validation loss 0.00698
Epoch 14639: reducing learning rate of group 0 to 2.5000e-04.
Epoch 14650: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7420, training loss 0.00368, validation loss 0.00315
Epoch 14661: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7423
Mean train loss for ascent epoch 7424: -0.0006155687151476741
Mean eval for ascent epoch 7424: 0.0032079112716019154
Epoch 14672: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7430, training loss 0.07731, validation loss 0.17411
Epoch 14683: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14694: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7440, training loss 0.04206, validation loss 0.02133
Epoch 14705: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7450, training loss 0.01037, validation loss 0.01083
Epoch 14716: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14727: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7460, training loss 0.00366, validation loss 0.00378
Epoch 14738: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7462
Mean train loss for ascent epoch 7463: -0.0009140613256022334
Mean eval for ascent epoch 7463: 0.0035205765161663294
Epoch 14749: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7470, training loss 0.11027, validation loss 0.16945
Epoch 14760: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14771: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7480, training loss 0.02562, validation loss 0.02614
Epoch 14782: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14793: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7490, training loss 0.00703, validation loss 0.01039
Epoch 14804: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7500, training loss 0.00384, validation loss 0.00386
Epoch 14815: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7501
Mean train loss for ascent epoch 7502: -0.0007039400516077876
Mean eval for ascent epoch 7502: 0.003230428323149681
Epoch 14826: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7510, training loss 0.05483, validation loss 0.16244
Epoch 14837: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14848: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7520, training loss 0.01808, validation loss 0.01680
Epoch 14859: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14870: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7530, training loss 0.00428, validation loss 0.00371
Epoch 14881: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14892: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7540
Doing Evaluation on the model now
This is Epoch 7540, training loss 0.00343, validation loss 0.00314
Mean train loss for ascent epoch 7541: -0.0005639944574795663
Mean eval for ascent epoch 7541: 0.00334069412201643
Epoch 14903: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7550, training loss 0.07607, validation loss 0.04885
Epoch 14914: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14925: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7560, training loss 0.02282, validation loss 0.02184
Epoch 14936: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14947: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7570, training loss 0.00447, validation loss 0.00398
Epoch 14958: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14969: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7579
Mean train loss for ascent epoch 7580: -0.0005528401234187186
Mean eval for ascent epoch 7580: 0.0033900560811161995
Doing Evaluation on the model now
This is Epoch 7580, training loss 0.00339, validation loss 0.00351
Epoch 14980: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14991: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7590, training loss 0.12112, validation loss 0.09340
Epoch 15002: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7600, training loss 0.01716, validation loss 0.01317
Resetting learning rate to 0.01000
Epoch 15013: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15024: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7610, training loss 0.00388, validation loss 0.00422
Epoch 15035: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15046: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7618
Mean train loss for ascent epoch 7619: -0.000908666115719825
Mean eval for ascent epoch 7619: 0.0034682711120694876
Doing Evaluation on the model now
This is Epoch 7620, training loss 0.23079, validation loss 0.17127
Epoch 15057: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15068: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7630, training loss 0.02835, validation loss 0.02297
Epoch 15079: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15090: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7640, training loss 0.02285, validation loss 0.03872
Epoch 15101: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7650, training loss 0.00379, validation loss 0.00409
Epoch 15112: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15123: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7657
Mean train loss for ascent epoch 7658: -0.0005228999070823193
Mean eval for ascent epoch 7658: 0.003010434564203024
Doing Evaluation on the model now
This is Epoch 7660, training loss 1.20147, validation loss 1.20785
Epoch 15134: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15145: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7670, training loss 0.02705, validation loss 0.04121
Epoch 15156: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15167: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7680, training loss 0.00952, validation loss 0.00922
Epoch 15178: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15189: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7690, training loss 0.00413, validation loss 0.00371
Epoch 15200: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7696
Mean train loss for ascent epoch 7697: -0.0005595135153271258
Mean eval for ascent epoch 7697: 0.002961673540994525
Doing Evaluation on the model now
This is Epoch 7700, training loss 1.19039, validation loss 0.76723
Epoch 15211: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15222: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7710, training loss 0.05139, validation loss 0.05539
Epoch 15233: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15244: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7720, training loss 0.00820, validation loss 0.00908
Epoch 15255: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15266: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7730, training loss 0.00372, validation loss 0.00315
Epoch 15277: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7735
Mean train loss for ascent epoch 7736: -0.0007827850640751421
Mean eval for ascent epoch 7736: 0.00306047429330647
Doing Evaluation on the model now
This is Epoch 7740, training loss 0.76044, validation loss 0.81492
Epoch 15288: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15299: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7750, training loss 0.05710, validation loss 0.04059
Epoch 15310: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15321: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7760, training loss 0.00721, validation loss 0.00829
Epoch 15332: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15343: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7770, training loss 0.00360, validation loss 0.00391
Epoch 15354: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7774
Mean train loss for ascent epoch 7775: -0.0009110122919082642
Mean eval for ascent epoch 7775: 0.0033557270653545856
Epoch 15365: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7780, training loss 0.07910, validation loss 0.06161
Epoch 15376: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7790, training loss 0.03489, validation loss 0.03810
Epoch 15387: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15398: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7800, training loss 0.00955, validation loss 0.01299
Resetting learning rate to 0.01000
Epoch 15409: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15420: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7810, training loss 0.00534, validation loss 0.00373
Epoch 15431: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15442: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7818
Mean train loss for ascent epoch 7819: -0.0006676434422843158
Mean eval for ascent epoch 7819: 0.0034598857164382935
Doing Evaluation on the model now
This is Epoch 7820, training loss 0.41286, validation loss 0.64463
Epoch 15453: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15464: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7830, training loss 0.03489, validation loss 0.05244
Epoch 15475: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7840, training loss 0.01338, validation loss 0.00668
Epoch 15486: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15497: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7850, training loss 0.00499, validation loss 0.00447
Epoch 15508: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15519: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7857
Mean train loss for ascent epoch 7858: -0.0011201067827641964
Mean eval for ascent epoch 7858: 0.004059141501784325
Doing Evaluation on the model now
This is Epoch 7860, training loss 0.29960, validation loss 0.81965
Epoch 15530: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15541: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7870, training loss 0.06274, validation loss 0.03544
Epoch 15552: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15563: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7880, training loss 0.00949, validation loss 0.00838
Epoch 15574: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7890, training loss 0.00421, validation loss 0.00367
Epoch 15585: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15596: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7896
Mean train loss for ascent epoch 7897: -0.0007385581266134977
Mean eval for ascent epoch 7897: 0.0034008845686912537
Doing Evaluation on the model now
This is Epoch 7900, training loss 0.75750, validation loss 0.22974
Epoch 15607: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15618: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7910, training loss 0.04438, validation loss 0.03069
Epoch 15629: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15640: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7920, training loss 0.00763, validation loss 0.00488
Epoch 15651: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15662: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7930, training loss 0.00390, validation loss 0.00314
Epoch 15673: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7935
Mean train loss for ascent epoch 7936: -0.0007898999028839171
Mean eval for ascent epoch 7936: 0.0034918144810944796
Doing Evaluation on the model now
This is Epoch 7940, training loss 1.00519, validation loss 0.14304
Epoch 15684: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15695: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7950, training loss 0.05096, validation loss 0.06506
Epoch 15706: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15717: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7960, training loss 0.01128, validation loss 0.01576
Epoch 15728: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15739: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7970, training loss 0.00354, validation loss 0.00384
Epoch 15750: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7974
Mean train loss for ascent epoch 7975: -0.0006700385711155832
Mean eval for ascent epoch 7975: 0.0034017888829112053
Epoch 15761: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7980, training loss 0.60612, validation loss 0.53399
Epoch 15772: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7990, training loss 0.04678, validation loss 0.04108
Epoch 15783: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15794: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8000, training loss 0.00675, validation loss 0.00746
Resetting learning rate to 0.01000
Epoch 15805: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15816: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8010, training loss 0.00439, validation loss 0.00355
Epoch 15827: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15838: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8019
Mean train loss for ascent epoch 8020: -0.0006003594608046114
Mean eval for ascent epoch 8020: 0.0030596682336181402
Doing Evaluation on the model now
This is Epoch 8020, training loss 0.00306, validation loss 0.00305
Epoch 15849: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15860: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8030, training loss 0.16892, validation loss 0.09419
Epoch 15871: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8040, training loss 0.01654, validation loss 0.01694
Epoch 15882: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15893: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8050, training loss 0.00822, validation loss 0.00547
Epoch 15904: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15915: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8058
Mean train loss for ascent epoch 8059: -0.0010218234965577722
Mean eval for ascent epoch 8059: 0.0033084184397011995
Doing Evaluation on the model now
This is Epoch 8060, training loss 0.09485, validation loss 0.13394
Epoch 15926: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15937: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8070, training loss 0.05515, validation loss 0.06258
Epoch 15948: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15959: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8080, training loss 0.01602, validation loss 0.00702
Epoch 15970: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8090, training loss 0.00425, validation loss 0.00632
Epoch 15981: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15992: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8097
Mean train loss for ascent epoch 8098: -0.0006207999540492892
Mean eval for ascent epoch 8098: 0.003069025231525302
Doing Evaluation on the model now
This is Epoch 8100, training loss 1.37228, validation loss 1.74618
Epoch 16003: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16014: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8110, training loss 0.05753, validation loss 0.07754
Epoch 16025: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16036: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8120, training loss 0.01261, validation loss 0.01029
Epoch 16047: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16058: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8130, training loss 0.00416, validation loss 0.00439
Epoch 16069: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8136
Mean train loss for ascent epoch 8137: -0.0009541836916469038
Mean eval for ascent epoch 8137: 0.0031360771972686052
Doing Evaluation on the model now
This is Epoch 8140, training loss 0.84193, validation loss 0.24319
Epoch 16080: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16091: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8150, training loss 0.02397, validation loss 0.02752
Epoch 16102: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16113: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8160, training loss 0.01127, validation loss 0.01054
Epoch 16124: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16135: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8170, training loss 0.00400, validation loss 0.00308
Epoch 16146: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8175
Mean train loss for ascent epoch 8176: -0.0005646611680276692
Mean eval for ascent epoch 8176: 0.0032899987418204546
Doing Evaluation on the model now
This is Epoch 8180, training loss 0.45370, validation loss 0.14055
Epoch 16157: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16168: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8190, training loss 0.04668, validation loss 0.02309
Epoch 16179: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16190: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8200, training loss 0.00755, validation loss 0.00634
Resetting learning rate to 0.01000
Epoch 16201: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16212: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8210, training loss 0.00389, validation loss 0.00362
Epoch 16223: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16234: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8219
Mean train loss for ascent epoch 8220: -0.0006510104285553098
Mean eval for ascent epoch 8220: 0.003183073829859495
Doing Evaluation on the model now
This is Epoch 8220, training loss 0.00318, validation loss 0.00292
Epoch 16245: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8230, training loss 0.11821, validation loss 0.10208
Epoch 16256: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16267: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8240, training loss 0.02191, validation loss 0.02657
Epoch 16278: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16289: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8250, training loss 0.00354, validation loss 0.00321
Epoch 16300: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16311: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8258
Mean train loss for ascent epoch 8259: -0.0005408033612184227
Mean eval for ascent epoch 8259: 0.003009306499734521
Doing Evaluation on the model now
This is Epoch 8260, training loss 0.18846, validation loss 0.13816
Epoch 16322: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16333: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8270, training loss 0.02699, validation loss 0.01804
Epoch 16344: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8280, training loss 0.01873, validation loss 0.01956
Epoch 16355: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16366: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8290, training loss 0.00382, validation loss 0.00352
Epoch 16377: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16388: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8297
Mean train loss for ascent epoch 8298: -0.0006284808041527867
Mean eval for ascent epoch 8298: 0.0028509648982435465
Doing Evaluation on the model now
This is Epoch 8300, training loss 0.43868, validation loss 0.66885
Epoch 16399: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16410: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8310, training loss 0.04199, validation loss 0.05942
Epoch 16421: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16432: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8320, training loss 0.01405, validation loss 0.01071
Epoch 16443: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8330, training loss 0.00380, validation loss 0.00379
Epoch 16454: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16465: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8336
Mean train loss for ascent epoch 8337: -0.0007729961653240025
Mean eval for ascent epoch 8337: 0.0029451120644807816
Doing Evaluation on the model now
This is Epoch 8340, training loss 0.46530, validation loss 0.47784
Epoch 16476: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16487: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8350, training loss 0.02199, validation loss 0.01723
Epoch 16498: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16509: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8360, training loss 0.00667, validation loss 0.00774
Epoch 16520: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16531: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8370, training loss 0.00297, validation loss 0.00347
Epoch 16542: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8375
Mean train loss for ascent epoch 8376: -0.0007683071307837963
Mean eval for ascent epoch 8376: 0.0028422181494534016
Doing Evaluation on the model now
This is Epoch 8380, training loss 0.66146, validation loss 0.53544
Epoch 16553: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16564: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8390, training loss 0.03532, validation loss 0.03851
Epoch 16575: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16586: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8400, training loss 0.00584, validation loss 0.00569
Resetting learning rate to 0.01000
Epoch 16597: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16608: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8410, training loss 0.00334, validation loss 0.00366
Epoch 16619: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16630: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8420
Doing Evaluation on the model now
This is Epoch 8420, training loss 0.00263, validation loss 0.00298
Mean train loss for ascent epoch 8421: -0.0005904835416004062
Mean eval for ascent epoch 8421: 0.0025639317464083433
Epoch 16641: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8430, training loss 0.15668, validation loss 0.19572
Epoch 16652: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16663: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8440, training loss 0.01807, validation loss 0.01987
Epoch 16674: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16685: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8450, training loss 0.00452, validation loss 0.00447
Epoch 16696: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16707: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8459
Mean train loss for ascent epoch 8460: -0.0006249268772080541
Mean eval for ascent epoch 8460: 0.0027799459639936686
Doing Evaluation on the model now
This is Epoch 8460, training loss 0.00278, validation loss 0.00252
Epoch 16718: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16729: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8470, training loss 0.05616, validation loss 0.13166
Epoch 16740: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8480, training loss 0.01075, validation loss 0.02184
Epoch 16751: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16762: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8490, training loss 0.00377, validation loss 0.00509
Epoch 16773: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16784: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8498
Mean train loss for ascent epoch 8499: -0.0007694978849031031
Mean eval for ascent epoch 8499: 0.0026399102061986923
Doing Evaluation on the model now
This is Epoch 8500, training loss 0.12694, validation loss 0.15425
Epoch 16795: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16806: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8510, training loss 0.11813, validation loss 0.06557
Epoch 16817: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16828: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8520, training loss 0.02230, validation loss 0.02018
Epoch 16839: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8530, training loss 0.00401, validation loss 0.00315
Epoch 16850: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16861: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8537
Mean train loss for ascent epoch 8538: -0.0008285529911518097
Mean eval for ascent epoch 8538: 0.0025894504506140947
Doing Evaluation on the model now
This is Epoch 8540, training loss 0.17648, validation loss 0.21856
Epoch 16872: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16883: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8550, training loss 0.03087, validation loss 0.01616
Epoch 16894: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16905: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8560, training loss 0.01278, validation loss 0.01190
Epoch 16916: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16927: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8570, training loss 0.00501, validation loss 0.00522
Epoch 16938: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8576
Mean train loss for ascent epoch 8577: -0.000865634938236326
Mean eval for ascent epoch 8577: 0.0033244737423956394
Doing Evaluation on the model now
This is Epoch 8580, training loss 1.43532, validation loss 0.49897
Epoch 16949: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16960: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8590, training loss 0.05388, validation loss 0.04596
Epoch 16971: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16982: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8600, training loss 0.01160, validation loss 0.00599
Resetting learning rate to 0.01000
Epoch 16993: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17004: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8610, training loss 0.00461, validation loss 0.00442
Epoch 17015: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17026: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8620
Doing Evaluation on the model now
This is Epoch 8620, training loss 0.00303, validation loss 0.00296
Mean train loss for ascent epoch 8621: -0.0007112894090823829
Mean eval for ascent epoch 8621: 0.0029584208969026804
Epoch 17037: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8630, training loss 0.11246, validation loss 0.08298
Epoch 17048: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17059: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8640, training loss 0.02287, validation loss 0.01374
Epoch 17070: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17081: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8650, training loss 0.00409, validation loss 0.00519
Epoch 17092: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17103: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8659
Mean train loss for ascent epoch 8660: -0.0006661129300482571
Mean eval for ascent epoch 8660: 0.0034730492625385523
Doing Evaluation on the model now
This is Epoch 8660, training loss 0.00347, validation loss 0.00277
Epoch 17114: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8670, training loss 0.05550, validation loss 0.04881
Epoch 17125: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17136: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8680, training loss 0.02041, validation loss 0.01693
Epoch 17147: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17158: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8690, training loss 0.00388, validation loss 0.00442
Epoch 17169: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17180: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8698
Mean train loss for ascent epoch 8699: -0.0007778378203511238
Mean eval for ascent epoch 8699: 0.0035119743552058935
Doing Evaluation on the model now
This is Epoch 8700, training loss 0.13659, validation loss 0.05255
Epoch 17191: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17202: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8710, training loss 0.04486, validation loss 0.04240
Epoch 17213: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8720, training loss 0.01380, validation loss 0.02082
Epoch 17224: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17235: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8730, training loss 0.00473, validation loss 0.00381
Epoch 17246: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17257: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8737
Mean train loss for ascent epoch 8738: -0.0009766939328983426
Mean eval for ascent epoch 8738: 0.003080105409026146
Doing Evaluation on the model now
This is Epoch 8740, training loss 0.58962, validation loss 0.44853
Epoch 17268: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17279: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8750, training loss 0.01849, validation loss 0.02386
Epoch 17290: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17301: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8760, training loss 0.00842, validation loss 0.00725
Epoch 17312: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8770, training loss 0.00401, validation loss 0.00363
Epoch 17323: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17334: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8776
Mean train loss for ascent epoch 8777: -0.0007172098266892135
Mean eval for ascent epoch 8777: 0.003056248417124152
Doing Evaluation on the model now
This is Epoch 8780, training loss 0.46903, validation loss 0.35174
Epoch 17345: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17356: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8790, training loss 0.01817, validation loss 0.02872
Epoch 17367: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17378: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8800, training loss 0.00767, validation loss 0.00441
Resetting learning rate to 0.01000
Epoch 17389: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17400: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8810, training loss 0.00403, validation loss 0.00598
Epoch 17411: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8820, training loss 0.00327, validation loss 0.00331
Epoch 17422: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8821
Mean train loss for ascent epoch 8822: -0.0007495586760342121
Mean eval for ascent epoch 8822: 0.0028352595400065184
Epoch 17433: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8830, training loss 0.03298, validation loss 0.04178
Epoch 17444: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17455: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8840, training loss 0.01584, validation loss 0.00725
Epoch 17466: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17477: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8850, training loss 0.00499, validation loss 0.00268
Epoch 17488: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17499: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8860
Doing Evaluation on the model now
This is Epoch 8860, training loss 0.00261, validation loss 0.00242
Mean train loss for ascent epoch 8861: -0.0010046049719676375
Mean eval for ascent epoch 8861: 0.0026241163723170757
Epoch 17510: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8870, training loss 0.06351, validation loss 0.03204
Epoch 17521: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17532: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8880, training loss 0.01882, validation loss 0.02274
Epoch 17543: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17554: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8890, training loss 0.00517, validation loss 0.00399
Epoch 17565: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17576: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8899
Mean train loss for ascent epoch 8900: -0.0005780112114734948
Mean eval for ascent epoch 8900: 0.0029561317060142756
Doing Evaluation on the model now
This is Epoch 8900, training loss 0.00296, validation loss 0.00277
Epoch 17587: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17598: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8910, training loss 0.08121, validation loss 0.07203
Epoch 17609: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8920, training loss 0.01811, validation loss 0.00858
Epoch 17620: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17631: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8930, training loss 0.00486, validation loss 0.00452
Epoch 17642: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17653: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8938
Mean train loss for ascent epoch 8939: -0.0006875775288790464
Mean eval for ascent epoch 8939: 0.003036306006833911
Doing Evaluation on the model now
This is Epoch 8940, training loss 0.16600, validation loss 0.10919
Epoch 17664: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17675: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8950, training loss 0.04540, validation loss 0.05234
Epoch 17686: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17697: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8960, training loss 0.02697, validation loss 0.01689
Epoch 17708: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8970, training loss 0.00437, validation loss 0.00763
Epoch 17719: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17730: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8977
Mean train loss for ascent epoch 8978: -0.0007061467040330172
Mean eval for ascent epoch 8978: 0.0032277610152959824
Doing Evaluation on the model now
This is Epoch 8980, training loss 0.42434, validation loss 0.64856
Epoch 17741: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17752: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8990, training loss 0.02824, validation loss 0.01480
Epoch 17763: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17774: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9000, training loss 0.01123, validation loss 0.00734
Resetting learning rate to 0.01000
Epoch 17785: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17796: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9010, training loss 0.00951, validation loss 0.01042
Epoch 17807: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9020, training loss 0.00306, validation loss 0.00364
Epoch 17818: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9021
Mean train loss for ascent epoch 9022: -0.0006495023262687027
Mean eval for ascent epoch 9022: 0.002805634867399931
Epoch 17829: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9030, training loss 0.10675, validation loss 0.24887
Epoch 17840: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17851: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9040, training loss 0.01726, validation loss 0.01573
Epoch 17862: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17873: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9050, training loss 0.00521, validation loss 0.00488
Epoch 17884: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17895: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9060
Doing Evaluation on the model now
This is Epoch 9060, training loss 0.00326, validation loss 0.00325
Mean train loss for ascent epoch 9061: -0.0008764704107306898
Mean eval for ascent epoch 9061: 0.0030492672231048346
Epoch 17906: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9070, training loss 0.07196, validation loss 0.09226
Epoch 17917: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17928: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9080, training loss 0.01457, validation loss 0.00740
Epoch 17939: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17950: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9090, training loss 0.00434, validation loss 0.00343
Epoch 17961: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17972: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9099
Mean train loss for ascent epoch 9100: -0.000814145605545491
Mean eval for ascent epoch 9100: 0.002772663487121463
Doing Evaluation on the model now
This is Epoch 9100, training loss 0.00277, validation loss 0.00296
Epoch 17983: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9110, training loss 0.06379, validation loss 0.05310
Epoch 17994: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18005: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9120, training loss 0.01588, validation loss 0.02320
Epoch 18016: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18027: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9130, training loss 0.00370, validation loss 0.00469
Epoch 18038: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18049: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9138
Mean train loss for ascent epoch 9139: -0.0009673904278315604
Mean eval for ascent epoch 9139: 0.003497013123705983
Doing Evaluation on the model now
This is Epoch 9140, training loss 0.29351, validation loss 0.31804
Epoch 18060: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18071: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9150, training loss 0.02524, validation loss 0.02509
Epoch 18082: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9160, training loss 0.01217, validation loss 0.01351
Epoch 18093: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18104: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9170, training loss 0.00571, validation loss 0.00358
Epoch 18115: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18126: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9177
Mean train loss for ascent epoch 9178: -0.0008776136673986912
Mean eval for ascent epoch 9178: 0.0032164498697966337
Doing Evaluation on the model now
This is Epoch 9180, training loss 0.25235, validation loss 0.52870
Epoch 18137: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18148: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9190, training loss 0.02979, validation loss 0.04938
Epoch 18159: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18170: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9200, training loss 0.00970, validation loss 0.00364
Resetting learning rate to 0.01000
Epoch 18181: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9210, training loss 0.00441, validation loss 0.00405
Epoch 18192: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18203: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9220, training loss 0.00324, validation loss 0.00297
Epoch 18214: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9222
Mean train loss for ascent epoch 9223: -0.0010047022951766849
Mean eval for ascent epoch 9223: 0.0026643455494195223
Epoch 18225: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9230, training loss 0.06184, validation loss 0.05985
Epoch 18236: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18247: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9240, training loss 0.01045, validation loss 0.01602
Epoch 18258: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18269: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9250, training loss 0.00563, validation loss 0.00414
Epoch 18280: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9260, training loss 0.00296, validation loss 0.00339
Epoch 18291: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9261
Mean train loss for ascent epoch 9262: -0.0007820177706889808
Mean eval for ascent epoch 9262: 0.0028325708117336035
Epoch 18302: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9270, training loss 0.18187, validation loss 0.32451
Epoch 18313: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18324: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9280, training loss 0.02137, validation loss 0.01829
Epoch 18335: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18346: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9290, training loss 0.00438, validation loss 0.00297
Epoch 18357: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18368: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9300
Doing Evaluation on the model now
This is Epoch 9300, training loss 0.00305, validation loss 0.00295
Mean train loss for ascent epoch 9301: -0.0009317683288827538
Mean eval for ascent epoch 9301: 0.0027437375392764807
Epoch 18379: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9310, training loss 0.10823, validation loss 0.04434
Epoch 18390: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18401: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9320, training loss 0.01821, validation loss 0.02325
Epoch 18412: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18423: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9330, training loss 0.00454, validation loss 0.00382
Epoch 18434: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18445: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9339
Mean train loss for ascent epoch 9340: -0.0007540612132288516
Mean eval for ascent epoch 9340: 0.002750825835391879
Doing Evaluation on the model now
This is Epoch 9340, training loss 0.00275, validation loss 0.00253
Epoch 18456: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18467: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9350, training loss 0.06596, validation loss 0.07772
Epoch 18478: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9360, training loss 0.00892, validation loss 0.00740
Epoch 18489: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18500: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9370, training loss 0.00370, validation loss 0.00351
Epoch 18511: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18522: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9378
Mean train loss for ascent epoch 9379: -0.0006872548256069422
Mean eval for ascent epoch 9379: 0.0034352450165897608
Doing Evaluation on the model now
This is Epoch 9380, training loss 0.13832, validation loss 0.08389
Epoch 18533: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18544: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9390, training loss 0.02381, validation loss 0.03273
Epoch 18555: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18566: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9400, training loss 0.01215, validation loss 0.02428
Resetting learning rate to 0.01000
Epoch 18577: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9410, training loss 0.00504, validation loss 0.00334
Epoch 18588: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18599: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9420, training loss 0.00300, validation loss 0.00270
Epoch 18610: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9422
Mean train loss for ascent epoch 9423: -0.0008681535255163908
Mean eval for ascent epoch 9423: 0.003069936763495207
Epoch 18621: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9430, training loss 0.08919, validation loss 0.10611
Epoch 18632: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18643: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9440, training loss 0.01849, validation loss 0.00629
Epoch 18654: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18665: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9450, training loss 0.00603, validation loss 0.00610
Epoch 18676: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9460, training loss 0.00303, validation loss 0.00294
Epoch 18687: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9461
Mean train loss for ascent epoch 9462: -0.0006888004136271775
Mean eval for ascent epoch 9462: 0.003712149802595377
Epoch 18698: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9470, training loss 0.05265, validation loss 0.03486
Epoch 18709: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18720: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9480, training loss 0.01601, validation loss 0.00636
Epoch 18731: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18742: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9490, training loss 0.00602, validation loss 0.00569
Epoch 18753: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18764: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9500
Doing Evaluation on the model now
This is Epoch 9500, training loss 0.00334, validation loss 0.00339
Mean train loss for ascent epoch 9501: -0.0006082006148062646
Mean eval for ascent epoch 9501: 0.002999836578965187
Epoch 18775: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9510, training loss 0.04019, validation loss 0.03547
Epoch 18786: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18797: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9520, training loss 0.00905, validation loss 0.02059
Epoch 18808: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18819: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9530, training loss 0.00372, validation loss 0.00329
Epoch 18830: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18841: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9539
Mean train loss for ascent epoch 9540: -0.0006326916045509279
Mean eval for ascent epoch 9540: 0.002966892207041383
Doing Evaluation on the model now
This is Epoch 9540, training loss 0.00297, validation loss 0.00301
Epoch 18852: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9550, training loss 0.05391, validation loss 0.16608
Epoch 18863: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18874: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9560, training loss 0.02534, validation loss 0.01278
Epoch 18885: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18896: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9570, training loss 0.00351, validation loss 0.00403
Epoch 18907: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18918: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9578
Mean train loss for ascent epoch 9579: -0.0006775159854441881
Mean eval for ascent epoch 9579: 0.0026371844578534365
Doing Evaluation on the model now
This is Epoch 9580, training loss 0.10259, validation loss 0.19497
Epoch 18929: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18940: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9590, training loss 0.04934, validation loss 0.03467
Epoch 18951: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9600, training loss 0.00990, validation loss 0.01248
Epoch 18962: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 18973: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9610, training loss 0.00522, validation loss 0.00350
Epoch 18984: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18995: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9620, training loss 0.00313, validation loss 0.00301
Epoch 19006: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9623
Mean train loss for ascent epoch 9624: -0.0005739436019212008
Mean eval for ascent epoch 9624: 0.003136934246867895
Epoch 19017: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9630, training loss 0.02859, validation loss 0.03503
Epoch 19028: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19039: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9640, training loss 0.01662, validation loss 0.02499
Epoch 19050: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9650, training loss 0.00524, validation loss 0.00551
Epoch 19061: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19072: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9660, training loss 0.00308, validation loss 0.00341
Epoch 19083: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9662
Mean train loss for ascent epoch 9663: -0.0009469399810768664
Mean eval for ascent epoch 9663: 0.0029106950387358665
Epoch 19094: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9670, training loss 0.08582, validation loss 0.05200
Epoch 19105: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19116: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9680, training loss 0.00952, validation loss 0.00498
Epoch 19127: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19138: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9690, training loss 0.00621, validation loss 0.00448
Epoch 19149: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9700, training loss 0.00303, validation loss 0.00318
Epoch 19160: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9701
Mean train loss for ascent epoch 9702: -0.0009401754359714687
Mean eval for ascent epoch 9702: 0.0027387633454054594
Epoch 19171: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9710, training loss 0.14934, validation loss 0.04232
Epoch 19182: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19193: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9720, training loss 0.01634, validation loss 0.01945
Epoch 19204: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19215: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9730, training loss 0.00522, validation loss 0.00436
Epoch 19226: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19237: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9740
Doing Evaluation on the model now
This is Epoch 9740, training loss 0.00314, validation loss 0.00303
Mean train loss for ascent epoch 9741: -0.0004860171175096184
Mean eval for ascent epoch 9741: 0.002628555055707693
Epoch 19248: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9750, training loss 0.09374, validation loss 0.11520
Epoch 19259: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19270: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9760, training loss 0.00944, validation loss 0.00876
Epoch 19281: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19292: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9770, training loss 0.00539, validation loss 0.00592
Epoch 19303: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19314: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9779
Mean train loss for ascent epoch 9780: -0.0006951958639547229
Mean eval for ascent epoch 9780: 0.0030989395454525948
Doing Evaluation on the model now
This is Epoch 9780, training loss 0.00310, validation loss 0.00291
Epoch 19325: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19336: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9790, training loss 0.05137, validation loss 0.02514
Epoch 19347: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9800, training loss 0.01446, validation loss 0.01123
Resetting learning rate to 0.01000
Epoch 19358: reducing learning rate of group 0 to 5.0000e-04.
Epoch 19369: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9810, training loss 0.00352, validation loss 0.00379
Epoch 19380: reducing learning rate of group 0 to 1.2500e-04.
Epoch 19391: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9818
Mean train loss for ascent epoch 9819: -0.0005579289281740785
Mean eval for ascent epoch 9819: 0.002915621967986226
Doing Evaluation on the model now
This is Epoch 9820, training loss 0.10791, validation loss 0.06291
Epoch 19402: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19413: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9830, training loss 0.02376, validation loss 0.02717
Epoch 19424: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19435: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9840, training loss 0.01443, validation loss 0.00538
Epoch 19446: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9850, training loss 0.00342, validation loss 0.00337
Epoch 19457: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19468: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9857
Mean train loss for ascent epoch 9858: -0.0010981953237205744
Mean eval for ascent epoch 9858: 0.002627120353281498
Doing Evaluation on the model now
This is Epoch 9860, training loss 1.08957, validation loss 1.03684
Epoch 19479: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19490: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9870, training loss 0.03580, validation loss 0.00730
Epoch 19501: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19512: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9880, training loss 0.00825, validation loss 0.00612
Epoch 19523: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19534: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9890, training loss 0.00343, validation loss 0.00386
Epoch 19545: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9896
Mean train loss for ascent epoch 9897: -0.0007619983516633511
Mean eval for ascent epoch 9897: 0.0031275106593966484
Doing Evaluation on the model now
This is Epoch 9900, training loss 0.25240, validation loss 0.23557
Epoch 19556: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19567: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9910, training loss 0.02466, validation loss 0.03466
Epoch 19578: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19589: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9920, training loss 0.00891, validation loss 0.00869
Epoch 19600: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19611: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9930, training loss 0.00311, validation loss 0.00279
Epoch 19622: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9935
Mean train loss for ascent epoch 9936: -0.0008103334112092853
Mean eval for ascent epoch 9936: 0.0028558725025504827
Doing Evaluation on the model now
This is Epoch 9940, training loss 0.40096, validation loss 0.04412
Epoch 19633: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19644: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9950, training loss 0.01153, validation loss 0.01556
Epoch 19655: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19666: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9960, training loss 0.00899, validation loss 0.01186
Epoch 19677: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19688: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9970, training loss 0.00313, validation loss 0.00277
Epoch 19699: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9974
Mean train loss for ascent epoch 9975: -0.0010662040440365672
Mean eval for ascent epoch 9975: 0.0029243158642202616
Epoch 19710: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9980, training loss 0.54847, validation loss 0.31516
Epoch 19721: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9990, training loss 0.11144, validation loss 0.06432
Epoch 19732: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19743: reducing learning rate of group 0 to 6.2500e-04.
POS: 
[8.903437, 8.825569, 8.83453, 8.830828, 8.832532, 8.830516, 8.834835, 8.835177, 8.821765, 8.840443, 8.828397, 8.832459, 8.835454, 8.828667, 8.8396845, 8.827702, 8.8357725, 8.833752, 8.837382, 8.838221, 8.833907, 8.826707, 8.832629, 8.83024, 8.836495, 8.832881, 8.833919, 8.8284855, 8.834149, 8.836454, 8.83469, 8.832457, 8.833299, 8.833987, 8.82516, 8.838718, 8.837323, 8.826137, 8.837283, 8.833915, 8.830977, 8.82764, 8.835255, 8.836912, 8.8354645, 8.828418, 8.817263, 8.4503, 3.5628169, 1.5407443, 1.0737222, 0.91895634, 0.895818, 0.7452596, 0.5486042, 0.4548926, 0.39101586, 0.3560705, 0.33788562, 0.24164797, 0.27381083, 0.24914445, 0.27285457, 0.2740758, 0.23305747, 2.5860984, 1.3384277, 1.2389691, 1.0954559, 1.0211099, 0.65707844, 0.6251502, 0.32660484, 0.70448637, 0.9313067, 0.59806144, 0.2923662, 0.17623572, 0.20095111, 0.27286008, 0.23335414, 0.20690669, 0.24174598, 0.21685284, 0.19588475, 0.13199537, 0.14816879, 0.11376348, 0.13549753, 0.12683089, 0.14105162, 0.17681126, 0.118662454, 0.15681845, 0.1352695, 0.12275972, 0.10127928, 0.13043742, 0.1365394, 0.14602514, 0.14738858, 0.1262458, 0.11106398, 0.80877113, 0.5983904, 1.0413979, 0.98659146, 1.0098617, 0.32783085, 0.4293352, 0.37925354, 0.2700676, 0.36190578, 0.36231175, 0.3190826, 0.26824078, 0.22226194, 0.2828303, 0.19205947, 0.31489635, 0.21546353, 0.24170253, 0.14929357, 0.1530585, 0.15459636, 0.1190981, 0.11701083, 0.16782533, 0.17726164, 0.1794957, 0.21782131, 0.14895876, 0.12440324, 0.13445717, 0.12785718, 0.117266595, 0.13718317, 0.09702043, 0.1178662, 0.12402223, 0.11424169, 0.5131114, 0.6743612, 0.6924693, 0.6325801, 0.4527968, 0.2869716, 0.47770068, 0.37218523, 0.25330505, 0.19056438, 0.16879284, 0.21740979, 0.21605837, 0.34849185, 0.20112129, 0.24379082, 0.5162212, 0.267158, 0.17502953, 0.24755603, 0.18897472, 0.15693572, 0.13010895, 0.11475336, 0.17074817, 0.14016035, 0.10622106, 0.12002172, 0.12944636, 0.13638349, 0.14972946, 0.14178395, 0.13028388, 0.14510386, 0.14751126, 0.17556393, 0.11781623, 0.1333976, 0.36227676, 0.22278947, 0.35113057, 1.1160859, 1.0047303, 0.42118445, 0.2653833, 0.648682, 0.39607882, 0.37992367, 0.3113072, 0.15685482, 0.2182191, 0.23803894, 0.20472515, 0.21555251, 0.13619421, 0.10509738, 0.16906618, 0.26762214, 0.1756948, 0.22519709, 0.112353064, 0.1657144, 0.14857544, 0.12306641, 0.10688641, 0.13076802, 0.14388667, 0.099217765, 0.12364521, 0.11791101, 0.095724516, 0.1110078, 0.12748067, 0.15628533, 0.14227954, 0.10262501, 0.4017901, 0.4240795, 0.6229051, 0.6604116, 0.5883279, 0.6104441, 0.22081713, 0.29899812, 0.44682774, 0.36850402, 0.2927765, 0.3231528, 0.5140071, 0.4709962, 0.16905557, 0.3265102, 0.13081041, 0.16290168, 0.20672418, 0.21000205, 0.21392599, 0.23120019, 0.16069205, 0.1048792, 0.16606039, 0.17250827, 0.16388185, 0.12390234, 0.16700485, 0.11209607, 0.10248053, 0.101796694, 0.08289519, 0.07992446, 0.11739127, 0.11647874, 0.11500158, 0.116223484, 0.3141278, 1.0130317, 0.62913847, 0.46720922, 0.66332865, 0.21135092, 0.17569284, 0.18298692, 0.41564524, 0.6068782, 0.47430906, 0.2211449, 0.21534319, 0.120625205, 0.2686348, 0.35937527, 0.17011674, 0.24490973, 0.12238334, 0.1395425, 0.14679372, 0.17275009, 0.15796982, 0.077826366, 0.1336016, 0.105182864, 0.15818927, 0.13041545, 0.13663143, 0.09804025, 0.10665026, 0.08461064, 0.09447631, 0.1139563, 0.086910345, 0.10163093, 0.09704675, 0.124785125, 0.43114752, 0.6180143, 0.48192698, 0.499436, 1.023761, 0.40854907, 0.21627812, 0.20514882, 0.41734943, 0.3022627, 0.2428447, 0.16181819, 0.12258405, 0.15979195, 0.19448672, 0.13887377, 0.10127425, 0.20399757, 0.14834605, 0.12697054, 0.13510971, 0.08040776, 0.12579747, 0.10020366, 0.09345449, 0.09062192, 0.08882018, 0.11089398, 0.107667774, 0.101405896, 0.19150133, 0.084399566, 0.13490506, 0.10007112, 0.09819665, 0.108246215, 0.08087669, 0.11982121, 0.44045222, 0.39354098, 0.61818814, 0.31159425, 0.6232954, 0.31127468, 0.33165765, 0.22668248, 0.4400348, 0.44278914, 0.20338964, 0.24958651, 0.23472518, 0.18420212, 0.13226546, 0.16532375, 0.10936966, 0.108934864, 0.15161331, 0.1275432, 0.108968794, 0.09426517, 0.13978505, 0.10701531, 0.11023085, 0.11728094, 0.10112787, 0.12306434, 0.1242445, 0.12660736, 0.09631726, 0.15146847, 0.09608603, 0.10691156, 0.11131021, 0.1189664, 0.10558959, 0.054132964, 0.90735584, 0.7800397, 0.3175832, 0.37081635, 0.61646503, 0.25802782, 0.31431046, 0.23704612, 0.4641075, 0.28910142, 0.13998283, 0.20477353, 0.32709417, 0.2358697, 0.1838563, 0.27920237, 0.13038693, 0.111681886, 0.1393043, 0.18448476, 0.18661828, 0.13708945, 0.118600495, 0.104407944, 0.13454878, 0.18351042, 0.2517137, 0.16977075, 0.08596003, 0.084070615, 0.16747817, 0.08612852, 0.09420339, 0.12871708, 0.08561336, 0.08604231, 0.14314918, 0.14716788, 0.11300338, 0.10121437, 0.07431001, 0.07237497, 0.10793297, 0.15691581, 0.38726312, 0.69667083, 0.685787, 0.449337, 0.4556163, 0.47752827, 0.2114131, 0.26285, 0.24950325, 0.21739608, 0.23580526, 0.2541695, 0.24710046, 0.16608322, 0.23378076, 0.14903395, 0.20255671, 0.104930185, 0.12289014, 0.15908666, 0.14599937, 0.09084832, 0.15539883, 0.12260518, 0.106810935, 0.15209585, 0.110125355, 0.120819055, 0.124780156, 0.068913765, 0.10582457, 0.093470864, 0.0883886, 0.08067751, 0.08358055, 0.07495766, 0.056943387, 0.11060185, 0.31870458, 0.9501118, 0.36935675, 0.29557315, 0.30461255, 0.47852686, 0.23337162, 0.43611392, 0.4817537, 0.25473142, 0.15524709, 0.10343888, 0.13389449, 0.13945495, 0.13655083, 0.14884141, 0.11819218, 0.13722005, 0.16976291, 0.1007412, 0.14038968, 0.12934424, 0.14473078, 0.13193274, 0.14153728, 0.07058206, 0.07545803, 0.12985596, 0.08130791, 0.14238414, 0.124465115, 0.08667352, 0.08241558, 0.1010674, 0.09088804, 0.10074269, 0.08802822, 0.14071101, 0.46251377, 0.6341071, 0.43259642, 0.40488634, 0.3798512, 0.21437153, 0.47426262, 0.24486615, 0.3398651, 0.41737264, 0.396521, 0.12377862, 0.13079064, 0.17193268, 0.19001617, 0.08570486, 0.08743452, 0.14577593, 0.10882834, 0.13719209, 0.24001566, 0.12820613, 0.13282458, 0.10797376, 0.10592677, 0.114588186, 0.097512744, 0.13577633, 0.10435923, 0.090357594, 0.09412512, 0.08422748, 0.09513384, 0.08484082, 0.11488434, 0.07488408, 0.12842062, 0.098613195, 0.61702704, 0.45390058, 0.37565473, 0.43430203, 0.43314186, 0.13651049, 0.25574917, 0.24914972, 0.3871014, 0.41316462, 0.16601944, 0.13389294, 0.11401429, 0.20228504, 0.12448498, 0.12411011, 0.089158796, 0.115876175, 0.10780298, 0.14265586, 0.1737313, 0.13513558, 0.0711982, 0.10896703, 0.15222271, 0.11022402, 0.103391945, 0.1459823, 0.1233069, 0.11084718, 0.09310212, 0.11630267, 0.07265784, 0.0986052, 0.09702674, 0.0790321, 0.08263037, 0.10325537, 0.36336026, 0.33193028, 0.62082183, 0.2627256, 0.24656336, 0.3287886, 0.30984932, 0.2786721, 0.20476599, 0.22738285, 0.22852057, 0.17836882, 0.18744907, 0.23437588, 0.17558709, 0.18900484, 0.14915264, 0.14039299, 0.11775035, 0.10566622, 0.095016815, 0.07299851, 0.10327032, 0.17258888, 0.10985565, 0.12626027, 0.13327245, 0.0911707, 0.12790117, 0.12955555, 0.09855435, 0.110613845, 0.09252357, 0.10833258, 0.08262002, 0.1149316, 0.06893786, 0.07719213, 0.095279805, 0.10691712, 0.07897594, 0.06498874, 0.08303814, 0.26657444, 0.48606998, 0.61951774, 0.37460542, 0.44246936, 0.29694107, 0.15953283, 0.40477315, 0.33299136, 0.11970768, 0.14675635, 0.13487892, 0.12352278, 0.16467601, 0.29559416, 0.18412022, 0.11000912, 0.1376002, 0.14632879, 0.12184823, 0.06272104, 0.13438438, 0.097659014, 0.1337916, 0.095257916, 0.06695285, 0.11534852, 0.08345147, 0.12046506, 0.075534105, 0.09065114, 0.0815194, 0.117571756, 0.08907689, 0.081950694, 0.09923214, 0.088222496, 0.106101766, 0.23551622, 0.24199885, 0.64541584, 0.3894464, 0.3959113, 0.36469007, 0.16618714, 0.2327053, 0.25374857, 0.19086213, 0.25814044, 0.17075792, 0.17985763, 0.117898665, 0.14094666, 0.2276453, 0.085775815, 0.09702669, 0.13990465, 0.1513483, 0.15834272, 0.1323594, 0.17082122, 0.1205344, 0.07978012, 0.09855413, 0.13077536, 0.08710577, 0.08319176, 0.109634615, 0.076345235, 0.10882064, 0.107456945, 0.12212442, 0.07717606, 0.061664186, 0.08981474, 0.09846795, 0.23373432, 0.28219974, 0.4142369, 0.43283454, 0.4972998, 0.20055142, 0.25560045, 0.40002507, 0.23771878, 0.22948599, 0.1582924, 0.15015613, 0.18421014, 0.17893262, 0.15070264, 0.15755704, 0.2545685, 0.18779634, 0.18049307, 0.076551825, 0.09249097, 0.086979054, 0.13359101, 0.12164782, 0.08126006, 0.116336934, 0.11717198, 0.093902834, 0.077261336, 0.07905481, 0.07333962, 0.06802985, 0.1294692, 0.07935729, 0.06243456, 0.10466468, 0.15724048, 0.07515465, 0.27032864, 0.5539831, 0.43988058, 0.24852939, 0.3222467, 0.1804037, 0.23642544, 0.14869395, 0.166747, 0.17740746, 0.14671052, 0.18920718, 0.13115698, 0.10203006, 0.1230243, 0.22242366, 0.14375901, 0.09799844, 0.108450904, 0.12208524, 0.10568294, 0.11718478, 0.09527517, 0.107375845, 0.14172947, 0.09928056, 0.12021198, 0.121235035, 0.06404854, 0.0707787, 0.11007047, 0.091146275, 0.09111672, 0.10469965, 0.079031594, 0.0912452, 0.08761894, 0.09346351, 0.3868451, 0.49410468, 0.21014073, 0.28680405, 0.59174997, 0.5233088, 0.12873185, 0.16721952, 0.37313154, 0.25411278, 0.21849622, 0.15428318, 0.19292916, 0.11131019, 0.14979827, 0.14808872, 0.09697976, 0.14352873, 0.1958543, 0.1567651, 0.11956327, 0.11379027, 0.09399484, 0.124220625, 0.12767714, 0.11244647, 0.078700885, 0.06464332, 0.07614744, 0.107745826, 0.10107035, 0.121041514, 0.076585434, 0.08650897, 0.0983011, 0.113869414, 0.07370146, 0.09318883, 0.075399555, 0.06805669, 0.080586, 0.06421569, 0.07620173, 0.06934506, 0.34471515, 0.31916636, 0.34710577, 0.3440824, 0.2138577, 0.11383724, 0.2844294, 0.21668757, 0.12612557, 0.19641414, 0.108617805, 0.10936136, 0.17511435, 0.16644168, 0.11013804, 0.082761765, 0.110058926, 0.14501327, 0.13587655, 0.10167381, 0.10541934, 0.09791572, 0.109334856, 0.078011505, 0.07396412, 0.08186261, 0.08320307, 0.07315341, 0.08009267, 0.073810585, 0.069383465, 0.080851205, 0.08258325, 0.055573236, 0.06728424, 0.09654341, 0.07369581, 0.07664988, 0.40396625, 0.3584465, 0.5652167, 0.25832844, 0.25258043, 0.14891933, 0.25200415, 0.23865825, 0.14643793, 0.24192178, 0.16879831, 0.16948517, 0.14040823, 0.16376655, 0.09143919, 0.11896526, 0.09645708, 0.09722172, 0.1010323, 0.14323665, 0.111348756, 0.069462925, 0.072683156, 0.073821634, 0.057943035, 0.088300616, 0.07202282, 0.13167094, 0.08268362, 0.07746924, 0.06548355, 0.10409165, 0.06978094, 0.068642996, 0.07481748, 0.06619941, 0.08262303, 0.07437463, 0.2704011, 0.9372153, 0.4069639, 0.36937684, 0.19708551, 0.20847562, 0.3482872, 0.19042017, 0.2128135, 0.18019068, 0.12896608, 0.08505188, 0.0762634, 0.08041199, 0.12381643, 0.07776955, 0.0579933, 0.11269905, 0.06609026, 0.12310543, 0.09041475, 0.072969645, 0.14841814, 0.083341226, 0.1244645, 0.10696523, 0.119280905, 0.08857784, 0.058832694, 0.055287782, 0.07337126, 0.071689956, 0.094294764, 0.083640315, 0.0637264, 0.065589555, 0.07350598, 0.070728146, 0.13375194, 0.17763658, 0.4540167, 0.21571839, 0.26340237, 0.16989344, 0.21451172, 0.18529306, 0.12311312, 0.09843121, 0.08171135, 0.12477662, 0.10306316, 0.15213397, 0.121938616, 0.07678244, 0.07950952, 0.12089458, 0.17021729, 0.15670939, 0.13206632, 0.11099571, 0.092323184, 0.075449325, 0.0723304, 0.08405681, 0.084461324, 0.08759072, 0.0996657, 0.07035708, 0.065160096, 0.08118692, 0.07596245, 0.066621326, 0.08350254, 0.07949157, 0.07160538, 0.056762394, 0.31637153, 0.3418677, 0.3561306, 0.27648947, 0.17001767, 0.13668542, 0.18758993, 0.35600594, 0.16049282, 0.26032123, 0.1600787, 0.16661145, 0.18626463, 0.15784217, 0.12241642, 0.08233982, 0.11492498, 0.10824708, 0.1247289, 0.08890875, 0.09640704, 0.11041586, 0.06877826, 0.1171429, 0.0984839, 0.07745955, 0.08887361, 0.08009373, 0.06586526, 0.07045355, 0.096674696, 0.06573229, 0.0629334, 0.05060453, 0.06509668, 0.061771248, 0.10078463, 0.06793971, 0.35687706, 0.4115702, 0.3527365, 0.41423464, 0.3365284, 0.19910078, 0.14117973, 0.1606394, 0.42169982, 0.24883205, 0.12064545, 0.09712443, 0.12817726, 0.14346185, 0.13376607, 0.09040626, 0.12970515, 0.10472004, 0.117225535, 0.102386184, 0.06674844, 0.10488941, 0.16611524, 0.10959827, 0.08317691, 0.09043741, 0.097920634, 0.09599884, 0.055728134, 0.084224366, 0.08532423, 0.07511885, 0.09425816, 0.068631716, 0.09831348, 0.0970054, 0.10027788, 0.05671868, 0.090528466, 0.45956382, 0.25460675, 0.29425114, 0.11565995, 0.26397395, 0.18356517, 0.27118483, 0.11879585, 0.12946115, 0.22835584, 0.113942444, 0.13051958, 0.13972548, 0.109104946, 0.11094, 0.09701051, 0.07889649, 0.11987741, 0.09518512, 0.12718655, 0.11433884, 0.06315056, 0.07212076, 0.062270917, 0.121210314, 0.076917574, 0.054989092, 0.09470079, 0.074123, 0.07896947, 0.09635832, 0.08025181, 0.08018308, 0.062878795, 0.059220627, 0.0487344, 0.05338025, 0.2173492, 0.26620594, 0.29573503, 0.30728745, 0.21369201, 0.30996433, 0.32489982, 0.31451312, 0.25834957, 0.13889535, 0.1733011, 0.14951965, 0.11977786, 0.09041293, 0.13953213, 0.0951945, 0.112293534, 0.06705634, 0.104156606, 0.09294203, 0.12500091, 0.09296437, 0.07384579, 0.058715004, 0.07056076, 0.077586554, 0.0676577, 0.072497584, 0.04665522, 0.0820374, 0.06762, 0.07046325, 0.07090092, 0.08995339, 0.050038405, 0.033620127, 0.07295456, 0.074295916, 0.11669627, 0.15267599, 0.18335912, 0.15429488, 0.1758048, 0.13692974, 0.22214513, 0.23252763, 0.17917114, 0.19338164, 0.06749016, 0.11145401, 0.09209119, 0.09391198, 0.18937445, 0.100200295, 0.093049176, 0.115097106, 0.06385558, 0.06811931, 0.07476443, 0.056673802, 0.046043403, 0.071098685, 0.06706039, 0.057979528, 0.0492127, 0.065776974, 0.050776806, 0.07505852, 0.06141711, 0.061891492, 0.053128913, 0.05097858, 0.07030905, 0.0652447, 0.060341176, 0.05341938, 0.25948676, 0.27904165, 0.41872752, 0.39794928, 0.17684537, 0.20883244, 0.18813758, 0.15778539, 0.14132328, 0.23331554, 0.24958548, 0.2822452, 0.15635906, 0.083660245, 0.08151847, 0.1254457, 0.123756625, 0.083199084, 0.08302758, 0.08563187, 0.10705209, 0.12038968, 0.07126181, 0.09770672, 0.07151704, 0.0803447, 0.07635348, 0.06763719, 0.069533035, 0.06199117, 0.08087227, 0.06757969, 0.0784026, 0.046581987, 0.07830589, 0.07169524, 0.06667619, 0.055962197, 0.066219464, 0.05735654, 0.070038736, 0.088647045, 0.052752286, 0.18783335, 0.19823608, 0.2969252, 0.22636384, 0.3641636, 0.23041517, 0.16202994, 0.16559081, 0.1390803, 0.22538115, 0.09933253, 0.08266087, 0.1292284, 0.0833541, 0.08040587, 0.17766, 0.10903195, 0.08777981, 0.080407575, 0.0851741, 0.07209157, 0.09511168, 0.06972231, 0.089495815, 0.09170981, 0.062201336, 0.056070603, 0.051241454, 0.061606895, 0.066856556, 0.087428406, 0.08658617, 0.079122245, 0.06030543, 0.04432419, 0.06443671, 0.071045645, 0.056025714, 0.3121632, 0.28622806, 0.21495676, 0.2770323, 0.17579716, 0.3891149, 0.13637337, 0.18383457, 0.15931147, 0.17030269, 0.163429, 0.0951517, 0.117271334, 0.100721896, 0.09848156, 0.08189287, 0.05949452, 0.060074616, 0.10119775, 0.10002941, 0.09181395, 0.08054759, 0.06632168, 0.06323464, 0.076049276, 0.07675211, 0.08702466, 0.05490665, 0.06287439, 0.053689968, 0.041602403, 0.057930462, 0.060973994, 0.06249059, 0.056737147, 0.07149675, 0.054422185, 0.06420688, 0.15151438, 0.18101446, 0.15120962, 0.20943898, 0.19381748, 0.08207024, 0.21614532, 0.10087357, 0.122518174, 0.1609398, 0.11310682, 0.07757185, 0.14495592, 0.14202385, 0.14503407, 0.108888485, 0.10068215, 0.0833664, 0.059547763, 0.09606777, 0.08717352, 0.11814816, 0.06452706, 0.09680279, 0.08346198, 0.06736435, 0.08771074, 0.066475086, 0.045876354, 0.048799604, 0.06410561, 0.06693117, 0.073511265, 0.038776655, 0.04591714, 0.06292368, 0.05904782, 0.05578964, 0.2817793, 0.30825147, 0.43767607, 0.27313632, 0.33620763, 0.14051203, 0.098380454, 0.2248702, 0.17969266, 0.251721, 0.106949255, 0.1056288, 0.14399652, 0.11833398, 0.18494257, 0.1140136, 0.13317834, 0.08546375, 0.06334275, 0.067457624, 0.07158555, 0.073962815, 0.07781284, 0.084051766, 0.061274264, 0.07796932, 0.06463792, 0.052535564, 0.06372956, 0.07692102, 0.06516796, 0.04014403, 0.060677912, 0.06500925, 0.09111247, 0.05569566, 0.07275281, 0.047044843, 0.16134542, 0.25522602, 0.5826796, 0.27862453, 0.3292816, 0.113766894, 0.2707667, 0.1290914, 0.09696392, 0.21706021, 0.08658892, 0.10718285, 0.12832873, 0.07575509, 0.09769573, 0.15522376, 0.067139365, 0.09123706, 0.09223431, 0.101248935, 0.064387195, 0.05816987, 0.06616414, 0.086608015, 0.101595044, 0.077777244, 0.07491292, 0.052616127, 0.051951207, 0.059745826, 0.063431196, 0.05227919, 0.0559859, 0.057895172, 0.03632958, 0.073576726, 0.06539509, 0.07562853, 0.053617388, 0.035160687, 0.05724397, 0.06568391, 0.054282196, 0.047047306, 0.2738295, 0.22642507, 0.27804372, 0.16224691, 0.21914716, 0.20862764, 0.1769563, 0.32065955, 0.15919657, 0.117545515, 0.10444511, 0.14173773, 0.15119673, 0.124523416, 0.07975689, 0.06042675, 0.12243711, 0.0763492, 0.08520893, 0.055285722, 0.09752704, 0.090898335, 0.04346062, 0.099369, 0.09688822, 0.07324264, 0.065542884, 0.049195927, 0.05985174, 0.0693542, 0.05626883, 0.059781782, 0.05741388, 0.0478171, 0.055225227, 0.056225307, 0.070589885, 0.056669205, 0.09601138, 0.17572595, 0.15567084, 0.14234707, 0.12153338, 0.1193802, 0.07907241, 0.15628955, 0.11446045, 0.17456427, 0.07820251, 0.21397723, 0.09311264, 0.1010043, 0.096151665, 0.08786143, 0.097321965, 0.06794538, 0.07374102, 0.060751934, 0.08287241, 0.08445303, 0.09075884, 0.07039835, 0.07673169, 0.050783783, 0.077299744, 0.049390867, 0.08018521, 0.086922035, 0.052106112, 0.050485708, 0.048839085, 0.059848696, 0.052467648, 0.035318088, 0.045717265, 0.09635587, 0.12050295, 0.17134935, 0.3659397, 0.16829, 0.1294329, 0.09793848, 0.13822798, 0.21322374, 0.21004778, 0.14360729, 0.09433423, 0.122338295, 0.14711396, 0.07106624, 0.06877919, 0.058162343, 0.052801203, 0.07870855, 0.07345131, 0.08670527, 0.050220665, 0.096165515, 0.09187441, 0.091230184, 0.05065217, 0.07407995, 0.058140665, 0.059076007, 0.060630992, 0.066463664, 0.07019294, 0.07315276, 0.068628035, 0.06676615, 0.06643833, 0.06026573, 0.055022698, 0.070189536, 0.15519011, 0.21364851, 0.18131106, 0.23306781, 0.21398214, 0.10529084, 0.108870305, 0.20542395, 0.088693134, 0.14389722, 0.1291131, 0.099418715, 0.0984265, 0.0839797, 0.066508904, 0.11281921, 0.08994194, 0.08899635, 0.06394098, 0.08026447, 0.053855043, 0.04961465, 0.07709692, 0.05170258, 0.08987435, 0.046047334, 0.06076535, 0.065089665, 0.040934626, 0.0636382, 0.052753806, 0.0611113, 0.057359427, 0.051056873, 0.051800862, 0.06717517, 0.050066058, 0.054685447, 0.16869755, 0.25061315, 0.2987742, 0.17628367, 0.25069785, 0.25974983, 0.095237486, 0.081902176, 0.1566462, 0.22989579, 0.1843247, 0.11470618, 0.12993364, 0.12776288, 0.08558092, 0.06435993, 0.08638835, 0.106366575, 0.07141636, 0.04880337, 0.073358715, 0.045968007, 0.05965462, 0.063736565, 0.06312296, 0.07744743, 0.0827227, 0.058907993, 0.05218151, 0.055228848, 0.057988476, 0.06914119, 0.093470044, 0.064075865, 0.07911565, 0.065891325, 0.039341677, 0.06395274, 0.042922374, 0.05021611, 0.04853709, 0.052022897, 0.05342902, 0.11558375, 0.21164946, 0.1417792, 0.26988038, 0.07973863, 0.08193704, 0.08891883, 0.13877356, 0.18688276, 0.1972451, 0.21346772, 0.10960338, 0.07485122, 0.19884145, 0.17440607, 0.06363467, 0.058010202, 0.07001039, 0.07568936, 0.07215768, 0.06464515, 0.04636384, 0.044939205, 0.061416816, 0.05386136, 0.072183706, 0.07547404, 0.06697243, 0.0528773, 0.03230354, 0.038254812, 0.036956523, 0.056526028, 0.06580774, 0.07237909, 0.049818825, 0.03827911, 0.07419401, 0.17056446, 0.4314781, 0.35722676, 0.25973982, 0.21888253, 0.09612898, 0.11212625, 0.12181673, 0.19283743, 0.094657786, 0.08740651, 0.104402505, 0.09418769, 0.11470989, 0.087243386, 0.12623349, 0.0634678, 0.07498076, 0.06222646, 0.10322099, 0.074370466, 0.08685494, 0.03870586, 0.041641016, 0.05864085, 0.057304595, 0.05569788, 0.07288786, 0.0493132, 0.050927486, 0.075906016, 0.08538779, 0.05756965, 0.054950688, 0.04823755, 0.064830445, 0.04902436, 0.040852115, 0.113166474, 0.11630711, 0.28810218, 0.25508597, 0.19973211, 0.124571376, 0.12463837, 0.11927034, 0.12942214, 0.11143107, 0.100844, 0.08064081, 0.115563594, 0.094375655, 0.07783718, 0.08005786, 0.13623309, 0.08070614, 0.06866161, 0.06554724, 0.07172659, 0.07860626, 0.04694896, 0.045794614, 0.047153067, 0.054440856, 0.06406606, 0.04171468, 0.046446275, 0.069633126, 0.05215463, 0.044926304, 0.058940127, 0.042390138, 0.044183742, 0.05478623, 0.05498967, 0.087627694, 0.10677408, 0.34128106, 0.44346547, 0.2839112, 0.24023706, 0.136034, 0.13198891, 0.10323791, 0.08405777, 0.224953, 0.17683199, 0.08934734, 0.09706143, 0.08682368, 0.08686532, 0.07680599, 0.07443867, 0.05934232, 0.10942186, 0.088951275, 0.11099459, 0.055421572, 0.050165225, 0.046830446, 0.07527807, 0.06398485, 0.0760496, 0.051460575, 0.058824293, 0.038652476, 0.055215634, 0.04711281, 0.04392212, 0.05217289, 0.04479068, 0.0387868, 0.056093078, 0.045627445, 0.14229733, 0.21194407, 0.1186111, 0.19489299, 0.25748602, 0.13085407, 0.12581073, 0.101769544, 0.10651794, 0.112275474, 0.13787729, 0.061770044, 0.072531976, 0.10123315, 0.10042318, 0.10474539, 0.08063406, 0.048819255, 0.07869193, 0.0871671, 0.06780172, 0.049346767, 0.049007803, 0.06366309, 0.05720843, 0.059438396, 0.067467116, 0.070101894, 0.05194659, 0.06871603, 0.05821561, 0.056461554, 0.0564732, 0.06405675, 0.045608185, 0.036800914, 0.047351554, 0.06406021, 0.038136017, 0.07530954, 0.050318606, 0.034638, 0.054757923, 0.04456323, 0.12348771, 0.09410326, 0.14161234, 0.1665039, 0.11579881, 0.12453611, 0.13597114, 0.17081894, 0.09840333, 0.093593, 0.06814945, 0.059086945, 0.12600708, 0.22696961, 0.10011077, 0.06964151, 0.08441663, 0.03455032, 0.05546726, 0.04649823, 0.05778063, 0.050141122, 0.085775234, 0.061274182, 0.06911692, 0.03515051, 0.061227523, 0.06701517, 0.04824181, 0.040538255, 0.052079864, 0.059053183, 0.07038203, 0.062314678, 0.04950205, 0.044280335, 0.05179466, 0.035535123, 0.113676384, 0.13055122, 0.1775937, 0.14384773, 0.14725687, 0.11429744, 0.1273916, 0.061299276, 0.14705624, 0.09543178, 0.15191033, 0.077687256, 0.08317359, 0.109335914, 0.065457635, 0.06694592, 0.04796306, 0.033066444, 0.045941707, 0.060989864, 0.06820361, 0.06089523, 0.05964753, 0.04256619, 0.037198372, 0.054379188, 0.04492027, 0.053186473, 0.046560615, 0.05107819, 0.055592105, 0.051750682, 0.04251027, 0.049693644, 0.05464523, 0.044706013, 0.053878985, 0.05205673, 0.27441022, 0.22153172, 0.1424853, 0.2550836, 0.13946676, 0.099571906, 0.14029333, 0.09708401, 0.14765689, 0.20003757, 0.094978824, 0.057026733, 0.043703597, 0.10654506, 0.099653974, 0.060970865, 0.06135938, 0.05985582, 0.070865214, 0.08664677, 0.04848509, 0.058455117, 0.057402655, 0.056722514, 0.052747946, 0.050814416, 0.04876882, 0.054011457, 0.032042038, 0.04903813, 0.058924105, 0.0476124, 0.06410956, 0.03325812, 0.0426378, 0.045154545, 0.054306023, 0.041868906, 0.15273567, 0.16216491, 0.19550073, 0.1647811, 0.26991364, 0.14012204, 0.06541755, 0.09796649, 0.10404412, 0.15987635, 0.07028517, 0.10602146, 0.14485767, 0.07161926, 0.07810333, 0.056733828, 0.04260455, 0.057866868, 0.04031586, 0.07509498, 0.061926134, 0.04689625, 0.035999704, 0.03728563, 0.05334248, 0.05780055, 0.058769528, 0.03443664, 0.06763167, 0.03551903, 0.039097674, 0.033542164, 0.053253863, 0.042797755, 0.04315637, 0.03733557, 0.049845155, 0.06343953, 0.12829316, 0.18741183, 0.33972073, 0.3249012, 0.13398461, 0.060309127, 0.07456935, 0.087629795, 0.08091363, 0.08718485, 0.15586492, 0.12791423, 0.055009853, 0.06921885, 0.0687255, 0.10348115, 0.07917882, 0.06139714, 0.05874943, 0.056129463, 0.04923622, 0.06494486, 0.06653578, 0.05414439, 0.052938644, 0.05598656, 0.057334103, 0.048513204, 0.040681463, 0.04116397, 0.064130224, 0.06042563, 0.053517498, 0.04642756, 0.05841498, 0.032391943, 0.031107005, 0.04695929, 0.041144304, 0.030472005, 0.045604784, 0.038338866, 0.036422417, 0.19517688, 0.3350277, 0.21225764, 0.33306912, 0.23682652, 0.114830844, 0.11133306, 0.2035557, 0.28383854, 0.24218324, 0.1239337, 0.11000646, 0.06012825, 0.06418239, 0.05142114, 0.04367593, 0.064173676, 0.05589814, 0.08176837, 0.05985261, 0.057402235, 0.05140151, 0.052039973, 0.059986636, 0.040701352, 0.051280443, 0.044101287, 0.05064826, 0.052506756, 0.046192717, 0.046322595, 0.043855432, 0.050044607, 0.05378405, 0.049694814, 0.05650179, 0.028652444, 0.036637712, 0.20192946, 0.17487, 0.1363327, 0.19936727, 0.16960461, 0.14645454, 0.15078071, 0.12418772, 0.08999537, 0.08049855, 0.10669345, 0.056224987, 0.05559248, 0.09170352, 0.09229176, 0.070246376, 0.057075225, 0.04783146, 0.07331065, 0.07327261, 0.070629336, 0.066142455, 0.047688548, 0.064360194, 0.044484664, 0.040041, 0.0470444, 0.03568497, 0.042010814, 0.04350397, 0.04735091, 0.052561775, 0.04947282, 0.03880662, 0.031237392, 0.04365481, 0.050343536, 0.055541176, 0.12520158, 0.22265622, 0.14192773, 0.06765284, 0.15322472, 0.091092214, 0.090056844, 0.1391969, 0.13670826, 0.080270864, 0.07851609, 0.100690186, 0.053715102, 0.071537085, 0.06688439, 0.09042081, 0.05510123, 0.0614031, 0.07862232, 0.047089532, 0.06277074, 0.046655796, 0.06031483, 0.041975666, 0.048625417, 0.034904003, 0.03949275, 0.054805215, 0.02939106, 0.03674526, 0.045849897, 0.05191393, 0.05805102, 0.035372723, 0.03173914, 0.047220107, 0.048456322, 0.024589524, 0.11996007, 0.09251033, 0.11486175, 0.21686606, 0.18628809, 0.06986091, 0.1058573, 0.07375854, 0.13625522, 0.16118304, 0.05977, 0.06811601, 0.11021082, 0.062285136, 0.06862558, 0.046229705, 0.042577617, 0.06922231, 0.037695874, 0.09014953, 0.07145053, 0.04864212, 0.04705683, 0.062950075, 0.0453346, 0.06975143, 0.048823066, 0.02895341, 0.060494963, 0.03782089, 0.03859464, 0.033448175, 0.034784064, 0.024972469, 0.03402492, 0.038834613, 0.046609133, 0.034590352, 0.1900999, 0.18318608, 0.16309598, 0.15480453, 0.24665213, 0.09867318, 0.12962821, 0.11065259, 0.12808047, 0.09123984, 0.12650636, 0.071916774, 0.077498, 0.035486132, 0.05277909, 0.07789943, 0.071438245, 0.04382899, 0.047764607, 0.05282645, 0.09472841, 0.07543041, 0.04811129, 0.040505458, 0.04695165, 0.07983074, 0.0726723, 0.048368424, 0.04730342, 0.047717314, 0.04015173, 0.047901873, 0.026695468, 0.05322849, 0.036037732, 0.04287453, 0.064436615, 0.041311096, 0.035874348, 0.038789425, 0.027446821, 0.035843484, 0.030087521, 0.03222234, 0.18467636, 0.25406823, 0.1518168, 0.10604563, 0.12029477, 0.18273212, 0.12881696, 0.09486939, 0.064633824, 0.06717563, 0.065116264, 0.08328102, 0.07287723, 0.06189671, 0.0714543, 0.05145238, 0.048417497, 0.071834825, 0.056686126, 0.040035415, 0.043272488, 0.058480214, 0.0450664, 0.033247747, 0.055347275, 0.047609616, 0.04575355, 0.034542214, 0.039381556, 0.023707878, 0.05090123, 0.04886755, 0.037555717, 0.028541395, 0.031962182, 0.038204014, 0.039626863, 0.041344088, 0.15745883, 0.23879553, 0.23688972, 0.21875587, 0.11305346, 0.09305606, 0.09553111, 0.08411231, 0.11973002, 0.20938076, 0.13137484, 0.0566729, 0.041931536, 0.05833625, 0.07373421, 0.054172985, 0.037956648, 0.058166888, 0.058278725, 0.06236813, 0.056068704, 0.05495938, 0.06420528, 0.04571547, 0.039678134, 0.026316725, 0.029217418, 0.04262914, 0.034272753, 0.042301517, 0.051947206, 0.03839885, 0.043547284, 0.04093881, 0.037192684, 0.03705769, 0.031146586, 0.030464375, 0.11404982, 0.14197756, 0.21642615, 0.11310704, 0.09862492, 0.09393534, 0.07254317, 0.08604369, 0.06636322, 0.14583415, 0.08196731, 0.09787621, 0.06842249, 0.06713136, 0.040391095, 0.060194273, 0.045179565, 0.084061086, 0.04103867, 0.037462577, 0.043963637, 0.03610598, 0.04139364, 0.03431611, 0.03908149, 0.038498394, 0.05078072, 0.04545748, 0.038862526, 0.05882553, 0.07337278, 0.038985528, 0.037006408, 0.055811387, 0.029658739, 0.03169128, 0.025390964, 0.03529995, 0.10027605, 0.27255714, 0.26236102, 0.14524414, 0.10227988, 0.0935349, 0.07545823, 0.21216561, 0.113466404, 0.054821197, 0.06957741, 0.109574504, 0.056162633, 0.0741189, 0.06742853, 0.0567061, 0.04223175, 0.062298104, 0.061932527, 0.046489723, 0.05993677, 0.036046002, 0.042966757, 0.042327326, 0.03643192, 0.041844174, 0.027875897, 0.04039635, 0.057252325, 0.054137457, 0.05256368, 0.042032287, 0.05062831, 0.032212142, 0.03555062, 0.03465998, 0.03952011, 0.03345051, 0.111456044, 0.21305333, 0.4155304, 0.47339922, 0.18063772, 0.17001076, 0.07825499, 0.09521035, 0.088216625, 0.07180681, 0.0768456, 0.07200779, 0.104061924, 0.056455124, 0.041923974, 0.039969794, 0.06693657, 0.07375697, 0.0525231, 0.053302363, 0.06206146, 0.066315316, 0.057561588, 0.059610374, 0.058444675, 0.05925901, 0.04527228, 0.049729362, 0.040286396, 0.029552404, 0.050236467, 0.04792358, 0.049880475, 0.037870217, 0.050030977, 0.04163425, 0.03483342, 0.034542315, 0.040103715, 0.027663441, 0.02871221, 0.021282624, 0.043253273, 0.113720804, 0.09318008, 0.16229711, 0.27239358, 0.1549927, 0.11838322, 0.18668818, 0.11506484, 0.0892982, 0.09012069, 0.05905962, 0.06418514, 0.05128804, 0.043530308, 0.10950632, 0.11135385, 0.09181886, 0.057313807, 0.032249637, 0.062197823, 0.05439964, 0.048318036, 0.051209893, 0.052421205, 0.047954988, 0.044683557, 0.041479953, 0.0446205, 0.041079056, 0.05126597, 0.042204253, 0.04042491, 0.04652014, 0.037149668, 0.042339556, 0.048863076, 0.040756173, 0.043222286, 0.12709592, 0.17164654, 0.13428979, 0.10846808, 0.14492458, 0.14878906, 0.09174623, 0.11820935, 0.10789462, 0.05603535, 0.069535114, 0.09338921, 0.07622066, 0.06250892, 0.075001955, 0.08759382, 0.048170216, 0.03250382, 0.05439871, 0.06743787, 0.04491781, 0.046000585, 0.039785214, 0.06798074, 0.04320519, 0.03257965, 0.044890795, 0.038117338, 0.04387515, 0.028427923, 0.028026178, 0.043782253, 0.03994472, 0.028609216, 0.029347474, 0.043509264, 0.04495795, 0.035876147, 0.16867952, 0.43601212, 0.16445829, 0.14201066, 0.12544152, 0.08937721, 0.07921329, 0.088085406, 0.09166216, 0.042543318, 0.08053179, 0.059826147, 0.08206173, 0.061845303, 0.03376168, 0.033188004, 0.05291391, 0.046274442, 0.06761962, 0.062116932, 0.040343113, 0.038152684, 0.04744568, 0.056860875, 0.03469541, 0.04948264, 0.050435454, 0.026378082, 0.042478748, 0.036657322, 0.047261298, 0.044531684, 0.022906478, 0.02569706, 0.040261522, 0.036679003, 0.054827664, 0.029172897, 0.10829209, 0.11498253, 0.21264508, 0.46393776, 0.3543468, 0.14043218, 0.09072187, 0.07685077, 0.1018708, 0.080112495, 0.073759906, 0.058961637, 0.03494706, 0.040830053, 0.07794035, 0.03938581, 0.0636356, 0.06262023, 0.04447719, 0.0619147, 0.037667934, 0.03527165, 0.044717327, 0.044928696, 0.043045867, 0.040344384, 0.05640976, 0.049742263, 0.037822437, 0.038474716, 0.023392152, 0.036671534, 0.041628074, 0.03573466, 0.03316544, 0.020417104, 0.026386596, 0.053823184, 0.07141412, 0.11132224, 0.120718755, 0.16083664, 0.21213233, 0.14593855, 0.095318705, 0.075423785, 0.13246524, 0.097170174, 0.07230998, 0.05252928, 0.061746195, 0.07613612, 0.048100922, 0.070145994, 0.049088787, 0.039720293, 0.04974237, 0.06597321, 0.05084126, 0.04937459, 0.063225605, 0.04291834, 0.053502, 0.045562383, 0.037169788, 0.022725465, 0.042422365, 0.02703283, 0.030638704, 0.042426318, 0.048441086, 0.03512363, 0.032678, 0.028892549, 0.036336936, 0.05671616, 0.033095095, 0.031062942, 0.049646422, 0.036152255, 0.029255286, 0.030467832, 0.19547732, 0.22783422, 0.21457376, 0.29058334, 0.13544111, 0.08795566, 0.117114045, 0.056962904, 0.076236635, 0.06379354, 0.11786466, 0.07781273, 0.08795337, 0.0858888, 0.056577235, 0.03290616, 0.03598415, 0.06367085, 0.0380393, 0.0492316, 0.055967137, 0.04328978, 0.04503294, 0.05844824, 0.04558593, 0.03162077, 0.040425926, 0.03443862, 0.041531302, 0.033857867, 0.03217903, 0.036966953, 0.03363048, 0.02681017, 0.034926556, 0.030406443, 0.055977933, 0.03682621, 0.08750147, 0.09613865, 0.06629386, 0.10353813, 0.057455577, 0.101716034, 0.074699394, 0.091628395, 0.10261259, 0.10762152, 0.06629013, 0.05549251, 0.061633836, 0.07825861, 0.08965745, 0.071836464, 0.029998628, 0.057326905, 0.0668955, 0.041364253, 0.07064024, 0.053140044, 0.032044526, 0.045572247, 0.048998576, 0.038567357, 0.042731714, 0.04382822, 0.039959826, 0.0326598, 0.02900623, 0.034473766, 0.027370691, 0.031498123, 0.05904573, 0.032636426, 0.030064398, 0.039030723, 0.11138537, 0.20768563, 0.18854153, 0.13866702, 0.116536155, 0.16159818, 0.13936031, 0.11585853, 0.08130945, 0.068687975, 0.08466148, 0.0853279, 0.12381095, 0.097163774, 0.1188808, 0.03246182, 0.042140707, 0.0383562, 0.04227102, 0.03853447, 0.04322673, 0.026067775, 0.051873527, 0.031549025, 0.03770367, 0.04116114, 0.046327077, 0.03790753, 0.028259773, 0.029049832, 0.031143991, 0.035727676, 0.030989205, 0.034647472, 0.03877709, 0.03199028, 0.044404022, 0.029990532, 0.052918553, 0.25875607, 0.12289535, 0.14928766, 0.10763363, 0.04641651, 0.12819736, 0.14164372, 0.091723934, 0.054105245, 0.09299773, 0.038265727, 0.05909605, 0.0381807, 0.03852918, 0.03020748, 0.03678188, 0.047245573, 0.04968587, 0.054827735, 0.04588782, 0.030720623, 0.035598088, 0.051452592, 0.047673445, 0.032485057, 0.04317096, 0.037184384, 0.033567138, 0.036741026, 0.028741306, 0.041657727, 0.02974017, 0.024829017, 0.03150828, 0.028852679, 0.03184853, 0.03614073, 0.11013187, 0.30359787, 0.30451703, 0.10635275, 0.117497705, 0.08979786, 0.05428823, 0.06995088, 0.13211808, 0.118733585, 0.15615426, 0.05449362, 0.06393448, 0.11041111, 0.048246324, 0.046513967, 0.047837142, 0.044187017, 0.055782024, 0.049547445, 0.047451712, 0.06688997, 0.031039242, 0.054868925, 0.053736478, 0.08280796, 0.069530085, 0.04719176, 0.045405723, 0.033049855, 0.026301198, 0.048191823, 0.040210556, 0.032741178, 0.025946481, 0.02514502, 0.026140355, 0.03713945, 0.03700438, 0.036474038, 0.04014628, 0.041423887, 0.03378023, 0.091999546, 0.108575016, 0.22099859, 0.10475933, 0.2040854, 0.12799056, 0.07307249, 0.06621028, 0.08661664, 0.117515676, 0.057546943, 0.07871722, 0.05129002, 0.040347368, 0.054815818, 0.04603815, 0.03505561, 0.04641853, 0.048389282, 0.061885446, 0.054047357, 0.046107084, 0.02770832, 0.03497273, 0.030370662, 0.036733437, 0.033799578, 0.04837207, 0.023136115, 0.034647375, 0.035324678, 0.03234476, 0.03580935, 0.034549993, 0.036795612, 0.027250513, 0.03500486, 0.034050457, 0.08862708, 0.23448503, 0.15404849, 0.11139276, 0.077965, 0.04101659, 0.08973422, 0.08261332, 0.0784098, 0.060434967, 0.040436957, 0.03642025, 0.05943013, 0.08308048, 0.08020917, 0.085938, 0.06305859, 0.042464916, 0.07974361, 0.059141617, 0.04448383, 0.033950534, 0.040279802, 0.03070133, 0.041589074, 0.03525151, 0.04679186, 0.03608039, 0.024969071, 0.03137918, 0.031384442, 0.031443074, 0.032663845, 0.0302655, 0.03570282, 0.049617838, 0.028508164, 0.028249258, 0.05712544, 0.118208006, 0.1400592, 0.31200892, 0.37566397, 0.15097255, 0.085380316, 0.11847056, 0.14717372, 0.075211175, 0.06746727, 0.08271643, 0.055872872, 0.066494085, 0.0604222, 0.065590456, 0.042501416, 0.037681475, 0.07534863, 0.050819147, 0.05396621, 0.031156553, 0.031637926, 0.040793665, 0.039829317, 0.03774925, 0.03305572, 0.027029052, 0.047105182, 0.042826504, 0.025462577, 0.044971358, 0.02942569, 0.029247591, 0.029338611, 0.027390517, 0.022556072, 0.049258303, 0.07341484, 0.25554854, 0.1601475, 0.11754084, 0.15980816, 0.13927017, 0.07398751, 0.070880495, 0.07011997, 0.06309219, 0.0689591, 0.05159763, 0.050499167, 0.07237577, 0.05846907, 0.06810334, 0.057623178, 0.038425993, 0.040669225, 0.05426098, 0.047339078, 0.031060407, 0.03097958, 0.036258787, 0.039042298, 0.0399602, 0.03484324, 0.0242554, 0.041003805, 0.030267512, 0.029715318, 0.03133564, 0.03034068, 0.030078242, 0.03302266, 0.050451193, 0.02934736, 0.023542607, 0.0830728, 0.14509507, 0.054008633, 0.12635948, 0.14564137, 0.13343151, 0.1675762, 0.06194495, 0.06557587, 0.103205524, 0.08757405, 0.044202972, 0.035448365, 0.046352338, 0.08672505, 0.05809986, 0.050199874, 0.03950293, 0.035018295, 0.032832507, 0.048398107, 0.059005763, 0.051666114, 0.058116492, 0.08099697, 0.038001906, 0.028647637, 0.031792678, 0.03405006, 0.03742504, 0.03059648, 0.027590008, 0.032989357, 0.051472332, 0.046504103, 0.030696625, 0.02557312, 0.03741505, 0.027952341, 0.030627882, 0.026487345, 0.03116421, 0.042652786, 0.03107177, 0.07769102, 0.096856646, 0.1656615, 0.13845608, 0.093302, 0.18664443, 0.10139272, 0.12559439, 0.09519152, 0.089373045, 0.0714704, 0.06279093, 0.07112346, 0.036244012, 0.052615583, 0.034071937, 0.045565993, 0.04328145, 0.05280952, 0.041525472, 0.047811, 0.076341, 0.029106379, 0.02136568, 0.03226061, 0.022280961, 0.040311318, 0.021444635, 0.019642647, 0.029985135, 0.040163867, 0.029768392, 0.06367094, 0.02728242, 0.022994194, 0.03822874, 0.02369536, 0.041620985, 0.08190679, 0.14153676, 0.16269961, 0.22058146, 0.21856381, 0.07487929, 0.12805893, 0.057872742, 0.04523475, 0.054743823, 0.05299104, 0.04228754, 0.09807634, 0.04335597, 0.06776059, 0.03760947, 0.034862753, 0.031749334, 0.03000111, 0.036855232, 0.04112551, 0.04037876, 0.024194837, 0.046554767, 0.045140628, 0.056316055, 0.026885513, 0.029384201, 0.03501404, 0.026736297, 0.03664076, 0.025812218, 0.029405804, 0.024930848, 0.030639771, 0.026733994, 0.04613148, 0.027371453, 0.06792448, 0.14202729, 0.16578923, 0.16220061, 0.059039682, 0.054561824, 0.08807471, 0.0508741, 0.069776595, 0.05365436, 0.029619483, 0.039505765, 0.065371685, 0.037984464, 0.054084603, 0.053707626, 0.057469275, 0.028876308, 0.042063583, 0.049980383, 0.053857904, 0.036197037, 0.027480835, 0.038018346, 0.027256222, 0.027149884, 0.019440448, 0.034078695, 0.037651047, 0.043165132, 0.021684578, 0.036319803, 0.027837282, 0.030254332, 0.027076459, 0.033300094, 0.023746042, 0.028156685, 0.11971929, 0.11444279, 0.13770144, 0.20484658, 0.19261777, 0.09403592, 0.04942605, 0.04187789, 0.03445843, 0.05434582, 0.027458683, 0.04249085, 0.07823884, 0.0732853, 0.07828248, 0.045180194, 0.035834037, 0.044668447, 0.04578341, 0.037848573, 0.03535507, 0.033497974, 0.025848653, 0.04073531, 0.03904995, 0.02180527, 0.023889018, 0.031981744, 0.04436098, 0.03853036, 0.02190002, 0.025827872, 0.025346369, 0.028679421, 0.027994435, 0.027015246, 0.025079697, 0.022172892, 0.052102305, 0.04071501, 0.1275022, 0.12944846, 0.087907135, 0.067753136, 0.100276485, 0.05899486, 0.034488674, 0.07457773, 0.047461778, 0.05099419, 0.06487915, 0.08358689, 0.0767731, 0.05548984, 0.03672333, 0.03469634, 0.031558067, 0.030194677, 0.057713985, 0.026440423, 0.031424657, 0.028318377, 0.024933591, 0.03653726, 0.030760683, 0.021168353, 0.03269013, 0.024523059, 0.024619872, 0.032359026, 0.037300836, 0.029677626, 0.020621032, 0.0263268, 0.029133985, 0.02190813, 0.07013625, 0.07544926, 0.09231478, 0.1412148, 0.07586199, 0.20028841, 0.11300292, 0.11530691, 0.05640776, 0.047329657, 0.09629295, 0.056866344, 0.08399118, 0.05387816, 0.030455673, 0.02444855, 0.037750628, 0.04184565, 0.037021443, 0.067937605, 0.04880789, 0.03125663, 0.025521124, 0.021513527, 0.036717422, 0.031866383, 0.04015281, 0.025457367, 0.026446128, 0.022738807, 0.03346226, 0.018539771, 0.021708313, 0.03585299, 0.023717236, 0.025239192, 0.03473575, 0.039680492, 0.073811986, 0.25066662, 0.22932471, 0.14974813, 0.09206076, 0.08488399, 0.06706671, 0.09162592, 0.07537467, 0.06027309, 0.05178251, 0.03790638, 0.0663177, 0.03724784, 0.088072054, 0.04986722, 0.03843442, 0.046814695, 0.03679849, 0.031140832, 0.035839062, 0.035566356, 0.028730813, 0.03915474, 0.027610032, 0.034628402, 0.031273328, 0.040101342, 0.030890092, 0.02621189, 0.030507797, 0.041662, 0.033533536, 0.04236463, 0.033495426, 0.036738288, 0.025541471, 0.04396539, 0.105049424, 0.101686165, 0.19343503, 0.11591832, 0.0694367, 0.11599915, 0.08804943, 0.06651763, 0.108566135, 0.04387578, 0.08428108, 0.047844686, 0.039009288, 0.032940626, 0.043572176, 0.045844447, 0.027661765, 0.039183374, 0.06583432, 0.05028414, 0.0375148, 0.037953448, 0.03558317, 0.036432333, 0.029284729, 0.031714875, 0.028242202, 0.02660635, 0.027789377, 0.030248646, 0.03126209, 0.028354183, 0.025310604, 0.029583907, 0.030459756, 0.037920155, 0.02351037, 0.02648498, 0.08195133, 0.16667223, 0.29305944, 0.09811282, 0.076979674, 0.08706234, 0.11872525, 0.045652267, 0.09745594, 0.1557066, 0.12938458, 0.046470996, 0.071565546, 0.05811107, 0.03624483, 0.03672704, 0.044749733, 0.03769456, 0.075116016, 0.06917386, 0.055261694, 0.03619735, 0.051811926, 0.026405092, 0.03168348, 0.02349019, 0.028097507, 0.045539204, 0.026420528, 0.03352781, 0.044187546, 0.022810277, 0.03546845, 0.022930646, 0.038212743, 0.018540202, 0.056623984, 0.033410717, 0.113138236, 0.13161783, 0.15852116, 0.18733038, 0.07560089, 0.06662327, 0.065798424, 0.057282742, 0.050411273, 0.069567375, 0.054414548, 0.050180137, 0.069761455, 0.048767127, 0.046011068, 0.026825983, 0.035686295, 0.023075078, 0.05560501, 0.049441937, 0.03034556, 0.04040167, 0.030922418, 0.02642294, 0.046814773, 0.033517007, 0.044502657, 0.042184226, 0.03182324, 0.041844785, 0.028603617, 0.035026666, 0.035207894, 0.035278507, 0.037920136, 0.023202697, 0.022788983, 0.03311707, 0.033078868, 0.020177796, 0.021830907, 0.0284302, 0.023662684, 0.13782728, 0.09769988, 0.081070475, 0.11839115, 0.12814832, 0.0858837, 0.055053283, 0.070718795, 0.05722795, 0.036020942, 0.059403293, 0.07644031, 0.038292788, 0.034230035, 0.03625433, 0.023047667, 0.033146866, 0.027202988, 0.03685503, 0.025311634, 0.031477917, 0.041304756, 0.030862555, 0.026762858, 0.027701281, 0.032341477, 0.024477806, 0.031118298, 0.03532293, 0.033525873, 0.028640756, 0.023802364, 0.028927175, 0.026723918, 0.021438759, 0.025354255, 0.02323859, 0.025215872, 0.20000239, 0.19460347, 0.1502593, 0.12366346, 0.2454689, 0.07595835, 0.06871464, 0.065278865, 0.05370772, 0.04629476, 0.03974908, 0.051929474, 0.06583044, 0.04627998, 0.08292323, 0.048336994, 0.036215335, 0.022283072, 0.023749975, 0.039995283, 0.035563488, 0.038057096, 0.033431247, 0.030210957, 0.06263499, 0.051805846, 0.036158454, 0.03334774, 0.03013144, 0.02689711, 0.023815516, 0.025059668, 0.02635556, 0.023581741, 0.025908148, 0.015188791, 0.016757602, 0.02189096, 0.10448477, 0.07367498, 0.07049313, 0.0825335, 0.13505888, 0.0615527, 0.051820647, 0.04235974, 0.072584905, 0.08679915, 0.030516226, 0.048064362, 0.045522653, 0.025007848, 0.02641808, 0.031635594, 0.028955162, 0.048555423, 0.026145244, 0.030764166, 0.0473226, 0.023330301, 0.03662865, 0.0390711, 0.02559519, 0.03076806, 0.030274887, 0.024222555, 0.035160094, 0.029736733, 0.031983763, 0.024678255, 0.029120304, 0.03332599, 0.018276446, 0.022477366, 0.026209606, 0.034394886, 0.078481734, 0.12299853, 0.14239925, 0.122316435, 0.09256055, 0.11891922, 0.09817567, 0.06905831, 0.047586467, 0.04879812, 0.039643142, 0.051013306, 0.07444414, 0.03121598, 0.053940937, 0.05477803, 0.029472455, 0.03571344, 0.039607197, 0.05531655, 0.05238532, 0.024642576, 0.019633535, 0.03081306, 0.03204208, 0.03219324, 0.036521923, 0.02418607, 0.029905483, 0.026215438, 0.025291244, 0.031273417, 0.031121785, 0.032321226, 0.022508755, 0.021671291, 0.03304446, 0.02379087, 0.08237255, 0.1101132, 0.1439189, 0.24559897, 0.1350287, 0.069484994, 0.04662038, 0.05397702, 0.081334345, 0.08253313, 0.047423508, 0.07506576, 0.046467505, 0.057753023, 0.08134653, 0.07066342, 0.051654246, 0.020243978, 0.031264693, 0.034190822, 0.023556413, 0.02701979, 0.030451482, 0.028502813, 0.04096666, 0.027454982, 0.039133184, 0.032757867, 0.027083648, 0.03613917, 0.02945136, 0.030930199, 0.022406533, 0.034924228, 0.028403677, 0.022482729, 0.018606303, 0.027377129, 0.030145576, 0.023943553, 0.022382136, 0.03419065, 0.020161303, 0.030186264, 0.078682885, 0.15136194, 0.09038652, 0.18995519, 0.099454336, 0.056060627, 0.067860335, 0.050813563, 0.07717595, 0.1058595, 0.13000372, 0.062352676, 0.046332713, 0.11353996, 0.12375577, 0.042496286, 0.032711018, 0.037135635, 0.03616078, 0.028908974, 0.0352486, 0.040600985, 0.031766523, 0.024703342, 0.01899533, 0.02989979, 0.029533103, 0.031035598, 0.022170052, 0.022354065, 0.031278357, 0.030219918, 0.021178061, 0.02461044, 0.027188146, 0.02841718, 0.031548433, 0.024948, 0.042370044, 0.08841434, 0.079137705, 0.10049737, 0.110405505, 0.10253809, 0.07517041, 0.10847451, 0.038685508, 0.08957421, 0.045634855, 0.09321224, 0.043443304, 0.04826663, 0.05289903, 0.030478455, 0.033372298, 0.03147468, 0.046731263, 0.03489257, 0.03647311, 0.035400018, 0.022576498, 0.031763047, 0.04559546, 0.023375157, 0.022056282, 0.03451376, 0.026588246, 0.02629088, 0.016736701, 0.025014175, 0.020418927, 0.022551069, 0.023204898, 0.024936968, 0.01914481, 0.03526311, 0.07190303, 0.08890909, 0.10820073, 0.26941603, 0.092487045, 0.0631663, 0.061306845, 0.07828402, 0.20245135, 0.08939419, 0.04830147, 0.031618953, 0.029355727, 0.052964877, 0.0700967, 0.05325972, 0.037858922, 0.031310324, 0.03875107, 0.043194674, 0.030577436, 0.033013478, 0.027547365, 0.025150513, 0.042072088, 0.02773378, 0.038966145, 0.0214131, 0.022856297, 0.037475474, 0.03166034, 0.018699924, 0.019766822, 0.02968465, 0.020394618, 0.02553392, 0.018121738, 0.026711455, 0.063843824, 0.1753909, 0.25193155, 0.13454688, 0.10948746, 0.16966558, 0.14477032, 0.13988622, 0.04891679, 0.07207043, 0.03858139, 0.031354856, 0.05686049, 0.04542463, 0.032272123, 0.038222093, 0.0781036, 0.048472315, 0.056856897, 0.0332062, 0.042503305, 0.036954433, 0.035260167, 0.025592368, 0.020580854, 0.02966928, 0.02891321, 0.022168415, 0.02361577, 0.022201613, 0.023569865, 0.027154738, 0.017066352, 0.030089337, 0.028547859, 0.025620326, 0.023065161, 0.040923785, 0.12912622, 0.08726479, 0.075363524, 0.14776576, 0.08463607, 0.08111199, 0.05889631, 0.06502153, 0.042037293, 0.045697913, 0.061030675, 0.07290234, 0.049229264, 0.04806227, 0.059550423, 0.034131218, 0.053914014, 0.033985563, 0.030438794, 0.024232619, 0.029828027, 0.0363716, 0.026007222, 0.027251378, 0.032118276, 0.03209764, 0.031088289, 0.028174685, 0.027963573, 0.020811435, 0.02541506, 0.020107372, 0.022377422, 0.023812376, 0.020764567, 0.025244175, 0.02040851, 0.016651621, 0.018826747, 0.013720605, 0.022049114, 0.020223225, 0.02174356, 0.06263021, 0.14859658, 0.15255757, 0.17809142, 0.16594173, 0.057825778, 0.05226576, 0.054543257, 0.08397972, 0.041028846, 0.06837047, 0.044750154, 0.07776763, 0.070328094, 0.029676978, 0.053855136, 0.08421973, 0.05369533, 0.052862406, 0.049759123, 0.045227952, 0.031362407, 0.027241174, 0.040491592, 0.03873575, 0.033947844, 0.03472644, 0.02012466, 0.021345682, 0.033769462, 0.024984842, 0.02124287, 0.027978072, 0.02687701, 0.019238513, 0.024636637, 0.04713101, 0.031395447, 0.085273094, 0.19832003, 0.16458724, 0.22683936, 0.15736462, 0.08370513, 0.16401102, 0.10833234, 0.086571194, 0.049080655, 0.057880927, 0.040915776, 0.040840436, 0.059988983, 0.039448395, 0.046440195, 0.03924508, 0.048163425, 0.03958394, 0.032153565, 0.033686027, 0.02429104, 0.027921127, 0.022484293, 0.022936564, 0.0220466, 0.017814847, 0.02374815, 0.024330996, 0.01687592, 0.027123982, 0.023376096, 0.023268165, 0.026433123, 0.022981554, 0.02995896, 0.029487614, 0.021999633, 0.24548039, 0.20456159, 0.18694499, 0.10393336, 0.18782873, 0.11569562, 0.08786867, 0.10020268, 0.04947256, 0.041609954, 0.06784279, 0.077237666, 0.055677325, 0.055361316, 0.053810727, 0.06857163, 0.046378452, 0.025274517, 0.038044073, 0.053841874, 0.04770378, 0.035961762, 0.02812525, 0.023909982, 0.04083709, 0.03337611, 0.025985232, 0.03674786, 0.03940157, 0.027415464, 0.021976912, 0.029579978, 0.031091502, 0.025266673, 0.028854655, 0.023098202, 0.017585518, 0.035206646, 0.08939184, 0.16307048, 0.15354335, 0.20161505, 0.0947322, 0.08921426, 0.06670765, 0.077032425, 0.08724367, 0.08056123, 0.04901546, 0.042398356, 0.042845983, 0.061041147, 0.039113645, 0.04001331, 0.033587106, 0.039132465, 0.03563888, 0.036444116, 0.024824627, 0.023074701, 0.031147417, 0.035247162, 0.034122106, 0.033851173, 0.027258685, 0.03458517, 0.023605486, 0.022385629, 0.022946935, 0.025116714, 0.031096637, 0.033288073, 0.029223466, 0.024590855, 0.034922894, 0.025690103, 0.04440923, 0.11976458, 0.074612945, 0.09455647, 0.16445498, 0.110726416, 0.042540755, 0.06513861, 0.051858336, 0.1228171, 0.057393577, 0.07420572, 0.062368695, 0.033190582, 0.04766857, 0.046755828, 0.03419786, 0.043469477, 0.0344786, 0.030186167, 0.0308483, 0.027156657, 0.028466875, 0.031176085, 0.023672726, 0.054876495, 0.031150809, 0.0429651, 0.022424389, 0.026216205, 0.026115403, 0.03050104, 0.018214202, 0.021349454, 0.021901676, 0.022795474, 0.039663687, 0.025149101, 0.045150436, 0.040433228, 0.024855234, 0.029000288, 0.024051212, 0.026843442, 0.083915174, 0.108331464, 0.3118988, 0.18894272, 0.16134472, 0.10456493, 0.111659475, 0.098592095, 0.060143407, 0.05371026, 0.03536172, 0.058546755, 0.04032854, 0.028665507, 0.041397627, 0.051835984, 0.033518307, 0.030682128, 0.029348537, 0.033527993, 0.04743366, 0.050523132, 0.034991227, 0.027992671, 0.02851837, 0.036038697, 0.031675037, 0.0252364, 0.025188424, 0.03905693, 0.024277752, 0.030335825, 0.035310335, 0.030816972, 0.022780133, 0.026023783, 0.019850213, 0.017345663, 0.0652859, 0.05509773, 0.077695146, 0.07261723, 0.08050595, 0.06475796, 0.03936715, 0.051028643, 0.064437166, 0.075694464, 0.03956796, 0.03448089, 0.07393446, 0.030045854, 0.034079954, 0.033093892, 0.030548187, 0.02350688, 0.028282005, 0.036144342, 0.03298555, 0.018877162, 0.024827445, 0.03002823, 0.028517278, 0.03748999, 0.029637441, 0.022333365, 0.019220844, 0.02493064, 0.025832424, 0.032211717, 0.026692675, 0.018444574, 0.030244397, 0.018135572, 0.025005454, 0.027347421, 0.12010367, 0.12735362, 0.094411515, 0.11579117, 0.09521617, 0.058325026, 0.038902167, 0.0863198, 0.0500872, 0.057832837, 0.047494303, 0.054874346, 0.033561092, 0.024994478, 0.052828062, 0.03271849, 0.02058499, 0.023227394, 0.045045182, 0.045186676, 0.031905606, 0.028737646, 0.021758335, 0.027246937, 0.03527829, 0.026523883, 0.027529115, 0.021765122, 0.01973847, 0.02433981, 0.02629252, 0.031219864, 0.02453101, 0.034875415, 0.040154684, 0.021484798, 0.024294257, 0.027093982, 0.053581346, 0.11925191, 0.20496488, 0.27559257, 0.109239504, 0.11334374, 0.03244118, 0.07930511, 0.06727322, 0.07200738, 0.067374006, 0.043893307, 0.04258997, 0.03902275, 0.053941652, 0.026044175, 0.051616907, 0.043426577, 0.026163802, 0.048763085, 0.03383906, 0.034369927, 0.026035791, 0.031455327, 0.029327678, 0.02573601, 0.043670565, 0.02712419, 0.027357593, 0.0271143, 0.019370034, 0.026553398, 0.022985712, 0.02446054, 0.02179186, 0.027856523, 0.022130903, 0.021049142, 0.11305972, 0.13950321, 0.15251012, 0.103678204, 0.04669858, 0.053842966, 0.08781503, 0.07433782, 0.08246273, 0.07725953, 0.054945424, 0.03607816, 0.034812026, 0.038570635, 0.036352593, 0.032402962, 0.030733582, 0.03875455, 0.023493774, 0.038543213, 0.026924271, 0.03613042, 0.019219523, 0.027784673, 0.029268507, 0.053052105, 0.024118796, 0.023273176, 0.025211776, 0.024577376, 0.025843887, 0.025785398, 0.026033735, 0.019285062, 0.024776423, 0.030156994, 0.034038816, 0.025067426, 0.025162438, 0.022482509, 0.021165939, 0.02195173, 0.020693155, 0.07072146, 0.10466632, 0.08320413, 0.19789979, 0.10565978, 0.05836155, 0.061218563, 0.062169727, 0.07743301, 0.11359054, 0.042666253, 0.08423216, 0.052493565, 0.038128942, 0.03228789, 0.040662322, 0.033143, 0.030648056, 0.03399894, 0.031822186, 0.04219041, 0.035163157, 0.024754249, 0.02847816, 0.04170254, 0.031969916, 0.02384247, 0.022193894, 0.024746649, 0.026998777, 0.035303358, 0.029982971, 0.023358002, 0.020387596, 0.02672691, 0.0257361, 0.023608947, 0.0202147, 0.15479214, 0.19586587, 0.17339557, 0.124126785, 0.07328648, 0.100934446, 0.0585322, 0.05176222, 0.058437906, 0.066143416, 0.048888624, 0.03771267, 0.04911854, 0.099816635, 0.059242807, 0.057040974, 0.06963744, 0.03725215, 0.036469243, 0.04430976, 0.037268233, 0.041409116, 0.030637577, 0.032438822, 0.027000299, 0.042062707, 0.039052397, 0.03396127, 0.024752287, 0.025184605, 0.022108933, 0.027490474, 0.03353823, 0.036166824, 0.028814966, 0.032175943, 0.03760483, 0.028280908, 0.083194934, 0.12086083, 0.099006996, 0.16019118, 0.13697472, 0.101644054, 0.089351684, 0.098514535, 0.06508758, 0.06941477, 0.061857775, 0.062280204, 0.03833175, 0.036507145, 0.027841954, 0.044633295, 0.100225925, 0.06792607, 0.034332085, 0.043977235, 0.04127567, 0.063134834, 0.036542002, 0.036782257, 0.04372961, 0.042249735, 0.026929924, 0.05937003, 0.029716516, 0.03147125, 0.04313381, 0.024091527, 0.032706093, 0.031502906, 0.025462521, 0.029573275, 0.028474921, 0.034249254, 0.07723937, 0.09059918, 0.07561814, 0.14134903, 0.15742177, 0.077058844, 0.10132939, 0.09672258, 0.08415282, 0.090738945, 0.06246516, 0.057091545, 0.066493034, 0.07131125, 0.04282615, 0.06542242, 0.05142076, 0.04241352, 0.035986263, 0.02613452, 0.032181606, 0.053451225, 0.05483684, 0.023167515, 0.036799263, 0.036470354, 0.034672588, 0.03363142, 0.036493994, 0.02301616, 0.020995481, 0.041682135, 0.027083118, 0.033640604, 0.025230873, 0.041058555, 0.031039929, 0.02449684, 0.06454423, 0.17276068, 0.10891829, 0.118038744, 0.10727087, 0.0694949, 0.056371085, 0.06297474, 0.09188972, 0.093587756, 0.04880215, 0.037544206, 0.06138895, 0.049728468, 0.040618036, 0.042692706, 0.022276888, 0.036624633, 0.030345747, 0.040870365, 0.035740566, 0.050462596, 0.025817329, 0.038033843, 0.038164347, 0.03408349, 0.03440734, 0.030363724, 0.029674076, 0.033326186, 0.033504996, 0.030632079, 0.027167128, 0.033564378, 0.033058356, 0.022370884, 0.023055116, 0.028805062, 0.029691953, 0.026535725, 0.021568224, 0.032426465, 0.03416984, 0.017666252, 0.07229871, 0.09164853, 0.1162369, 0.29219234, 0.1228449, 0.13199715, 0.09314535, 0.04348176, 0.121161185, 0.07049301, 0.05438492, 0.04688556, 0.04603566, 0.051684324, 0.057188895, 0.053096864, 0.048749313, 0.02837534, 0.042245623, 0.031335678, 0.03396735, 0.026389332, 0.032972958, 0.028519198, 0.036477588, 0.03221654, 0.025037542, 0.036699075, 0.041220777, 0.026859773, 0.025723584, 0.03338988, 0.026561223, 0.025359763, 0.021048395, 0.027775055, 0.026615225, 0.021257974, 0.06838439, 0.22299789, 0.1991225, 0.19962925, 0.16480958, 0.060742635, 0.0757242, 0.079569474, 0.057855677, 0.057467543, 0.04206687, 0.03875404, 0.037291948, 0.052994855, 0.04353937, 0.05376497, 0.03788534, 0.03679775, 0.03248286, 0.054144457, 0.04916321, 0.036073618, 0.031289015, 0.030079236, 0.03129401, 0.036866993, 0.028426465, 0.02934022, 0.022480141, 0.038678575, 0.020683669, 0.01637268, 0.023406854, 0.026113702, 0.026165888, 0.030171597, 0.03212117, 0.021102587, 0.0583136, 0.082886785, 0.25977075, 0.10655363, 0.10731833, 0.044239115, 0.046825677, 0.12114994, 0.102972694, 0.10758189, 0.09821115, 0.034018878, 0.04406009, 0.0544471, 0.03941275, 0.043668248, 0.038378734, 0.028810862, 0.036495123, 0.0375925, 0.037732817, 0.027944647, 0.037177447, 0.028961988, 0.022808656, 0.037555642, 0.025428759, 0.020588918, 0.032033578, 0.02735286, 0.028464451, 0.037512444, 0.034570344, 0.026391689, 0.03617884, 0.03476409, 0.035709295, 0.02226814, 0.11170962, 0.13059124, 0.12561278, 0.12147456, 0.09192842, 0.0632846, 0.079968415, 0.04800303, 0.04454792, 0.048452422, 0.037179958, 0.075448, 0.07676365, 0.0370059, 0.026701793, 0.043194093, 0.055918757, 0.033090208, 0.032563556, 0.037745297, 0.05912736, 0.038349677, 0.03628983, 0.02946453, 0.032266118, 0.022194218, 0.021516597, 0.026599124, 0.046212025, 0.027598862, 0.032943115, 0.032988787, 0.030297386, 0.026542362, 0.028620169, 0.025703816, 0.02336753, 0.029050335, 0.06938657, 0.06994612, 0.112735875, 0.25380844, 0.15460984, 0.117570624, 0.08827124, 0.05240014, 0.06333059, 0.08979769, 0.056635186, 0.06742844, 0.05236291, 0.036257137, 0.07161228, 0.044465892, 0.047865022, 0.029231453, 0.029300708, 0.04073317, 0.0395526, 0.022668593, 0.03919497, 0.039149035, 0.03150997, 0.039906897, 0.032486673, 0.03429302, 0.043389462, 0.04209897, 0.025194952, 0.023104984, 0.028242227, 0.031409275, 0.02671294, 0.02618551, 0.02145537, 0.02608839, 0.023189714, 0.019830735, 0.023168698, 0.028225675, 0.027849313, 0.1316926, 0.12823425, 0.13187115, 0.091817394, 0.13147533, 0.07985856, 0.06685255, 0.087234, 0.1103441, 0.11007995, 0.048280716, 0.029128814, 0.07629941, 0.051659673, 0.045485333, 0.037283342, 0.028062532, 0.04004252, 0.041987818, 0.039702106, 0.033318978, 0.034896962, 0.0359706, 0.026649503, 0.029915815, 0.025764829, 0.03248808, 0.023690308, 0.024921548, 0.025436526, 0.03774727, 0.024390722, 0.037781496, 0.0247435, 0.021902636, 0.018285338, 0.03218102, 0.026570514, 0.1254392, 0.09547116, 0.09146802, 0.24538553, 0.1485458, 0.05525069, 0.04543089, 0.07361797, 0.12325877, 0.05075468, 0.045830943, 0.03978357, 0.060075723, 0.047594696, 0.04813841, 0.040468544, 0.06350571, 0.054541964, 0.05675623, 0.03889339, 0.034245066, 0.023093868, 0.029421376, 0.033070564, 0.038771886, 0.026787965, 0.027573036, 0.030484423, 0.034973383, 0.0340256, 0.033662893, 0.027125822, 0.031247387, 0.020332798, 0.025519365, 0.017877478, 0.022824429, 0.019777179, 0.05395573, 0.11260365, 0.0922954, 0.098526776, 0.12345142, 0.11911267, 0.085918576, 0.04868124, 0.0409216, 0.03260791, 0.07376241, 0.03716545, 0.030765396, 0.043379333, 0.03074381, 0.037854455, 0.039413285, 0.037655782, 0.037655048, 0.04021976, 0.04135381, 0.023441928, 0.02782178, 0.03113799, 0.035320546, 0.020841887, 0.023464527, 0.027598266, 0.041253187, 0.02391422, 0.03254709, 0.024646185, 0.018477628, 0.01920183, 0.023871569, 0.016058898, 0.024638481, 0.029206073, 0.06382797, 0.09648051, 0.07408401, 0.083694644, 0.055893414, 0.13031997, 0.059466913, 0.0744309, 0.07829498, 0.08497693, 0.03146426, 0.034479946, 0.03444631, 0.043103836, 0.023199761, 0.028126886, 0.033983003, 0.021734199, 0.034192186, 0.022457466, 0.03735104, 0.025280012, 0.020321576, 0.031342056, 0.03018912, 0.018255945, 0.016005293, 0.022683244, 0.018551877, 0.016338559, 0.027257329, 0.020012015, 0.025014065, 0.026981222, 0.019812683, 0.021572487, 0.01849649, 0.01847134, 0.07708896, 0.08117417, 0.37850487, 0.18166539, 0.06843696, 0.05967297, 0.06163202, 0.08338628, 0.06608619, 0.058717757, 0.046321254, 0.04377944, 0.033256136, 0.04710226, 0.08466102, 0.09074016, 0.03510231, 0.031961072, 0.030520389, 0.03338464, 0.021586346, 0.038835075, 0.028427655, 0.031763643, 0.030001113, 0.024906974, 0.038886074, 0.030644184, 0.021833133, 0.027182922, 0.0578477, 0.043820884, 0.03323149, 0.018928861, 0.034706384, 0.023155058, 0.02590992, 0.028960545, 0.02263438, 0.04458734, 0.020386219, 0.025084687, 0.023966677, 0.029349621, 0.10858935, 0.11391108, 0.11431901, 0.16478653, 0.23238868, 0.10036144, 0.07212614, 0.07514739, 0.051212937, 0.04775573, 0.03935753, 0.042127594, 0.045253944, 0.03269602, 0.02718466, 0.032997224, 0.02750106, 0.04235961, 0.063422464, 0.04833052, 0.033856265, 0.032249626, 0.030516908, 0.032013644, 0.04347829, 0.027588455, 0.048198666, 0.023110583, 0.026281238, 0.030615395, 0.0394898, 0.018089795, 0.026661092, 0.028543077, 0.025467018, 0.02408491, 0.022121064, 0.029227527, 0.039883144, 0.10278994, 0.095559806, 0.09722819, 0.059957594, 0.05149597, 0.09194756, 0.052285276, 0.058470525, 0.058742397, 0.042627025, 0.03344994, 0.059827153, 0.0629418, 0.043823797, 0.036877833, 0.031464055, 0.033469826, 0.02821651, 0.027442409, 0.030736411, 0.030024651, 0.025113577, 0.037470113, 0.028669799, 0.0418968, 0.020856323, 0.03257705, 0.021213586, 0.027014274, 0.023623744, 0.034733355, 0.03277992, 0.021323983, 0.022308791, 0.023483533, 0.027197348, 0.026799368, 0.10731921, 0.10874721, 0.15175293, 0.12603338, 0.14414547, 0.072778486, 0.20390463, 0.09532342, 0.09873849, 0.06194971, 0.066657126, 0.0833475, 0.04566948, 0.044175792, 0.053462856, 0.037678618, 0.03023962, 0.03068818, 0.036082476, 0.03406522, 0.025702832, 0.031828653, 0.026428085, 0.032896526, 0.027110294, 0.022124562, 0.021573022, 0.023898857, 0.05305671, 0.028413689, 0.02442693, 0.033976365, 0.023718169, 0.029439054, 0.028450407, 0.026715633, 0.02766644, 0.023152594, 0.10544419, 0.10981591, 0.18141285, 0.09739439, 0.055208158, 0.06290008, 0.06809918, 0.081962295, 0.053480595, 0.061010554, 0.043675732, 0.045779783, 0.04151381, 0.07675811, 0.052307926, 0.034936275, 0.044462617, 0.04042933, 0.028652724, 0.031476665, 0.037179966, 0.04055659, 0.023488801, 0.027124707, 0.033347424, 0.039929174, 0.032660753, 0.02369684, 0.02072367, 0.03072911, 0.022009233, 0.032002874, 0.029566608, 0.03701749, 0.024034671, 0.023002598, 0.033750754, 0.027276428, 0.09056849, 0.100469954, 0.22978269, 0.20677595, 0.09918487, 0.06447959, 0.086453065, 0.059069946, 0.05572022, 0.054349862, 0.051692236, 0.04542434, 0.058978714, 0.041104645, 0.02869958, 0.031941857, 0.02931018, 0.029884044, 0.046551954, 0.03604717, 0.040415436, 0.043380618, 0.03129915, 0.03242057, 0.027161306, 0.046538264, 0.039345916, 0.04183215, 0.029888565, 0.028956212, 0.02647072, 0.032037467, 0.027595097, 0.030157227, 0.02616814, 0.01996067, 0.031694073, 0.017862817, 0.023653626, 0.028596131, 0.026672263, 0.03408189, 0.022631431, 0.1844278, 0.15823174, 0.26842332, 0.16644886, 0.1571889, 0.16931988, 0.11044605, 0.047472898, 0.07048741, 0.0854026, 0.041603565, 0.03517254, 0.05553437, 0.052880242, 0.078465246, 0.057715397, 0.039715677, 0.041712504, 0.04069963, 0.037552726, 0.03536217, 0.032023996, 0.03189231, 0.026011888, 0.026655799, 0.031976826, 0.029028224, 0.03672737, 0.02780914, 0.034696773, 0.019943371, 0.026706422, 0.038013663, 0.033766057, 0.024786403, 0.024980122, 0.027383754, 0.02529399, 0.05554818, 0.124089815, 0.064876385, 0.11544541, 0.081664324, 0.08096392, 0.097560525, 0.08162305, 0.11065294, 0.07843468, 0.078704014, 0.06688529, 0.03993584, 0.048652545, 0.084769, 0.060408782, 0.04165409, 0.031737432, 0.031089595, 0.033065815, 0.054516297, 0.03877292, 0.027315594, 0.02897724, 0.01894585, 0.023509998, 0.022614773, 0.025483683, 0.032623615, 0.03022834, 0.029945621, 0.031594995, 0.019979829, 0.03022263, 0.02702779, 0.028103337, 0.024535742, 0.023002587, 0.12762092, 0.19490595, 0.17290461, 0.14684032, 0.15785079, 0.055764593, 0.06497703, 0.052917443, 0.06273417, 0.102852166, 0.05739078, 0.043462075, 0.032958984, 0.054116387, 0.055915657, 0.03167993, 0.0367725, 0.03825537, 0.021124301, 0.029055605, 0.03593518, 0.035720073, 0.045554217, 0.04431972, 0.03050181, 0.024747575, 0.02150467, 0.025694689, 0.02317603, 0.020106921, 0.036343426, 0.030414209, 0.032167986, 0.042806603, 0.021757985, 0.042128906, 0.022436388, 0.017009528, 0.04745933, 0.18832777, 0.13357344, 0.11117374, 0.07925424, 0.051169336, 0.051425807, 0.05433998, 0.028023202, 0.04033171, 0.038658015, 0.04637592, 0.042931627, 0.06781918, 0.04070375, 0.054886248, 0.056615736, 0.026834281, 0.07991345, 0.036763053, 0.027471272, 0.037141193, 0.023095464, 0.02294769, 0.024299392, 0.026441798, 0.041867483, 0.038263056, 0.024295127, 0.022719676, 0.027141824, 0.029209122, 0.027387422, 0.019748399, 0.016237946, 0.03123498, 0.027396798, 0.024679437, 0.1109778, 0.0910243, 0.11717745, 0.1396105, 0.0809617, 0.14177619, 0.072634906, 0.044434663, 0.032692976, 0.058111046, 0.03489847, 0.038864147, 0.030265812, 0.025648931, 0.032956935, 0.040539432, 0.036916897, 0.034583658, 0.030630156, 0.028944002, 0.04452445, 0.027917987, 0.034207452, 0.0368596, 0.021375353, 0.044537388, 0.0448777, 0.033256855, 0.032791164, 0.021911873, 0.026382552, 0.034569312, 0.019663049, 0.04230979, 0.02128999, 0.023726989, 0.024951005, 0.0293414, 0.02417276, 0.016580826, 0.03013129, 0.030325394, 0.02111854, 0.025139475, 0.079903185, 0.07210412, 0.14263777, 0.078232735, 0.07985477, 0.108084075, 0.10368104, 0.07517233, 0.05986211, 0.060850225, 0.06172179, 0.07001649, 0.043116424, 0.050485674, 0.037445642, 0.042846635, 0.031469952, 0.03782959, 0.038483363, 0.03169194, 0.022409424, 0.025623754, 0.029587626, 0.02616067, 0.0271418, 0.039849453, 0.023582524, 0.019857617, 0.022922939, 0.039121043, 0.024063807, 0.037269317, 0.02898628, 0.027936367, 0.022167398, 0.026455158, 0.025612691, 0.025322633, 0.06195413, 0.13146208, 0.091858335, 0.13127746, 0.14529802, 0.10751403, 0.06853864, 0.07230149, 0.041738052, 0.042313352, 0.048557006, 0.03812349, 0.03650812, 0.03652258, 0.057882152, 0.047207706, 0.030124262, 0.02961138, 0.03547552, 0.037693262, 0.031305384, 0.028555186, 0.02935614, 0.03684296, 0.019778695, 0.024378475, 0.02420933, 0.018950129, 0.019749472, 0.025975233, 0.02988768, 0.03128127, 0.026162935, 0.025333876, 0.018377852, 0.024553189, 0.038747903, 0.01867997, 0.092222296, 0.07962293, 0.22547773, 0.19519855, 0.06050229, 0.05784936, 0.07080876, 0.12535343, 0.07584409, 0.10767484, 0.088767774, 0.033467025, 0.028869802, 0.048642177, 0.048725523, 0.054902572, 0.035056345, 0.033911053, 0.03332125, 0.045980297, 0.03927601, 0.03172774, 0.031923555, 0.020018928, 0.027010426, 0.037472483, 0.035796873, 0.034139574, 0.029192943, 0.039315797, 0.01869695, 0.05162286, 0.023219928, 0.029029282, 0.030772852, 0.021282304, 0.024537873, 0.030057801, 0.07672821, 0.06708809, 0.17933506, 0.16384794, 0.08496496, 0.12019033, 0.083752096, 0.081798606, 0.09422167, 0.050522313, 0.06338885, 0.06902644, 0.091238976, 0.05411677, 0.041434776, 0.030047191, 0.029705744, 0.02949742, 0.037479173, 0.0305135, 0.04089515, 0.034893427, 0.039317302, 0.032347728, 0.033740416, 0.026597293, 0.02557229, 0.01877506, 0.034704674, 0.03296921, 0.029754544, 0.028181449, 0.030959195, 0.03368322, 0.02929079, 0.024862876, 0.021178545, 0.02789338, 0.062390868, 0.05245216, 0.09371118, 0.12285022, 0.05658557, 0.081230365, 0.049495917, 0.06039356, 0.084061414, 0.08969493, 0.04214354, 0.03835962, 0.02816357, 0.045779917, 0.036646087, 0.035804313, 0.03253321, 0.043897457, 0.04324566, 0.0428542, 0.023541031, 0.024178691, 0.026404047, 0.032259043, 0.017921183, 0.029148849, 0.03232412, 0.021611812, 0.03685118, 0.020237489, 0.027407426, 0.019666588, 0.027617102, 0.02872595, 0.033542927, 0.023817241, 0.02508173, 0.026210634, 0.09682164, 0.09032518, 0.1416098, 0.17238925, 0.06237335, 0.05069286, 0.04697902, 0.11123291, 0.09512532, 0.049182247, 0.0595257, 0.06401478, 0.029257586, 0.044255, 0.07305684, 0.04791337, 0.06509894, 0.027701808, 0.024230963, 0.042614665, 0.032158785, 0.027574671, 0.034175884, 0.022428924, 0.038142897, 0.028973544, 0.022927308, 0.022834888, 0.025272742, 0.025473023, 0.022094691, 0.024997458, 0.02337696, 0.026107598, 0.022021564, 0.025196638, 0.023600409, 0.026893506, 0.08337522, 0.10746666, 0.12428695, 0.0804569, 0.057247564, 0.07226112, 0.09806047, 0.05636372, 0.039617985, 0.052894402, 0.05072219, 0.07021597, 0.04143418, 0.035790306, 0.038497772, 0.028994953, 0.024832295, 0.04246618, 0.022382729, 0.030089837, 0.037558906, 0.023110684, 0.029953003, 0.0633301, 0.027883068, 0.040438082, 0.020062542, 0.03087904, 0.023929402, 0.043191124, 0.026189225, 0.037787512, 0.021991545, 0.036068868, 0.024184115, 0.023301324, 0.023684215, 0.01702876, 0.043848306, 0.18714604, 0.21270183, 0.27578157, 0.09736335, 0.078137144, 0.06436926, 0.04619965, 0.06653296, 0.058053356, 0.059229538, 0.07277564, 0.04917754, 0.02888587, 0.06599532, 0.05283522, 0.032432266, 0.02396826, 0.03335312, 0.041424304, 0.028642984, 0.03955818, 0.037983507, 0.063718975, 0.044635456, 0.040509287, 0.030649882, 0.026477654, 0.030141884, 0.026843704, 0.023831999, 0.032392636, 0.027912108, 0.034078162, 0.03220557, 0.028891588, 0.027719155, 0.028843328, 0.053198073, 0.07100032, 0.095869325, 0.06288046, 0.12363297, 0.058824714, 0.047200326, 0.08778734, 0.07597151, 0.058144413, 0.07328452, 0.04686892, 0.0344378, 0.049101077, 0.03526599, 0.03606326, 0.030555578, 0.05415911, 0.034962296, 0.055666797, 0.0386978, 0.055783793, 0.03783882, 0.023812162, 0.020613067, 0.023154711, 0.030266717, 0.025538761, 0.027843187, 0.019230172, 0.024064919, 0.027801815, 0.024772543, 0.025778934, 0.024998372, 0.03143224, 0.027119165, 0.018052246, 0.069714345, 0.10446618, 0.06399932, 0.074503176, 0.05619418, 0.04253683, 0.046595804, 0.042113703, 0.038274497, 0.06547522, 0.056216866, 0.04169728, 0.03062385, 0.03704903, 0.049577925, 0.027657693, 0.032646727, 0.027952706, 0.032651402, 0.051374692, 0.022770444, 0.02447098, 0.027647978, 0.030551735, 0.032643214, 0.031968657, 0.040303487, 0.028857714, 0.030717447, 0.026341451, 0.031641744, 0.02749935, 0.025112191, 0.018801725, 0.023731057, 0.023266546, 0.020839328, 0.02362402, 0.027032105, 0.028499745, 0.01845543, 0.02566337, 0.026712373, 0.06686082, 0.08899271, 0.1319227, 0.16668914, 0.09805063, 0.085217945, 0.09575371, 0.08004457, 0.10009048, 0.0775957, 0.060166735, 0.059184663, 0.06585152, 0.060581747, 0.07980134, 0.039188184, 0.050077133, 0.0299649, 0.028468627, 0.052002184, 0.03755665, 0.04548598, 0.03203087, 0.03967897, 0.03788826, 0.023591598, 0.037381273, 0.034799967, 0.036478546, 0.022051388, 0.021281127, 0.034016766, 0.03300426, 0.02489012, 0.030401716, 0.025867252, 0.02261282, 0.02397382, 0.04795491, 0.1158344, 0.10675971, 0.08422554, 0.08190967, 0.06693275, 0.07037185, 0.05720044, 0.108524345, 0.15116592, 0.06300242, 0.057836365, 0.046627305, 0.035451658, 0.046559524, 0.039570566, 0.04169279, 0.028230865, 0.036449026, 0.038768485, 0.051356707, 0.025375903, 0.026064212, 0.038479183, 0.030843144, 0.031890273, 0.022185879, 0.02124852, 0.024941862, 0.028078314, 0.026679635, 0.020984972, 0.024499053, 0.017011646, 0.030842248, 0.02351471, 0.02392083, 0.017952204, 0.07483316, 0.13336885, 0.0701875, 0.14543578, 0.17171358, 0.08803072, 0.08763172, 0.08624854, 0.072930284, 0.07542497, 0.061461844, 0.069142446, 0.04373299, 0.04948758, 0.038784698, 0.04718272, 0.046943754, 0.03767551, 0.029330397, 0.04420399, 0.044132203, 0.04164648, 0.034557886, 0.034803342, 0.027520532, 0.032249376, 0.025757244, 0.020913335, 0.022728775, 0.026912931, 0.024019899, 0.02949942, 0.025603808, 0.030274065, 0.026204074, 0.033312876, 0.025983388, 0.027419906, 0.050751, 0.13737375, 0.12704052, 0.06682971, 0.076945916, 0.049363047, 0.04514629, 0.05942162, 0.09027193, 0.08912104, 0.035882484, 0.05959955, 0.047105152, 0.054135278, 0.05675612, 0.035082918, 0.039991535, 0.043839976, 0.03533187, 0.052831605, 0.025889691, 0.033600777, 0.026522622, 0.0345134, 0.041567627, 0.024358388, 0.04018786, 0.029899761, 0.043902177, 0.030039525, 0.027251996, 0.021512967, 0.021311939, 0.029863806, 0.020343387, 0.017770285, 0.02170406, 0.021327753, 0.09659459, 0.1515675, 0.14215678, 0.084809005, 0.09991226, 0.07949554, 0.09012291, 0.06206726, 0.03918191, 0.090779886, 0.049196478, 0.028767366, 0.04701073, 0.056810554, 0.049110413, 0.034879923, 0.048850678, 0.046184447, 0.047700427, 0.04941232, 0.045431055, 0.027856087, 0.023188943, 0.021432282, 0.027119018, 0.02765491, 0.01628831, 0.023791859, 0.033517383, 0.026715439, 0.022297606, 0.019797359, 0.028729549, 0.014669804, 0.019104734, 0.029414965, 0.01733055, 0.021178419, 0.018859798, 0.022296246, 0.017030254, 0.025605181, 0.016991306, 0.020485794, 0.05449762, 0.11866547, 0.059666578, 0.07425556, 0.07156354, 0.046245407, 0.06597604, 0.06424677, 0.05495712, 0.060928416, 0.037632078, 0.035191808, 0.027538583, 0.0518863, 0.090062946, 0.05164372, 0.046850163, 0.028997613, 0.024926389, 0.025478117, 0.03640717, 0.04713816, 0.0300212, 0.038435947, 0.024522021, 0.03022628, 0.019848283, 0.025760239, 0.02148285, 0.025912343, 0.027835388, 0.028310662, 0.03235408, 0.024769858, 0.021403208, 0.024508499, 0.021374127, 0.020455169, 0.059625, 0.06853794, 0.12671997, 0.11422159, 0.10922426, 0.1663494, 0.057604525, 0.08192717, 0.04938114, 0.119085886, 0.054159924, 0.054518133, 0.03515219, 0.032993715, 0.040611558, 0.028511038, 0.021955468, 0.038938824, 0.026983703, 0.0210617, 0.028559038, 0.031040499, 0.029198898, 0.023369076, 0.028198587, 0.033809833, 0.026260123, 0.024063077, 0.03251469, 0.02590288, 0.025648251, 0.021350173, 0.02349945, 0.024862664, 0.03205939, 0.028210029, 0.03933365, 0.02870518, 0.11372823, 0.06372772, 0.106743954, 0.12947245, 0.14984304, 0.10671986, 0.15650053, 0.123028, 0.06024282, 0.03740908, 0.06964809, 0.07272098, 0.080758974, 0.06199038, 0.038512517, 0.03493992, 0.026034778, 0.041373175, 0.03594525, 0.03806294, 0.03765372, 0.04007365, 0.029220348, 0.016473869, 0.031678084, 0.03569448, 0.021109225, 0.02420317, 0.024999851, 0.024294144, 0.030854134, 0.026941648, 0.025657112, 0.025253378, 0.024539217, 0.017363751, 0.018649492, 0.02257584, 0.067820154, 0.17084046, 0.1327377, 0.12898439, 0.087364286, 0.05512852, 0.047004055, 0.059296656, 0.11911919, 0.047184, 0.023458255, 0.033832137, 0.062981956, 0.042581737, 0.039275423, 0.019887617, 0.024466427, 0.0322822, 0.024102893, 0.044364065, 0.031655442, 0.02521812, 0.024146639, 0.025185063, 0.038405243, 0.028630514, 0.037055787, 0.037798863, 0.021683145, 0.030035987, 0.026166055, 0.023939034, 0.030878356, 0.020135831, 0.022646949, 0.031808794, 0.019618262, 0.028075475, 0.059332736, 0.039146136, 0.20825103, 0.15469229, 0.081931196, 0.05507352, 0.11048621, 0.054464553, 0.057811987, 0.0819798, 0.07228382, 0.041515656, 0.07608511, 0.036654755, 0.03504502, 0.039965328, 0.03370332, 0.03614141, 0.023413444, 0.039665747, 0.023039551, 0.02638867, 0.025985051, 0.021492695, 0.03242289, 0.02293148, 0.035891104, 0.027063195, 0.039052755, 0.022532713, 0.023278179, 0.022172878, 0.02096009, 0.024487622, 0.036400467, 0.029788828, 0.0277452, 0.030805277, 0.030325852, 0.022196358, 0.026005676, 0.030393347, 0.025309255, 0.08473962, 0.11054423, 0.08163599, 0.17151177, 0.045719553, 0.07051001, 0.04410273, 0.11079937, 0.05200713, 0.06431661, 0.051565, 0.031608067, 0.036685534, 0.05644374, 0.05248251, 0.053536836, 0.0407624, 0.039449498, 0.032131188, 0.035245426, 0.032872606, 0.040614087, 0.02213433, 0.0270307, 0.027121302, 0.017854394, 0.03343084, 0.025032422, 0.026482578, 0.020266403, 0.020584656, 0.027008176, 0.02336488, 0.020042425, 0.02601003, 0.021421572, 0.026740018, 0.020934736, 0.10480594, 0.13347027, 0.0735459, 0.053085327, 0.054585878, 0.06474011, 0.07952785, 0.06567643, 0.051918816, 0.06893291, 0.048257504, 0.034381658, 0.030877009, 0.030650754, 0.04272341, 0.04370311, 0.023613632, 0.021571541, 0.022302017, 0.040477604, 0.023277825, 0.025819147, 0.031868797, 0.024846572, 0.016921045, 0.025832444, 0.019787394, 0.01890686, 0.023623323, 0.020130217, 0.02429188, 0.027191525, 0.020335065, 0.027019056, 0.024184553, 0.017272541, 0.024256652, 0.033908375, 0.10814023, 0.13779047, 0.09420696, 0.13600598, 0.099510856, 0.07366021, 0.07107085, 0.04525064, 0.08966403, 0.07017542, 0.039125696, 0.047074016, 0.043714598, 0.018996065, 0.029246395, 0.029948894, 0.03693975, 0.035678346, 0.0468911, 0.02855671, 0.01761786, 0.024789019, 0.035789326, 0.019985044, 0.023160568, 0.022670899, 0.024477366, 0.016558606, 0.022144713, 0.015048493, 0.025681876, 0.019709397, 0.025755048, 0.02053674, 0.019101793, 0.021492723, 0.021828303, 0.016675355, 0.043180823, 0.12481757, 0.080572605, 0.084119335, 0.071351245, 0.048504557, 0.037213873, 0.09601864, 0.12588753, 0.05066703, 0.032109387, 0.02926104, 0.043504335, 0.02689052, 0.022371765, 0.021763803, 0.040489547, 0.029541278, 0.028691815, 0.046989392, 0.022959394, 0.021587746, 0.018693319, 0.026478987, 0.036716945, 0.024131473, 0.020577475, 0.021015331, 0.019247914, 0.016560184, 0.01828566, 0.018255949, 0.01475015, 0.01678531, 0.014978707, 0.01311666, 0.016092937, 0.023814894, 0.040866178, 0.14236318, 0.07021259, 0.12701468, 0.16476922, 0.058159158, 0.10369577, 0.045045584, 0.06350452, 0.07989421, 0.04553413, 0.033646613, 0.03731723, 0.04474613, 0.028786141, 0.03161308, 0.022378553, 0.023740675, 0.025096018, 0.016462458, 0.025817405, 0.019342547, 0.024124432, 0.024982294, 0.02113392, 0.027501363, 0.020575363, 0.020375544, 0.019938791, 0.021602964, 0.016046064, 0.02726458, 0.017563116, 0.015580711, 0.020739548, 0.017909868, 0.01666587, 0.020838907, 0.019015241, 0.024425134, 0.022413224, 0.01796092, 0.01788265, 0.015737271, 0.067301, 0.07760259, 0.10404548, 0.05035205, 0.059349447, 0.08036054, 0.076899424, 0.06852933, 0.02890574, 0.04919342, 0.04696215, 0.02996625, 0.027552843, 0.020645602, 0.03226026, 0.018150307, 0.022223929, 0.025131216, 0.021124344, 0.022865597, 0.024922647, 0.020491442, 0.021834005, 0.024490288, 0.017711291, 0.012310253, 0.017996877, 0.017423615, 0.022343818, 0.018918447, 0.014267882, 0.015091117, 0.012599902, 0.012712585, 0.026402222, 0.025097933, 0.013826288, 0.012133605, 0.053219896, 0.12699397, 0.11445502, 0.070562184, 0.093333505, 0.041607648, 0.062002998, 0.060649175, 0.07894808, 0.12250874, 0.0755661, 0.0285476, 0.044295873, 0.03317553, 0.029521255, 0.035678826, 0.038981725, 0.031078828, 0.024624003, 0.030576078, 0.020417877, 0.02708978, 0.027836053, 0.025558015, 0.031004691, 0.025954686, 0.02220637, 0.02661258, 0.0152884545, 0.024419501, 0.02032927, 0.01696437, 0.026213862, 0.016021982, 0.021817243, 0.019741654, 0.023282452, 0.014124008, 0.05995854, 0.06878509, 0.112918854, 0.07160196, 0.080466576, 0.03733776, 0.06728873, 0.12504987, 0.09739469, 0.03946826, 0.042185366, 0.033654016, 0.02624991, 0.040135466, 0.03497883, 0.022592789, 0.02250219, 0.021067843, 0.028706918, 0.022996165, 0.026786955, 0.027785713, 0.0172504, 0.018461779, 0.022526257, 0.028756449, 0.022013819, 0.016572708, 0.018302316, 0.021427732, 0.015180519, 0.019088008, 0.017146537, 0.018489342, 0.01580963, 0.015692916, 0.015491578, 0.014784091, 0.046063993, 0.100248754, 0.123602964, 0.058568716, 0.07429602, 0.047411133, 0.041694492, 0.063575864, 0.08644631, 0.053186003, 0.07745669, 0.040002394, 0.034619067, 0.03034124, 0.027284017, 0.032720774, 0.029674549, 0.0328281, 0.018737143, 0.027319564, 0.023729082, 0.019291246, 0.015896238, 0.021934157, 0.01675234, 0.017952263, 0.016240194, 0.01906741, 0.018199984, 0.019059977, 0.019056432, 0.017949551, 0.012066589, 0.019503288, 0.014120149, 0.019858846, 0.01249987, 0.011818302, 0.047876235, 0.08403523, 0.064816795, 0.08746207, 0.06403228, 0.06816155, 0.054429047, 0.043764852, 0.06304029, 0.14232875, 0.06991206, 0.033719275, 0.018793147, 0.030556386, 0.04687525, 0.03280058, 0.021671003, 0.023689324, 0.026755687, 0.03448868, 0.025434887, 0.026299244, 0.01298314, 0.014473545, 0.019935384, 0.021212235, 0.017958367, 0.021222722, 0.02617419, 0.015030393, 0.033559065, 0.016576098, 0.01689038, 0.013024769, 0.01854034, 0.018383104, 0.021122023, 0.01318601, 0.017493065, 0.013293571, 0.012433861, 0.015387998, 0.020402068, 0.06322771, 0.08413126, 0.07659504, 0.10429013, 0.071861856, 0.06372749, 0.049199637, 0.061111156, 0.027214875, 0.049553636, 0.02517704, 0.026709052, 0.01653345, 0.030764496, 0.050152957, 0.04243481, 0.02268202, 0.022359544, 0.013157122, 0.021806004, 0.033813607, 0.020923868, 0.016900158, 0.01568619, 0.015195748, 0.023026727, 0.021578195, 0.020669466, 0.020892194, 0.013050724, 0.011317644, 0.014862892, 0.014241871, 0.011261787, 0.010904272, 0.010502763, 0.024316275, 0.014138937, 0.15492803, 0.17143174, 0.07222261, 0.084688745, 0.086011864, 0.07022494, 0.043494646, 0.031661704, 0.048272055, 0.0562274, 0.03916972, 0.036122452, 0.027823761, 0.021528436, 0.026936019, 0.02874888, 0.026733264, 0.03556208, 0.023864461, 0.041277982, 0.02952009, 0.018752687, 0.016068175, 0.020793397, 0.017929383, 0.016489401, 0.0204484, 0.018905737, 0.020386329, 0.010451091, 0.012482621, 0.0149695575, 0.01368883, 0.013456525, 0.016684216, 0.013062961, 0.013112753, 0.021833329, 0.039938163, 0.077359386, 0.09569017, 0.20860316, 0.16880032, 0.07097067, 0.06752583, 0.050945047, 0.03577922, 0.04458954, 0.049085747, 0.028630376, 0.035087854, 0.039172217, 0.027065521, 0.03049277, 0.019821867, 0.019314582, 0.026678117, 0.022945063, 0.025685454, 0.02283786, 0.018722028, 0.022107847, 0.0221212, 0.017112026, 0.019319786, 0.013338149, 0.014678751, 0.025225496, 0.019430524, 0.012904456, 0.013767841, 0.020915173, 0.013613155, 0.016629485, 0.013421233, 0.017869359, 0.031010365, 0.109177865, 0.18412675, 0.13798015, 0.1566332, 0.07111816, 0.07991556, 0.086184226, 0.041178197, 0.04243346, 0.01804736, 0.03694699, 0.032882255, 0.03905541, 0.022680622, 0.027883722, 0.038371753, 0.035049826, 0.020952635, 0.014551991, 0.025272064, 0.022275325, 0.022427998, 0.029482199, 0.012239824, 0.025857763, 0.027707819, 0.016561046, 0.017152851, 0.016757429, 0.01821787, 0.018578831, 0.016959237, 0.020754883, 0.016264794, 0.017251896, 0.01714837, 0.018766925, 0.049282566, 0.14894493, 0.07752139, 0.074343584, 0.07157061, 0.050254248, 0.04853381, 0.0539024, 0.041891325, 0.04368014, 0.022357157, 0.023282684, 0.020243103, 0.025325628, 0.033084214, 0.020536419, 0.019920688, 0.026813006, 0.020285154, 0.01846606, 0.022841189, 0.025401035, 0.018567504, 0.024056897, 0.0384351, 0.032314688, 0.020058237, 0.015319157, 0.020841166, 0.02581599, 0.017043902, 0.024160165, 0.0142662935, 0.014477585, 0.026054407, 0.015761986, 0.013497577, 0.013939001, 0.015951576, 0.011177933, 0.015410941, 0.013016419, 0.0137314685, 0.013879883, 0.118264526, 0.11669586, 0.22824413, 0.22855021, 0.11989818, 0.050388608, 0.062423833, 0.07288069, 0.06859663, 0.035197638, 0.03318646, 0.029103238, 0.035317, 0.03606557, 0.028753327, 0.01458608, 0.02733111, 0.024788126, 0.029208923, 0.0257885, 0.03289441, 0.021851476, 0.028474027, 0.027217265, 0.030076241, 0.017921057, 0.01747109, 0.017913045, 0.01637284, 0.02095741, 0.014899031, 0.020829171, 0.017324045, 0.01901168, 0.023330556, 0.026297206, 0.015398452, 0.015351327, 0.028087322, 0.034983747, 0.17854577, 0.09333678, 0.047585916, 0.079240814, 0.06604785, 0.041745514, 0.07608457, 0.056385547, 0.047455724, 0.01870881, 0.021474756, 0.028489226, 0.0322342, 0.026997337, 0.037745323, 0.026448255, 0.024139695, 0.020010935, 0.023260096, 0.017680738, 0.021503365, 0.023067176, 0.01593943, 0.02045597, 0.019638505, 0.01895191, 0.02069582, 0.019933585, 0.015216303, 0.014862856, 0.012665031, 0.017408943, 0.014951944, 0.013419145, 0.0197285, 0.020261373, 0.103968434, 0.15840511, 0.12500727, 0.15593348, 0.046172727, 0.06064063, 0.043477952, 0.08577198, 0.12417566, 0.027687734, 0.021928297, 0.023631053, 0.024799988, 0.021605188, 0.029913768, 0.033370268, 0.035838977, 0.02686677, 0.014280865, 0.025676891, 0.023634141, 0.020219767, 0.014064569, 0.013968802, 0.019233987, 0.016989399, 0.018297194, 0.019885277, 0.014959805, 0.011951646, 0.014436085, 0.015964527, 0.01572254, 0.016088104, 0.013942627, 0.014464576, 0.010349739, 0.016911918, 0.15537181, 0.15842782, 0.08541185, 0.09638946, 0.07350912, 0.060541958, 0.090493456, 0.11191293, 0.040954296, 0.040397357, 0.045232024, 0.042415425, 0.073196195, 0.0790337, 0.049631953, 0.067466445, 0.034709655, 0.029291028, 0.022926502, 0.035008043, 0.036462728, 0.03458977, 0.041280016, 0.027704611, 0.024967108, 0.028127626, 0.03446936, 0.024146063, 0.025848873, 0.022590593, 0.02504868, 0.03453639, 0.021145264, 0.02374854, 0.026259217, 0.019126588, 0.020108117, 0.028978026, 0.051856603, 0.07460218, 0.1377809, 0.16049884, 0.113512024, 0.05756611, 0.04643734, 0.049550485, 0.08571507, 0.053338043, 0.0553224, 0.059703533, 0.03324319, 0.05342367, 0.04230361, 0.043777913, 0.044411864, 0.033107124, 0.029685799, 0.036460504, 0.026466426, 0.037230015, 0.031497393, 0.025833085, 0.04334948, 0.042899884, 0.037865292, 0.031768013, 0.023807026, 0.024359548, 0.022252643, 0.02265051, 0.02721765, 0.024873814, 0.024658807, 0.027102932, 0.020325825, 0.026280276, 0.026288962, 0.022388274, 0.022880673, 0.025236042, 0.018600918, 0.07457898, 0.120940745, 0.08297624, 0.1739993, 0.072788894, 0.065048344, 0.057742465, 0.09010909, 0.048120808, 0.041410547, 0.038433924, 0.02954562, 0.05127694, 0.057518546, 0.06660932, 0.053734392, 0.06376723, 0.04740116, 0.041829918, 0.032403026, 0.03629404, 0.02924884, 0.034254, 0.024666235, 0.04330976, 0.02655133, 0.028765747, 0.017865593, 0.024371102, 0.02859173, 0.02468165, 0.02495571, 0.021085616, 0.025860129, 0.03149637, 0.033821832, 0.022359699, 0.024377054, 0.05222395, 0.09762908, 0.1551384, 0.13377303, 0.2072828, 0.05704878, 0.062282134, 0.08513123, 0.14639981, 0.08078315, 0.0674066, 0.04405633, 0.032008998, 0.038921908, 0.032572936, 0.045147687, 0.03929287, 0.036347065, 0.040725898, 0.037493344, 0.05078533, 0.035924245, 0.041117676, 0.0344315, 0.026784882, 0.036605183, 0.028788874, 0.031530835, 0.023885155, 0.033707514, 0.022767322, 0.027291294, 0.026449608, 0.029799938, 0.027151795, 0.019895539, 0.02019754, 0.024602834, 0.1093321, 0.079929546, 0.09388318, 0.16721678, 0.13124844, 0.08404523, 0.11037379, 0.095785834, 0.06084298, 0.07069741, 0.041538287, 0.031081146, 0.037698638, 0.04613859, 0.047359496, 0.04842632, 0.030655915, 0.029607277, 0.032305084, 0.031926226, 0.022949437, 0.030384779, 0.03462099, 0.033438157, 0.039204568, 0.025477143, 0.025685577, 0.021371115, 0.02577307, 0.025688235, 0.016939536, 0.034239724, 0.018719142, 0.027158521, 0.035146926, 0.018631699, 0.024169108, 0.031307656, 0.048965186, 0.060648844, 0.15630408, 0.06596166, 0.056290343, 0.05192252, 0.048505545, 0.04691888, 0.0812074, 0.071801074, 0.033179343, 0.03114162, 0.03337255, 0.030592566, 0.032770418, 0.06045799, 0.04637914, 0.03967435, 0.030919213, 0.028369738, 0.026763147, 0.03403107, 0.046045054, 0.02559659, 0.035259843, 0.035273783, 0.034957957, 0.025485065, 0.024281727, 0.023636352, 0.02977318, 0.023669308, 0.025832916, 0.02907801, 0.031592023, 0.0266843, 0.026599342, 0.028243557, 0.063331485, 0.13728145, 0.06645372, 0.20581248, 0.21498184, 0.09231826, 0.12569436, 0.03703631, 0.0892159, 0.04978209, 0.044607636, 0.03916498, 0.03926448, 0.026497213, 0.035032265, 0.03569326, 0.02869551, 0.03579897, 0.029024862, 0.025871823, 0.022122301, 0.02457145, 0.027210748, 0.029622292, 0.037523847, 0.044371817, 0.026739271, 0.020023294, 0.03127222, 0.0331278, 0.023975579, 0.025291817, 0.020916708, 0.019453919, 0.022787547, 0.023000527, 0.026124666, 0.020034881, 0.025270855, 0.021188617, 0.030893818, 0.018681064, 0.016550612, 0.018883592, 0.122469515, 0.18301283, 0.20991291, 0.15365775, 0.056896027, 0.0758389, 0.03201598, 0.032462474, 0.02954858, 0.03640521, 0.039553445, 0.03479924, 0.032965016, 0.03332086, 0.028882274, 0.030963892, 0.02572916, 0.031149155, 0.03229933, 0.04092705, 0.026224097, 0.022161251, 0.01574834, 0.028982794, 0.01762951, 0.017898407, 0.021037098, 0.024151213, 0.026862705, 0.02196981, 0.01910694, 0.019687092, 0.026129022, 0.01608152, 0.021733511, 0.018682616, 0.025733193, 0.022761205, 0.040896796, 0.103119574, 0.12602068, 0.07070166, 0.0562819, 0.09333253, 0.083682366, 0.050521202, 0.04790697, 0.05842263, 0.047658965, 0.03156385, 0.037717387, 0.041650545, 0.036572784, 0.03587402, 0.027672162, 0.038523976, 0.026117489, 0.04302657, 0.042866126, 0.020265302, 0.031805698, 0.03231638, 0.019478366, 0.02983975, 0.029885128, 0.021683091, 0.02695323, 0.023487763, 0.018655222, 0.025259757, 0.02080451, 0.02373292, 0.022429168, 0.022593357, 0.015337721, 0.017849015, 0.06828117, 0.09429858, 0.15448008, 0.15347306, 0.08557579, 0.076859966, 0.045648813, 0.089492045, 0.05841655, 0.047833994, 0.030996965, 0.030221716, 0.058645837, 0.04490047, 0.030947393, 0.02692488, 0.025607215, 0.020537153, 0.04773113, 0.021914795, 0.021108424, 0.026346674, 0.020479074, 0.018280627, 0.021267649, 0.023289405, 0.01879969, 0.014751285, 0.02018039, 0.013514457, 0.019716742, 0.014645689, 0.015395321, 0.01283335, 0.025421543, 0.018489998, 0.018591028, 0.016102305, 0.054910157, 0.061455093, 0.08899052, 0.08289927, 0.05699871, 0.049722612, 0.060164057, 0.049160678, 0.023086157, 0.0251501, 0.03853805, 0.02933853, 0.028811844, 0.031750947, 0.023844443, 0.041725796, 0.028979145, 0.01872215, 0.017631285, 0.021467311, 0.013114323, 0.02497591, 0.021134643, 0.02098476, 0.016981436, 0.02067342, 0.021041676, 0.020360082, 0.024549866, 0.01781748, 0.020978725, 0.01848104, 0.015338371, 0.024525575, 0.014781219, 0.012988624, 0.014430585, 0.011140155, 0.08879481, 0.045493253, 0.056294277, 0.071231626, 0.0732852, 0.041643556, 0.063392155, 0.11707414, 0.07255804, 0.076211974, 0.060548462, 0.022575365, 0.031940635, 0.03272791, 0.057765413, 0.020250417, 0.027496278, 0.028368313, 0.030814039, 0.023923201, 0.017548207, 0.023127064, 0.013099933, 0.025647158, 0.022862662, 0.021248171, 0.024057204, 0.017170008, 0.015288103, 0.012954231, 0.019936096, 0.023578605, 0.014531551, 0.017928958, 0.019804563, 0.015225288, 0.016542757, 0.012998818, 0.019166056, 0.010976314, 0.021938557, 0.013725758, 0.01864986, 0.033891056, 0.103334196, 0.19745614, 0.09022632, 0.15065965, 0.04586248, 0.034115132, 0.041624088, 0.026333313, 0.03034042, 0.04224788, 0.045524694, 0.05512388, 0.042312056, 0.02716484, 0.04541719, 0.044828527, 0.015016556, 0.018768433, 0.025815839, 0.028985692, 0.019905578, 0.023252236, 0.030034725, 0.030448986, 0.020294357, 0.017145492, 0.019762632, 0.022488063, 0.018019661, 0.01257338, 0.017876867, 0.017574318, 0.03120252, 0.020863008, 0.01718366, 0.011473764, 0.013785977, 0.050140005, 0.12526332, 0.3040299, 0.13734968, 0.10104057, 0.055582236, 0.041715726, 0.043464214, 0.043297607, 0.04256701, 0.030839914, 0.026438681, 0.022562286, 0.031265795, 0.022949914, 0.029856874, 0.017865498, 0.03071486, 0.022286266, 0.015564921, 0.029647514, 0.026017776, 0.019814178, 0.022224812, 0.026843313, 0.018401595, 0.01565947, 0.023084156, 0.024876656, 0.010098478, 0.013309541, 0.020069025, 0.014065377, 0.01360133, 0.020166362, 0.011203349, 0.011528018, 0.0121918125, 0.04729136, 0.04176131, 0.04781295, 0.05507514, 0.15204486, 0.073617525, 0.062875286, 0.02423604, 0.03885798, 0.02122583, 0.025648123, 0.03344231, 0.027865425, 0.023464473, 0.029602433, 0.0354789, 0.025131065, 0.030643081, 0.0164606, 0.021260083, 0.02033415, 0.016851075, 0.02087001, 0.011347261, 0.014402107, 0.027527194, 0.020545596, 0.017525323, 0.013768558, 0.015454101, 0.014635464, 0.014700656, 0.011748916, 0.014349041, 0.015508115, 0.012256451, 0.014852072, 0.016318664, 0.031998888, 0.07383666, 0.0603425, 0.05383245, 0.1357389, 0.08189594, 0.07599727, 0.08509985, 0.06324859, 0.028346227, 0.040456597, 0.096948594, 0.049063865, 0.03757083, 0.01516937, 0.03371474, 0.023472173, 0.0266751, 0.025306333, 0.0207444, 0.02157033, 0.016452415, 0.025885861, 0.01999119, 0.02018311, 0.012277475, 0.02577479, 0.020685976, 0.013934176, 0.014482823, 0.015702259, 0.017948586, 0.015165118, 0.011153647, 0.029431174, 0.013351445, 0.01907995, 0.014036997, 0.1269592, 0.050850082, 0.049546145, 0.10309247, 0.058607314, 0.042707007, 0.05216296, 0.035997532, 0.029955877, 0.050933104, 0.056141645, 0.021666244, 0.020993412, 0.03249841, 0.02388996, 0.039127912, 0.023651076, 0.013703259, 0.01805171, 0.0149559425, 0.020017974, 0.0230277, 0.020897161, 0.017741367, 0.01669386, 0.0147783095, 0.022064948, 0.024233818, 0.013606275, 0.014465456, 0.01749883, 0.018338539, 0.018121323, 0.017321132, 0.013986972, 0.018930122, 0.011922201, 0.018912816, 0.018405309, 0.020133788, 0.015904676, 0.0114479605, 0.020370862, 0.019043371, 0.10424889, 0.099719174, 0.19257693, 0.1800996, 0.030654581, 0.063675985, 0.0702774, 0.034406874, 0.054290105, 0.02837586, 0.03552175, 0.031378284, 0.018358951, 0.01914906, 0.019018427, 0.019933099, 0.025248073, 0.017439151, 0.016479239, 0.013727993, 0.019168084, 0.018750925, 0.02075354, 0.012472076, 0.018672278, 0.016758632, 0.019692136, 0.020221567, 0.016875453, 0.015037394, 0.014925147, 0.013706423, 0.01487308, 0.014451199, 0.015252483, 0.013750321, 0.018887248, 0.018893206, 0.06142733, 0.06701834, 0.064821795, 0.15187614, 0.07430488, 0.0604149, 0.077162676, 0.066943035, 0.039224833, 0.113675736, 0.06936789, 0.055627942, 0.037404876, 0.052139867, 0.03194784, 0.022143355, 0.03268789, 0.02895016, 0.044644825, 0.031834323, 0.0235671, 0.017118627, 0.01928557, 0.023416037, 0.023290865, 0.023918249, 0.021981055, 0.028226053, 0.016493505, 0.015367698, 0.015709406, 0.02539951, 0.015650243, 0.018514741, 0.0154528525, 0.014362636, 0.016432986, 0.02392284, 0.031667933, 0.065879814, 0.06689506, 0.05398543, 0.0315467, 0.04754379, 0.049000226, 0.055008896, 0.061116237, 0.123527706, 0.04083412, 0.028102173, 0.03526127, 0.045201704, 0.027983421, 0.0348189, 0.027156143, 0.024267754, 0.024355711, 0.02143459, 0.031949468, 0.022166986, 0.016162237, 0.018675203, 0.030067343, 0.020367673, 0.014442049, 0.013386665, 0.021076303, 0.011473742, 0.019934107, 0.015160435, 0.013667982, 0.01800434, 0.015436078, 0.015497609, 0.022108857, 0.012362848, 0.043558806, 0.07561551, 0.20050058, 0.08739081, 0.046154674, 0.050925065, 0.04148937, 0.038275268, 0.050889805, 0.04278978, 0.03657052, 0.044112653, 0.035603378, 0.043140486, 0.03356755, 0.025181953, 0.029044673, 0.017472655, 0.027760468, 0.034078944, 0.018579468, 0.019456737, 0.023952149, 0.014725277, 0.026805062, 0.03617247, 0.017291022, 0.018521065, 0.014204601, 0.017577806, 0.015091471, 0.022945955, 0.01680534, 0.015032172, 0.018573292, 0.01569349, 0.01593314, 0.02097641, 0.067099735, 0.15982543, 0.10996298, 0.21747842, 0.065647855, 0.03325887, 0.065076515, 0.05557403, 0.048986148, 0.042695973, 0.05460081, 0.037408587, 0.016035562, 0.051424287, 0.024603704, 0.02754006, 0.027903346, 0.015716828, 0.020976838, 0.019095985, 0.01576848, 0.015489376, 0.021379152, 0.02677592, 0.018198714, 0.024372963, 0.019615209, 0.02028242, 0.020226479, 0.013914924, 0.011827641, 0.022471827, 0.017208325, 0.017460924, 0.030814854, 0.017148424, 0.014644084, 0.011894007, 0.043795526, 0.065572575, 0.06796104, 0.061831605, 0.048410114, 0.03903975, 0.064427435, 0.1157688, 0.045374747, 0.039371938, 0.044432104, 0.025992151, 0.03564489, 0.02015026, 0.022971626, 0.029156571, 0.04310386, 0.031387687, 0.02321059, 0.019145103, 0.020955345, 0.021790328, 0.01612652, 0.009850171, 0.013095428, 0.015565125, 0.014897379, 0.020493105, 0.014566916, 0.01262648, 0.013321748, 0.0149489185, 0.015122649, 0.0115767345, 0.016383646, 0.012937823, 0.015629228, 0.0147546, 0.10529877, 0.07900862, 0.15508933, 0.094805144, 0.084792845, 0.047115713, 0.04354517, 0.05618319, 0.024819303, 0.028740915, 0.039433926, 0.02228622, 0.033365805, 0.020039504, 0.028635515, 0.017208189, 0.032491416, 0.017833466, 0.016347103, 0.022435218, 0.02454878, 0.016864592, 0.016937584, 0.018478757, 0.020219317, 0.028910985, 0.017868424, 0.025215397, 0.014855034, 0.021226352, 0.017023379, 0.020399759, 0.012537661, 0.013022375, 0.014397107, 0.014522871, 0.016841007, 0.01275804, 0.1523628, 0.1499721, 0.09081677, 0.11295896, 0.15315123, 0.061820175, 0.06016063, 0.048633706, 0.042775128, 0.08086941, 0.05024395, 0.025344785, 0.030528558, 0.049063027, 0.038716182, 0.038171556, 0.018441388, 0.02514657, 0.015331424, 0.01536207, 0.029393293, 0.017925262, 0.017915849, 0.015974596, 0.018300533, 0.0258718, 0.017116275, 0.014985417, 0.01567137, 0.022831095, 0.012353552, 0.016525773, 0.020201659, 0.018167678, 0.01783505, 0.012672117, 0.019805366, 0.01290718, 0.055644438, 0.095794894, 0.1321599, 0.1357227, 0.059289884, 0.06486074, 0.07782254, 0.06364588, 0.047137015, 0.049118865, 0.024058834, 0.032074578, 0.022341078, 0.030653184, 0.029662823, 0.027512055, 0.020299291, 0.02123444, 0.017816782, 0.025108108, 0.026497172, 0.024262486, 0.020378724, 0.021315282, 0.025400791, 0.019147284, 0.032137603, 0.012136959, 0.022567222, 0.019494854, 0.016456818, 0.021730764, 0.01572817, 0.018418394, 0.01703246, 0.013626028, 0.016749447, 0.012995971, 0.093428135, 0.06996716, 0.1150927, 0.052626837, 0.03322809, 0.040139075, 0.08438152, 0.03522966, 0.038115166, 0.02960034, 0.019947426, 0.0312554, 0.027014237, 0.026378646, 0.02519601, 0.016302854, 0.018276464, 0.020888278, 0.014878266, 0.018675923, 0.024258964, 0.017254272, 0.019805858, 0.019697383, 0.01680785, 0.023984823, 0.026343951, 0.027182862, 0.0139761865, 0.022809241, 0.023319697, 0.0179492, 0.015782485, 0.01610327, 0.01568961, 0.017805591, 0.014253681, 0.016159935, 0.016381964, 0.016805625, 0.0136648305, 0.014353383, 0.016170828, 0.04585325, 0.053382486, 0.08854152, 0.095582426, 0.07443909, 0.03412268, 0.09141673, 0.063748345, 0.039458893, 0.04865758, 0.027285777, 0.032217193, 0.030263519, 0.027494982, 0.021636482, 0.032182563, 0.02462782, 0.017419396, 0.020157218, 0.016902773, 0.035217285, 0.033969935, 0.026382364, 0.022799004, 0.019654676, 0.024380313, 0.0222388, 0.021835132, 0.019587375, 0.021493187, 0.01948859, 0.019095637, 0.015395128, 0.015489769, 0.013012528, 0.0122471275, 0.017555805, 0.012854561, 0.06199026, 0.10897312, 0.059684966, 0.13093098, 0.1378991, 0.13568409, 0.04384358, 0.033706203, 0.03559417, 0.027764881, 0.024172489, 0.016264128, 0.027964812, 0.030692302, 0.023542875, 0.030439863, 0.027831828, 0.021264965, 0.017948441, 0.026327545, 0.014061612, 0.021155054, 0.01642523, 0.016247313, 0.015175615, 0.014815055, 0.014704047, 0.014644733, 0.0144631155, 0.020426964, 0.016625326, 0.014315748, 0.018470902, 0.021197073, 0.016266894, 0.014126991, 0.013618279, 0.014196155, 0.09404513, 0.1023957, 0.06607267, 0.06977246, 0.08848222, 0.063242495, 0.059014566, 0.031158997, 0.039628148, 0.060026694, 0.03212616, 0.01871967, 0.019943882, 0.021488901, 0.035504643, 0.02303129, 0.019756213, 0.028644841, 0.024733335, 0.02143327, 0.017616231, 0.016347747, 0.014461341, 0.015690863, 0.015522441, 0.026768226, 0.025150752, 0.016832566, 0.017087078, 0.0133803645, 0.017779585, 0.016922984, 0.026307508, 0.028208073, 0.017416283, 0.013561059, 0.016407724, 0.017519621, 0.08122505, 0.0783454, 0.10355447, 0.07959214, 0.20996816, 0.10749239, 0.039000146, 0.047753662, 0.06475483, 0.058167428, 0.061752, 0.03739397, 0.04136466, 0.03091442, 0.039721876, 0.025068792, 0.02434042, 0.025174573, 0.027861115, 0.021194125, 0.024598414, 0.029750127, 0.016268006, 0.02565747, 0.022595063, 0.013185269, 0.017895056, 0.020248113, 0.021900143, 0.021975463, 0.013832748, 0.025552122, 0.013150017, 0.014488749, 0.019714346, 0.016570179, 0.0157615, 0.014454057, 0.029287979, 0.0488661, 0.19194694, 0.13084619, 0.11545318, 0.12401187, 0.048922088, 0.09033673, 0.117297746, 0.0816535, 0.036287975, 0.031127933, 0.028745854, 0.026108706, 0.030494282, 0.018915398, 0.01678808, 0.019966051, 0.031376537, 0.03443617, 0.022067089, 0.03665641, 0.024551334, 0.0215085, 0.020382967, 0.022353869, 0.020965425, 0.013269528, 0.01787515, 0.02421962, 0.017103648, 0.018836359, 0.022641324, 0.019490307, 0.01968723, 0.014648952, 0.021090847, 0.018871352, 0.013190888, 0.015641613, 0.0166888, 0.018155709, 0.015358122, 0.01797734, 0.06796279, 0.082968935, 0.307551, 0.14882123, 0.05105267, 0.03538523, 0.0574601, 0.06451725, 0.037217546, 0.04611494, 0.055179615, 0.025680635, 0.032015063, 0.021647545, 0.04259884, 0.030950483, 0.028109467, 0.030173158, 0.022308277, 0.016164288, 0.024777494, 0.025480017, 0.017150026, 0.01800177, 0.01904913, 0.015891066, 0.012738787, 0.016845688, 0.023742368, 0.024321318, 0.021444201, 0.018638378, 0.0143842995, 0.015981896, 0.014447407, 0.019168692, 0.013984499, 0.013260935, 0.05923283, 0.059279453, 0.12034827, 0.17311592, 0.14455137, 0.052656826, 0.032404702, 0.061265092, 0.06284584, 0.0473471, 0.022768648, 0.043952793, 0.04174907, 0.040267583, 0.026538307, 0.023885742, 0.01610799, 0.024725653, 0.021073679, 0.03762231, 0.02254043, 0.01861425, 0.017014274, 0.014548788, 0.023543643, 0.022116847, 0.027752312, 0.0137588065, 0.014697128, 0.016085243, 0.027254697, 0.014605932, 0.017417507, 0.024127778, 0.012292914, 0.015107639, 0.01321987, 0.0139280325, 0.061038088, 0.26916745, 0.12663448, 0.16992916, 0.039093543, 0.050910123, 0.038099054, 0.03270955, 0.0269214, 0.026006628, 0.02379804, 0.038534775, 0.026948892, 0.018699074, 0.042313594, 0.021420043, 0.017984685, 0.01855937, 0.016086591, 0.020180082, 0.021603296, 0.013725348, 0.02266929, 0.012180436, 0.015104846, 0.013108374, 0.016843649, 0.0152349, 0.01395883, 0.015051705, 0.016445264, 0.018260032, 0.011431879, 0.017663898, 0.018543962, 0.014137, 0.013259455, 0.020165825, 0.061545666, 0.0946729, 0.07585586, 0.10194538, 0.06369045, 0.03678367, 0.032197896, 0.029721564, 0.047880985, 0.041936766, 0.023707613, 0.06986368, 0.04670298, 0.028446442, 0.035302464, 0.021815827, 0.018600652, 0.0267357, 0.02017823, 0.016603947, 0.01972602, 0.01743247, 0.022746304, 0.01713192, 0.02280791, 0.020272594, 0.015780883, 0.024641905, 0.016923154, 0.018197399, 0.02045016, 0.019178662, 0.013513653, 0.014479979, 0.011635284, 0.01786694, 0.017777754, 0.01438863, 0.04228929, 0.08170408, 0.10341063, 0.05409028, 0.037455283, 0.06560105, 0.042842917, 0.03362666, 0.1156987, 0.05901321, 0.046063293, 0.029355632, 0.03523915, 0.05382989, 0.035583142, 0.035004422, 0.025308482, 0.023545517, 0.02190439, 0.0153429145, 0.015395087, 0.01629166, 0.02282552, 0.020964684, 0.018130109, 0.013788762, 0.023115927, 0.017809559, 0.018655075, 0.017180666, 0.01828722, 0.02057665, 0.012686778, 0.022951974, 0.018785754, 0.020869868, 0.017191065, 0.022580538, 0.015285991, 0.018096093, 0.018646901, 0.012066197, 0.017936323, 0.081762515, 0.24894717, 0.23044029, 0.19549534, 0.22746503, 0.09152083, 0.07944876, 0.048004393, 0.088985465, 0.052415226, 0.044103064, 0.031337034, 0.036128968, 0.07033363, 0.04992186, 0.025619304, 0.021703547, 0.026714073, 0.038645346, 0.02029208, 0.021928811, 0.02003711, 0.022647679, 0.018271986, 0.022707231, 0.019607842, 0.02392048, 0.016929245, 0.017196372, 0.016075721, 0.024610853, 0.016517436, 0.02462731, 0.024243847, 0.02440685, 0.018670976, 0.016249295, 0.017926969, 0.04878369, 0.11550099, 0.19418, 0.14133672, 0.11243701, 0.03898628, 0.03606376, 0.05300119, 0.07055106, 0.041279502, 0.042922128, 0.042265683, 0.038816325, 0.04639391, 0.04613443, 0.043913223, 0.035762537, 0.021837339, 0.04379192, 0.018763779, 0.030019525, 0.03019944, 0.025544696, 0.023932206, 0.023803761, 0.02188495, 0.017512662, 0.023554467, 0.01780301, 0.014062889, 0.018296614, 0.018383011, 0.014271186, 0.01795618, 0.01760482, 0.024079528, 0.01337747, 0.015913855, 0.040744454, 0.07838369, 0.13056087, 0.082831986, 0.094716705, 0.0369639, 0.0957777, 0.0615845, 0.050421108, 0.03616874, 0.038932983, 0.041195676, 0.032757036, 0.038182557, 0.035218608, 0.031689014, 0.02061374, 0.021963157, 0.01967985, 0.022799984, 0.027696675, 0.019725468, 0.018719265, 0.01802594, 0.03345592, 0.026391124, 0.022925287, 0.01587348, 0.027777158, 0.020186672, 0.016302917, 0.026463812, 0.023177084, 0.010805633, 0.017206937, 0.020987235, 0.022878742, 0.0145166265, 0.04481091, 0.12221753, 0.08958385, 0.11798131, 0.18392792, 0.06259081, 0.047851473, 0.034016352, 0.06983468, 0.04621446, 0.02934752, 0.028411571, 0.02746909, 0.028505798, 0.034034636, 0.03411974, 0.030781617, 0.02495575, 0.020002391, 0.018956875, 0.01851742, 0.016556943, 0.023419514, 0.026886366, 0.017836763, 0.020028427, 0.01627594, 0.017705685, 0.013338304, 0.020421414, 0.019166721, 0.021209873, 0.011654029, 0.019389126, 0.016667606, 0.016041301, 0.017320178, 0.013965501, 0.07863473, 0.09208367, 0.15004015, 0.10678664, 0.08793429, 0.059282467, 0.04505995, 0.026118724, 0.029466266, 0.037112165, 0.026475681, 0.039016753, 0.041154075, 0.053420234, 0.026281664, 0.015926942, 0.026741043, 0.021609949, 0.026926298, 0.02308836, 0.02221141, 0.016361898, 0.015180778, 0.01719389, 0.030781921, 0.023855535, 0.020666823, 0.02204315, 0.020805158, 0.01901942, 0.015358847, 0.01480729, 0.015485387, 0.012316695, 0.018591255, 0.011680144, 0.011722272, 0.013322287, 0.021655783, 0.01898329, 0.016105223, 0.01266731, 0.015698308, 0.014670322, 0.115441464, 0.114194416, 0.14842609, 0.16813096, 0.0907063, 0.0372049, 0.045465313, 0.055263337, 0.054961354, 0.069148354, 0.035543226, 0.03484256, 0.021858811, 0.030194167, 0.019126106, 0.031318042, 0.03862238, 0.020490572, 0.02185635, 0.019359197, 0.0152058005, 0.029943017, 0.023008818, 0.025510898, 0.018202657, 0.02114364, 0.019889176, 0.015040932, 0.022538964, 0.014375481, 0.012472193, 0.021167953, 0.019113952, 0.014142953, 0.016178405, 0.01704586, 0.013795509, 0.011693978, 0.04999157, 0.15608129, 0.14801379, 0.19626532, 0.086526655, 0.08026284, 0.04193716, 0.0594886, 0.094863474, 0.027183002, 0.03664958, 0.05977167, 0.04635524, 0.032841653, 0.042045638, 0.029860346, 0.039870247, 0.031324364, 0.029777642, 0.027822172, 0.021424785, 0.022864057, 0.024151413, 0.03086617, 0.024637576, 0.020550143, 0.014996749, 0.027983302, 0.015865974, 0.019226128, 0.012049812, 0.020706967, 0.017527854, 0.023188068, 0.02487326, 0.015864568, 0.01853707, 0.017438818, 0.07972792, 0.117640436, 0.12634476, 0.04617839, 0.03911215, 0.07091776, 0.054950576, 0.02599821, 0.0195066, 0.026761808, 0.025092492, 0.026614726, 0.026095094, 0.029825792, 0.022676086, 0.017577743, 0.016634548, 0.01600496, 0.020021752, 0.014562135, 0.025868831, 0.014654738, 0.016297298, 0.016881011, 0.015946204, 0.01717663, 0.01296579, 0.01442542, 0.013834699, 0.01226418, 0.0150139835, 0.021457776, 0.020565439, 0.017068272, 0.014187151, 0.024348263, 0.0138822915, 0.014378309, 0.03442353, 0.054594565, 0.06963525, 0.052868623, 0.038105577, 0.041665565, 0.048387922, 0.068156146, 0.041326538, 0.038496774, 0.035701547, 0.029400298, 0.07384042, 0.036811918, 0.05048714, 0.02942525, 0.025829531, 0.020909859, 0.025400775, 0.019876957, 0.021016775, 0.016927904, 0.026261454, 0.019922772, 0.012481364, 0.01383426, 0.0158602, 0.014478111, 0.016175242, 0.017795242, 0.016457526, 0.018455675, 0.014516294, 0.017963367, 0.023549391, 0.013173773, 0.015116971, 0.014289072, 0.052316114, 0.16351025, 0.21142754, 0.15137936, 0.06645694, 0.046358917, 0.042809416, 0.05376483, 0.049991634, 0.033950794, 0.021440376, 0.025181303, 0.02557425, 0.022192037, 0.019519435, 0.021629881, 0.026634956, 0.018481776, 0.019683678, 0.022365617, 0.020263359, 0.022092486, 0.024957163, 0.027885864, 0.024682127, 0.016964221, 0.010756019, 0.014992364, 0.01597438, 0.015541116, 0.016267452, 0.018709421, 0.018642342, 0.017605469, 0.010606786, 0.020212302, 0.01911943, 0.017414926, 0.016730146, 0.014804225, 0.015464845, 0.018434651, 0.0136188585, 0.045725476, 0.05038956, 0.14742914, 0.08626462, 0.14570923, 0.09126423, 0.06439259, 0.08647789, 0.12158509, 0.043017752, 0.031224443, 0.02031695, 0.022995861, 0.04508405, 0.031719975, 0.05421799, 0.02888062, 0.027825948, 0.027372884, 0.028766178, 0.029091459, 0.013576269, 0.015915874, 0.021377625, 0.01690269, 0.027529657, 0.018806979, 0.02357692, 0.021161657, 0.014125336, 0.012801714, 0.016479507, 0.03332253, 0.016598709, 0.016211377, 0.017598592, 0.015705, 0.018671054, 0.04627026, 0.09961403, 0.1335649, 0.05102716, 0.05536371, 0.09192333, 0.08399906, 0.05097313, 0.059714638, 0.07269676, 0.03776968, 0.037948404, 0.041561943, 0.036998898, 0.02568375, 0.026660882, 0.023928137, 0.022034999, 0.016606702, 0.037338763, 0.025882114, 0.014961165, 0.030780692, 0.02061903, 0.02050212, 0.013504979, 0.023171613, 0.018392097, 0.017191125, 0.0110544395, 0.020124683, 0.016465599, 0.023407504, 0.017220298, 0.01313332, 0.011210925, 0.015591392, 0.018548543, 0.038107775, 0.110899664, 0.12497779, 0.07239376, 0.12219407, 0.06804741, 0.036774576, 0.035843898, 0.043709278, 0.06162186, 0.02787601, 0.038683563, 0.04723359, 0.056059223, 0.041828576, 0.07753116, 0.029947726, 0.02609321, 0.02785246, 0.030372404, 0.019826362, 0.020436933, 0.025633141, 0.013084051, 0.019819174, 0.022868032, 0.019210087, 0.024320781, 0.020469982, 0.021491313, 0.013963589, 0.017175272, 0.018314146, 0.018217986, 0.019605909, 0.019373855, 0.01838782, 0.012988813, 0.03872754, 0.049198363, 0.088346265, 0.11140321, 0.09140659, 0.036137246, 0.024844553, 0.037656195, 0.076522015, 0.07891256, 0.04473976, 0.0353891, 0.02017489, 0.02296054, 0.030994456, 0.03146857, 0.028462965, 0.021523254, 0.026801223, 0.027443808, 0.017104551, 0.019104453, 0.019305747, 0.016512869, 0.021738961, 0.029665498, 0.015005595, 0.01658556, 0.022597354, 0.017271975, 0.02352404, 0.015607682, 0.016091013, 0.01479697, 0.017659245, 0.017913543, 0.020267444, 0.017258307, 0.060670633, 0.12546058, 0.07475808, 0.061829135, 0.2317664, 0.07194493, 0.06766666, 0.03422277, 0.046679825, 0.06823435, 0.040288, 0.03484294, 0.040736463, 0.031336103, 0.03474817, 0.020442633, 0.029535413, 0.026777193, 0.022889452, 0.011578072, 0.012571715, 0.02341893, 0.022526793, 0.021118397, 0.033363085, 0.018765599, 0.021519786, 0.013659871, 0.01912801, 0.016485412, 0.014902963, 0.010853136, 0.021347107, 0.018099831, 0.018620478, 0.023610447, 0.013539876, 0.009444216, 0.014953462, 0.017741261, 0.015578775, 0.014437201, 0.01650876, 0.012903995, 0.048759192, 0.062273577, 0.09318396, 0.052527454, 0.05569736, 0.05671897, 0.03929092, 0.03384715, 0.030902138, 0.05349959, 0.024444217, 0.02362219, 0.04757328, 0.037481952, 0.029765734, 0.018546486, 0.021202924, 0.02021798, 0.01494705, 0.022061111, 0.021026755, 0.015606417, 0.022288373, 0.020009179, 0.01866869, 0.014696149, 0.018231586, 0.017777294, 0.01273498, 0.014961582, 0.015265651, 0.014536537, 0.014729149, 0.011628432, 0.017859824, 0.022471273, 0.012917191, 0.013324614, 0.03706836, 0.10670596, 0.04366743, 0.07787956, 0.047969233, 0.03705188, 0.039269995, 0.029266283, 0.026527122, 0.029530222, 0.032494508, 0.027177801, 0.021791294, 0.02432824, 0.023972778, 0.025038606, 0.016934497, 0.021244915, 0.028946478, 0.020445788, 0.020812484, 0.016211446, 0.01454636, 0.017669016, 0.017909907, 0.015357063, 0.016682565, 0.016214436, 0.021459917, 0.014482743, 0.015219649, 0.020055408, 0.010386315, 0.015634572, 0.019949682, 0.015693368, 0.012704474, 0.018484928, 0.039840203, 0.12059289, 0.085007, 0.091458745, 0.031340953, 0.023585115, 0.041139834, 0.048940368, 0.04618018, 0.07736633, 0.026765792, 0.036428116, 0.027067909, 0.028861087, 0.030047907, 0.018665865, 0.023562348, 0.019416183, 0.03593698, 0.032056097, 0.024618072, 0.014552334, 0.014809728, 0.019034533, 0.018960508, 0.018352333, 0.016545786, 0.016369984, 0.013257428, 0.0137210665, 0.013057707, 0.022805633, 0.019093817, 0.012980859, 0.015937528, 0.015486609, 0.011791799, 0.014398896, 0.06964608, 0.2142934, 0.19965568, 0.07089448, 0.04043936, 0.051058397, 0.061628845, 0.030921886, 0.038997423, 0.069558084, 0.037129365, 0.036554303, 0.043212846, 0.032903567, 0.030798292, 0.024813756, 0.019120423, 0.025467811, 0.018979035, 0.02383545, 0.03174278, 0.020706985, 0.021655647, 0.02475369, 0.014845591, 0.01608186, 0.015471747, 0.01557139, 0.017601484, 0.019138988, 0.021574512, 0.014654073, 0.01976537, 0.018542526, 0.010432984, 0.012176766, 0.020127932, 0.022038136, 0.04632384, 0.074273236, 0.06583209, 0.063531555, 0.07920613, 0.04603423, 0.05480173, 0.080170296, 0.035466053, 0.048366062, 0.034537178, 0.036060486, 0.040182315, 0.033616405, 0.033784326, 0.02387857, 0.022716856, 0.025714861, 0.029732648, 0.024595581, 0.019676886, 0.017191624, 0.018101059, 0.023694562, 0.018994702, 0.019499063, 0.02347745, 0.018754426, 0.015466562, 0.025260488, 0.019837672, 0.021272238, 0.011938484, 0.011751222, 0.014004804, 0.017678538, 0.015500034, 0.029717932, 0.011618111, 0.014997058, 0.022057211, 0.017165275, 0.009886417, 0.05935878, 0.1528569, 0.151281, 0.09240338, 0.14452831, 0.08084654, 0.054161858, 0.10646868, 0.111131966, 0.07101686, 0.08634684, 0.10471753, 0.040578764, 0.033221908, 0.028260535, 0.0882619, 0.035048183, 0.02355136, 0.034002174, 0.024679517, 0.018270345, 0.02425111, 0.03399145, 0.018873015, 0.024981374, 0.015606268, 0.020847702, 0.01937835, 0.01727917, 0.024128838, 0.018936092, 0.019699724, 0.02438212, 0.016175963, 0.016217954, 0.023902709, 0.016490674, 0.021124266, 0.052589573, 0.039359104, 0.08276253, 0.06394214, 0.10582623, 0.117346525, 0.046786413, 0.048637435, 0.032601308, 0.02351822, 0.022325555, 0.029765174, 0.027946986, 0.020745747, 0.02552448, 0.033163518, 0.016626127, 0.027007489, 0.027477933, 0.017190566, 0.021054758, 0.01699452, 0.016342575, 0.018210998, 0.017512504, 0.019973114, 0.020761032, 0.019775506, 0.014405547, 0.013723491, 0.015890377, 0.019753909, 0.010482766, 0.016802704, 0.0134302145, 0.012232114, 0.011862186, 0.01674897, 0.042934265, 0.12874602, 0.06621647, 0.12983327, 0.095568754, 0.036682278, 0.029782427, 0.031090027, 0.037618604, 0.028426573, 0.034628246, 0.032825604, 0.018708115, 0.035291728, 0.02783736, 0.026948296, 0.028637176, 0.02354658, 0.020342948, 0.021141764, 0.014978838, 0.016417949, 0.015545863, 0.01806368, 0.018347392, 0.019471386, 0.019529194, 0.017363, 0.020818902, 0.02754442, 0.01744276, 0.023035267, 0.01628784, 0.018848885, 0.02110771, 0.011652062, 0.014158859, 0.021822432, 0.094379805, 0.061015397, 0.05126477, 0.12633899, 0.12981111, 0.0704179, 0.035926897, 0.032273278, 0.073588885, 0.04019802, 0.044806175, 0.043547835, 0.03068902, 0.024590513, 0.03494975, 0.03604306, 0.029533988, 0.01939178, 0.02197283, 0.024419423, 0.02353482, 0.020103106, 0.015373606, 0.02110437, 0.013994283, 0.017664324, 0.016907811, 0.021462077, 0.016107142, 0.021573221, 0.01599436, 0.015708817, 0.019816978, 0.01568562, 0.014674572, 0.016770508, 0.013113576, 0.013533161, 0.04620268, 0.103338376, 0.06388262, 0.075441755, 0.06823708, 0.037662264, 0.04321405, 0.03402193, 0.025646191, 0.03563879, 0.023510136, 0.039370507, 0.031524394, 0.024211051, 0.034745075, 0.027790647, 0.022876889, 0.020910496, 0.028527869, 0.023584254, 0.020347988, 0.019519066, 0.021266824, 0.020886308, 0.021610335, 0.02550675, 0.016846245, 0.025269132, 0.020904493, 0.016034264, 0.0139892055, 0.016796345, 0.013538775, 0.013754927, 0.015858449, 0.014641915, 0.013355221, 0.016266325, 0.015528383, 0.020753426, 0.019750291, 0.015484151, 0.011579476, 0.0131, 0.13777427, 0.12845661, 0.085201144, 0.064445674, 0.07864372, 0.059897963, 0.07228631, 0.03570179, 0.082826376, 0.10452821, 0.04645731, 0.026370779, 0.022124305, 0.029787866, 0.039621715, 0.0152776055, 0.025552932, 0.021044953, 0.02305212, 0.021835858, 0.022701235, 0.02462546, 0.01861986, 0.012700903, 0.024883056, 0.029286046, 0.01699204, 0.019296704, 0.01571621, 0.017803147, 0.017622113, 0.014905049, 0.016881898, 0.015783638, 0.02071661, 0.015708985, 0.02340974, 0.023319552, 0.1483, 0.13360032, 0.077332415, 0.07499784, 0.037991956, 0.052047357, 0.03716056, 0.11970375, 0.059784524, 0.043433376, 0.03868953, 0.028101629, 0.027050192, 0.032054674, 0.020733211, 0.02320702, 0.02281468, 0.013708154, 0.020568382, 0.024434559, 0.023152808, 0.01902389, 0.017646438, 0.016396245, 0.019192612, 0.022467148, 0.017954513, 0.011301162, 0.019019052, 0.016815167, 0.016770907, 0.012423038, 0.025492676, 0.014330125, 0.015827892, 0.017134422, 0.0138989985, 0.016282784, 0.058718048, 0.087997116, 0.092742965, 0.20564143, 0.046970453, 0.049194403, 0.04978044, 0.043788165, 0.05851941, 0.05788923, 0.030822236, 0.03075034, 0.03677127, 0.023491375, 0.038678285, 0.03643981, 0.04268564, 0.015617047, 0.018279202, 0.025256248, 0.01839023, 0.0165268, 0.023285264, 0.019264875, 0.020398574, 0.018274615, 0.014408245, 0.01665336, 0.016128762, 0.010368358, 0.01297306, 0.011672954, 0.015720045, 0.01937199, 0.010346966, 0.021582967, 0.012333198, 0.0116473595, 0.11878916, 0.12829585, 0.060931344, 0.0776179, 0.064602755, 0.05037732, 0.051829964, 0.04801942, 0.026710821, 0.035943896, 0.03336915, 0.029798975, 0.03872788, 0.024116201, 0.029779458, 0.035142314, 0.023400037, 0.019288985, 0.018303081, 0.015502972, 0.02765925, 0.018914225, 0.022722922, 0.019466283, 0.022951115, 0.017270243, 0.01921797, 0.011230374, 0.024808213, 0.015373212, 0.01772019, 0.024882719, 0.016878951, 0.014127883, 0.015251093, 0.010258617, 0.016815469, 0.018157154, 0.04928579, 0.060097005, 0.12970896, 0.16885352, 0.07943535, 0.03905046, 0.06279528, 0.03191631, 0.04951464, 0.04485185, 0.03980048, 0.018801752, 0.025400326, 0.03929933, 0.019636577, 0.022387387, 0.026324337, 0.019294059, 0.029187955, 0.035962936, 0.024647579, 0.019578962, 0.0205409, 0.022205666, 0.020473516, 0.030133087, 0.021449678, 0.018919101, 0.019515797, 0.022209078, 0.012539926, 0.017225025, 0.01556173, 0.017338822, 0.01507228, 0.0129408, 0.015878752, 0.014109497, 0.013326918, 0.013931203, 0.012189925, 0.016640738, 0.012504326, 0.034231335, 0.043831673, 0.09235396, 0.0651893, 0.07437388, 0.034161955, 0.06759929, 0.04372884, 0.063294575, 0.04442276, 0.045275997, 0.030545784, 0.047474843, 0.036256887, 0.022912236, 0.02022788, 0.022448305, 0.017423095, 0.014894428, 0.024793958, 0.024915917, 0.02301046, 0.020453656, 0.018928261, 0.015354464, 0.017751735, 0.022184685, 0.0146032935, 0.015685687, 0.01589625, 0.012624197, 0.024151428, 0.02812233, 0.019831875, 0.021651596, 0.012236625, 0.011652987, 0.015483168, 0.08902184, 0.10433902, 0.08705623, 0.089290045, 0.06442861, 0.034993477, 0.044179, 0.055457402, 0.04092109, 0.04873666, 0.059506405, 0.06810453, 0.042230785, 0.030540517, 0.03906704, 0.046587214, 0.027653402, 0.020735096, 0.021621669, 0.02378608, 0.027056694, 0.022149278, 0.014988118, 0.022233054, 0.018329417, 0.018736325, 0.015357428, 0.021292841, 0.015508347, 0.01596566, 0.019781448, 0.01519896, 0.01711544, 0.0190711, 0.015558985, 0.01400526, 0.017812818, 0.01346653, 0.036103506, 0.10361408, 0.08736178, 0.32260883, 0.27780172, 0.085295565, 0.10937718, 0.04396494, 0.048268717, 0.041179985, 0.032874566, 0.027142217, 0.02378091, 0.02222167, 0.030075168, 0.026334932, 0.031831276, 0.030809766, 0.024257913, 0.02633133, 0.023973713, 0.014223135, 0.015734717, 0.016852846, 0.023283677, 0.018690621, 0.020117937, 0.016134663, 0.019971117, 0.014306399, 0.018556934, 0.014800918, 0.017552463, 0.014572959, 0.016519882, 0.017079946, 0.014205231, 0.019565264, 0.04435791, 0.10236934, 0.10223929, 0.08031574, 0.061631598, 0.04131664, 0.025221037, 0.043509975, 0.055718962, 0.06498109, 0.035444748, 0.024716353, 0.023914535, 0.040163543, 0.027423495, 0.02389428, 0.037173085, 0.0367725, 0.026792701, 0.03621032, 0.017521089, 0.024704989, 0.018733326, 0.023441162, 0.015425485, 0.016445968, 0.01413473, 0.018737918, 0.012417045, 0.019142633, 0.01991702, 0.021063471, 0.020528922, 0.013346227, 0.02221888, 0.017969491, 0.013561277, 0.016428992, 0.07070719, 0.09730667, 0.18832378, 0.11524681, 0.04857834, 0.030163119, 0.036613684, 0.048804175, 0.049998462, 0.029733296, 0.046572775, 0.045076847, 0.025880354, 0.026584195, 0.024749573, 0.018724779, 0.016141033, 0.032653287, 0.016580014, 0.029598063, 0.024585202, 0.022197463, 0.021028448, 0.014536244, 0.020909464, 0.019656911, 0.020384464, 0.018239984, 0.03244428, 0.016759288, 0.015569138, 0.01723197, 0.010621209, 0.016052907, 0.021037424, 0.016615752, 0.015238608, 0.023606107, 0.022354987, 0.018664107, 0.015106885, 0.040687658, 0.017855057, 0.013681056, 0.04426493, 0.058884244, 0.08186329, 0.072352216, 0.05195862, 0.053432327, 0.034419335, 0.05362458, 0.050135903, 0.04007206, 0.022450194, 0.045700695, 0.048019167, 0.04512677, 0.027572092, 0.027290985, 0.01781202, 0.016479872, 0.014379884, 0.019317849, 0.014242594, 0.022344554, 0.01879639, 0.019132027, 0.018004712, 0.013153596, 0.019394921, 0.017790925, 0.02047579, 0.013697358, 0.019627724, 0.019931618, 0.01793323, 0.009726709, 0.01190091, 0.015119803, 0.017775044, 0.019247208, 0.06850432, 0.109443024, 0.058040895, 0.05676595, 0.029354755, 0.06428343, 0.05436026, 0.054965947, 0.04807109, 0.0384608, 0.042092077, 0.030698579, 0.028030658, 0.032055657, 0.028906474, 0.01809379, 0.023587799, 0.021196838, 0.017248493, 0.029739732, 0.018320503, 0.019626984, 0.023915062, 0.0217588, 0.020699335, 0.013849926, 0.019967731, 0.011617113, 0.013887976, 0.015862841, 0.017377296, 0.015580036, 0.013548705, 0.010087445, 0.021068886, 0.0129415365, 0.010828009, 0.01609485, 0.10656197, 0.11101893, 0.06767913, 0.08614968, 0.10108567, 0.031184977, 0.030589804, 0.094017416, 0.032393914, 0.028263642, 0.024782384, 0.022771066, 0.038350172, 0.026698178, 0.050167494, 0.05979006, 0.028721489, 0.020113248, 0.02072885, 0.021740254, 0.032079197, 0.025532657, 0.0230841, 0.015829096, 0.015518543, 0.025950938, 0.016445806, 0.02313314, 0.018756352, 0.010702046, 0.02665887, 0.015842492, 0.014895707, 0.0146694, 0.023036556, 0.014836662, 0.016679123, 0.011258901, 0.11110425, 0.12819272, 0.098859735, 0.17501463, 0.051840696, 0.03667199, 0.103969835, 0.07796404, 0.05504777, 0.13473907, 0.05634174, 0.035978712, 0.05405093, 0.024916098, 0.03354378, 0.03761829, 0.02944916, 0.018793525, 0.016440077, 0.016802501, 0.024953859, 0.025601806, 0.018366553, 0.021983916, 0.024350075, 0.025508907, 0.01988611, 0.012602018, 0.021494595, 0.018909395, 0.012330632, 0.015609004, 0.014679349, 0.015176974, 0.012463667, 0.019007728, 0.012779003, 0.018136436, 0.120350614, 0.047501035, 0.08687823, 0.25514627, 0.10045738, 0.0448846, 0.06805321, 0.06876495, 0.039438426, 0.053871583, 0.044291005, 0.038860776, 0.023790505, 0.048404884, 0.042584922, 0.021847399, 0.017540958, 0.03516397, 0.02103769, 0.02467379, 0.016711783, 0.021870712, 0.01833206, 0.019243706, 0.02209691, 0.02118098, 0.015580331, 0.013355997, 0.022613728, 0.01980858, 0.013301073, 0.015677694, 0.020450022, 0.021981105, 0.016249847, 0.022192912, 0.014534332, 0.013886901, 0.05021025, 0.07139512, 0.047053836, 0.07925554, 0.0416529, 0.03183794, 0.037222, 0.026571475, 0.030951105, 0.036707025, 0.029778833, 0.030262088, 0.015488358, 0.038423486, 0.029831434, 0.02974561, 0.02342762, 0.019146437, 0.017060386, 0.01460626, 0.022831166, 0.014383199, 0.019673206, 0.015138844, 0.02004693, 0.013828303, 0.013487063, 0.014162428, 0.014538848, 0.015022762, 0.013155368, 0.017562088, 0.015760409, 0.014863645, 0.015032342, 0.016965572, 0.014000402, 0.009199308, 0.12040876, 0.08688888, 0.060823873, 0.06291418, 0.06835141, 0.05750403, 0.047786433, 0.028165735, 0.03210141, 0.03737454, 0.036336843, 0.03554609, 0.032716278, 0.047580253, 0.038843762, 0.02633037, 0.027439456, 0.015819764, 0.019393172, 0.01913245, 0.026454994, 0.027017085, 0.018076586, 0.016341576, 0.02035813, 0.012914281, 0.014685257, 0.020567467, 0.014557463, 0.013848162, 0.013880141, 0.016439313, 0.020311056, 0.017510042, 0.014577489, 0.01686571, 0.014227993, 0.015402438, 0.063166596, 0.060912196, 0.042230126, 0.044575017, 0.040806144, 0.036148347, 0.06100802, 0.049854904, 0.065740466, 0.0401991, 0.030718317, 0.030656334, 0.026590247, 0.020203248, 0.024409294, 0.017316805, 0.023906086, 0.013219476, 0.021819603, 0.023828793, 0.021116192, 0.019445814, 0.015880162, 0.018305596, 0.012036297, 0.018052032, 0.016801188, 0.020018453, 0.023805369, 0.015804395, 0.0176945, 0.015834425, 0.015850376, 0.0119571695, 0.015541119, 0.0136520155, 0.015819356, 0.010953114, 0.044144463, 0.077155195, 0.053628895, 0.057835728, 0.06520835, 0.0515659, 0.05261789, 0.06124999, 0.034284044, 0.039306007, 0.028545907, 0.026741322, 0.024817491, 0.03476575, 0.05199157, 0.031765636, 0.021768995, 0.029665593, 0.019338166, 0.020204723, 0.016138768, 0.023412572, 0.018704059, 0.018597098, 0.014672683, 0.021426197, 0.014167721, 0.017578483, 0.016184522, 0.01835742, 0.013813445, 0.01440777, 0.01872892, 0.017706277, 0.014524992, 0.018367564, 0.014534054, 0.016780248, 0.038530577, 0.061589573, 0.08569626, 0.05771892, 0.13278838, 0.073768705, 0.059305567, 0.05805407, 0.06007545, 0.041883003, 0.03691125, 0.03941868, 0.036531698, 0.033706937, 0.032939076, 0.030084541, 0.022027193, 0.020968681, 0.02696752, 0.019116374, 0.017460732, 0.013507765, 0.016324505, 0.016063865]

Ascent losses:

{26: -0.4419129, 66: -0.018962273, 105: -0.008222019, 144: -0.005968905, 183: -0.0056813606, 222: -0.0055320887, 261: -0.0044476916, 300: -0.0056763375, 339: -0.0040263096, 378: -0.0047549424, 423: -0.004533042, 462: -0.0045416593, 501: -0.0036645061, 540: -0.0047086664, 579: -0.005218214, 623: -0.004652526, 662: -0.0040281373, 701: -0.003548408, 740: -0.0042321975, 779: -0.005239027, 824: -0.003692934, 863: -0.003151801, 902: -0.0025126934, 941: -0.0034617444, 980: -0.003966071, 1019: -0.005205455, 1058: -0.0028110854, 1097: -0.0035221125, 1136: -0.0027321277, 1175: -0.0027727308, 1219: -0.003114205, 1258: -0.0034045577, 1297: -0.0029572765, 1336: -0.0020815511, 1375: -0.0020045766, 1420: -0.0025675728, 1459: -0.0025857699, 1498: -0.0020642928, 1537: -0.003367097, 1576: -0.0018877144, 1620: -0.0022024177, 1659: -0.0016016728, 1698: -0.0030265087, 1737: -0.002095236, 1776: -0.0036309825, 1821: -0.0028238527, 1860: -0.001823856, 1899: -0.0017772618, 1938: -0.002059962, 1977: -0.0020330355, 2021: -0.0016876222, 2060: -0.0014774279, 2099: -0.0017701713, 2138: -0.0024985801, 2177: -0.0014759545, 2222: -0.0019497058, 2261: -0.0014681538, 2300: -0.0014572762, 2339: -0.0020879686, 2378: -0.0016739595, 2422: -0.0019233223, 2461: -0.0015870774, 2500: -0.0017200662, 2539: -0.0022187163, 2578: -0.0015626466, 2623: -0.0016868211, 2662: -0.0012330504, 2701: -0.0012923208, 2740: -0.0014051924, 2779: -0.001606038, 2823: -0.0020618679, 2862: -0.0013820899, 2901: -0.0020818207, 2940: -0.001537433, 2979: -0.0016423194, 3024: -0.0010527562, 3063: -0.002061826, 3102: -0.0014422924, 3141: -0.0010559956, 3180: -0.0011854707, 3219: -0.0012061731, 3258: -0.0010927238, 3297: -0.0011152112, 3336: -0.001436391, 3375: -0.001693549, 3419: -0.0010912787, 3458: -0.0010244509, 3497: -0.0016163621, 3536: -0.001394186, 3575: -0.0016158598, 3620: -0.00090146397, 3659: -0.00095876073, 3698: -0.0009918499, 3737: -0.0012507115, 3776: -0.0019552836, 3820: -0.00118609, 3859: -0.0019985444, 3898: -0.0011103355, 3937: -0.0011264187, 3976: -0.0016575645, 4021: -0.00087362726, 4060: -0.0010762519, 4099: -0.00093991123, 4138: -0.0010992956, 4177: -0.0014218318, 4221: -0.0013683069, 4260: -0.0013275802, 4299: -0.0019243362, 4338: -0.0015067776, 4377: -0.0019954033, 4422: -0.0012939471, 4461: -0.0014061708, 4500: -0.0012012264, 4539: -0.000987724, 4578: -0.0011951227, 4622: -0.0014962689, 4661: -0.0013795554, 4700: -0.0020123299, 4739: -0.0007618395, 4778: -0.0008425751, 4823: -0.0013572588, 4862: -0.0011604595, 4901: -0.0013298155, 4940: -0.0010680801, 4979: -0.001036449, 5023: -0.0014913087, 5062: -0.0017654343, 5101: -0.0014619819, 5140: -0.0014499974, 5179: -0.001234176, 5224: -0.00098555, 5263: -0.0011862941, 5302: -0.0011122667, 5341: -0.001581953, 5380: -0.0012092538, 5419: -0.0011549057, 5458: -0.0016804137, 5497: -0.0009899184, 5536: -0.0010541854, 5575: -0.0009858301, 5619: -0.0015200634, 5658: -0.001133857, 5697: -0.0011332752, 5736: -0.0015401151, 5775: -0.0013425011, 5820: -0.00083386136, 5859: -0.000690564, 5898: -0.0015792703, 5937: -0.0012564618, 5976: -0.0009256107, 6020: -0.0009874387, 6059: -0.0012397949, 6098: -0.0015398641, 6137: -0.0009490011, 6176: -0.0009923716, 6221: -0.00094417296, 6260: -0.0008094506, 6299: -0.0011072366, 6338: -0.00080056506, 6377: -0.0010525492, 6421: -0.0012119288, 6460: -0.00049747044, 6499: -0.00063493056, 6538: -0.0006254262, 6577: -0.00087379507, 6622: -0.00072295347, 6661: -0.0012956973, 6700: -0.00066651514, 6739: -0.0008742677, 6778: -0.001620077, 6822: -0.001077347, 6861: -0.0010573058, 6900: -0.0011643362, 6939: -0.0012591997, 6978: -0.0013651287, 7023: -0.0009809431, 7062: -0.0009206539, 7101: -0.00078000035, 7140: -0.00097583176, 7179: -0.001197665, 7223: -0.00062445906, 7262: -0.0009419124, 7301: -0.00072741153, 7340: -0.00082658534, 7379: -0.00054521253, 7424: -0.0006155687, 7463: -0.0009140613, 7502: -0.00070394005, 7541: -0.00056399446, 7580: -0.0005528401, 7619: -0.0009086661, 7658: -0.0005228999, 7697: -0.0005595135, 7736: -0.00078278506, 7775: -0.0009110123, 7819: -0.00066764344, 7858: -0.0011201068, 7897: -0.0007385581, 7936: -0.0007898999, 7975: -0.0006700386, 8020: -0.00060035946, 8059: -0.0010218235, 8098: -0.00062079995, 8137: -0.0009541837, 8176: -0.00056466117, 8220: -0.0006510104, 8259: -0.00054080336, 8298: -0.0006284808, 8337: -0.00077299617, 8376: -0.00076830713, 8421: -0.00059048354, 8460: -0.0006249269, 8499: -0.0007694979, 8538: -0.000828553, 8577: -0.00086563494, 8621: -0.0007112894, 8660: -0.00066611293, 8699: -0.0007778378, 8738: -0.0009766939, 8777: -0.0007172098, 8822: -0.0007495587, 8861: -0.001004605, 8900: -0.0005780112, 8939: -0.0006875775, 8978: -0.0007061467, 9022: -0.0006495023, 9061: -0.0008764704, 9100: -0.0008141456, 9139: -0.0009673904, 9178: -0.00087761367, 9223: -0.0010047023, 9262: -0.00078201777, 9301: -0.0009317683, 9340: -0.0007540612, 9379: -0.0006872548, 9423: -0.0008681535, 9462: -0.0006888004, 9501: -0.0006082006, 9540: -0.0006326916, 9579: -0.000677516, 9624: -0.0005739436, 9663: -0.00094694, 9702: -0.00094017544, 9741: -0.00048601712, 9780: -0.00069519586, 9819: -0.0005579289, 9858: -0.0010981953, 9897: -0.00076199835, 9936: -0.0008103334, 9975: -0.001066204}
Importing data files...
['mm1d_1.csv', 'mm1d_4.csv', 'mm1d_0.csv', 'mm1d_2.csv', 'mm1d_3.csv', 'mm1d_5.csv']
Splitting data into training and test sets with a ratio of: 0.2
Total number of training samples is 38400
Total number of test samples is 9600
Length of an output spectrum is 300
Generating torch datasets
training using gradient ascent
Starting training process
Doing Evaluation on the model now
This is Epoch 0, training loss 7.36013, validation loss 6.43435
Resetting learning rate to 0.01000
Doing Evaluation on the model now
This is Epoch 10, training loss 0.28342, validation loss 0.31009
Epoch    32: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 20, training loss 0.19433, validation loss 0.20598
Epoch    46: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 30, training loss 0.08911, validation loss 0.09544
Epoch    62: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 40, training loss 0.06925, validation loss 0.06710
Epoch    84: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 42
Mean train loss for ascent epoch 43: -0.02255186438560486
Mean eval for ascent epoch 43: 0.08048870414495468
Epoch    98: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 50, training loss 0.95359, validation loss 1.91978
Epoch   109: reducing learning rate of group 0 to 2.5000e-03.
Epoch   120: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 60, training loss 0.34530, validation loss 0.73882
Epoch   131: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 70, training loss 0.05391, validation loss 0.06752
Epoch   142: reducing learning rate of group 0 to 3.1250e-04.
Epoch   153: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 80, training loss 0.03445, validation loss 0.03592
Epoch   164: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 82
Mean train loss for ascent epoch 83: -0.021324124187231064
Mean eval for ascent epoch 83: 0.03513345122337341
Epoch   175: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 90, training loss 0.14336, validation loss 0.13744
Epoch   186: reducing learning rate of group 0 to 2.5000e-03.
Epoch   197: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 100, training loss 0.09462, validation loss 0.08930
Epoch   208: reducing learning rate of group 0 to 6.2500e-04.
Epoch   219: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 110, training loss 0.05929, validation loss 0.04338
Epoch   230: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 120, training loss 0.03794, validation loss 0.02387
Epoch   241: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 121
Mean train loss for ascent epoch 122: -0.01831977069377899
Mean eval for ascent epoch 122: 0.036576494574546814
Epoch   252: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 130, training loss 0.51631, validation loss 0.15929
Epoch   263: reducing learning rate of group 0 to 2.5000e-03.
Epoch   274: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 140, training loss 0.09728, validation loss 0.05028
Epoch   285: reducing learning rate of group 0 to 6.2500e-04.
Epoch   296: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 150, training loss 0.05055, validation loss 0.02683
Epoch   307: reducing learning rate of group 0 to 1.5625e-04.
Epoch   318: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 160
Doing Evaluation on the model now
This is Epoch 160, training loss 0.03063, validation loss 0.03737
Mean train loss for ascent epoch 161: -0.01967974565923214
Mean eval for ascent epoch 161: 0.029375309124588966
Epoch   329: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 170, training loss 0.30988, validation loss 0.26826
Epoch   340: reducing learning rate of group 0 to 2.5000e-03.
Epoch   351: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 180, training loss 0.10349, validation loss 0.07080
Epoch   362: reducing learning rate of group 0 to 6.2500e-04.
Epoch   373: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 190, training loss 0.02533, validation loss 0.02163
Epoch   384: reducing learning rate of group 0 to 1.5625e-04.
Epoch   395: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 199
Mean train loss for ascent epoch 200: -0.012165432795882225
Mean eval for ascent epoch 200: 0.019398046657443047
Doing Evaluation on the model now
This is Epoch 200, training loss 0.01940, validation loss 0.01899
Resetting learning rate to 0.01000
Epoch   406: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 210, training loss 0.03114, validation loss 0.04487
Epoch   417: reducing learning rate of group 0 to 2.5000e-04.
Epoch   428: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 220, training loss 0.01806, validation loss 0.01800
Epoch   439: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 222
Mean train loss for ascent epoch 223: -0.018898889422416687
Mean eval for ascent epoch 223: 0.01861974038183689
Epoch   450: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 230, training loss 0.08796, validation loss 0.06950
Epoch   461: reducing learning rate of group 0 to 2.5000e-03.
Epoch   472: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 240, training loss 0.06335, validation loss 0.04825
Epoch   483: reducing learning rate of group 0 to 6.2500e-04.
Epoch   494: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 250, training loss 0.03615, validation loss 0.03973
Epoch   505: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 260, training loss 0.02478, validation loss 0.02188
Epoch   516: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 261
Mean train loss for ascent epoch 262: -0.01618780381977558
Mean eval for ascent epoch 262: 0.021839315071702003
Epoch   527: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 270, training loss 0.41622, validation loss 0.45060
Epoch   538: reducing learning rate of group 0 to 2.5000e-03.
Epoch   549: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 280, training loss 0.15065, validation loss 0.10121
Epoch   560: reducing learning rate of group 0 to 6.2500e-04.
Epoch   571: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 290, training loss 0.03400, validation loss 0.06243
Epoch   582: reducing learning rate of group 0 to 1.5625e-04.
Epoch   593: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 300
Doing Evaluation on the model now
This is Epoch 300, training loss 0.02134, validation loss 0.02098
Mean train loss for ascent epoch 301: -0.01212607603520155
Mean eval for ascent epoch 301: 0.021892834454774857
Epoch   604: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 310, training loss 0.40524, validation loss 0.20214
Epoch   615: reducing learning rate of group 0 to 2.5000e-03.
Epoch   626: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 320, training loss 0.05527, validation loss 0.05575
Epoch   637: reducing learning rate of group 0 to 6.2500e-04.
Epoch   648: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 330, training loss 0.02101, validation loss 0.02961
Epoch   659: reducing learning rate of group 0 to 1.5625e-04.
Epoch   670: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 339
Mean train loss for ascent epoch 340: -0.018089402467012405
Mean eval for ascent epoch 340: 0.019609158858656883
Doing Evaluation on the model now
This is Epoch 340, training loss 0.01961, validation loss 0.02083
Epoch   681: reducing learning rate of group 0 to 5.0000e-03.
Epoch   692: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 350, training loss 0.26853, validation loss 0.25155
Epoch   703: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 360, training loss 0.10258, validation loss 0.04120
Epoch   714: reducing learning rate of group 0 to 6.2500e-04.
Epoch   725: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 370, training loss 0.01980, validation loss 0.02019
Epoch   736: reducing learning rate of group 0 to 1.5625e-04.
Epoch   747: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 378
Mean train loss for ascent epoch 379: -0.015926584601402283
Mean eval for ascent epoch 379: 0.020068418234586716
Doing Evaluation on the model now
This is Epoch 380, training loss 0.26851, validation loss 0.24060
Epoch   758: reducing learning rate of group 0 to 5.0000e-03.
Epoch   769: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 390, training loss 0.15695, validation loss 0.21035
Epoch   780: reducing learning rate of group 0 to 1.2500e-03.
Epoch   791: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 400, training loss 0.06661, validation loss 0.04726
Resetting learning rate to 0.01000
Epoch   802: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 410, training loss 0.01888, validation loss 0.01648
Epoch   813: reducing learning rate of group 0 to 2.5000e-04.
Epoch   824: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 420, training loss 0.01539, validation loss 0.01621
Epoch   835: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 422
Mean train loss for ascent epoch 423: -0.014001692645251751
Mean eval for ascent epoch 423: 0.01542574167251587
Epoch   846: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 430, training loss 0.51332, validation loss 0.87836
Epoch   857: reducing learning rate of group 0 to 2.5000e-03.
Epoch   868: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 440, training loss 0.04796, validation loss 0.04372
Epoch   879: reducing learning rate of group 0 to 6.2500e-04.
Epoch   890: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 450, training loss 0.02332, validation loss 0.02164
Epoch   901: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 460, training loss 0.01925, validation loss 0.01760
Epoch   912: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 461
Mean train loss for ascent epoch 462: -0.012345531024038792
Mean eval for ascent epoch 462: 0.01877569407224655
Epoch   923: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 470, training loss 0.38444, validation loss 0.14500
Epoch   934: reducing learning rate of group 0 to 2.5000e-03.
Epoch   945: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 480, training loss 0.06719, validation loss 0.06413
Epoch   956: reducing learning rate of group 0 to 6.2500e-04.
Epoch   967: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 490, training loss 0.02713, validation loss 0.02594
Epoch   978: reducing learning rate of group 0 to 1.5625e-04.
Epoch   989: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 500
Doing Evaluation on the model now
This is Epoch 500, training loss 0.01932, validation loss 0.02404
Mean train loss for ascent epoch 501: -0.011108099482953548
Mean eval for ascent epoch 501: 0.030714645981788635
Epoch  1000: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 510, training loss 0.10844, validation loss 0.07414
Epoch  1011: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1022: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 520, training loss 0.05373, validation loss 0.03012
Epoch  1033: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1044: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 530, training loss 0.02013, validation loss 0.02121
Epoch  1055: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1066: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 539
Mean train loss for ascent epoch 540: -0.009741153568029404
Mean eval for ascent epoch 540: 0.015672100707888603
Doing Evaluation on the model now
This is Epoch 540, training loss 0.01567, validation loss 0.01519
Epoch  1077: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 550, training loss 0.11564, validation loss 0.19917
Epoch  1088: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1099: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 560, training loss 0.05991, validation loss 0.12617
Epoch  1110: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1121: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 570, training loss 0.02485, validation loss 0.02623
Epoch  1132: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1143: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 578
Mean train loss for ascent epoch 579: -0.014904031530022621
Mean eval for ascent epoch 579: 0.017231399193406105
Doing Evaluation on the model now
This is Epoch 580, training loss 0.12864, validation loss 0.13067
Epoch  1154: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1165: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 590, training loss 0.18637, validation loss 0.20653
Epoch  1176: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 600, training loss 0.04217, validation loss 0.05655
Epoch  1187: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  1198: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 610, training loss 0.06311, validation loss 0.07826
Epoch  1209: reducing learning rate of group 0 to 2.5000e-04.
Epoch  1220: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 620, training loss 0.01655, validation loss 0.01649
Epoch  1231: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 623
Mean train loss for ascent epoch 624: -0.009010584093630314
Mean eval for ascent epoch 624: 0.01692391000688076
Epoch  1242: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 630, training loss 0.15162, validation loss 0.05362
Epoch  1253: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1264: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 640, training loss 0.06100, validation loss 0.03547
Epoch  1275: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 650, training loss 0.03692, validation loss 0.03317
Epoch  1286: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1297: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 660, training loss 0.02242, validation loss 0.01895
Epoch  1308: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 662
Mean train loss for ascent epoch 663: -0.010942955501377583
Mean eval for ascent epoch 663: 0.017808949574828148
Epoch  1319: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 670, training loss 0.14058, validation loss 0.20259
Epoch  1330: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1341: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 680, training loss 0.03792, validation loss 0.02658
Epoch  1352: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1363: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 690, training loss 0.01997, validation loss 0.01680
Epoch  1374: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 700, training loss 0.01730, validation loss 0.01553
Epoch  1385: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 701
Mean train loss for ascent epoch 702: -0.011529641225934029
Mean eval for ascent epoch 702: 0.015273935161530972
Epoch  1396: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 710, training loss 0.10480, validation loss 0.07517
Epoch  1407: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1418: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 720, training loss 0.03426, validation loss 0.09572
Epoch  1429: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1440: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 730, training loss 0.01456, validation loss 0.01301
Epoch  1451: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1462: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 740
Doing Evaluation on the model now
This is Epoch 740, training loss 0.01380, validation loss 0.01909
Mean train loss for ascent epoch 741: -0.01075080968439579
Mean eval for ascent epoch 741: 0.014033200219273567
Epoch  1473: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 750, training loss 0.26208, validation loss 0.09923
Epoch  1484: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1495: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 760, training loss 0.06762, validation loss 0.06837
Epoch  1506: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1517: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 770, training loss 0.01535, validation loss 0.01539
Epoch  1528: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1539: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 779
Mean train loss for ascent epoch 780: -0.011094619520008564
Mean eval for ascent epoch 780: 0.013932408764958382
Doing Evaluation on the model now
This is Epoch 780, training loss 0.01393, validation loss 0.01261
Epoch  1550: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1561: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 790, training loss 0.28169, validation loss 0.41441
Epoch  1572: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 800, training loss 0.03500, validation loss 0.10628
Resetting learning rate to 0.01000
Epoch  1583: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1594: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 810, training loss 0.01479, validation loss 0.01397
Epoch  1605: reducing learning rate of group 0 to 1.2500e-04.
Epoch  1616: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 818
Mean train loss for ascent epoch 819: -0.009891163557767868
Mean eval for ascent epoch 819: 0.013698789291083813
Doing Evaluation on the model now
This is Epoch 820, training loss 0.20189, validation loss 0.37513
Epoch  1627: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1638: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 830, training loss 0.07257, validation loss 0.05755
Epoch  1649: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1660: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 840, training loss 0.06444, validation loss 0.03263
Epoch  1671: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 850, training loss 0.01629, validation loss 0.01846
Epoch  1682: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1693: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 857
Mean train loss for ascent epoch 858: -0.009364943951368332
Mean eval for ascent epoch 858: 0.013101291842758656
Doing Evaluation on the model now
This is Epoch 860, training loss 0.79294, validation loss 1.11346
Epoch  1704: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1715: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 870, training loss 0.10973, validation loss 0.05307
Epoch  1726: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1737: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 880, training loss 0.03529, validation loss 0.04260
Epoch  1748: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1759: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 890, training loss 0.01471, validation loss 0.01221
Epoch  1770: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 896
Mean train loss for ascent epoch 897: -0.007978183217346668
Mean eval for ascent epoch 897: 0.012788924388587475
Doing Evaluation on the model now
This is Epoch 900, training loss 0.24932, validation loss 0.20694
Epoch  1781: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1792: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 910, training loss 0.13976, validation loss 0.03008
Epoch  1803: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1814: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 920, training loss 0.01629, validation loss 0.02187
Epoch  1825: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1836: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 930, training loss 0.01413, validation loss 0.01801
Epoch  1847: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 935
Mean train loss for ascent epoch 936: -0.010279926471412182
Mean eval for ascent epoch 936: 0.012288831174373627
Doing Evaluation on the model now
This is Epoch 940, training loss 0.43963, validation loss 0.96400
Epoch  1858: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1869: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 950, training loss 0.04942, validation loss 0.02593
Epoch  1880: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1891: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 960, training loss 0.01646, validation loss 0.02154
Epoch  1902: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1913: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 970, training loss 0.01217, validation loss 0.01453
Epoch  1924: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 974
Mean train loss for ascent epoch 975: -0.00991601962596178
Mean eval for ascent epoch 975: 0.011160767637193203
Epoch  1935: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 980, training loss 0.09434, validation loss 0.06706
Epoch  1946: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 990, training loss 0.03208, validation loss 0.03377
Epoch  1957: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1968: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1000, training loss 0.01594, validation loss 0.01603
Resetting learning rate to 0.01000
Epoch  1979: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1990: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1010, training loss 0.01341, validation loss 0.01111
Epoch  2001: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2012: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1018
Mean train loss for ascent epoch 1019: -0.009850780479609966
Mean eval for ascent epoch 1019: 0.013312630355358124
Doing Evaluation on the model now
This is Epoch 1020, training loss 0.19688, validation loss 0.22520
Epoch  2023: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2034: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1030, training loss 0.04469, validation loss 0.04382
Epoch  2045: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1040, training loss 0.05104, validation loss 0.01856
Epoch  2056: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2067: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1050, training loss 0.01356, validation loss 0.01476
Epoch  2078: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2089: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1057
Mean train loss for ascent epoch 1058: -0.007678030524402857
Mean eval for ascent epoch 1058: 0.015696361660957336
Doing Evaluation on the model now
This is Epoch 1060, training loss 0.20648, validation loss 0.20689
Epoch  2100: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2111: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1070, training loss 0.08976, validation loss 0.07019
Epoch  2122: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2133: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1080, training loss 0.03187, validation loss 0.02066
Epoch  2144: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1090, training loss 0.01381, validation loss 0.01359
Epoch  2155: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2166: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1096
Mean train loss for ascent epoch 1097: -0.012988266535103321
Mean eval for ascent epoch 1097: 0.013483604416251183
Doing Evaluation on the model now
This is Epoch 1100, training loss 0.33081, validation loss 0.23084
Epoch  2177: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2188: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1110, training loss 0.03777, validation loss 0.03772
Epoch  2199: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2210: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1120, training loss 0.01662, validation loss 0.01652
Epoch  2221: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2232: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1130, training loss 0.01452, validation loss 0.02759
Epoch  2243: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1135
Mean train loss for ascent epoch 1136: -0.009840818122029305
Mean eval for ascent epoch 1136: 0.0131166847422719
Doing Evaluation on the model now
This is Epoch 1140, training loss 0.23301, validation loss 0.35327
Epoch  2254: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2265: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1150, training loss 0.03789, validation loss 0.04273
Epoch  2276: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2287: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1160, training loss 0.01517, validation loss 0.02511
Epoch  2298: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2309: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1170, training loss 0.01098, validation loss 0.00935
Epoch  2320: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1174
Mean train loss for ascent epoch 1175: -0.00816345401108265
Mean eval for ascent epoch 1175: 0.012950393371284008
Epoch  2331: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1180, training loss 0.39506, validation loss 0.19008
Epoch  2342: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1190, training loss 0.03616, validation loss 0.02524
Epoch  2353: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2364: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1200, training loss 0.01624, validation loss 0.01378
Resetting learning rate to 0.01000
Epoch  2375: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2386: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1210, training loss 0.01149, validation loss 0.01069
Epoch  2397: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2408: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1219
Mean train loss for ascent epoch 1220: -0.0077605098485946655
Mean eval for ascent epoch 1220: 0.011628161184489727
Doing Evaluation on the model now
This is Epoch 1220, training loss 0.01163, validation loss 0.01314
Epoch  2419: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2430: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1230, training loss 0.11036, validation loss 0.03907
Epoch  2441: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1240, training loss 0.01876, validation loss 0.02735
Epoch  2452: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2463: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1250, training loss 0.01057, validation loss 0.00994
Epoch  2474: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2485: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1258
Mean train loss for ascent epoch 1259: -0.008004025556147099
Mean eval for ascent epoch 1259: 0.011629804968833923
Doing Evaluation on the model now
This is Epoch 1260, training loss 0.21659, validation loss 0.20760
Epoch  2496: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2507: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1270, training loss 0.04041, validation loss 0.04331
Epoch  2518: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2529: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1280, training loss 0.01761, validation loss 0.01262
Epoch  2540: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1290, training loss 0.01062, validation loss 0.01270
Epoch  2551: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2562: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1297
Mean train loss for ascent epoch 1298: -0.009791053831577301
Mean eval for ascent epoch 1298: 0.013482393696904182
Doing Evaluation on the model now
This is Epoch 1300, training loss 0.33152, validation loss 0.15329
Epoch  2573: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2584: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1310, training loss 0.04980, validation loss 0.10092
Epoch  2595: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2606: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1320, training loss 0.01978, validation loss 0.01579
Epoch  2617: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2628: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1330, training loss 0.01519, validation loss 0.01215
Epoch  2639: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1336
Mean train loss for ascent epoch 1337: -0.004803112242370844
Mean eval for ascent epoch 1337: 0.014439466409385204
Doing Evaluation on the model now
This is Epoch 1340, training loss 0.37792, validation loss 0.35139
Epoch  2650: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2661: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1350, training loss 0.03717, validation loss 0.02984
Epoch  2672: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2683: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1360, training loss 0.01298, validation loss 0.01212
Epoch  2694: reducing learning rate of group 0 to 3.1250e-04.
Epoch  2705: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1370, training loss 0.01307, validation loss 0.01216
Epoch  2716: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1375
Mean train loss for ascent epoch 1376: -0.007891726680099964
Mean eval for ascent epoch 1376: 0.01080633606761694
Doing Evaluation on the model now
This is Epoch 1380, training loss 0.35317, validation loss 0.50803
Epoch  2727: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2738: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1390, training loss 0.05887, validation loss 0.10587
Epoch  2749: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2760: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1400, training loss 0.01423, validation loss 0.01675
Resetting learning rate to 0.01000
Epoch  2771: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2782: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1410, training loss 0.01435, validation loss 0.02007
Epoch  2793: reducing learning rate of group 0 to 1.2500e-04.
Epoch  2804: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1419
Mean train loss for ascent epoch 1420: -0.009191733784973621
Mean eval for ascent epoch 1420: 0.014162948355078697
Doing Evaluation on the model now
This is Epoch 1420, training loss 0.01416, validation loss 0.01218
Epoch  2815: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1430, training loss 0.12415, validation loss 0.13148
Epoch  2826: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2837: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1440, training loss 0.01575, validation loss 0.02664
Epoch  2848: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2859: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1450, training loss 0.01227, validation loss 0.01183
Epoch  2870: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2881: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1458
Mean train loss for ascent epoch 1459: -0.006540769245475531
Mean eval for ascent epoch 1459: 0.013243383727967739
Doing Evaluation on the model now
This is Epoch 1460, training loss 0.17872, validation loss 0.13063
Epoch  2892: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2903: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1470, training loss 0.05544, validation loss 0.07194
Epoch  2914: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1480, training loss 0.02337, validation loss 0.04065
Epoch  2925: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2936: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1490, training loss 0.01038, validation loss 0.00962
Epoch  2947: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2958: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1497
Mean train loss for ascent epoch 1498: -0.0065371510572731495
Mean eval for ascent epoch 1498: 0.01072377897799015
Doing Evaluation on the model now
This is Epoch 1500, training loss 0.11231, validation loss 0.06226
Epoch  2969: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2980: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1510, training loss 0.05269, validation loss 0.03663
Epoch  2991: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3002: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1520, training loss 0.01612, validation loss 0.01208
Epoch  3013: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1530, training loss 0.00967, validation loss 0.00956
Epoch  3024: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3035: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1536
Mean train loss for ascent epoch 1537: -0.006953588221222162
Mean eval for ascent epoch 1537: 0.010831334628164768
Doing Evaluation on the model now
This is Epoch 1540, training loss 0.22766, validation loss 0.14105
Epoch  3046: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3057: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1550, training loss 0.03828, validation loss 0.02460
Epoch  3068: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3079: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1560, training loss 0.01543, validation loss 0.01432
Epoch  3090: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3101: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1570, training loss 0.00982, validation loss 0.01104
Epoch  3112: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1575
Mean train loss for ascent epoch 1576: -0.006771622691303492
Mean eval for ascent epoch 1576: 0.010614238679409027
Doing Evaluation on the model now
This is Epoch 1580, training loss 0.27018, validation loss 0.33455
Epoch  3123: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3134: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1590, training loss 0.03108, validation loss 0.02680
Epoch  3145: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3156: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1600, training loss 0.01101, validation loss 0.01578
Resetting learning rate to 0.01000
Epoch  3167: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3178: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1610, training loss 0.01168, validation loss 0.01196
Epoch  3189: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3200: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1620
Doing Evaluation on the model now
This is Epoch 1620, training loss 0.00990, validation loss 0.00810
Mean train loss for ascent epoch 1621: -0.004973743110895157
Mean eval for ascent epoch 1621: 0.008926637470722198
Epoch  3211: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1630, training loss 0.06614, validation loss 0.05301
Epoch  3222: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3233: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1640, training loss 0.01909, validation loss 0.01485
Epoch  3244: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3255: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1650, training loss 0.01171, validation loss 0.01238
Epoch  3266: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3277: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1659
Mean train loss for ascent epoch 1660: -0.008008960634469986
Mean eval for ascent epoch 1660: 0.016205402091145515
Doing Evaluation on the model now
This is Epoch 1660, training loss 0.01621, validation loss 0.01064
Epoch  3288: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3299: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1670, training loss 0.10747, validation loss 0.39301
Epoch  3310: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1680, training loss 0.02459, validation loss 0.02910
Epoch  3321: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3332: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1690, training loss 0.01203, validation loss 0.00920
Epoch  3343: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3354: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1698
Mean train loss for ascent epoch 1699: -0.0063022770918905735
Mean eval for ascent epoch 1699: 0.008749794214963913
Doing Evaluation on the model now
This is Epoch 1700, training loss 0.09564, validation loss 0.21617
Epoch  3365: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3376: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1710, training loss 0.03006, validation loss 0.02191
Epoch  3387: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3398: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1720, training loss 0.04563, validation loss 0.02554
Epoch  3409: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1730, training loss 0.00976, validation loss 0.01040
Epoch  3420: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3431: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1737
Mean train loss for ascent epoch 1738: -0.010349810123443604
Mean eval for ascent epoch 1738: 0.008224621415138245
Doing Evaluation on the model now
This is Epoch 1740, training loss 0.09760, validation loss 0.14505
Epoch  3442: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3453: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1750, training loss 0.02078, validation loss 0.01899
Epoch  3464: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3475: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1760, training loss 0.01170, validation loss 0.00970
Epoch  3486: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3497: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1770, training loss 0.00830, validation loss 0.00770
Epoch  3508: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1776
Mean train loss for ascent epoch 1777: -0.0044651641510427
Mean eval for ascent epoch 1777: 0.007442411035299301
Doing Evaluation on the model now
This is Epoch 1780, training loss 0.35138, validation loss 0.76624
Epoch  3519: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3530: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1790, training loss 0.03880, validation loss 0.01625
Epoch  3541: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3552: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1800, training loss 0.01210, validation loss 0.01223
Resetting learning rate to 0.01000
Epoch  3563: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3574: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1810, training loss 0.01058, validation loss 0.00960
Epoch  3585: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3596: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1820
Doing Evaluation on the model now
This is Epoch 1820, training loss 0.00937, validation loss 0.01028
Mean train loss for ascent epoch 1821: -0.007802631705999374
Mean eval for ascent epoch 1821: 0.011228042654693127
Epoch  3607: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1830, training loss 0.10140, validation loss 0.03536
Epoch  3618: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3629: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1840, training loss 0.01902, validation loss 0.01820
Epoch  3640: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3651: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1850, training loss 0.00861, validation loss 0.00794
Epoch  3662: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3673: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1859
Mean train loss for ascent epoch 1860: -0.005546551663428545
Mean eval for ascent epoch 1860: 0.007731036748737097
Doing Evaluation on the model now
This is Epoch 1860, training loss 0.00773, validation loss 0.00726
Epoch  3684: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1870, training loss 0.05567, validation loss 0.21432
Epoch  3695: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3706: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1880, training loss 0.02782, validation loss 0.06119
Epoch  3717: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3728: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1890, training loss 0.00821, validation loss 0.00824
Epoch  3739: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3750: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1898
Mean train loss for ascent epoch 1899: -0.005606308113783598
Mean eval for ascent epoch 1899: 0.008771775290369987
Doing Evaluation on the model now
This is Epoch 1900, training loss 0.16796, validation loss 0.32079
Epoch  3761: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3772: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1910, training loss 0.02408, validation loss 0.02426
Epoch  3783: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1920, training loss 0.01829, validation loss 0.01529
Epoch  3794: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3805: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1930, training loss 0.01027, validation loss 0.00895
Epoch  3816: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3827: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1937
Mean train loss for ascent epoch 1938: -0.0049448804929852486
Mean eval for ascent epoch 1938: 0.008596307598054409
Doing Evaluation on the model now
This is Epoch 1940, training loss 0.09227, validation loss 0.03614
Epoch  3838: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3849: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1950, training loss 0.03592, validation loss 0.05309
Epoch  3860: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3871: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1960, training loss 0.02199, validation loss 0.01255
Epoch  3882: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1970, training loss 0.00864, validation loss 0.01003
Epoch  3893: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3904: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1976
Mean train loss for ascent epoch 1977: -0.007373384665697813
Mean eval for ascent epoch 1977: 0.009812474250793457
Doing Evaluation on the model now
This is Epoch 1980, training loss 0.50475, validation loss 0.32278
Epoch  3915: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3926: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1990, training loss 0.05157, validation loss 0.04759
Epoch  3937: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3948: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2000, training loss 0.01127, validation loss 0.01102
Resetting learning rate to 0.01000
Epoch  3959: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3970: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2010, training loss 0.00863, validation loss 0.01049
Epoch  3981: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2020, training loss 0.00802, validation loss 0.00846
Epoch  3992: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2021
Mean train loss for ascent epoch 2022: -0.00582859618589282
Mean eval for ascent epoch 2022: 0.007356719113886356
Epoch  4003: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2030, training loss 0.14654, validation loss 0.08808
Epoch  4014: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4025: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2040, training loss 0.01529, validation loss 0.01178
Epoch  4036: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4047: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2050, training loss 0.00832, validation loss 0.01078
Epoch  4058: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4069: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2060
Doing Evaluation on the model now
This is Epoch 2060, training loss 0.00874, validation loss 0.00835
Mean train loss for ascent epoch 2061: -0.004554362501949072
Mean eval for ascent epoch 2061: 0.00809384509921074
Epoch  4080: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2070, training loss 0.10587, validation loss 0.04825
Epoch  4091: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4102: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2080, training loss 0.02003, validation loss 0.04313
Epoch  4113: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4124: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2090, training loss 0.00870, validation loss 0.00959
Epoch  4135: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4146: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2099
Mean train loss for ascent epoch 2100: -0.005506778135895729
Mean eval for ascent epoch 2100: 0.007018173113465309
Doing Evaluation on the model now
This is Epoch 2100, training loss 0.00702, validation loss 0.00664
Epoch  4157: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4168: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2110, training loss 0.17496, validation loss 0.11506
Epoch  4179: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2120, training loss 0.01628, validation loss 0.02146
Epoch  4190: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4201: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2130, training loss 0.00862, validation loss 0.01341
Epoch  4212: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4223: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2138
Mean train loss for ascent epoch 2139: -0.004775365348905325
Mean eval for ascent epoch 2139: 0.008326442912220955
Doing Evaluation on the model now
This is Epoch 2140, training loss 0.11442, validation loss 0.12288
Epoch  4234: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4245: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2150, training loss 0.02939, validation loss 0.03603
Epoch  4256: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4267: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2160, training loss 0.01437, validation loss 0.01042
Epoch  4278: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2170, training loss 0.00940, validation loss 0.00851
Epoch  4289: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4300: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2177
Mean train loss for ascent epoch 2178: -0.005181273445487022
Mean eval for ascent epoch 2178: 0.007409487850964069
Doing Evaluation on the model now
This is Epoch 2180, training loss 0.39397, validation loss 0.25904
Epoch  4311: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4322: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2190, training loss 0.02956, validation loss 0.02493
Epoch  4333: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4344: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2200, training loss 0.01588, validation loss 0.01234
Resetting learning rate to 0.01000
Epoch  4355: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4366: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2210, training loss 0.00977, validation loss 0.00875
Epoch  4377: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2220, training loss 0.00930, validation loss 0.01175
Epoch  4388: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2221
Mean train loss for ascent epoch 2222: -0.004520059563219547
Mean eval for ascent epoch 2222: 0.00771452346816659
Epoch  4399: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2230, training loss 0.07440, validation loss 0.04102
Epoch  4410: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4421: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2240, training loss 0.01834, validation loss 0.01860
Epoch  4432: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4443: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2250, training loss 0.00900, validation loss 0.01047
Epoch  4454: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4465: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2260
Doing Evaluation on the model now
This is Epoch 2260, training loss 0.00713, validation loss 0.00663
Mean train loss for ascent epoch 2261: -0.004611522890627384
Mean eval for ascent epoch 2261: 0.007595773786306381
Epoch  4476: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2270, training loss 0.03476, validation loss 0.07335
Epoch  4487: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4498: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2280, training loss 0.01822, validation loss 0.03361
Epoch  4509: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4520: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2290, training loss 0.00783, validation loss 0.00717
Epoch  4531: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4542: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2299
Mean train loss for ascent epoch 2300: -0.007659802213311195
Mean eval for ascent epoch 2300: 0.007784401997923851
Doing Evaluation on the model now
This is Epoch 2300, training loss 0.00778, validation loss 0.00700
Epoch  4553: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2310, training loss 0.07493, validation loss 0.03000
Epoch  4564: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4575: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2320, training loss 0.01363, validation loss 0.00977
Epoch  4586: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4597: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2330, training loss 0.00808, validation loss 0.00958
Epoch  4608: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4619: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2338
Mean train loss for ascent epoch 2339: -0.003925096243619919
Mean eval for ascent epoch 2339: 0.006757928524166346
Doing Evaluation on the model now
This is Epoch 2340, training loss 0.06364, validation loss 0.07984
Epoch  4630: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4641: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2350, training loss 0.04077, validation loss 0.05110
Epoch  4652: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2360, training loss 0.01227, validation loss 0.01596
Epoch  4663: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4674: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2370, training loss 0.00755, validation loss 0.00647
Epoch  4685: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4696: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2377
Mean train loss for ascent epoch 2378: -0.0041191112250089645
Mean eval for ascent epoch 2378: 0.00811926368623972
Doing Evaluation on the model now
This is Epoch 2380, training loss 0.07386, validation loss 0.08747
Epoch  4707: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4718: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2390, training loss 0.02579, validation loss 0.01704
Epoch  4729: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4740: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2400, training loss 0.01677, validation loss 0.01007
Resetting learning rate to 0.01000
Epoch  4751: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2410, training loss 0.00765, validation loss 0.00927
Epoch  4762: reducing learning rate of group 0 to 2.5000e-04.
Epoch  4773: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2420, training loss 0.00882, validation loss 0.00720
Epoch  4784: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2422
Mean train loss for ascent epoch 2423: -0.002981381257995963
Mean eval for ascent epoch 2423: 0.007086103782057762
Epoch  4795: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2430, training loss 0.05079, validation loss 0.04150
Epoch  4806: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4817: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2440, training loss 0.02653, validation loss 0.01630
Epoch  4828: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4839: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2450, training loss 0.00995, validation loss 0.01005
Epoch  4850: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2460, training loss 0.00679, validation loss 0.00670
Epoch  4861: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2461
Mean train loss for ascent epoch 2462: -0.003815832082182169
Mean eval for ascent epoch 2462: 0.006680343300104141
Epoch  4872: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2470, training loss 0.06212, validation loss 0.03697
Epoch  4883: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4894: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2480, training loss 0.01568, validation loss 0.01934
Epoch  4905: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4916: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2490, training loss 0.00615, validation loss 0.00589
Epoch  4927: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4938: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2500
Doing Evaluation on the model now
This is Epoch 2500, training loss 0.00655, validation loss 0.00671
Mean train loss for ascent epoch 2501: -0.00330374320037663
Mean eval for ascent epoch 2501: 0.006825882941484451
Epoch  4949: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2510, training loss 0.09901, validation loss 0.03026
Epoch  4960: reducing learning rate of group 0 to 2.5000e-03.
Epoch  4971: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2520, training loss 0.02228, validation loss 0.00974
Epoch  4982: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4993: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2530, training loss 0.00834, validation loss 0.00693
Epoch  5004: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5015: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2539
Mean train loss for ascent epoch 2540: -0.003379669738933444
Mean eval for ascent epoch 2540: 0.006711533758789301
Doing Evaluation on the model now
This is Epoch 2540, training loss 0.00671, validation loss 0.00635
Epoch  5026: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5037: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2550, training loss 0.05090, validation loss 0.08455
Epoch  5048: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2560, training loss 0.01613, validation loss 0.00724
Epoch  5059: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5070: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2570, training loss 0.00604, validation loss 0.00590
Epoch  5081: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5092: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2578
Mean train loss for ascent epoch 2579: -0.003395635401830077
Mean eval for ascent epoch 2579: 0.0061173951253294945
Doing Evaluation on the model now
This is Epoch 2580, training loss 0.36500, validation loss 1.42588
Epoch  5103: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5114: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2590, training loss 0.03718, validation loss 0.01659
Epoch  5125: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5136: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2600, training loss 0.01637, validation loss 0.04097
Resetting learning rate to 0.01000
Epoch  5147: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2610, training loss 0.00867, validation loss 0.00700
Epoch  5158: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5169: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2620, training loss 0.00532, validation loss 0.00542
Epoch  5180: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2622
Mean train loss for ascent epoch 2623: -0.004228054545819759
Mean eval for ascent epoch 2623: 0.005687933415174484
Epoch  5191: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2630, training loss 0.10462, validation loss 0.07519
Epoch  5202: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5213: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2640, training loss 0.01767, validation loss 0.01970
Epoch  5224: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5235: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2650, training loss 0.00691, validation loss 0.00772
Epoch  5246: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2660, training loss 0.00526, validation loss 0.00490
Epoch  5257: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2661
Mean train loss for ascent epoch 2662: -0.004006512463092804
Mean eval for ascent epoch 2662: 0.005556612275540829
Epoch  5268: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2670, training loss 0.04780, validation loss 0.03620
Epoch  5279: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5290: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2680, training loss 0.01543, validation loss 0.01244
Epoch  5301: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5312: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2690, training loss 0.00722, validation loss 0.00724
Epoch  5323: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5334: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2700
Doing Evaluation on the model now
This is Epoch 2700, training loss 0.00504, validation loss 0.00535
Mean train loss for ascent epoch 2701: -0.002021077089011669
Mean eval for ascent epoch 2701: 0.005159331951290369
Epoch  5345: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2710, training loss 0.06476, validation loss 0.02222
Epoch  5356: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5367: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2720, training loss 0.00980, validation loss 0.00873
Epoch  5378: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5389: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2730, training loss 0.00828, validation loss 0.00772
Epoch  5400: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5411: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2739
Mean train loss for ascent epoch 2740: -0.0025420107413083315
Mean eval for ascent epoch 2740: 0.007020187098532915
Doing Evaluation on the model now
This is Epoch 2740, training loss 0.00702, validation loss 0.00610
Epoch  5422: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2750, training loss 0.07549, validation loss 0.15714
Epoch  5433: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5444: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2760, training loss 0.02242, validation loss 0.01083
Epoch  5455: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5466: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2770, training loss 0.00681, validation loss 0.00861
Epoch  5477: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5488: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2778
Mean train loss for ascent epoch 2779: -0.0036666220985352993
Mean eval for ascent epoch 2779: 0.005884793121367693
Doing Evaluation on the model now
This is Epoch 2780, training loss 0.09763, validation loss 0.14445
Epoch  5499: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5510: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2790, training loss 0.03181, validation loss 0.00942
Epoch  5521: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2800, training loss 0.01194, validation loss 0.01702
Epoch  5532: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  5543: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 2810, training loss 0.00958, validation loss 0.00744
Epoch  5554: reducing learning rate of group 0 to 2.5000e-04.
Epoch  5565: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 2820, training loss 0.00524, validation loss 0.00484
Epoch  5576: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2823
Mean train loss for ascent epoch 2824: -0.0024667149409651756
Mean eval for ascent epoch 2824: 0.00562148867174983
Epoch  5587: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2830, training loss 0.06065, validation loss 0.07338
Epoch  5598: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5609: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2840, training loss 0.01353, validation loss 0.00742
Epoch  5620: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2850, training loss 0.00717, validation loss 0.00730
Epoch  5631: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5642: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2860, training loss 0.00554, validation loss 0.00580
Epoch  5653: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2862
Mean train loss for ascent epoch 2863: -0.0031721750274300575
Mean eval for ascent epoch 2863: 0.0062308055348694324
Epoch  5664: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2870, training loss 0.03725, validation loss 0.02306
Epoch  5675: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5686: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2880, training loss 0.01297, validation loss 0.01119
Epoch  5697: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5708: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2890, training loss 0.00686, validation loss 0.00487
Epoch  5719: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2900, training loss 0.00608, validation loss 0.00495
Epoch  5730: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2901
Mean train loss for ascent epoch 2902: -0.002908228663727641
Mean eval for ascent epoch 2902: 0.006780371069908142
Epoch  5741: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2910, training loss 0.07149, validation loss 0.14967
Epoch  5752: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5763: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2920, training loss 0.00970, validation loss 0.01006
Epoch  5774: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5785: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2930, training loss 0.00588, validation loss 0.00575
Epoch  5796: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5807: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2940
Doing Evaluation on the model now
This is Epoch 2940, training loss 0.00491, validation loss 0.00453
Mean train loss for ascent epoch 2941: -0.00231279619038105
Mean eval for ascent epoch 2941: 0.004932974930852652
Epoch  5818: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2950, training loss 0.09199, validation loss 0.10862
Epoch  5829: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5840: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2960, training loss 0.01474, validation loss 0.01205
Epoch  5851: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5862: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2970, training loss 0.00606, validation loss 0.00570
Epoch  5873: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5884: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2979
Mean train loss for ascent epoch 2980: -0.0034121377393603325
Mean eval for ascent epoch 2980: 0.005373951978981495
Doing Evaluation on the model now
This is Epoch 2980, training loss 0.00537, validation loss 0.00507
Epoch  5895: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5906: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2990, training loss 0.06074, validation loss 0.04524
Epoch  5917: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3000, training loss 0.01876, validation loss 0.02230
Resetting learning rate to 0.01000
Epoch  5928: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5939: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3010, training loss 0.00506, validation loss 0.00468
Epoch  5950: reducing learning rate of group 0 to 1.2500e-04.
Epoch  5961: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3018
Mean train loss for ascent epoch 3019: -0.0038300626911222935
Mean eval for ascent epoch 3019: 0.00575658306479454
Doing Evaluation on the model now
This is Epoch 3020, training loss 0.17577, validation loss 0.13867
Epoch  5972: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5983: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3030, training loss 0.06541, validation loss 0.10759
Epoch  5994: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6005: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3040, training loss 0.01030, validation loss 0.01169
Epoch  6016: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3050, training loss 0.00617, validation loss 0.00801
Epoch  6027: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6038: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3057
Mean train loss for ascent epoch 3058: -0.0031649847514927387
Mean eval for ascent epoch 3058: 0.0054205190390348434
Doing Evaluation on the model now
This is Epoch 3060, training loss 0.41119, validation loss 0.56787
Epoch  6049: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6060: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3070, training loss 0.04891, validation loss 0.05866
Epoch  6071: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6082: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3080, training loss 0.00848, validation loss 0.00830
Epoch  6093: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6104: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3090, training loss 0.00720, validation loss 0.00932
Epoch  6115: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3096
Mean train loss for ascent epoch 3097: -0.004196870140731335
Mean eval for ascent epoch 3097: 0.006233464926481247
Doing Evaluation on the model now
This is Epoch 3100, training loss 0.96462, validation loss 0.53189
Epoch  6126: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6137: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3110, training loss 0.02757, validation loss 0.02303
Epoch  6148: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6159: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3120, training loss 0.00704, validation loss 0.00825
Epoch  6170: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6181: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3130, training loss 0.00512, validation loss 0.00463
Epoch  6192: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3135
Mean train loss for ascent epoch 3136: -0.0032814654987305403
Mean eval for ascent epoch 3136: 0.005716486368328333
Doing Evaluation on the model now
This is Epoch 3140, training loss 0.10453, validation loss 0.19245
Epoch  6203: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6214: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3150, training loss 0.02488, validation loss 0.01789
Epoch  6225: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6236: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3160, training loss 0.00881, validation loss 0.00715
Epoch  6247: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6258: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3170, training loss 0.00587, validation loss 0.00625
Epoch  6269: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3174
Mean train loss for ascent epoch 3175: -0.0037842788733541965
Mean eval for ascent epoch 3175: 0.005334393121302128
Epoch  6280: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3180, training loss 0.10351, validation loss 0.04662
Epoch  6291: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3190, training loss 0.03183, validation loss 0.04089
Epoch  6302: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6313: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3200, training loss 0.00680, validation loss 0.00778
Resetting learning rate to 0.01000
Epoch  6324: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6335: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3210, training loss 0.00553, validation loss 0.00852
Epoch  6346: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6357: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3218
Mean train loss for ascent epoch 3219: -0.0033579147420823574
Mean eval for ascent epoch 3219: 0.004452396184206009
Doing Evaluation on the model now
This is Epoch 3220, training loss 0.11155, validation loss 0.22755
Epoch  6368: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6379: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3230, training loss 0.04475, validation loss 0.02587
Epoch  6390: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3240, training loss 0.01406, validation loss 0.01168
Epoch  6401: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6412: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3250, training loss 0.00620, validation loss 0.00525
Epoch  6423: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6434: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3257
Mean train loss for ascent epoch 3258: -0.003593589412048459
Mean eval for ascent epoch 3258: 0.006366013549268246
Doing Evaluation on the model now
This is Epoch 3260, training loss 0.63234, validation loss 0.28717
Epoch  6445: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6456: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3270, training loss 0.04846, validation loss 0.03559
Epoch  6467: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6478: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3280, training loss 0.00836, validation loss 0.01008
Epoch  6489: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3290, training loss 0.00645, validation loss 0.00747
Epoch  6500: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6511: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3296
Mean train loss for ascent epoch 3297: -0.002968244021758437
Mean eval for ascent epoch 3297: 0.005088391713798046
Doing Evaluation on the model now
This is Epoch 3300, training loss 0.13349, validation loss 0.26389
Epoch  6522: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6533: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3310, training loss 0.03679, validation loss 0.03424
Epoch  6544: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6555: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3320, training loss 0.00809, validation loss 0.00732
Epoch  6566: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6577: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3330, training loss 0.00673, validation loss 0.00428
Epoch  6588: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3335
Mean train loss for ascent epoch 3336: -0.0028383852913975716
Mean eval for ascent epoch 3336: 0.004976189695298672
Doing Evaluation on the model now
This is Epoch 3340, training loss 0.55166, validation loss 0.93547
Epoch  6599: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6610: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3350, training loss 0.04374, validation loss 0.04035
Epoch  6621: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6632: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3360, training loss 0.00990, validation loss 0.01276
Epoch  6643: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6654: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3370, training loss 0.00572, validation loss 0.00790
Epoch  6665: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3374
Mean train loss for ascent epoch 3375: -0.003171914955601096
Mean eval for ascent epoch 3375: 0.004984766710549593
Epoch  6676: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3380, training loss 0.26931, validation loss 0.66533
Epoch  6687: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3390, training loss 0.03408, validation loss 0.08303
Epoch  6698: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6709: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3400, training loss 0.01231, validation loss 0.00973
Resetting learning rate to 0.01000
Epoch  6720: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6731: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3410, training loss 0.00636, validation loss 0.00606
Epoch  6742: reducing learning rate of group 0 to 1.2500e-04.
Epoch  6753: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3419
Mean train loss for ascent epoch 3420: -0.003992136567831039
Mean eval for ascent epoch 3420: 0.005113260354846716
Doing Evaluation on the model now
This is Epoch 3420, training loss 0.00511, validation loss 0.00539
Epoch  6764: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6775: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3430, training loss 0.08255, validation loss 0.02287
Epoch  6786: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3440, training loss 0.02445, validation loss 0.01493
Epoch  6797: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6808: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3450, training loss 0.00958, validation loss 0.00583
Epoch  6819: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6830: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3458
Mean train loss for ascent epoch 3459: -0.0024466568138450384
Mean eval for ascent epoch 3459: 0.004607190378010273
Doing Evaluation on the model now
This is Epoch 3460, training loss 0.16687, validation loss 0.11069
Epoch  6841: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6852: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3470, training loss 0.10131, validation loss 0.05928
Epoch  6863: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6874: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3480, training loss 0.02461, validation loss 0.01593
Epoch  6885: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3490, training loss 0.00672, validation loss 0.00634
Epoch  6896: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6907: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3497
Mean train loss for ascent epoch 3498: -0.0029298528097569942
Mean eval for ascent epoch 3498: 0.004862552974373102
Doing Evaluation on the model now
This is Epoch 3500, training loss 0.64508, validation loss 1.00078
Epoch  6918: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6929: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3510, training loss 0.04150, validation loss 0.04973
Epoch  6940: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6951: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3520, training loss 0.01677, validation loss 0.00502
Epoch  6962: reducing learning rate of group 0 to 3.1250e-04.
Epoch  6973: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3530, training loss 0.00689, validation loss 0.00795
Epoch  6984: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3536
Mean train loss for ascent epoch 3537: -0.0030018289107829332
Mean eval for ascent epoch 3537: 0.00536776427179575
Doing Evaluation on the model now
This is Epoch 3540, training loss 0.42939, validation loss 0.29507
Epoch  6995: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7006: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3550, training loss 0.04403, validation loss 0.06265
Epoch  7017: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7028: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3560, training loss 0.01139, validation loss 0.01075
Epoch  7039: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7050: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3570, training loss 0.00652, validation loss 0.00508
Epoch  7061: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3575
Mean train loss for ascent epoch 3576: -0.0028718046378344297
Mean eval for ascent epoch 3576: 0.005087424535304308
Doing Evaluation on the model now
This is Epoch 3580, training loss 0.46058, validation loss 0.47174
Epoch  7072: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7083: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3590, training loss 0.05472, validation loss 0.03624
Epoch  7094: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7105: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3600, training loss 0.01355, validation loss 0.01820
Resetting learning rate to 0.01000
Epoch  7116: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7127: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3610, training loss 0.00584, validation loss 0.00583
Epoch  7138: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7149: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3619
Mean train loss for ascent epoch 3620: -0.0026118073146790266
Mean eval for ascent epoch 3620: 0.004369895439594984
Doing Evaluation on the model now
This is Epoch 3620, training loss 0.00437, validation loss 0.00446
Epoch  7160: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3630, training loss 0.15577, validation loss 0.17292
Epoch  7171: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7182: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3640, training loss 0.02549, validation loss 0.01491
Epoch  7193: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7204: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3650, training loss 0.00824, validation loss 0.01112
Epoch  7215: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7226: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3658
Mean train loss for ascent epoch 3659: -0.0026858034543693066
Mean eval for ascent epoch 3659: 0.004572808276861906
Doing Evaluation on the model now
This is Epoch 3660, training loss 0.35370, validation loss 0.39928
Epoch  7237: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7248: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3670, training loss 0.08141, validation loss 0.08858
Epoch  7259: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3680, training loss 0.02389, validation loss 0.02447
Epoch  7270: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7281: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3690, training loss 0.00931, validation loss 0.00481
Epoch  7292: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7303: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3697
Mean train loss for ascent epoch 3698: -0.003430974669754505
Mean eval for ascent epoch 3698: 0.004594890866428614
Doing Evaluation on the model now
This is Epoch 3700, training loss 1.00216, validation loss 1.11343
Epoch  7314: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7325: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3710, training loss 0.05346, validation loss 0.04859
Epoch  7336: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7347: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3720, training loss 0.02675, validation loss 0.02859
Epoch  7358: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3730, training loss 0.00554, validation loss 0.00518
Epoch  7369: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7380: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3736
Mean train loss for ascent epoch 3737: -0.0026227489579468966
Mean eval for ascent epoch 3737: 0.005394956562668085
Doing Evaluation on the model now
This is Epoch 3740, training loss 0.52226, validation loss 0.34279
Epoch  7391: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7402: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3750, training loss 0.06055, validation loss 0.03920
Epoch  7413: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7424: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3760, training loss 0.01419, validation loss 0.01157
Epoch  7435: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7446: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3770, training loss 0.00505, validation loss 0.00453
Epoch  7457: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3775
Mean train loss for ascent epoch 3776: -0.0019860477186739445
Mean eval for ascent epoch 3776: 0.004674380179494619
Doing Evaluation on the model now
This is Epoch 3780, training loss 0.23719, validation loss 0.47394
Epoch  7468: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7479: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3790, training loss 0.07396, validation loss 0.01821
Epoch  7490: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7501: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3800, training loss 0.01168, validation loss 0.01500
Resetting learning rate to 0.01000
Epoch  7512: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7523: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3810, training loss 0.00545, validation loss 0.00782
Epoch  7534: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7545: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3820
Doing Evaluation on the model now
This is Epoch 3820, training loss 0.00454, validation loss 0.00554
Mean train loss for ascent epoch 3821: -0.0024859558325260878
Mean eval for ascent epoch 3821: 0.0047517879866063595
Epoch  7556: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3830, training loss 0.10853, validation loss 0.07274
Epoch  7567: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7578: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3840, training loss 0.02413, validation loss 0.03231
Epoch  7589: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7600: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3850, training loss 0.00569, validation loss 0.00441
Epoch  7611: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7622: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3859
Mean train loss for ascent epoch 3860: -0.0026481470558792353
Mean eval for ascent epoch 3860: 0.004915228113532066
Doing Evaluation on the model now
This is Epoch 3860, training loss 0.00492, validation loss 0.00480
Epoch  7633: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7644: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3870, training loss 0.21590, validation loss 0.18996
Epoch  7655: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3880, training loss 0.02710, validation loss 0.02725
Epoch  7666: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7677: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3890, training loss 0.00659, validation loss 0.00787
Epoch  7688: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7699: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3898
Mean train loss for ascent epoch 3899: -0.0023609555792063475
Mean eval for ascent epoch 3899: 0.004129639361053705
Doing Evaluation on the model now
This is Epoch 3900, training loss 0.15831, validation loss 0.19950
Epoch  7710: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7721: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3910, training loss 0.02459, validation loss 0.02368
Epoch  7732: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7743: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3920, training loss 0.01919, validation loss 0.01651
Epoch  7754: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3930, training loss 0.00692, validation loss 0.00692
Epoch  7765: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7776: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3937
Mean train loss for ascent epoch 3938: -0.0037344424054026604
Mean eval for ascent epoch 3938: 0.004794393666088581
Doing Evaluation on the model now
This is Epoch 3940, training loss 0.42893, validation loss 0.91905
Epoch  7787: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7798: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3950, training loss 0.08486, validation loss 0.09790
Epoch  7809: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7820: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3960, training loss 0.01723, validation loss 0.01183
Epoch  7831: reducing learning rate of group 0 to 3.1250e-04.
Epoch  7842: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3970, training loss 0.00623, validation loss 0.00480
Epoch  7853: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3976
Mean train loss for ascent epoch 3977: -0.0036893063224852085
Mean eval for ascent epoch 3977: 0.005018953233957291
Doing Evaluation on the model now
This is Epoch 3980, training loss 0.16744, validation loss 0.10407
Epoch  7864: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7875: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3990, training loss 0.07063, validation loss 0.05612
Epoch  7886: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7897: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4000, training loss 0.01077, validation loss 0.00614
Resetting learning rate to 0.01000
Epoch  7908: reducing learning rate of group 0 to 5.0000e-04.
Epoch  7919: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4010, training loss 0.00713, validation loss 0.00627
Epoch  7930: reducing learning rate of group 0 to 1.2500e-04.
Epoch  7941: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4020
Doing Evaluation on the model now
This is Epoch 4020, training loss 0.00490, validation loss 0.00402
Mean train loss for ascent epoch 4021: -0.0023328347597271204
Mean eval for ascent epoch 4021: 0.004518379457294941
Epoch  7952: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4030, training loss 0.24219, validation loss 0.10308
Epoch  7963: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7974: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4040, training loss 0.02185, validation loss 0.03541
Epoch  7985: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7996: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4050, training loss 0.00694, validation loss 0.00689
Epoch  8007: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8018: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4059
Mean train loss for ascent epoch 4060: -0.0024109017103910446
Mean eval for ascent epoch 4060: 0.005228806287050247
Doing Evaluation on the model now
This is Epoch 4060, training loss 0.00523, validation loss 0.00525
Epoch  8029: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4070, training loss 0.06402, validation loss 0.04134
Epoch  8040: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8051: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4080, training loss 0.03053, validation loss 0.01943
Epoch  8062: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8073: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4090, training loss 0.00615, validation loss 0.00691
Epoch  8084: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8095: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4098
Mean train loss for ascent epoch 4099: -0.0024754919577389956
Mean eval for ascent epoch 4099: 0.00481081148609519
Doing Evaluation on the model now
This is Epoch 4100, training loss 0.14249, validation loss 0.04857
Epoch  8106: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8117: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4110, training loss 0.04595, validation loss 0.03460
Epoch  8128: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4120, training loss 0.02854, validation loss 0.02601
Epoch  8139: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8150: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4130, training loss 0.00676, validation loss 0.01085
Epoch  8161: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8172: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4137
Mean train loss for ascent epoch 4138: -0.0028316809330135584
Mean eval for ascent epoch 4138: 0.005278947297483683
Doing Evaluation on the model now
This is Epoch 4140, training loss 0.17347, validation loss 0.15243
Epoch  8183: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8194: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4150, training loss 0.06590, validation loss 0.07497
Epoch  8205: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8216: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4160, training loss 0.01268, validation loss 0.00613
Epoch  8227: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4170, training loss 0.00626, validation loss 0.01114
Epoch  8238: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8249: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4176
Mean train loss for ascent epoch 4177: -0.0024914254900068045
Mean eval for ascent epoch 4177: 0.0044555943459272385
Doing Evaluation on the model now
This is Epoch 4180, training loss 1.44812, validation loss 1.72784
Epoch  8260: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8271: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4190, training loss 0.04550, validation loss 0.04182
Epoch  8282: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8293: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4200, training loss 0.01428, validation loss 0.01161
Resetting learning rate to 0.01000
Epoch  8304: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8315: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4210, training loss 0.00715, validation loss 0.00752
Epoch  8326: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4220, training loss 0.00545, validation loss 0.00519
Epoch  8337: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4221
Mean train loss for ascent epoch 4222: -0.0021850066259503365
Mean eval for ascent epoch 4222: 0.0044218506664037704
Epoch  8348: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4230, training loss 0.06783, validation loss 0.07740
Epoch  8359: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8370: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4240, training loss 0.03516, validation loss 0.02117
Epoch  8381: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8392: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4250, training loss 0.00808, validation loss 0.00702
Epoch  8403: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8414: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4260
Doing Evaluation on the model now
This is Epoch 4260, training loss 0.00510, validation loss 0.00478
Mean train loss for ascent epoch 4261: -0.0020713733974844217
Mean eval for ascent epoch 4261: 0.0047740028239786625
Epoch  8425: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4270, training loss 0.07755, validation loss 0.14614
Epoch  8436: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8447: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4280, training loss 0.02955, validation loss 0.01121
Epoch  8458: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8469: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4290, training loss 0.00696, validation loss 0.00712
Epoch  8480: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8491: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4299
Mean train loss for ascent epoch 4300: -0.0025473260320723057
Mean eval for ascent epoch 4300: 0.004963518586009741
Doing Evaluation on the model now
This is Epoch 4300, training loss 0.00496, validation loss 0.00459
Epoch  8502: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8513: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4310, training loss 0.13023, validation loss 0.07968
Epoch  8524: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4320, training loss 0.03012, validation loss 0.04580
Epoch  8535: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8546: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4330, training loss 0.00555, validation loss 0.00550
Epoch  8557: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8568: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4338
Mean train loss for ascent epoch 4339: -0.0035353745333850384
Mean eval for ascent epoch 4339: 0.004890015348792076
Doing Evaluation on the model now
This is Epoch 4340, training loss 0.54025, validation loss 1.08288
Epoch  8579: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8590: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4350, training loss 0.03083, validation loss 0.05939
Epoch  8601: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8612: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4360, training loss 0.02802, validation loss 0.03124
Epoch  8623: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4370, training loss 0.00541, validation loss 0.00584
Epoch  8634: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8645: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4377
Mean train loss for ascent epoch 4378: -0.002579243155196309
Mean eval for ascent epoch 4378: 0.0046744393184781075
Doing Evaluation on the model now
This is Epoch 4380, training loss 0.24705, validation loss 0.07669
Epoch  8656: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8667: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4390, training loss 0.02865, validation loss 0.02816
Epoch  8678: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8689: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4400, training loss 0.01418, validation loss 0.01196
Resetting learning rate to 0.01000
Epoch  8700: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8711: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4410, training loss 0.00908, validation loss 0.00872
Epoch  8722: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4420, training loss 0.00477, validation loss 0.00433
Epoch  8733: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4421
Mean train loss for ascent epoch 4422: -0.0028467196971178055
Mean eval for ascent epoch 4422: 0.004798402544111013
Epoch  8744: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4430, training loss 0.05199, validation loss 0.07585
Epoch  8755: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8766: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4440, training loss 0.02336, validation loss 0.01698
Epoch  8777: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8788: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4450, training loss 0.00696, validation loss 0.00527
Epoch  8799: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8810: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4460
Doing Evaluation on the model now
This is Epoch 4460, training loss 0.00467, validation loss 0.00433
Mean train loss for ascent epoch 4461: -0.00255988840945065
Mean eval for ascent epoch 4461: 0.004836488049477339
Epoch  8821: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4470, training loss 0.05425, validation loss 0.07175
Epoch  8832: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8843: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4480, training loss 0.01943, validation loss 0.01235
Epoch  8854: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8865: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4490, training loss 0.00550, validation loss 0.00455
Epoch  8876: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8887: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4499
Mean train loss for ascent epoch 4500: -0.003336691064760089
Mean eval for ascent epoch 4500: 0.005301940720528364
Doing Evaluation on the model now
This is Epoch 4500, training loss 0.00530, validation loss 0.00473
Epoch  8898: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4510, training loss 0.06995, validation loss 0.11405
Epoch  8909: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8920: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4520, training loss 0.01786, validation loss 0.01360
Epoch  8931: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8942: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4530, training loss 0.00581, validation loss 0.00548
Epoch  8953: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8964: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4538
Mean train loss for ascent epoch 4539: -0.0022025289945304394
Mean eval for ascent epoch 4539: 0.004769328515976667
Doing Evaluation on the model now
This is Epoch 4540, training loss 0.23339, validation loss 0.36875
Epoch  8975: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8986: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4550, training loss 0.02578, validation loss 0.01545
Epoch  8997: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4560, training loss 0.02268, validation loss 0.01823
Epoch  9008: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9019: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4570, training loss 0.00542, validation loss 0.00571
Epoch  9030: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9041: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4577
Mean train loss for ascent epoch 4578: -0.002659781603142619
Mean eval for ascent epoch 4578: 0.0062606967985630035
Doing Evaluation on the model now
This is Epoch 4580, training loss 0.08344, validation loss 0.08946
Epoch  9052: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9063: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4590, training loss 0.03376, validation loss 0.06411
Epoch  9074: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9085: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4600, training loss 0.01108, validation loss 0.00634
Resetting learning rate to 0.01000
Epoch  9096: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4610, training loss 0.00608, validation loss 0.00444
Epoch  9107: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9118: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4620, training loss 0.00433, validation loss 0.00474
Epoch  9129: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4622
Mean train loss for ascent epoch 4623: -0.0025038409512490034
Mean eval for ascent epoch 4623: 0.004584126640111208
Epoch  9140: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4630, training loss 0.10433, validation loss 0.07058
Epoch  9151: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9162: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4640, training loss 0.02755, validation loss 0.02051
Epoch  9173: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9184: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4650, training loss 0.00694, validation loss 0.00664
Epoch  9195: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4660, training loss 0.00493, validation loss 0.00427
Epoch  9206: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4661
Mean train loss for ascent epoch 4662: -0.0022068414837121964
Mean eval for ascent epoch 4662: 0.004850615747272968
Epoch  9217: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4670, training loss 0.13265, validation loss 0.17987
Epoch  9228: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9239: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4680, training loss 0.01985, validation loss 0.01765
Epoch  9250: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9261: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4690, training loss 0.00528, validation loss 0.00424
Epoch  9272: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9283: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4700
Doing Evaluation on the model now
This is Epoch 4700, training loss 0.00450, validation loss 0.00456
Mean train loss for ascent epoch 4701: -0.0030902812723070383
Mean eval for ascent epoch 4701: 0.0048610009253025055
Epoch  9294: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4710, training loss 0.04773, validation loss 0.05088
Epoch  9305: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9316: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4720, training loss 0.01537, validation loss 0.01772
Epoch  9327: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9338: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4730, training loss 0.00588, validation loss 0.00695
Epoch  9349: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9360: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4739
Mean train loss for ascent epoch 4740: -0.0029032863676548004
Mean eval for ascent epoch 4740: 0.004748458042740822
Doing Evaluation on the model now
This is Epoch 4740, training loss 0.00475, validation loss 0.00461
Epoch  9371: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9382: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4750, training loss 0.05446, validation loss 0.08697
Epoch  9393: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4760, training loss 0.02297, validation loss 0.01556
Epoch  9404: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9415: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4770, training loss 0.00555, validation loss 0.00638
Epoch  9426: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9437: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4778
Mean train loss for ascent epoch 4779: -0.0026131027843803167
Mean eval for ascent epoch 4779: 0.004771817475557327
Doing Evaluation on the model now
This is Epoch 4780, training loss 0.33130, validation loss 0.60477
Epoch  9448: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9459: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4790, training loss 0.04329, validation loss 0.03887
Epoch  9470: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9481: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4800, training loss 0.02128, validation loss 0.01520
Resetting learning rate to 0.01000
Epoch  9492: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4810, training loss 0.00662, validation loss 0.00770
Epoch  9503: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9514: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4820, training loss 0.00410, validation loss 0.00427
Epoch  9525: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4822
Mean train loss for ascent epoch 4823: -0.003396012354642153
Mean eval for ascent epoch 4823: 0.004465880803763866
Epoch  9536: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4830, training loss 0.29769, validation loss 0.11355
Epoch  9547: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9558: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4840, training loss 0.03331, validation loss 0.03481
Epoch  9569: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9580: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4850, training loss 0.01056, validation loss 0.00965
Epoch  9591: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4860, training loss 0.00523, validation loss 0.00654
Epoch  9602: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4861
Mean train loss for ascent epoch 4862: -0.0036110905930399895
Mean eval for ascent epoch 4862: 0.005546956788748503
Epoch  9613: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4870, training loss 0.13614, validation loss 0.04788
Epoch  9624: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9635: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4880, training loss 0.02342, validation loss 0.03042
Epoch  9646: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9657: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4890, training loss 0.01102, validation loss 0.00805
Epoch  9668: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9679: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4900
Doing Evaluation on the model now
This is Epoch 4900, training loss 0.00535, validation loss 0.00603
Mean train loss for ascent epoch 4901: -0.003343417774885893
Mean eval for ascent epoch 4901: 0.005998782813549042
Epoch  9690: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4910, training loss 0.06470, validation loss 0.02710
Epoch  9701: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9712: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4920, training loss 0.01604, validation loss 0.01493
Epoch  9723: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9734: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4930, training loss 0.00599, validation loss 0.00789
Epoch  9745: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9756: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4939
Mean train loss for ascent epoch 4940: -0.0034411170054227114
Mean eval for ascent epoch 4940: 0.005222789943218231
Doing Evaluation on the model now
This is Epoch 4940, training loss 0.00522, validation loss 0.00462
Epoch  9767: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4950, training loss 0.07991, validation loss 0.16095
Epoch  9778: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9789: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4960, training loss 0.01500, validation loss 0.01974
Epoch  9800: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9811: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4970, training loss 0.00580, validation loss 0.00571
Epoch  9822: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9833: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4978
Mean train loss for ascent epoch 4979: -0.004144284408539534
Mean eval for ascent epoch 4979: 0.005446737632155418
Doing Evaluation on the model now
This is Epoch 4980, training loss 0.15351, validation loss 0.31888
Epoch  9844: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9855: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4990, training loss 0.03085, validation loss 0.03487
Epoch  9866: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5000, training loss 0.01968, validation loss 0.03658
Epoch  9877: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  9888: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5010, training loss 0.00583, validation loss 0.00619
Epoch  9899: reducing learning rate of group 0 to 2.5000e-04.
Epoch  9910: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5020, training loss 0.00563, validation loss 0.00574
Epoch  9921: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5023
Mean train loss for ascent epoch 5024: -0.0026815240271389484
Mean eval for ascent epoch 5024: 0.005106802098453045
Epoch  9932: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5030, training loss 0.03881, validation loss 0.10393
Epoch  9943: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9954: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5040, training loss 0.01586, validation loss 0.00945
Epoch  9965: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5050, training loss 0.00856, validation loss 0.00766
Epoch  9976: reducing learning rate of group 0 to 3.1250e-04.
Epoch  9987: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5060, training loss 0.00520, validation loss 0.00451
Epoch  9998: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5062
Mean train loss for ascent epoch 5063: -0.0026657446287572384
Mean eval for ascent epoch 5063: 0.004833349492400885
Epoch 10009: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5070, training loss 0.04939, validation loss 0.06728
Epoch 10020: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10031: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5080, training loss 0.01871, validation loss 0.01847
Epoch 10042: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10053: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5090, training loss 0.00554, validation loss 0.00500
Epoch 10064: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5100, training loss 0.00477, validation loss 0.00547
Epoch 10075: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5101
Mean train loss for ascent epoch 5102: -0.0037754354998469353
Mean eval for ascent epoch 5102: 0.004932395648211241
Epoch 10086: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5110, training loss 0.05733, validation loss 0.03350
Epoch 10097: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10108: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5120, training loss 0.01473, validation loss 0.01323
Epoch 10119: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10130: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5130, training loss 0.01011, validation loss 0.00598
Epoch 10141: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10152: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5140
Doing Evaluation on the model now
This is Epoch 5140, training loss 0.00511, validation loss 0.00537
Mean train loss for ascent epoch 5141: -0.0031636226922273636
Mean eval for ascent epoch 5141: 0.00530624995008111
Epoch 10163: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5150, training loss 0.04530, validation loss 0.06228
Epoch 10174: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10185: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5160, training loss 0.02055, validation loss 0.02929
Epoch 10196: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10207: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5170, training loss 0.00576, validation loss 0.00658
Epoch 10218: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10229: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5179
Mean train loss for ascent epoch 5180: -0.002821407513692975
Mean eval for ascent epoch 5180: 0.005448352545499802
Doing Evaluation on the model now
This is Epoch 5180, training loss 0.00545, validation loss 0.00507
Epoch 10240: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10251: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5190, training loss 0.02660, validation loss 0.01648
Epoch 10262: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5200, training loss 0.01394, validation loss 0.01864
Resetting learning rate to 0.01000
Epoch 10273: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10284: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5210, training loss 0.00537, validation loss 0.00491
Epoch 10295: reducing learning rate of group 0 to 1.2500e-04.
Epoch 10306: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5218
Mean train loss for ascent epoch 5219: -0.003142873989418149
Mean eval for ascent epoch 5219: 0.005178056191653013
Doing Evaluation on the model now
This is Epoch 5220, training loss 0.20755, validation loss 0.14962
Epoch 10317: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10328: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5230, training loss 0.02075, validation loss 0.01771
Epoch 10339: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10350: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5240, training loss 0.01563, validation loss 0.00851
Epoch 10361: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5250, training loss 0.00566, validation loss 0.00538
Epoch 10372: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10383: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5257
Mean train loss for ascent epoch 5258: -0.002440787386149168
Mean eval for ascent epoch 5258: 0.004990489222109318
Doing Evaluation on the model now
This is Epoch 5260, training loss 0.19098, validation loss 0.38885
Epoch 10394: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10405: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5270, training loss 0.04079, validation loss 0.01110
Epoch 10416: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10427: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5280, training loss 0.01006, validation loss 0.00906
Epoch 10438: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10449: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5290, training loss 0.00588, validation loss 0.00494
Epoch 10460: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5296
Mean train loss for ascent epoch 5297: -0.002386798383668065
Mean eval for ascent epoch 5297: 0.004949444439262152
Doing Evaluation on the model now
This is Epoch 5300, training loss 0.46350, validation loss 0.32240
Epoch 10471: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10482: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5310, training loss 0.02966, validation loss 0.03915
Epoch 10493: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10504: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5320, training loss 0.00944, validation loss 0.00752
Epoch 10515: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10526: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5330, training loss 0.00578, validation loss 0.00542
Epoch 10537: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5335
Mean train loss for ascent epoch 5336: -0.0021931841038167477
Mean eval for ascent epoch 5336: 0.0050940546207129955
Doing Evaluation on the model now
This is Epoch 5340, training loss 0.23427, validation loss 0.47624
Epoch 10548: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10559: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5350, training loss 0.03150, validation loss 0.03872
Epoch 10570: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10581: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5360, training loss 0.00716, validation loss 0.00671
Epoch 10592: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10603: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5370, training loss 0.00485, validation loss 0.00459
Epoch 10614: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5374
Mean train loss for ascent epoch 5375: -0.0020321153569966555
Mean eval for ascent epoch 5375: 0.005105066578835249
Epoch 10625: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5380, training loss 0.17095, validation loss 0.14590
Epoch 10636: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5390, training loss 0.03521, validation loss 0.04188
Epoch 10647: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10658: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5400, training loss 0.00973, validation loss 0.00979
Resetting learning rate to 0.01000
Epoch 10669: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10680: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5410, training loss 0.00602, validation loss 0.00567
Epoch 10691: reducing learning rate of group 0 to 1.2500e-04.
Epoch 10702: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5418
Mean train loss for ascent epoch 5419: -0.0027335186023265123
Mean eval for ascent epoch 5419: 0.005674596875905991
Doing Evaluation on the model now
This is Epoch 5420, training loss 0.12487, validation loss 0.19898
Epoch 10713: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10724: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5430, training loss 0.02812, validation loss 0.03181
Epoch 10735: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5440, training loss 0.01421, validation loss 0.01575
Epoch 10746: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10757: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5450, training loss 0.00519, validation loss 0.00526
Epoch 10768: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10779: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5457
Mean train loss for ascent epoch 5458: -0.0026271205861121416
Mean eval for ascent epoch 5458: 0.005293722730129957
Doing Evaluation on the model now
This is Epoch 5460, training loss 0.38738, validation loss 0.36353
Epoch 10790: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10801: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5470, training loss 0.02754, validation loss 0.01221
Epoch 10812: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10823: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5480, training loss 0.01253, validation loss 0.00782
Epoch 10834: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5490, training loss 0.00729, validation loss 0.00694
Epoch 10845: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10856: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5496
Mean train loss for ascent epoch 5497: -0.0026020787190645933
Mean eval for ascent epoch 5497: 0.006260060239583254
Doing Evaluation on the model now
This is Epoch 5500, training loss 0.15333, validation loss 0.06617
Epoch 10867: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10878: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5510, training loss 0.05416, validation loss 0.06563
Epoch 10889: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10900: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5520, training loss 0.00768, validation loss 0.00664
Epoch 10911: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10922: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5530, training loss 0.00558, validation loss 0.00608
Epoch 10933: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5535
Mean train loss for ascent epoch 5536: -0.0020235544070601463
Mean eval for ascent epoch 5536: 0.005171177908778191
Doing Evaluation on the model now
This is Epoch 5540, training loss 0.15699, validation loss 0.15508
Epoch 10944: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10955: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5550, training loss 0.04592, validation loss 0.04586
Epoch 10966: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10977: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5560, training loss 0.00883, validation loss 0.01561
Epoch 10988: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10999: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5570, training loss 0.00572, validation loss 0.00597
Epoch 11010: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5574
Mean train loss for ascent epoch 5575: -0.001869638916105032
Mean eval for ascent epoch 5575: 0.0054994686506688595
Epoch 11021: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5580, training loss 0.31306, validation loss 0.31566
Epoch 11032: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5590, training loss 0.05544, validation loss 0.05820
Epoch 11043: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11054: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5600, training loss 0.00670, validation loss 0.00762
Resetting learning rate to 0.01000
Epoch 11065: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11076: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5610, training loss 0.00543, validation loss 0.00560
Epoch 11087: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11098: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5619
Mean train loss for ascent epoch 5620: -0.003142740111798048
Mean eval for ascent epoch 5620: 0.005105047486722469
Doing Evaluation on the model now
This is Epoch 5620, training loss 0.00511, validation loss 0.00590
Epoch 11109: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11120: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5630, training loss 0.08604, validation loss 0.07026
Epoch 11131: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5640, training loss 0.01693, validation loss 0.00987
Epoch 11142: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11153: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5650, training loss 0.00692, validation loss 0.00584
Epoch 11164: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11175: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5658
Mean train loss for ascent epoch 5659: -0.0026702661998569965
Mean eval for ascent epoch 5659: 0.004781011492013931
Doing Evaluation on the model now
This is Epoch 5660, training loss 0.11243, validation loss 0.30095
Epoch 11186: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11197: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5670, training loss 0.03154, validation loss 0.02636
Epoch 11208: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11219: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5680, training loss 0.01977, validation loss 0.03610
Epoch 11230: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5690, training loss 0.00611, validation loss 0.00628
Epoch 11241: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11252: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5697
Mean train loss for ascent epoch 5698: -0.0029477174393832684
Mean eval for ascent epoch 5698: 0.004552416503429413
Doing Evaluation on the model now
This is Epoch 5700, training loss 0.21709, validation loss 0.29795
Epoch 11263: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11274: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5710, training loss 0.02632, validation loss 0.01856
Epoch 11285: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11296: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5720, training loss 0.01176, validation loss 0.01239
Epoch 11307: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11318: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5730, training loss 0.00531, validation loss 0.00495
Epoch 11329: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5736
Mean train loss for ascent epoch 5737: -0.0022296523675322533
Mean eval for ascent epoch 5737: 0.004005443304777145
Doing Evaluation on the model now
This is Epoch 5740, training loss 0.15503, validation loss 0.17738
Epoch 11340: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11351: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5750, training loss 0.04025, validation loss 0.02628
Epoch 11362: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11373: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5760, training loss 0.00787, validation loss 0.00518
Epoch 11384: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11395: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5770, training loss 0.00530, validation loss 0.00577
Epoch 11406: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5775
Mean train loss for ascent epoch 5776: -0.0026764017529785633
Mean eval for ascent epoch 5776: 0.004323041532188654
Doing Evaluation on the model now
This is Epoch 5780, training loss 0.42758, validation loss 0.62967
Epoch 11417: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11428: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5790, training loss 0.03555, validation loss 0.04825
Epoch 11439: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11450: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5800, training loss 0.01016, validation loss 0.01635
Resetting learning rate to 0.01000
Epoch 11461: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11472: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5810, training loss 0.00642, validation loss 0.00776
Epoch 11483: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11494: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5819
Mean train loss for ascent epoch 5820: -0.002467425772920251
Mean eval for ascent epoch 5820: 0.005398975685238838
Doing Evaluation on the model now
This is Epoch 5820, training loss 0.00540, validation loss 0.00505
Epoch 11505: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5830, training loss 0.04946, validation loss 0.09153
Epoch 11516: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11527: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5840, training loss 0.02105, validation loss 0.02423
Epoch 11538: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11549: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5850, training loss 0.00520, validation loss 0.00552
Epoch 11560: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11571: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5858
Mean train loss for ascent epoch 5859: -0.003508290508762002
Mean eval for ascent epoch 5859: 0.004880248568952084
Doing Evaluation on the model now
This is Epoch 5860, training loss 0.09894, validation loss 0.02458
Epoch 11582: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11593: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5870, training loss 0.04646, validation loss 0.06319
Epoch 11604: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5880, training loss 0.02106, validation loss 0.02440
Epoch 11615: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11626: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5890, training loss 0.00566, validation loss 0.00497
Epoch 11637: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11648: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5897
Mean train loss for ascent epoch 5898: -0.003893274115398526
Mean eval for ascent epoch 5898: 0.00447465293109417
Doing Evaluation on the model now
This is Epoch 5900, training loss 0.13307, validation loss 0.04609
Epoch 11659: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11670: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5910, training loss 0.06016, validation loss 0.07518
Epoch 11681: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11692: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5920, training loss 0.01507, validation loss 0.01209
Epoch 11703: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5930, training loss 0.00651, validation loss 0.00799
Epoch 11714: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11725: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5936
Mean train loss for ascent epoch 5937: -0.0025602413807064295
Mean eval for ascent epoch 5937: 0.004504929296672344
Doing Evaluation on the model now
This is Epoch 5940, training loss 0.22066, validation loss 0.40901
Epoch 11736: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11747: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5950, training loss 0.02861, validation loss 0.03405
Epoch 11758: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11769: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5960, training loss 0.00835, validation loss 0.00668
Epoch 11780: reducing learning rate of group 0 to 3.1250e-04.
Epoch 11791: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5970, training loss 0.00507, validation loss 0.00483
Epoch 11802: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5975
Mean train loss for ascent epoch 5976: -0.0023160974960774183
Mean eval for ascent epoch 5976: 0.004780267830938101
Doing Evaluation on the model now
This is Epoch 5980, training loss 0.19187, validation loss 0.40377
Epoch 11813: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11824: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5990, training loss 0.06073, validation loss 0.04501
Epoch 11835: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11846: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6000, training loss 0.00716, validation loss 0.01152
Resetting learning rate to 0.01000
Epoch 11857: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11868: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6010, training loss 0.00426, validation loss 0.00397
Epoch 11879: reducing learning rate of group 0 to 1.2500e-04.
Epoch 11890: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6020
Doing Evaluation on the model now
This is Epoch 6020, training loss 0.00432, validation loss 0.00382
Mean train loss for ascent epoch 6021: -0.0027797373477369547
Mean eval for ascent epoch 6021: 0.004349750932306051
Epoch 11901: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6030, training loss 0.10179, validation loss 0.05581
Epoch 11912: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11923: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6040, training loss 0.01791, validation loss 0.02677
Epoch 11934: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11945: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6050, training loss 0.00680, validation loss 0.00674
Epoch 11956: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11967: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6059
Mean train loss for ascent epoch 6060: -0.003182918531820178
Mean eval for ascent epoch 6060: 0.004696316551417112
Doing Evaluation on the model now
This is Epoch 6060, training loss 0.00470, validation loss 0.00424
Epoch 11978: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11989: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6070, training loss 0.14624, validation loss 0.17287
Epoch 12000: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6080, training loss 0.02780, validation loss 0.01928
Epoch 12011: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12022: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6090, training loss 0.00652, validation loss 0.00573
Epoch 12033: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12044: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6098
Mean train loss for ascent epoch 6099: -0.002684811595827341
Mean eval for ascent epoch 6099: 0.005177585873752832
Doing Evaluation on the model now
This is Epoch 6100, training loss 0.18807, validation loss 0.49150
Epoch 12055: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12066: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6110, training loss 0.02676, validation loss 0.01755
Epoch 12077: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12088: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6120, training loss 0.01788, validation loss 0.01771
Epoch 12099: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6130, training loss 0.00550, validation loss 0.00495
Epoch 12110: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12121: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6137
Mean train loss for ascent epoch 6138: -0.0034850332885980606
Mean eval for ascent epoch 6138: 0.005770138464868069
Doing Evaluation on the model now
This is Epoch 6140, training loss 0.07760, validation loss 0.09391
Epoch 12132: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12143: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6150, training loss 0.03527, validation loss 0.06969
Epoch 12154: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12165: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6160, training loss 0.01113, validation loss 0.00791
Epoch 12176: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12187: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6170, training loss 0.00608, validation loss 0.00578
Epoch 12198: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6176
Mean train loss for ascent epoch 6177: -0.002117800759151578
Mean eval for ascent epoch 6177: 0.005547157023102045
Doing Evaluation on the model now
This is Epoch 6180, training loss 0.41562, validation loss 0.12866
Epoch 12209: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12220: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6190, training loss 0.06223, validation loss 0.07290
Epoch 12231: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12242: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6200, training loss 0.01427, validation loss 0.01199
Resetting learning rate to 0.01000
Epoch 12253: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12264: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6210, training loss 0.00614, validation loss 0.00507
Epoch 12275: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12286: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6220
Doing Evaluation on the model now
This is Epoch 6220, training loss 0.00643, validation loss 0.00579
Mean train loss for ascent epoch 6221: -0.0020374071318656206
Mean eval for ascent epoch 6221: 0.00507609685882926
Epoch 12297: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6230, training loss 0.07819, validation loss 0.06465
Epoch 12308: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12319: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6240, training loss 0.02576, validation loss 0.01696
Epoch 12330: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12341: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6250, training loss 0.00596, validation loss 0.00537
Epoch 12352: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12363: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6259
Mean train loss for ascent epoch 6260: -0.0019506445387378335
Mean eval for ascent epoch 6260: 0.004622103180736303
Doing Evaluation on the model now
This is Epoch 6260, training loss 0.00462, validation loss 0.00464
Epoch 12374: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6270, training loss 0.11076, validation loss 0.13553
Epoch 12385: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12396: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6280, training loss 0.01498, validation loss 0.01188
Epoch 12407: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12418: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6290, training loss 0.00553, validation loss 0.00431
Epoch 12429: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12440: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6298
Mean train loss for ascent epoch 6299: -0.0014875333290547132
Mean eval for ascent epoch 6299: 0.004184606019407511
Doing Evaluation on the model now
This is Epoch 6300, training loss 0.15606, validation loss 0.24481
Epoch 12451: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12462: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6310, training loss 0.02952, validation loss 0.01006
Epoch 12473: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6320, training loss 0.01867, validation loss 0.01519
Epoch 12484: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12495: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6330, training loss 0.00500, validation loss 0.00503
Epoch 12506: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12517: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6337
Mean train loss for ascent epoch 6338: -0.0024143545888364315
Mean eval for ascent epoch 6338: 0.004636751022189856
Doing Evaluation on the model now
This is Epoch 6340, training loss 0.72057, validation loss 0.58809
Epoch 12528: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12539: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6350, training loss 0.02964, validation loss 0.02004
Epoch 12550: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12561: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6360, training loss 0.01061, validation loss 0.00924
Epoch 12572: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6370, training loss 0.00596, validation loss 0.00548
Epoch 12583: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12594: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6376
Mean train loss for ascent epoch 6377: -0.0023856512270867825
Mean eval for ascent epoch 6377: 0.004534410312771797
Doing Evaluation on the model now
This is Epoch 6380, training loss 0.23168, validation loss 0.32183
Epoch 12605: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12616: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6390, training loss 0.05867, validation loss 0.05843
Epoch 12627: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12638: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6400, training loss 0.01047, validation loss 0.00881
Resetting learning rate to 0.01000
Epoch 12649: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12660: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6410, training loss 0.00631, validation loss 0.00507
Epoch 12671: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6420, training loss 0.00490, validation loss 0.00431
Epoch 12682: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6421
Mean train loss for ascent epoch 6422: -0.0019564011599868536
Mean eval for ascent epoch 6422: 0.004261816386133432
Epoch 12693: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6430, training loss 0.11117, validation loss 0.06495
Epoch 12704: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12715: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6440, training loss 0.01546, validation loss 0.01055
Epoch 12726: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12737: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6450, training loss 0.00505, validation loss 0.00517
Epoch 12748: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12759: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6460
Doing Evaluation on the model now
This is Epoch 6460, training loss 0.00491, validation loss 0.00521
Mean train loss for ascent epoch 6461: -0.0024975109845399857
Mean eval for ascent epoch 6461: 0.005184505134820938
Epoch 12770: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6470, training loss 0.06195, validation loss 0.08677
Epoch 12781: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12792: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6480, training loss 0.01695, validation loss 0.00833
Epoch 12803: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12814: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6490, training loss 0.00520, validation loss 0.00565
Epoch 12825: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12836: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6499
Mean train loss for ascent epoch 6500: -0.0018846049206331372
Mean eval for ascent epoch 6500: 0.003984745126217604
Doing Evaluation on the model now
This is Epoch 6500, training loss 0.00398, validation loss 0.00458
Epoch 12847: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12858: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6510, training loss 0.04436, validation loss 0.10267
Epoch 12869: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6520, training loss 0.01426, validation loss 0.01113
Epoch 12880: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12891: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6530, training loss 0.00636, validation loss 0.00505
Epoch 12902: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12913: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6538
Mean train loss for ascent epoch 6539: -0.002003432484343648
Mean eval for ascent epoch 6539: 0.005099385976791382
Doing Evaluation on the model now
This is Epoch 6540, training loss 0.17621, validation loss 0.17628
Epoch 12924: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12935: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6550, training loss 0.03507, validation loss 0.03160
Epoch 12946: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12957: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6560, training loss 0.02506, validation loss 0.02409
Epoch 12968: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6570, training loss 0.00571, validation loss 0.00467
Epoch 12979: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12990: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6577
Mean train loss for ascent epoch 6578: -0.0022716408129781485
Mean eval for ascent epoch 6578: 0.004597077611833811
Doing Evaluation on the model now
This is Epoch 6580, training loss 0.35022, validation loss 0.27292
Epoch 13001: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13012: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6590, training loss 0.05143, validation loss 0.05374
Epoch 13023: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13034: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6600, training loss 0.01408, validation loss 0.01213
Resetting learning rate to 0.01000
Epoch 13045: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13056: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6610, training loss 0.00668, validation loss 0.00606
Epoch 13067: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6620, training loss 0.00462, validation loss 0.00590
Epoch 13078: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6621
Mean train loss for ascent epoch 6622: -0.002185094403102994
Mean eval for ascent epoch 6622: 0.004944258835166693
Epoch 13089: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6630, training loss 0.07377, validation loss 0.04693
Epoch 13100: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13111: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6640, training loss 0.02911, validation loss 0.03822
Epoch 13122: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13133: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6650, training loss 0.00684, validation loss 0.00626
Epoch 13144: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13155: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6660
Doing Evaluation on the model now
This is Epoch 6660, training loss 0.00505, validation loss 0.00407
Mean train loss for ascent epoch 6661: -0.0030343355610966682
Mean eval for ascent epoch 6661: 0.004955371841788292
Epoch 13166: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6670, training loss 0.11384, validation loss 0.06251
Epoch 13177: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13188: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6680, training loss 0.02533, validation loss 0.04092
Epoch 13199: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13210: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6690, training loss 0.00559, validation loss 0.00622
Epoch 13221: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13232: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6699
Mean train loss for ascent epoch 6700: -0.0042311688885092735
Mean eval for ascent epoch 6700: 0.006011713296175003
Doing Evaluation on the model now
This is Epoch 6700, training loss 0.00601, validation loss 0.00470
Epoch 13243: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6710, training loss 0.06650, validation loss 0.08460
Epoch 13254: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13265: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6720, training loss 0.01799, validation loss 0.01412
Epoch 13276: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13287: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6730, training loss 0.00603, validation loss 0.00717
Epoch 13298: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13309: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6738
Mean train loss for ascent epoch 6739: -0.0020825425162911415
Mean eval for ascent epoch 6739: 0.005570252891629934
Doing Evaluation on the model now
This is Epoch 6740, training loss 0.19162, validation loss 0.21883
Epoch 13320: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13331: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6750, training loss 0.04604, validation loss 0.01846
Epoch 13342: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6760, training loss 0.01882, validation loss 0.01744
Epoch 13353: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13364: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6770, training loss 0.00685, validation loss 0.00658
Epoch 13375: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13386: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6777
Mean train loss for ascent epoch 6778: -0.0027315348852425814
Mean eval for ascent epoch 6778: 0.004833655431866646
Doing Evaluation on the model now
This is Epoch 6780, training loss 0.42174, validation loss 0.89146
Epoch 13397: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13408: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6790, training loss 0.02991, validation loss 0.03222
Epoch 13419: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13430: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6800, training loss 0.01269, validation loss 0.01511
Resetting learning rate to 0.01000
Epoch 13441: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 6810, training loss 0.00726, validation loss 0.00715
Epoch 13452: reducing learning rate of group 0 to 2.5000e-04.
Epoch 13463: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6820, training loss 0.00512, validation loss 0.00581
Epoch 13474: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6822
Mean train loss for ascent epoch 6823: -0.0022842788603156805
Mean eval for ascent epoch 6823: 0.005024421028792858
Epoch 13485: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6830, training loss 0.10627, validation loss 0.08696
Epoch 13496: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13507: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6840, training loss 0.03116, validation loss 0.02224
Epoch 13518: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13529: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6850, training loss 0.00787, validation loss 0.00558
Epoch 13540: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6860, training loss 0.00479, validation loss 0.00443
Epoch 13551: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6861
Mean train loss for ascent epoch 6862: -0.0022510718554258347
Mean eval for ascent epoch 6862: 0.004907199647277594
Epoch 13562: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6870, training loss 0.07971, validation loss 0.14071
Epoch 13573: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13584: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6880, training loss 0.02477, validation loss 0.02802
Epoch 13595: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13606: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6890, training loss 0.00742, validation loss 0.00640
Epoch 13617: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13628: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6900
Doing Evaluation on the model now
This is Epoch 6900, training loss 0.00500, validation loss 0.00485
Mean train loss for ascent epoch 6901: -0.0028299852274358273
Mean eval for ascent epoch 6901: 0.005126295145601034
Epoch 13639: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6910, training loss 0.13222, validation loss 0.08256
Epoch 13650: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13661: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6920, training loss 0.03242, validation loss 0.02624
Epoch 13672: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13683: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6930, training loss 0.00729, validation loss 0.00642
Epoch 13694: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13705: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6939
Mean train loss for ascent epoch 6940: -0.0022521361242979765
Mean eval for ascent epoch 6940: 0.004823843017220497
Doing Evaluation on the model now
This is Epoch 6940, training loss 0.00482, validation loss 0.00441
Epoch 13716: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13727: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6950, training loss 0.07401, validation loss 0.11370
Epoch 13738: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6960, training loss 0.02703, validation loss 0.02546
Epoch 13749: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13760: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6970, training loss 0.00649, validation loss 0.00716
Epoch 13771: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13782: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6978
Mean train loss for ascent epoch 6979: -0.00299699860624969
Mean eval for ascent epoch 6979: 0.005124480929225683
Doing Evaluation on the model now
This is Epoch 6980, training loss 0.32101, validation loss 0.34475
Epoch 13793: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13804: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6990, training loss 0.02751, validation loss 0.02323
Epoch 13815: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13826: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7000, training loss 0.03528, validation loss 0.04289
Resetting learning rate to 0.01000
Epoch 13837: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7010, training loss 0.00975, validation loss 0.01283
Epoch 13848: reducing learning rate of group 0 to 2.5000e-04.
Epoch 13859: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7020, training loss 0.00543, validation loss 0.00487
Epoch 13870: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7022
Mean train loss for ascent epoch 7023: -0.002235604450106621
Mean eval for ascent epoch 7023: 0.005498817190527916
Epoch 13881: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7030, training loss 0.17976, validation loss 0.32274
Epoch 13892: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13903: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7040, training loss 0.03585, validation loss 0.01728
Epoch 13914: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13925: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7050, training loss 0.01054, validation loss 0.01160
Epoch 13936: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7060, training loss 0.00505, validation loss 0.00524
Epoch 13947: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7061
Mean train loss for ascent epoch 7062: -0.0023411139845848083
Mean eval for ascent epoch 7062: 0.004587455186992884
Epoch 13958: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7070, training loss 0.05604, validation loss 0.03527
Epoch 13969: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13980: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7080, training loss 0.02073, validation loss 0.01418
Epoch 13991: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14002: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7090, training loss 0.00618, validation loss 0.01084
Epoch 14013: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14024: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7100
Doing Evaluation on the model now
This is Epoch 7100, training loss 0.00509, validation loss 0.00584
Mean train loss for ascent epoch 7101: -0.003171577351167798
Mean eval for ascent epoch 7101: 0.0049100397154688835
Epoch 14035: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7110, training loss 0.04307, validation loss 0.03752
Epoch 14046: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14057: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7120, training loss 0.01457, validation loss 0.01397
Epoch 14068: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14079: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7130, training loss 0.00949, validation loss 0.00860
Epoch 14090: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14101: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7139
Mean train loss for ascent epoch 7140: -0.0028047962114214897
Mean eval for ascent epoch 7140: 0.0050794342532753944
Doing Evaluation on the model now
This is Epoch 7140, training loss 0.00508, validation loss 0.00484
Epoch 14112: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7150, training loss 0.16207, validation loss 0.14184
Epoch 14123: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14134: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7160, training loss 0.02354, validation loss 0.02373
Epoch 14145: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14156: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7170, training loss 0.00612, validation loss 0.00757
Epoch 14167: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14178: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7178
Mean train loss for ascent epoch 7179: -0.0029203067533671856
Mean eval for ascent epoch 7179: 0.005204310175031424
Doing Evaluation on the model now
This is Epoch 7180, training loss 0.23151, validation loss 0.27027
Epoch 14189: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14200: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7190, training loss 0.03908, validation loss 0.03280
Epoch 14211: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7200, training loss 0.02069, validation loss 0.01557
Epoch 14222: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 14233: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 7210, training loss 0.00872, validation loss 0.00708
Epoch 14244: reducing learning rate of group 0 to 2.5000e-04.
Epoch 14255: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7220, training loss 0.00597, validation loss 0.00556
Epoch 14266: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7223
Mean train loss for ascent epoch 7224: -0.0014567408943548799
Mean eval for ascent epoch 7224: 0.004240273032337427
Epoch 14277: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7230, training loss 0.08327, validation loss 0.03116
Epoch 14288: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14299: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7240, training loss 0.02715, validation loss 0.02868
Epoch 14310: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7250, training loss 0.00946, validation loss 0.00667
Epoch 14321: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14332: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7260, training loss 0.00481, validation loss 0.00420
Epoch 14343: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7262
Mean train loss for ascent epoch 7263: -0.002619565697386861
Mean eval for ascent epoch 7263: 0.005098877940326929
Epoch 14354: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7270, training loss 0.08549, validation loss 0.09226
Epoch 14365: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14376: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7280, training loss 0.02960, validation loss 0.04061
Epoch 14387: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14398: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7290, training loss 0.00642, validation loss 0.00454
Epoch 14409: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7300, training loss 0.00490, validation loss 0.00462
Epoch 14420: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7301
Mean train loss for ascent epoch 7302: -0.003064642893150449
Mean eval for ascent epoch 7302: 0.005387469194829464
Epoch 14431: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7310, training loss 0.08840, validation loss 0.08276
Epoch 14442: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14453: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7320, training loss 0.02357, validation loss 0.01732
Epoch 14464: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14475: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7330, training loss 0.00575, validation loss 0.00668
Epoch 14486: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14497: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7340
Doing Evaluation on the model now
This is Epoch 7340, training loss 0.00495, validation loss 0.00589
Mean train loss for ascent epoch 7341: -0.0021665862295776606
Mean eval for ascent epoch 7341: 0.004991021007299423
Epoch 14508: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7350, training loss 0.02675, validation loss 0.03101
Epoch 14519: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14530: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7360, training loss 0.03989, validation loss 0.02544
Epoch 14541: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14552: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7370, training loss 0.00667, validation loss 0.00657
Epoch 14563: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14574: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7379
Mean train loss for ascent epoch 7380: -0.002018781378865242
Mean eval for ascent epoch 7380: 0.004923864267766476
Doing Evaluation on the model now
This is Epoch 7380, training loss 0.00492, validation loss 0.00568
Epoch 14585: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14596: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7390, training loss 0.06416, validation loss 0.05627
Epoch 14607: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7400, training loss 0.02485, validation loss 0.01789
Resetting learning rate to 0.01000
Epoch 14618: reducing learning rate of group 0 to 5.0000e-04.
Epoch 14629: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7410, training loss 0.00621, validation loss 0.00683
Epoch 14640: reducing learning rate of group 0 to 1.2500e-04.
Epoch 14651: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7418
Mean train loss for ascent epoch 7419: -0.002767319791018963
Mean eval for ascent epoch 7419: 0.004935752600431442
Doing Evaluation on the model now
This is Epoch 7420, training loss 0.21889, validation loss 0.16072
Epoch 14662: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14673: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7430, training loss 0.02165, validation loss 0.02792
Epoch 14684: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14695: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7440, training loss 0.02144, validation loss 0.02575
Epoch 14706: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7450, training loss 0.00530, validation loss 0.00616
Epoch 14717: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14728: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7457
Mean train loss for ascent epoch 7458: -0.0030267799738794565
Mean eval for ascent epoch 7458: 0.004999872297048569
Doing Evaluation on the model now
This is Epoch 7460, training loss 0.46107, validation loss 0.81724
Epoch 14739: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14750: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7470, training loss 0.05002, validation loss 0.07940
Epoch 14761: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14772: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7480, training loss 0.01502, validation loss 0.00615
Epoch 14783: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14794: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7490, training loss 0.00601, validation loss 0.00552
Epoch 14805: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7496
Mean train loss for ascent epoch 7497: -0.0027849464677274227
Mean eval for ascent epoch 7497: 0.005995909683406353
Doing Evaluation on the model now
This is Epoch 7500, training loss 0.15116, validation loss 0.24705
Epoch 14816: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14827: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7510, training loss 0.04295, validation loss 0.03932
Epoch 14838: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14849: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7520, training loss 0.00923, validation loss 0.00600
Epoch 14860: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14871: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7530, training loss 0.00557, validation loss 0.00438
Epoch 14882: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7535
Mean train loss for ascent epoch 7536: -0.0026909462176263332
Mean eval for ascent epoch 7536: 0.005641218274831772
Doing Evaluation on the model now
This is Epoch 7540, training loss 0.50864, validation loss 0.20536
Epoch 14893: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14904: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7550, training loss 0.05599, validation loss 0.03798
Epoch 14915: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14926: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7560, training loss 0.00716, validation loss 0.00874
Epoch 14937: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14948: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7570, training loss 0.00512, validation loss 0.00497
Epoch 14959: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7574
Mean train loss for ascent epoch 7575: -0.002278857631608844
Mean eval for ascent epoch 7575: 0.0054436540231108665
Epoch 14970: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7580, training loss 0.17230, validation loss 0.21896
Epoch 14981: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7590, training loss 0.04617, validation loss 0.03833
Epoch 14992: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15003: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7600, training loss 0.01120, validation loss 0.01439
Resetting learning rate to 0.01000
Epoch 15014: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15025: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7610, training loss 0.00547, validation loss 0.00655
Epoch 15036: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15047: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7618
Mean train loss for ascent epoch 7619: -0.0024632804561406374
Mean eval for ascent epoch 7619: 0.006147341337054968
Doing Evaluation on the model now
This is Epoch 7620, training loss 0.23299, validation loss 0.22621
Epoch 15058: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15069: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7630, training loss 0.05251, validation loss 0.01472
Epoch 15080: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7640, training loss 0.02843, validation loss 0.01812
Epoch 15091: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15102: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7650, training loss 0.00635, validation loss 0.00508
Epoch 15113: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15124: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7657
Mean train loss for ascent epoch 7658: -0.0033204355277121067
Mean eval for ascent epoch 7658: 0.0054484205320477486
Doing Evaluation on the model now
This is Epoch 7660, training loss 0.37727, validation loss 0.24591
Epoch 15135: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15146: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7670, training loss 0.03406, validation loss 0.04267
Epoch 15157: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15168: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7680, training loss 0.01678, validation loss 0.01590
Epoch 15179: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7690, training loss 0.00672, validation loss 0.00652
Epoch 15190: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15201: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7696
Mean train loss for ascent epoch 7697: -0.002394136507064104
Mean eval for ascent epoch 7697: 0.005570477340370417
Doing Evaluation on the model now
This is Epoch 7700, training loss 0.62835, validation loss 0.67929
Epoch 15212: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15223: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7710, training loss 0.03562, validation loss 0.02527
Epoch 15234: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15245: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7720, training loss 0.00993, validation loss 0.00881
Epoch 15256: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15267: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7730, training loss 0.00635, validation loss 0.00498
Epoch 15278: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7735
Mean train loss for ascent epoch 7736: -0.0026431868318468332
Mean eval for ascent epoch 7736: 0.005201819818466902
Doing Evaluation on the model now
This is Epoch 7740, training loss 0.94575, validation loss 0.74779
Epoch 15289: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15300: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7750, training loss 0.06693, validation loss 0.06187
Epoch 15311: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15322: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7760, training loss 0.00843, validation loss 0.01084
Epoch 15333: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15344: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7770, training loss 0.00489, validation loss 0.00550
Epoch 15355: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7774
Mean train loss for ascent epoch 7775: -0.0029524986166507006
Mean eval for ascent epoch 7775: 0.005225401371717453
Epoch 15366: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7780, training loss 1.55371, validation loss 0.57025
Epoch 15377: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7790, training loss 0.05420, validation loss 0.05777
Epoch 15388: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15399: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7800, training loss 0.00940, validation loss 0.01577
Resetting learning rate to 0.01000
Epoch 15410: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15421: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7810, training loss 0.00497, validation loss 0.00567
Epoch 15432: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15443: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7819
Mean train loss for ascent epoch 7820: -0.002292454009875655
Mean eval for ascent epoch 7820: 0.004352057818323374
Doing Evaluation on the model now
This is Epoch 7820, training loss 0.00435, validation loss 0.00479
Epoch 15454: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15465: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7830, training loss 0.04281, validation loss 0.04280
Epoch 15476: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7840, training loss 0.01632, validation loss 0.00930
Epoch 15487: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15498: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7850, training loss 0.00626, validation loss 0.00704
Epoch 15509: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15520: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7858
Mean train loss for ascent epoch 7859: -0.002172900829464197
Mean eval for ascent epoch 7859: 0.004335310775786638
Doing Evaluation on the model now
This is Epoch 7860, training loss 0.28394, validation loss 0.34505
Epoch 15531: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15542: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7870, training loss 0.02943, validation loss 0.02071
Epoch 15553: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15564: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7880, training loss 0.01678, validation loss 0.01811
Epoch 15575: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7890, training loss 0.00552, validation loss 0.00502
Epoch 15586: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15597: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7897
Mean train loss for ascent epoch 7898: -0.002621202729642391
Mean eval for ascent epoch 7898: 0.005008848384022713
Doing Evaluation on the model now
This is Epoch 7900, training loss 0.53495, validation loss 0.88188
Epoch 15608: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15619: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7910, training loss 0.03012, validation loss 0.02656
Epoch 15630: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15641: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7920, training loss 0.00968, validation loss 0.00860
Epoch 15652: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15663: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7930, training loss 0.00552, validation loss 0.00779
Epoch 15674: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7936
Mean train loss for ascent epoch 7937: -0.0037879205774515867
Mean eval for ascent epoch 7937: 0.004706189036369324
Doing Evaluation on the model now
This is Epoch 7940, training loss 0.13022, validation loss 0.23670
Epoch 15685: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15696: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7950, training loss 0.06139, validation loss 0.07375
Epoch 15707: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15718: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7960, training loss 0.01246, validation loss 0.01217
Epoch 15729: reducing learning rate of group 0 to 3.1250e-04.
Epoch 15740: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7970, training loss 0.00540, validation loss 0.00419
Epoch 15751: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7975
Mean train loss for ascent epoch 7976: -0.0032540515530854464
Mean eval for ascent epoch 7976: 0.005090759601444006
Doing Evaluation on the model now
This is Epoch 7980, training loss 0.63921, validation loss 0.40394
Epoch 15762: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15773: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7990, training loss 0.03873, validation loss 0.01711
Epoch 15784: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15795: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8000, training loss 0.00985, validation loss 0.01021
Resetting learning rate to 0.01000
Epoch 15806: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15817: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8010, training loss 0.00474, validation loss 0.00574
Epoch 15828: reducing learning rate of group 0 to 1.2500e-04.
Epoch 15839: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8019
Mean train loss for ascent epoch 8020: -0.001977752661332488
Mean eval for ascent epoch 8020: 0.004250155296176672
Doing Evaluation on the model now
This is Epoch 8020, training loss 0.00425, validation loss 0.00514
Epoch 15850: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8030, training loss 0.08552, validation loss 0.11162
Epoch 15861: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15872: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8040, training loss 0.02312, validation loss 0.01805
Epoch 15883: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15894: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8050, training loss 0.00634, validation loss 0.00866
Epoch 15905: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15916: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8058
Mean train loss for ascent epoch 8059: -0.0021827633026987314
Mean eval for ascent epoch 8059: 0.005155973602086306
Doing Evaluation on the model now
This is Epoch 8060, training loss 0.14128, validation loss 0.33666
Epoch 15927: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15938: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8070, training loss 0.02829, validation loss 0.01967
Epoch 15949: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8080, training loss 0.01195, validation loss 0.02501
Epoch 15960: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15971: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8090, training loss 0.00524, validation loss 0.00475
Epoch 15982: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15993: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8097
Mean train loss for ascent epoch 8098: -0.002885366789996624
Mean eval for ascent epoch 8098: 0.004795193672180176
Doing Evaluation on the model now
This is Epoch 8100, training loss 0.23992, validation loss 0.55882
Epoch 16004: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16015: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8110, training loss 0.03220, validation loss 0.05709
Epoch 16026: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16037: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8120, training loss 0.02114, validation loss 0.01844
Epoch 16048: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8130, training loss 0.00533, validation loss 0.00509
Epoch 16059: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16070: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8136
Mean train loss for ascent epoch 8137: -0.002102044178172946
Mean eval for ascent epoch 8137: 0.005346877966076136
Doing Evaluation on the model now
This is Epoch 8140, training loss 1.06087, validation loss 0.95194
Epoch 16081: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16092: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8150, training loss 0.05001, validation loss 0.05517
Epoch 16103: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16114: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8160, training loss 0.00800, validation loss 0.00686
Epoch 16125: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16136: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8170, training loss 0.00551, validation loss 0.00485
Epoch 16147: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8175
Mean train loss for ascent epoch 8176: -0.0015064552426338196
Mean eval for ascent epoch 8176: 0.004766874946653843
Doing Evaluation on the model now
This is Epoch 8180, training loss 0.49869, validation loss 1.12866
Epoch 16158: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16169: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8190, training loss 0.05728, validation loss 0.01786
Epoch 16180: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16191: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8200, training loss 0.01164, validation loss 0.00864
Resetting learning rate to 0.01000
Epoch 16202: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16213: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8210, training loss 0.00526, validation loss 0.00519
Epoch 16224: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16235: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8220
Doing Evaluation on the model now
This is Epoch 8220, training loss 0.00434, validation loss 0.00670
Mean train loss for ascent epoch 8221: -0.0020643186289817095
Mean eval for ascent epoch 8221: 0.004980691242963076
Epoch 16246: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8230, training loss 0.10533, validation loss 0.06472
Epoch 16257: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16268: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8240, training loss 0.03331, validation loss 0.04721
Epoch 16279: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16290: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8250, training loss 0.00533, validation loss 0.00564
Epoch 16301: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16312: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8259
Mean train loss for ascent epoch 8260: -0.002263493835926056
Mean eval for ascent epoch 8260: 0.004815790802240372
Doing Evaluation on the model now
This is Epoch 8260, training loss 0.00482, validation loss 0.00444
Epoch 16323: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16334: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8270, training loss 0.07594, validation loss 0.07298
Epoch 16345: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8280, training loss 0.03003, validation loss 0.02622
Epoch 16356: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16367: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8290, training loss 0.00587, validation loss 0.00721
Epoch 16378: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16389: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8298
Mean train loss for ascent epoch 8299: -0.0024395945947617292
Mean eval for ascent epoch 8299: 0.004517002031207085
Doing Evaluation on the model now
This is Epoch 8300, training loss 0.17133, validation loss 0.09178
Epoch 16400: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16411: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8310, training loss 0.02956, validation loss 0.06937
Epoch 16422: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16433: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8320, training loss 0.02350, validation loss 0.02023
Epoch 16444: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8330, training loss 0.00547, validation loss 0.00626
Epoch 16455: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16466: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8337
Mean train loss for ascent epoch 8338: -0.0030299772042781115
Mean eval for ascent epoch 8338: 0.004891513381153345
Doing Evaluation on the model now
This is Epoch 8340, training loss 0.36960, validation loss 0.29360
Epoch 16477: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16488: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8350, training loss 0.02117, validation loss 0.04048
Epoch 16499: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16510: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8360, training loss 0.01942, validation loss 0.00795
Epoch 16521: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16532: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8370, training loss 0.00600, validation loss 0.00653
Epoch 16543: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8376
Mean train loss for ascent epoch 8377: -0.002574055688455701
Mean eval for ascent epoch 8377: 0.005576062947511673
Doing Evaluation on the model now
This is Epoch 8380, training loss 0.76393, validation loss 0.44055
Epoch 16554: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16565: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8390, training loss 0.05466, validation loss 0.03100
Epoch 16576: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16587: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8400, training loss 0.01126, validation loss 0.01297
Resetting learning rate to 0.01000
Epoch 16598: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16609: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8410, training loss 0.00719, validation loss 0.00542
Epoch 16620: reducing learning rate of group 0 to 1.2500e-04.
Epoch 16631: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8420
Doing Evaluation on the model now
This is Epoch 8420, training loss 0.00534, validation loss 0.00638
Mean train loss for ascent epoch 8421: -0.002456161193549633
Mean eval for ascent epoch 8421: 0.006429108325392008
Epoch 16642: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8430, training loss 0.04208, validation loss 0.08877
Epoch 16653: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16664: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8440, training loss 0.01828, validation loss 0.01424
Epoch 16675: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16686: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8450, training loss 0.00587, validation loss 0.00504
Epoch 16697: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16708: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8459
Mean train loss for ascent epoch 8460: -0.0027394175995141268
Mean eval for ascent epoch 8460: 0.0049748411402106285
Doing Evaluation on the model now
This is Epoch 8460, training loss 0.00497, validation loss 0.00443
Epoch 16719: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8470, training loss 0.05478, validation loss 0.03941
Epoch 16730: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16741: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8480, training loss 0.01573, validation loss 0.01466
Epoch 16752: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16763: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8490, training loss 0.00650, validation loss 0.00509
Epoch 16774: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16785: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8498
Mean train loss for ascent epoch 8499: -0.0015119584277272224
Mean eval for ascent epoch 8499: 0.0038230549544095993
Doing Evaluation on the model now
This is Epoch 8500, training loss 0.26820, validation loss 0.32674
Epoch 16796: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16807: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8510, training loss 0.03312, validation loss 0.03238
Epoch 16818: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8520, training loss 0.01728, validation loss 0.01839
Epoch 16829: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16840: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8530, training loss 0.00510, validation loss 0.00608
Epoch 16851: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16862: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8537
Mean train loss for ascent epoch 8538: -0.0026003834791481495
Mean eval for ascent epoch 8538: 0.004303684923797846
Doing Evaluation on the model now
This is Epoch 8540, training loss 0.24496, validation loss 0.25554
Epoch 16873: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16884: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8550, training loss 0.03406, validation loss 0.03871
Epoch 16895: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16906: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8560, training loss 0.01421, validation loss 0.00739
Epoch 16917: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8570, training loss 0.00587, validation loss 0.00573
Epoch 16928: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16939: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8576
Mean train loss for ascent epoch 8577: -0.002515171654522419
Mean eval for ascent epoch 8577: 0.0040389676578342915
Doing Evaluation on the model now
This is Epoch 8580, training loss 0.52414, validation loss 0.57601
Epoch 16950: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16961: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8590, training loss 0.04601, validation loss 0.04294
Epoch 16972: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16983: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8600, training loss 0.01196, validation loss 0.01157
Resetting learning rate to 0.01000
Epoch 16994: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17005: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8610, training loss 0.00611, validation loss 0.00450
Epoch 17016: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8620, training loss 0.00464, validation loss 0.00365
Epoch 17027: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8621
Mean train loss for ascent epoch 8622: -0.002018785336986184
Mean eval for ascent epoch 8622: 0.0042855404317379
Epoch 17038: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8630, training loss 0.09527, validation loss 0.02625
Epoch 17049: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17060: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8640, training loss 0.02691, validation loss 0.02358
Epoch 17071: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17082: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8650, training loss 0.00687, validation loss 0.00915
Epoch 17093: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17104: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8660
Doing Evaluation on the model now
This is Epoch 8660, training loss 0.00522, validation loss 0.00512
Mean train loss for ascent epoch 8661: -0.002145916223526001
Mean eval for ascent epoch 8661: 0.005195623729377985
Epoch 17115: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8670, training loss 0.07665, validation loss 0.04716
Epoch 17126: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17137: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8680, training loss 0.01867, validation loss 0.01791
Epoch 17148: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17159: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8690, training loss 0.00693, validation loss 0.00851
Epoch 17170: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17181: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8699
Mean train loss for ascent epoch 8700: -0.0024943940807133913
Mean eval for ascent epoch 8700: 0.004231218248605728
Doing Evaluation on the model now
This is Epoch 8700, training loss 0.00423, validation loss 0.00452
Epoch 17192: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17203: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8710, training loss 0.05230, validation loss 0.03407
Epoch 17214: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8720, training loss 0.02543, validation loss 0.02706
Epoch 17225: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17236: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8730, training loss 0.00574, validation loss 0.00685
Epoch 17247: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17258: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8738
Mean train loss for ascent epoch 8739: -0.0028638786170631647
Mean eval for ascent epoch 8739: 0.005487164482474327
Doing Evaluation on the model now
This is Epoch 8740, training loss 0.11259, validation loss 0.22616
Epoch 17269: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17280: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8750, training loss 0.04899, validation loss 0.06434
Epoch 17291: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17302: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8760, training loss 0.02458, validation loss 0.01440
Epoch 17313: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8770, training loss 0.00813, validation loss 0.00692
Epoch 17324: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17335: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8777
Mean train loss for ascent epoch 8778: -0.0024793881457298994
Mean eval for ascent epoch 8778: 0.005907670594751835
Doing Evaluation on the model now
This is Epoch 8780, training loss 0.56573, validation loss 0.28017
Epoch 17346: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17357: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8790, training loss 0.04534, validation loss 0.03063
Epoch 17368: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17379: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8800, training loss 0.01137, validation loss 0.00812
Resetting learning rate to 0.01000
Epoch 17390: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17401: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8810, training loss 0.00743, validation loss 0.00654
Epoch 17412: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8820, training loss 0.00464, validation loss 0.00555
Epoch 17423: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8821
Mean train loss for ascent epoch 8822: -0.002098377328366041
Mean eval for ascent epoch 8822: 0.005326893646270037
Epoch 17434: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8830, training loss 0.06947, validation loss 0.09824
Epoch 17445: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17456: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8840, training loss 0.02271, validation loss 0.01812
Epoch 17467: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17478: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8850, training loss 0.00924, validation loss 0.00568
Epoch 17489: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17500: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8860
Doing Evaluation on the model now
This is Epoch 8860, training loss 0.00522, validation loss 0.00515
Mean train loss for ascent epoch 8861: -0.0024352192413061857
Mean eval for ascent epoch 8861: 0.005463188048452139
Epoch 17511: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8870, training loss 0.05633, validation loss 0.08518
Epoch 17522: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17533: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8880, training loss 0.01783, validation loss 0.02252
Epoch 17544: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17555: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8890, training loss 0.00708, validation loss 0.00538
Epoch 17566: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17577: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8899
Mean train loss for ascent epoch 8900: -0.003693079110234976
Mean eval for ascent epoch 8900: 0.005117578897625208
Doing Evaluation on the model now
This is Epoch 8900, training loss 0.00512, validation loss 0.00597
Epoch 17588: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8910, training loss 0.04693, validation loss 0.02472
Epoch 17599: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17610: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8920, training loss 0.01539, validation loss 0.02261
Epoch 17621: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17632: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8930, training loss 0.00586, validation loss 0.00523
Epoch 17643: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17654: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8938
Mean train loss for ascent epoch 8939: -0.0018841867567971349
Mean eval for ascent epoch 8939: 0.005007815081626177
Doing Evaluation on the model now
This is Epoch 8940, training loss 0.14315, validation loss 0.08013
Epoch 17665: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17676: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8950, training loss 0.08815, validation loss 0.05186
Epoch 17687: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8960, training loss 0.01908, validation loss 0.02165
Epoch 17698: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17709: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8970, training loss 0.00538, validation loss 0.00557
Epoch 17720: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17731: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8977
Mean train loss for ascent epoch 8978: -0.0021488757338374853
Mean eval for ascent epoch 8978: 0.005923483520746231
Doing Evaluation on the model now
This is Epoch 8980, training loss 0.23081, validation loss 0.14224
Epoch 17742: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17753: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8990, training loss 0.02480, validation loss 0.04400
Epoch 17764: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17775: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9000, training loss 0.01054, validation loss 0.00751
Resetting learning rate to 0.01000
Epoch 17786: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9010, training loss 0.00588, validation loss 0.00685
Epoch 17797: reducing learning rate of group 0 to 2.5000e-04.
Epoch 17808: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9020, training loss 0.00478, validation loss 0.00416
Epoch 17819: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9022
Mean train loss for ascent epoch 9023: -0.0027311493176966906
Mean eval for ascent epoch 9023: 0.004579576663672924
Epoch 17830: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9030, training loss 0.08226, validation loss 0.05515
Epoch 17841: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17852: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9040, training loss 0.01826, validation loss 0.01904
Epoch 17863: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17874: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9050, training loss 0.00672, validation loss 0.00608
Epoch 17885: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9060, training loss 0.00496, validation loss 0.00463
Epoch 17896: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9061
Mean train loss for ascent epoch 9062: -0.0020860370714217424
Mean eval for ascent epoch 9062: 0.005334015004336834
Epoch 17907: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9070, training loss 0.08943, validation loss 0.04723
Epoch 17918: reducing learning rate of group 0 to 2.5000e-03.
Epoch 17929: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9080, training loss 0.01768, validation loss 0.02001
Epoch 17940: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17951: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9090, training loss 0.00540, validation loss 0.00451
Epoch 17962: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17973: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9100
Doing Evaluation on the model now
This is Epoch 9100, training loss 0.00482, validation loss 0.00519
Mean train loss for ascent epoch 9101: -0.0019875175785273314
Mean eval for ascent epoch 9101: 0.005195206496864557
Epoch 17984: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9110, training loss 0.07233, validation loss 0.19585
Epoch 17995: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18006: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9120, training loss 0.01799, validation loss 0.01669
Epoch 18017: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18028: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9130, training loss 0.00624, validation loss 0.00892
Epoch 18039: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18050: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9139
Mean train loss for ascent epoch 9140: -0.0029423111118376255
Mean eval for ascent epoch 9140: 0.005328766535967588
Doing Evaluation on the model now
This is Epoch 9140, training loss 0.00533, validation loss 0.00624
Epoch 18061: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18072: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9150, training loss 0.06156, validation loss 0.04626
Epoch 18083: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9160, training loss 0.02223, validation loss 0.02609
Epoch 18094: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18105: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9170, training loss 0.00507, validation loss 0.00510
Epoch 18116: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18127: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9178
Mean train loss for ascent epoch 9179: -0.0032021598890423775
Mean eval for ascent epoch 9179: 0.004985630512237549
Doing Evaluation on the model now
This is Epoch 9180, training loss 0.12828, validation loss 0.18353
Epoch 18138: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18149: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9190, training loss 0.04342, validation loss 0.04334
Epoch 18160: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18171: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9200, training loss 0.03009, validation loss 0.02049
Resetting learning rate to 0.01000
Epoch 18182: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9210, training loss 0.00645, validation loss 0.00637
Epoch 18193: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18204: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9220, training loss 0.00528, validation loss 0.00637
Epoch 18215: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9222
Mean train loss for ascent epoch 9223: -0.0031188682187348604
Mean eval for ascent epoch 9223: 0.004932762589305639
Epoch 18226: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9230, training loss 0.18432, validation loss 0.07187
Epoch 18237: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18248: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9240, training loss 0.03440, validation loss 0.03931
Epoch 18259: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18270: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9250, training loss 0.00956, validation loss 0.00905
Epoch 18281: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9260, training loss 0.00541, validation loss 0.00687
Epoch 18292: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9261
Mean train loss for ascent epoch 9262: -0.00200499570928514
Mean eval for ascent epoch 9262: 0.005061195697635412
Epoch 18303: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9270, training loss 0.08581, validation loss 0.07778
Epoch 18314: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18325: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9280, training loss 0.01850, validation loss 0.01692
Epoch 18336: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18347: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9290, training loss 0.00830, validation loss 0.00663
Epoch 18358: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18369: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9300
Doing Evaluation on the model now
This is Epoch 9300, training loss 0.00514, validation loss 0.00520
Mean train loss for ascent epoch 9301: -0.0021885521709918976
Mean eval for ascent epoch 9301: 0.005092974752187729
Epoch 18380: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9310, training loss 0.12085, validation loss 0.10876
Epoch 18391: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18402: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9320, training loss 0.02667, validation loss 0.02253
Epoch 18413: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18424: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9330, training loss 0.00636, validation loss 0.00505
Epoch 18435: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18446: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9339
Mean train loss for ascent epoch 9340: -0.0016884709475561976
Mean eval for ascent epoch 9340: 0.004567902535200119
Doing Evaluation on the model now
This is Epoch 9340, training loss 0.00457, validation loss 0.00477
Epoch 18457: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9350, training loss 0.05213, validation loss 0.10912
Epoch 18468: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18479: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9360, training loss 0.02002, validation loss 0.02291
Epoch 18490: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18501: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9370, training loss 0.00655, validation loss 0.00858
Epoch 18512: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18523: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9378
Mean train loss for ascent epoch 9379: -0.00228665373288095
Mean eval for ascent epoch 9379: 0.005029559601098299
Doing Evaluation on the model now
This is Epoch 9380, training loss 0.17439, validation loss 0.07445
Epoch 18534: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18545: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9390, training loss 0.05231, validation loss 0.04337
Epoch 18556: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9400, training loss 0.01714, validation loss 0.02129
Epoch 18567: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 18578: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 9410, training loss 0.00913, validation loss 0.00858
Epoch 18589: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18600: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9420, training loss 0.00533, validation loss 0.00473
Epoch 18611: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9423
Mean train loss for ascent epoch 9424: -0.002150758635252714
Mean eval for ascent epoch 9424: 0.005525187589228153
Epoch 18622: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9430, training loss 0.06514, validation loss 0.03995
Epoch 18633: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18644: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9440, training loss 0.01854, validation loss 0.01969
Epoch 18655: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9450, training loss 0.00946, validation loss 0.00923
Epoch 18666: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18677: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9460, training loss 0.00551, validation loss 0.00597
Epoch 18688: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9462
Mean train loss for ascent epoch 9463: -0.002593403682112694
Mean eval for ascent epoch 9463: 0.005613723769783974
Epoch 18699: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9470, training loss 0.10715, validation loss 0.11626
Epoch 18710: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18721: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9480, training loss 0.02041, validation loss 0.01586
Epoch 18732: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18743: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9490, training loss 0.00674, validation loss 0.00668
Epoch 18754: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9500, training loss 0.00500, validation loss 0.00470
Epoch 18765: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9501
Mean train loss for ascent epoch 9502: -0.0025681941770017147
Mean eval for ascent epoch 9502: 0.0053678229451179504
Epoch 18776: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9510, training loss 0.05760, validation loss 0.11607
Epoch 18787: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18798: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9520, training loss 0.02113, validation loss 0.01986
Epoch 18809: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18820: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9530, training loss 0.00614, validation loss 0.00615
Epoch 18831: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18842: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9540
Doing Evaluation on the model now
This is Epoch 9540, training loss 0.00516, validation loss 0.00493
Mean train loss for ascent epoch 9541: -0.0027196917217224836
Mean eval for ascent epoch 9541: 0.005501041188836098
Epoch 18853: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9550, training loss 0.06238, validation loss 0.03735
Epoch 18864: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18875: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9560, training loss 0.01715, validation loss 0.01382
Epoch 18886: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18897: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9570, training loss 0.00656, validation loss 0.00875
Epoch 18908: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18919: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9579
Mean train loss for ascent epoch 9580: -0.003208709182217717
Mean eval for ascent epoch 9580: 0.0062470161356031895
Doing Evaluation on the model now
This is Epoch 9580, training loss 0.00625, validation loss 0.00561
Epoch 18930: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18941: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9590, training loss 0.04361, validation loss 0.05275
Epoch 18952: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9600, training loss 0.01960, validation loss 0.02500
Resetting learning rate to 0.01000
Epoch 18963: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18974: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9610, training loss 0.00566, validation loss 0.00495
Epoch 18985: reducing learning rate of group 0 to 1.2500e-04.
Epoch 18996: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9618
Mean train loss for ascent epoch 9619: -0.0016429568640887737
Mean eval for ascent epoch 9619: 0.0048283776268363
Doing Evaluation on the model now
This is Epoch 9620, training loss 0.18858, validation loss 0.44178
Epoch 19007: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19018: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9630, training loss 0.03188, validation loss 0.03796
Epoch 19029: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19040: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9640, training loss 0.02924, validation loss 0.02421
Epoch 19051: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9650, training loss 0.00641, validation loss 0.00531
Epoch 19062: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19073: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9657
Mean train loss for ascent epoch 9658: -0.002085290616378188
Mean eval for ascent epoch 9658: 0.005585063248872757
Doing Evaluation on the model now
This is Epoch 9660, training loss 0.66252, validation loss 1.10949
Epoch 19084: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19095: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9670, training loss 0.04566, validation loss 0.06132
Epoch 19106: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19117: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9680, training loss 0.01745, validation loss 0.01762
Epoch 19128: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19139: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9690, training loss 0.00631, validation loss 0.00680
Epoch 19150: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9696
Mean train loss for ascent epoch 9697: -0.002341926796361804
Mean eval for ascent epoch 9697: 0.0052629136480391026
Doing Evaluation on the model now
This is Epoch 9700, training loss 0.92313, validation loss 2.79109
Epoch 19161: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19172: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9710, training loss 0.08005, validation loss 0.07768
Epoch 19183: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19194: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9720, training loss 0.01059, validation loss 0.00773
Epoch 19205: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19216: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9730, training loss 0.00645, validation loss 0.00573
Epoch 19227: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9735
Mean train loss for ascent epoch 9736: -0.0021609608083963394
Mean eval for ascent epoch 9736: 0.005981662776321173
Doing Evaluation on the model now
This is Epoch 9740, training loss 0.48887, validation loss 0.26497
Epoch 19238: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19249: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9750, training loss 0.04211, validation loss 0.02336
Epoch 19260: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19271: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9760, training loss 0.01451, validation loss 0.02183
Epoch 19282: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19293: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9770, training loss 0.00623, validation loss 0.00546
Epoch 19304: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9774
Mean train loss for ascent epoch 9775: -0.003045660676434636
Mean eval for ascent epoch 9775: 0.005996894557029009
Epoch 19315: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9780, training loss 0.06279, validation loss 0.05589
Epoch 19326: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9790, training loss 0.07274, validation loss 0.06482
Epoch 19337: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19348: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9800, training loss 0.01044, validation loss 0.01325
Resetting learning rate to 0.01000
Epoch 19359: reducing learning rate of group 0 to 5.0000e-04.
Epoch 19370: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9810, training loss 0.00606, validation loss 0.00605
Epoch 19381: reducing learning rate of group 0 to 1.2500e-04.
Epoch 19392: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9818
Mean train loss for ascent epoch 9819: -0.0027593730483204126
Mean eval for ascent epoch 9819: 0.006545312236994505
Doing Evaluation on the model now
This is Epoch 9820, training loss 0.25780, validation loss 0.65412
Epoch 19403: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19414: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9830, training loss 0.04283, validation loss 0.02642
Epoch 19425: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9840, training loss 0.01662, validation loss 0.00818
Epoch 19436: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19447: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9850, training loss 0.00618, validation loss 0.00650
Epoch 19458: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19469: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9857
Mean train loss for ascent epoch 9858: -0.0031227823346853256
Mean eval for ascent epoch 9858: 0.006470907479524612
Doing Evaluation on the model now
This is Epoch 9860, training loss 0.23754, validation loss 0.40870
Epoch 19480: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19491: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9870, training loss 0.06169, validation loss 0.06023
Epoch 19502: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19513: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9880, training loss 0.01631, validation loss 0.01014
Epoch 19524: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9890, training loss 0.00576, validation loss 0.00527
Epoch 19535: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19546: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9896
Mean train loss for ascent epoch 9897: -0.002224127994850278
Mean eval for ascent epoch 9897: 0.0048040784895420074
Doing Evaluation on the model now
This is Epoch 9900, training loss 0.31328, validation loss 0.24023
Epoch 19557: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19568: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9910, training loss 0.03971, validation loss 0.03749
Epoch 19579: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19590: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9920, training loss 0.01346, validation loss 0.01462
Epoch 19601: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19612: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9930, training loss 0.00593, validation loss 0.00719
Epoch 19623: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9935
Mean train loss for ascent epoch 9936: -0.002296269638463855
Mean eval for ascent epoch 9936: 0.004955700132995844
Doing Evaluation on the model now
This is Epoch 9940, training loss 0.60563, validation loss 0.81635
Epoch 19634: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19645: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9950, training loss 0.05638, validation loss 0.06097
Epoch 19656: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19667: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9960, training loss 0.01227, validation loss 0.00969
Epoch 19678: reducing learning rate of group 0 to 3.1250e-04.
Epoch 19689: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9970, training loss 0.00555, validation loss 0.00453
Epoch 19700: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9974
Mean train loss for ascent epoch 9975: -0.0025626893620938063
Mean eval for ascent epoch 9975: 0.0049874805845320225
Epoch 19711: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9980, training loss 0.54717, validation loss 0.40148
Epoch 19722: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9990, training loss 0.03927, validation loss 0.04812
Epoch 19733: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19744: reducing learning rate of group 0 to 6.2500e-04.
POS: 
[6.0401516, 3.636379, 1.8035002, 1.139327, 0.6816947, 0.6497507, 0.56901664, 0.5363311, 0.5203774, 0.67670417, 0.49177074, 0.53000593, 0.553946, 0.5247258, 0.6724995, 0.67261136, 0.38051853, 0.3185984, 0.38260317, 0.3564135, 0.34658647, 0.37746143, 0.36207047, 0.24134324, 0.19149828, 0.17342229, 0.23165073, 0.25099313, 0.25099096, 0.27081335, 0.221564, 0.24007943, 0.18403864, 0.18021198, 0.20294409, 0.1853352, 0.15684606, 0.20055187, 0.21474628, 0.31286088, 0.16119152, 0.20137964, 0.22199538, 1.436536, 1.3373597, 0.9510564, 1.4105896, 1.4637098, 1.6673368, 1.1638647, 0.6558084, 0.6771632, 0.93377775, 0.5032576, 0.43699676, 0.4325038, 0.35939512, 0.63948816, 0.43186796, 0.454788, 0.5487332, 0.42800713, 0.30494872, 0.2881147, 0.20226368, 0.36974493, 0.31323466, 0.28886408, 0.27280247, 0.22330885, 0.16082467, 0.20779137, 0.29188973, 0.25088003, 0.21657775, 0.1883163, 0.20836179, 0.18213832, 0.17305171, 0.1668408, 0.14114106, 0.1274242, 0.5999699, 1.0336424, 1.1837766, 1.2311434, 0.8982509, 0.50401646, 0.291648, 0.46679005, 0.36509085, 0.4322419, 0.48962995, 0.28711787, 0.22111912, 0.41146562, 0.38644236, 0.41652024, 0.25916642, 0.22609769, 0.24425551, 0.3340382, 0.32451335, 0.18688792, 0.17567062, 0.19114143, 0.21237065, 0.17919207, 0.19720481, 0.16843072, 0.2252258, 0.17054623, 0.13036367, 0.14422311, 0.18919544, 0.1824913, 0.22236949, 0.13586135, 0.24673426, 0.13023575, 0.3924832, 1.141223, 1.0512441, 1.0266036, 1.0599235, 0.4250117, 0.8479117, 0.8389788, 0.38022766, 0.5228746, 0.37434894, 0.29596248, 0.30550575, 0.33298784, 0.32641473, 0.42107034, 0.3988345, 0.22760688, 0.4454818, 0.29063445, 0.20700487, 0.15830958, 0.1584765, 0.21188721, 0.21809955, 0.15196633, 0.23064093, 0.1671072, 0.1742044, 0.24576034, 0.18596151, 0.14221394, 0.13766076, 0.15012738, 0.16771999, 0.19463605, 0.18910085, 0.19663511, 0.7398444, 0.85746413, 1.0154898, 0.4913515, 0.9807472, 0.5418742, 0.5241048, 0.2835995, 0.4688081, 0.31316248, 0.2904668, 0.21696673, 0.5108686, 0.3705291, 0.26146382, 0.4154089, 0.19773717, 0.22073989, 0.2845413, 0.17208089, 0.23634107, 0.2948489, 0.2874263, 0.29792622, 0.26543975, 0.1671871, 0.27486286, 0.18158957, 0.1717684, 0.18466702, 0.23631649, 0.17588407, 0.13094418, 0.171765, 0.15654369, 0.17876647, 0.16357864, 0.122227006, 0.2225447, 0.25353923, 0.15508266, 0.28089505, 0.16180444, 0.16307223, 0.21789892, 0.21821657, 0.16126241, 0.15308948, 0.23091838, 0.10943152, 0.17345011, 0.20803502, 0.18983853, 0.15615585, 0.16198808, 0.14774388, 0.103604, 0.18175757, 0.1667912, 0.14722587, 1.0157775, 1.1965564, 0.7482455, 0.8934288, 0.5963655, 0.52450365, 0.22003284, 0.5437767, 0.460311, 0.55109566, 0.40666842, 0.2522004, 0.2987945, 0.35176164, 0.25510496, 0.30261818, 0.18035865, 0.24863547, 0.23859504, 0.1824897, 0.3722856, 0.22368324, 0.20461027, 0.20735766, 0.1829813, 0.13789187, 0.24694167, 0.1183676, 0.20369864, 0.15373497, 0.11807817, 0.15752728, 0.19437037, 0.18253261, 0.19620062, 0.16351782, 0.1383456, 0.10722124, 0.46810302, 0.5531808, 0.61972344, 0.55930257, 0.31439003, 0.5380111, 0.3005263, 0.55869055, 0.82746005, 0.83322537, 0.2492412, 0.24673176, 0.31724238, 0.21132927, 0.38919023, 0.26525557, 0.2967858, 0.36313003, 0.15262179, 0.17266183, 0.314414, 0.119336, 0.1441581, 0.20881541, 0.14204462, 0.18882763, 0.14350046, 0.249448, 0.12701854, 0.13869306, 0.1791414, 0.22997633, 0.19397892, 0.21346052, 0.22022769, 0.2623066, 0.12503263, 0.1363335, 0.55874604, 0.65171355, 0.8880259, 0.89782494, 0.7595825, 0.4372671, 1.0310634, 0.83608454, 0.49350563, 0.42945406, 0.26377827, 0.4648277, 0.35390958, 0.29090527, 0.38785025, 0.3589877, 0.25734955, 0.15941401, 0.17989115, 0.27729258, 0.25327322, 0.25476795, 0.17001837, 0.112666905, 0.14407824, 0.12611045, 0.17176391, 0.1129323, 0.18149722, 0.14929394, 0.22272685, 0.15575996, 0.113535754, 0.1906225, 0.15795209, 0.15184835, 0.18820803, 0.16458282, 0.98623997, 0.58137465, 0.6717162, 0.3372816, 0.41204727, 1.015729, 0.6343067, 0.23415199, 0.26435146, 0.60067743, 0.47532874, 0.3856887, 0.37667695, 0.33506078, 0.26432514, 0.20269005, 0.13686083, 0.20007361, 0.33909246, 0.3105139, 0.21664447, 0.21189153, 0.22364096, 0.10875693, 0.109928004, 0.22990493, 0.16072023, 0.14650793, 0.11300048, 0.11748619, 0.1541638, 0.21269944, 0.1278062, 0.15411381, 0.14271209, 0.21819, 0.15937306, 0.16326675, 0.4370223, 0.33376095, 0.54376847, 0.4474752, 0.28730592, 0.36367536, 0.28779465, 0.19171889, 0.26837718, 0.52336615, 0.3391936, 0.27242047, 0.2301256, 0.30432793, 0.3571454, 0.2998408, 0.17835851, 0.1975113, 0.14091843, 0.24146233, 0.22742932, 0.21160181, 0.27548257, 0.23039557, 0.14797874, 0.19578134, 0.18554825, 0.18196364, 0.14314139, 0.13922417, 0.11315087, 0.16661134, 0.10460859, 0.13427019, 0.13924739, 0.09649552, 0.12578422, 0.13597073, 0.1398785, 0.15728015, 0.115469754, 0.13491766, 0.11182981, 0.53632843, 0.67765933, 1.1474565, 0.9857452, 0.5665153, 0.40097547, 0.64107376, 0.40658322, 0.36446562, 0.6967169, 0.60776335, 0.24390833, 0.3819168, 0.18025635, 0.44349918, 0.20802066, 0.19783278, 0.19116303, 0.26502594, 0.2690476, 0.21198207, 0.14337687, 0.18649396, 0.122174814, 0.14308067, 0.19609423, 0.0949219, 0.10981399, 0.21596695, 0.10407378, 0.1033675, 0.18428162, 0.13325894, 0.095839605, 0.200241, 0.17275465, 0.13227722, 0.12033194, 0.5747647, 0.7223167, 0.88113725, 0.48148385, 0.30495095, 0.4255375, 0.552043, 0.6624201, 0.5011051, 0.5667294, 0.24459743, 0.48050097, 0.26957926, 0.22442865, 0.20336677, 0.26707327, 0.1370676, 0.21462184, 0.18639489, 0.1600469, 0.18836802, 0.13330142, 0.1636156, 0.18667948, 0.17350355, 0.19400238, 0.17376356, 0.18216367, 0.1321884, 0.1527328, 0.1398699, 0.103656046, 0.13322955, 0.11919465, 0.14278954, 0.12701769, 0.2301694, 0.15778871, 0.553249, 0.8037705, 0.91435826, 0.85178095, 0.6795438, 0.173112, 0.33343807, 0.41452253, 0.20374247, 0.1860641, 0.4178529, 0.6686853, 0.26638713, 0.15234053, 0.37256598, 0.2830313, 0.20493804, 0.18277717, 0.20822228, 0.106045395, 0.25711653, 0.17806743, 0.091673486, 0.15337317, 0.13876717, 0.122673325, 0.14881344, 0.16798826, 0.1263643, 0.15926069, 0.14422004, 0.14784044, 0.13989908, 0.11068614, 0.1084796, 0.15903294, 0.12151837, 0.09659287, 0.21478246, 0.5182958, 0.29964188, 0.60539776, 1.138135, 0.31652603, 0.38659835, 0.3719962, 0.2717683, 0.19667317, 0.36832276, 0.29521644, 0.26279655, 0.19477427, 0.22234304, 0.23154771, 0.21162565, 0.17401466, 0.1802575, 0.2229161, 0.24643274, 0.17320761, 0.091283284, 0.21099347, 0.23818313, 0.24174537, 0.18248242, 0.20912175, 0.13104935, 0.16790567, 0.13613027, 0.1336479, 0.120507754, 0.1120993, 0.111846305, 0.1113032, 0.16017818, 0.14451747, 0.29026586, 0.34594658, 0.4923078, 0.437052, 0.8481968, 0.72790843, 0.38164493, 0.28400913, 0.28990516, 0.1711856, 0.38822785, 0.24239807, 0.18680444, 0.16553767, 0.17471938, 0.3215182, 0.27806854, 0.19218858, 0.16461141, 0.13445678, 0.18306948, 0.17811541, 0.20629482, 0.14726104, 0.21963742, 0.24757583, 0.16121255, 0.14125305, 0.09784903, 0.15072906, 0.21473086, 0.13733655, 0.23430644, 0.14326756, 0.12073821, 0.13010347, 0.15078674, 0.07940223, 0.1609297, 0.09670652, 0.18570311, 0.11208886, 0.10526474, 0.18168373, 0.43274704, 0.47473344, 0.44929475, 0.7545758, 0.67282134, 0.3009494, 0.16633289, 0.3854525, 0.3655111, 0.47905993, 0.2647566, 0.36431244, 0.31304127, 0.32747728, 0.21740305, 0.20517239, 0.16132244, 0.14903265, 0.15714943, 0.12142064, 0.1683006, 0.15800507, 0.11937942, 0.16347338, 0.16562283, 0.21257429, 0.18634793, 0.14375375, 0.165758, 0.08595779, 0.12934186, 0.12213841, 0.14637907, 0.17681542, 0.12835889, 0.16635883, 0.13126248, 0.112176746, 0.23656185, 0.36063156, 0.35246757, 0.9006971, 0.7127488, 0.3689312, 0.2732058, 0.61178535, 0.47090605, 0.40975744, 0.15345535, 0.17938824, 0.2504981, 0.21555519, 0.106695354, 0.16686417, 0.12954877, 0.17400147, 0.22651811, 0.3378587, 0.16079082, 0.16652183, 0.14526807, 0.112287834, 0.21086194, 0.1261427, 0.23243317, 0.12908253, 0.12445771, 0.114902675, 0.11204006, 0.13784651, 0.11319221, 0.10536069, 0.11071154, 0.13994671, 0.1686014, 0.10281042, 0.35276458, 0.33876202, 0.7915886, 0.4772732, 0.23875256, 0.19381805, 0.23408376, 0.22897588, 0.3068563, 0.3396316, 0.14788264, 0.132938, 0.18523872, 0.30720934, 0.19765446, 0.18405542, 0.13584368, 0.11984167, 0.22146107, 0.11571734, 0.124587595, 0.111300945, 0.11031736, 0.094409555, 0.107470036, 0.08921235, 0.116497025, 0.12767191, 0.076880515, 0.08139053, 0.15033415, 0.107202195, 0.10021653, 0.07310539, 0.13174215, 0.1428394, 0.08130866, 0.10596692, 0.17782909, 0.85958964, 0.72259074, 0.53414494, 0.3806338, 0.4338797, 0.32148483, 0.20866746, 0.54692173, 0.227335, 0.15331677, 0.23837928, 0.31197518, 0.14738503, 0.15690336, 0.13266395, 0.07817093, 0.15486063, 0.24362886, 0.19527717, 0.26003444, 0.114891306, 0.12146246, 0.110056326, 0.21539462, 0.123470634, 0.17980094, 0.077981465, 0.12001432, 0.10272722, 0.117104255, 0.09879284, 0.17556839, 0.0955072, 0.15304692, 0.09209154, 0.077014625, 0.13119894, 0.4754184, 0.5631334, 0.28316396, 0.32423005, 0.42335987, 0.17397411, 0.33625177, 0.31867054, 0.5229099, 0.47574335, 0.353212, 0.23187494, 0.22408871, 0.2970435, 0.21581687, 0.17405383, 0.20731626, 0.16071042, 0.15067892, 0.10456109, 0.1643964, 0.118046924, 0.12419064, 0.10026293, 0.16587432, 0.1360013, 0.13374762, 0.13402134, 0.08009039, 0.107252814, 0.08668786, 0.118638664, 0.11362972, 0.14220868, 0.08497273, 0.12772949, 0.10141486, 0.11921255, 0.30110553, 0.65176517, 0.6163933, 0.63818395, 0.43056673, 0.5350588, 0.2623346, 0.19856416, 0.26300502, 0.20229982, 0.20298038, 0.1280003, 0.1949774, 0.1211949, 0.22328347, 0.13850714, 0.14510001, 0.103866994, 0.24872328, 0.13290973, 0.15549597, 0.12756014, 0.12043038, 0.14823632, 0.11360589, 0.1388373, 0.13841848, 0.11861742, 0.1723545, 0.14253795, 0.10909103, 0.11836013, 0.124293186, 0.099283576, 0.0910198, 0.09847901, 0.09511846, 0.14757314, 0.49562594, 0.96896774, 0.62226796, 0.5308839, 0.32254985, 0.13908239, 0.2167375, 0.20201075, 0.15944771, 0.3895656, 0.17306474, 0.24731886, 0.18671565, 0.2439205, 0.2608331, 0.25033128, 0.12160645, 0.11885834, 0.12763014, 0.25689322, 0.23948434, 0.1335052, 0.0972577, 0.13662827, 0.14421974, 0.081784055, 0.097778775, 0.084147185, 0.13339691, 0.089382075, 0.09522837, 0.0690421, 0.09535944, 0.11162529, 0.10736418, 0.10911065, 0.09436429, 0.122138076, 0.397399, 0.34105155, 0.32044247, 0.5612648, 0.35662335, 0.18480591, 0.17694002, 0.2092154, 0.09884598, 0.23826206, 0.24784184, 0.257779, 0.2069501, 0.16920847, 0.11796985, 0.16692899, 0.1640787, 0.10727677, 0.104606, 0.061826542, 0.13590136, 0.11414631, 0.06054636, 0.13246739, 0.11784309, 0.10974644, 0.1145505, 0.12226548, 0.07830957, 0.09288681, 0.08324466, 0.117228985, 0.11428039, 0.053662132, 0.1166906, 0.069920056, 0.11428442, 0.1013752, 0.22574608, 0.6297732, 0.6145017, 0.42638734, 0.6155706, 0.35417992, 0.35128146, 0.22539674, 0.30157813, 0.17438121, 0.25996894, 0.1794333, 0.15897417, 0.14014116, 0.12461198, 0.13173817, 0.12857178, 0.14579938, 0.18652534, 0.1793867, 0.15057191, 0.076848984, 0.08490088, 0.11430205, 0.13551593, 0.19067578, 0.06971131, 0.08925616, 0.09922574, 0.09055455, 0.114310086, 0.08888265, 0.08963818, 0.09992193, 0.08157725, 0.09185592, 0.11299766, 0.07215452, 0.42394572, 0.31970415, 0.46405625, 0.25408325, 0.21742465, 0.41686288, 0.14711547, 0.3528245, 0.23316988, 0.22470434, 0.18273023, 0.21343018, 0.31321344, 0.17276405, 0.13542308, 0.101032555, 0.097136796, 0.114399634, 0.17627569, 0.26343137, 0.14024739, 0.15001078, 0.12568456, 0.10825014, 0.09127849, 0.1196691, 0.12537085, 0.07528415, 0.09531136, 0.12798177, 0.09606395, 0.07582948, 0.0837127, 0.10862954, 0.11658065, 0.08253803, 0.13546519, 0.1360703, 0.07289352, 0.11579631, 0.05847189, 0.056088522, 0.11038679, 0.33724272, 0.18066609, 0.34105182, 0.52342314, 0.91934705, 0.42804408, 0.37743777, 0.23864646, 0.16331689, 0.21227473, 0.11867124, 0.17343263, 0.18105803, 0.16387951, 0.1073545, 0.26334846, 0.17956772, 0.124311864, 0.08644033, 0.18202214, 0.13359223, 0.10732788, 0.0859626, 0.092582405, 0.107310474, 0.123020664, 0.096560515, 0.11028523, 0.09806553, 0.1102734, 0.098207325, 0.08992064, 0.10113344, 0.11996988, 0.08799642, 0.1304435, 0.12703438, 0.13053714, 0.367617, 0.2954028, 0.25863057, 0.430452, 0.62738496, 0.41933337, 0.19304009, 0.1782431, 0.22856367, 0.24251883, 0.17020953, 0.16999933, 0.18840528, 0.16882396, 0.16389993, 0.1122008, 0.13787599, 0.15815523, 0.16810153, 0.16065569, 0.16043414, 0.119602896, 0.12952375, 0.08059602, 0.11866236, 0.06501176, 0.09928869, 0.15226474, 0.11836127, 0.0942683, 0.10699637, 0.105997466, 0.07481796, 0.07708188, 0.07165471, 0.07572046, 0.08219483, 0.08052047, 0.14412333, 0.4296875, 0.3486568, 0.31952283, 0.25825334, 0.2740748, 0.52681077, 0.21221974, 0.11200549, 0.13531983, 0.12820804, 0.17814404, 0.15220097, 0.14101644, 0.14128175, 0.172903, 0.18254249, 0.122778274, 0.10540913, 0.11353645, 0.119978614, 0.11595559, 0.103615694, 0.12350578, 0.120457485, 0.13541658, 0.085895374, 0.05960317, 0.09093795, 0.09107945, 0.11104085, 0.10821368, 0.12921223, 0.08623385, 0.07176434, 0.10548267, 0.13130042, 0.07795467, 0.42788497, 0.53134394, 0.2347963, 0.30430615, 0.42400044, 0.22122695, 0.12769747, 0.18940553, 0.20650382, 0.22809915, 0.15964761, 0.13431613, 0.14009541, 0.0947791, 0.15316333, 0.16233891, 0.1387083, 0.123738416, 0.20892677, 0.1089339, 0.102729216, 0.13786502, 0.07342576, 0.095935345, 0.1125482, 0.0823145, 0.09217666, 0.088908434, 0.093736775, 0.0977099, 0.08689052, 0.077634, 0.060322825, 0.10546983, 0.10182488, 0.077614926, 0.09706662, 0.082510866, 0.25927258, 0.45293, 0.3811818, 0.48810044, 0.4814594, 0.13158643, 0.19325809, 0.1849585, 0.31381816, 0.20526585, 0.15228061, 0.21516642, 0.19395089, 0.16484846, 0.18476026, 0.1294347, 0.16975005, 0.092701174, 0.10526809, 0.112523735, 0.111991666, 0.11582155, 0.11877876, 0.085055895, 0.13618813, 0.15674004, 0.13201037, 0.10311305, 0.114395365, 0.08932924, 0.07725227, 0.08779204, 0.09618046, 0.0835084, 0.07490394, 0.09182479, 0.088476665, 0.08215159, 0.10494052, 0.08625856, 0.09749658, 0.056936145, 0.09883215, 0.07890993, 0.1411768, 0.28520858, 0.1979731, 0.27834186, 0.38474137, 0.14910035, 0.21224807, 0.15477215, 0.15815127, 0.2279337, 0.15144272, 0.19896963, 0.13557826, 0.09462178, 0.10319275, 0.09630396, 0.10048547, 0.1203269, 0.13844077, 0.090554535, 0.14037642, 0.12113771, 0.11456867, 0.107931994, 0.13651438, 0.11280909, 0.124795586, 0.05726918, 0.083704375, 0.07096815, 0.087416396, 0.1081967, 0.08779747, 0.06805544, 0.098227404, 0.09163731, 0.06048737, 0.092155, 0.28734297, 0.38880014, 0.22037694, 0.22864632, 0.16982932, 0.1282044, 0.13463907, 0.13019273, 0.20345053, 0.17432025, 0.12899247, 0.1110912, 0.08772045, 0.09473216, 0.10367431, 0.12914218, 0.13918902, 0.13773657, 0.13912793, 0.17662519, 0.07988845, 0.085653044, 0.08671604, 0.095682524, 0.05922168, 0.075792976, 0.103457145, 0.07923495, 0.106150575, 0.0746308, 0.07384521, 0.08343289, 0.078688815, 0.07269359, 0.058175225, 0.0989243, 0.06319175, 0.08793216, 0.2905983, 0.30249915, 0.22429948, 0.21539718, 0.24412544, 0.25609818, 0.20555095, 0.19894582, 0.18838744, 0.1400316, 0.12252382, 0.18148896, 0.32398438, 0.108368255, 0.14761029, 0.080337755, 0.086136036, 0.113604255, 0.13771722, 0.114809774, 0.077146515, 0.107989475, 0.10864742, 0.10438432, 0.102111764, 0.092020966, 0.07560923, 0.084525794, 0.100909665, 0.07025603, 0.09939792, 0.0676544, 0.042278934, 0.08428363, 0.09105944, 0.078955, 0.05957851, 0.11759625, 0.1986322, 0.36680272, 0.40768525, 0.27640826, 0.16846432, 0.20461288, 0.23576358, 0.17062697, 0.097976245, 0.24660999, 0.13993065, 0.16452453, 0.10891953, 0.15036711, 0.27360496, 0.11465129, 0.16475207, 0.12164406, 0.10837304, 0.10005154, 0.12640241, 0.1025574, 0.06262027, 0.101849936, 0.06973741, 0.09941949, 0.0821595, 0.096249275, 0.060420062, 0.073360555, 0.09532571, 0.097603396, 0.06929026, 0.07025966, 0.07351693, 0.085942514, 0.07157419, 0.06108659, 0.21848397, 0.45893037, 0.33388814, 0.29429322, 0.19101022, 0.17539428, 0.10362202, 0.122804865, 0.17049669, 0.16572785, 0.0797373, 0.09023035, 0.09779201, 0.15378334, 0.19656575, 0.15792294, 0.1286155, 0.079171695, 0.07831691, 0.0797682, 0.12799998, 0.091367394, 0.07106752, 0.075222254, 0.07844536, 0.11079445, 0.12524448, 0.10763902, 0.07714568, 0.10037607, 0.07280238, 0.09075198, 0.07315211, 0.09200734, 0.07873311, 0.0986918, 0.09446006, 0.06519443, 0.06333691, 0.060597807, 0.08613776, 0.07219274, 0.066268966, 0.3018872, 0.41495827, 0.22469267, 0.2962675, 0.26976752, 0.2404462, 0.2389171, 0.12394951, 0.15008797, 0.28056306, 0.16294803, 0.08806254, 0.07866425, 0.117243, 0.13216916, 0.118254386, 0.12450134, 0.10557782, 0.094018556, 0.08197191, 0.09202198, 0.103532925, 0.09024646, 0.12369323, 0.08286317, 0.14095555, 0.12948687, 0.07420454, 0.08788316, 0.09509438, 0.06932873, 0.08883699, 0.0842216, 0.07244245, 0.07584253, 0.064535, 0.07721633, 0.12727763, 0.28553283, 0.15761286, 0.2713104, 0.25073984, 0.2830975, 0.21662539, 0.24579121, 0.14139862, 0.17726497, 0.1894303, 0.109689735, 0.15656656, 0.16700165, 0.09457836, 0.083606295, 0.08552067, 0.14703792, 0.07480829, 0.05734401, 0.06816405, 0.086202934, 0.09436094, 0.04810833, 0.080949076, 0.10663157, 0.11073349, 0.08728158, 0.086692065, 0.097536616, 0.07370983, 0.083297476, 0.07866183, 0.06526361, 0.077845536, 0.07041321, 0.063067906, 0.07085495, 0.07169418, 0.118735775, 0.1624578, 0.19465216, 0.3313265, 0.19516218, 0.14088412, 0.07546486, 0.19333635, 0.18473937, 0.29921505, 0.23444279, 0.13600786, 0.19426069, 0.15911385, 0.20097426, 0.111738265, 0.08400732, 0.10454721, 0.07289761, 0.083402015, 0.095002785, 0.08181105, 0.09320869, 0.09593709, 0.110747, 0.066619255, 0.07617345, 0.083020374, 0.0758948, 0.07746159, 0.075062394, 0.07733988, 0.08424427, 0.07585962, 0.067975864, 0.07604716, 0.11766164, 0.08459992, 0.27228537, 0.40076122, 0.2643479, 0.2396362, 0.19323449, 0.3762412, 0.24936719, 0.1943779, 0.0812437, 0.17701112, 0.13517542, 0.13138828, 0.13672416, 0.09074383, 0.19439971, 0.077138215, 0.07281692, 0.1031816, 0.0965978, 0.054074124, 0.09754126, 0.09813628, 0.08073689, 0.06329102, 0.0648614, 0.09217366, 0.061665762, 0.08003866, 0.07299906, 0.058131456, 0.09470959, 0.06006107, 0.08425731, 0.06885639, 0.09016947, 0.061723024, 0.066424705, 0.08787674, 0.12619719, 0.15102376, 0.32037297, 0.2853942, 0.23646846, 0.14332323, 0.21355262, 0.09379965, 0.18733661, 0.10940972, 0.10268322, 0.13888164, 0.12063285, 0.123968564, 0.106567256, 0.11315034, 0.08926672, 0.08886991, 0.11122317, 0.09117219, 0.0937489, 0.10145018, 0.080682606, 0.06943495, 0.059159003, 0.11200059, 0.06204152, 0.07945356, 0.09837792, 0.08666855, 0.08077497, 0.0956404, 0.11580894, 0.09127069, 0.07955306, 0.08040071, 0.06454964, 0.051891755, 0.06880303, 0.058142137, 0.058065224, 0.097484015, 0.058271885, 0.09019234, 0.14878602, 0.39984217, 0.32134783, 0.27287117, 0.17409146, 0.16226923, 0.096256, 0.12095736, 0.16288605, 0.14339547, 0.16000539, 0.07960565, 0.10682684, 0.10464289, 0.17050515, 0.0967369, 0.06101067, 0.11603572, 0.11809971, 0.085879125, 0.073398255, 0.055570886, 0.09319026, 0.10315945, 0.09290259, 0.0604415, 0.07004445, 0.06518223, 0.077414975, 0.0861699, 0.08626201, 0.05603493, 0.050952848, 0.05927921, 0.076358154, 0.09320425, 0.06652293, 0.07149593, 0.28799838, 0.21346311, 0.20321813, 0.40108627, 0.14080633, 0.14503653, 0.20279995, 0.18624511, 0.10262497, 0.17536962, 0.14538828, 0.16452783, 0.09299499, 0.1996251, 0.10642079, 0.0821589, 0.14661483, 0.09861298, 0.07349872, 0.11032659, 0.11186888, 0.07944781, 0.07536199, 0.09246655, 0.07470087, 0.08750286, 0.056106612, 0.054597717, 0.06979495, 0.060619574, 0.0830804, 0.07508246, 0.05982066, 0.06480198, 0.07158917, 0.06718656, 0.058032174, 0.058324143, 0.1954761, 0.20429313, 0.28193417, 0.25770125, 0.13856809, 0.24804695, 0.14625871, 0.24664882, 0.26351348, 0.16160865, 0.10024462, 0.09924442, 0.08375927, 0.107736014, 0.0750074, 0.098175764, 0.08427889, 0.13392873, 0.07178811, 0.11042393, 0.11503044, 0.054062534, 0.08244038, 0.068973444, 0.05495448, 0.09257602, 0.07136678, 0.07274003, 0.060268972, 0.06505882, 0.068278104, 0.05055142, 0.053889945, 0.05582889, 0.060685728, 0.06522778, 0.06102108, 0.04730913, 0.14083628, 0.15861464, 0.34438816, 0.140979, 0.11549631, 0.13451174, 0.23096724, 0.25744, 0.26234642, 0.1490653, 0.12797408, 0.11749145, 0.15184253, 0.1954007, 0.1011244, 0.09841263, 0.1127128, 0.094592765, 0.09343411, 0.10140596, 0.06517288, 0.064533085, 0.055227544, 0.07233209, 0.052024476, 0.07670277, 0.076426685, 0.049776044, 0.06186542, 0.066409566, 0.067771696, 0.050348487, 0.050433114, 0.06436869, 0.08097391, 0.05294006, 0.058517944, 0.053571004, 0.28894863, 0.2624806, 0.25167117, 0.29935548, 0.15110447, 0.24898082, 0.14698333, 0.14942761, 0.22944176, 0.24886274, 0.25536832, 0.14996044, 0.1256019, 0.16392826, 0.14031899, 0.16374019, 0.079629324, 0.08511769, 0.05913186, 0.059315544, 0.098259956, 0.0761259, 0.06518075, 0.056569364, 0.07587759, 0.07194799, 0.06351947, 0.090091266, 0.07567558, 0.07724855, 0.056384016, 0.07590481, 0.042954076, 0.045292225, 0.062217686, 0.05860251, 0.06059102, 0.05903542, 0.06993061, 0.05820302, 0.058132052, 0.0689821, 0.06459498, 0.1532384, 0.4051231, 0.23343834, 0.1408326, 0.22276668, 0.22638446, 0.12281059, 0.1099877, 0.23068345, 0.10196249, 0.08230286, 0.093000256, 0.13253613, 0.12690468, 0.108901836, 0.16120362, 0.09650753, 0.08287074, 0.06965834, 0.062354058, 0.08045633, 0.08818342, 0.06776049, 0.08413593, 0.09485305, 0.068047, 0.07196138, 0.061893694, 0.04788829, 0.065178216, 0.053159412, 0.054174673, 0.07605371, 0.09325974, 0.060806524, 0.0441561, 0.060780838, 0.0713552, 0.20942526, 0.3971128, 0.13734032, 0.18276784, 0.19315828, 0.1462289, 0.17363201, 0.12081073, 0.096500374, 0.09837335, 0.23790774, 0.088566, 0.15460774, 0.14245984, 0.12151964, 0.12668744, 0.07795268, 0.12692748, 0.10271948, 0.091824435, 0.076377176, 0.05509361, 0.08574449, 0.079338185, 0.062326945, 0.067631625, 0.06932369, 0.04298458, 0.05118637, 0.04898671, 0.060238767, 0.056050997, 0.04060084, 0.05121785, 0.07342961, 0.07330478, 0.053335994, 0.050167795, 0.19059908, 0.31495023, 0.21486606, 0.2508437, 0.34996295, 0.19708548, 0.18102857, 0.1753163, 0.15205975, 0.14447731, 0.07522278, 0.15106046, 0.1019934, 0.11253557, 0.07997124, 0.13710642, 0.100398034, 0.088825546, 0.09864805, 0.086778566, 0.0847125, 0.10101967, 0.077210866, 0.0625371, 0.050903052, 0.06949084, 0.05051292, 0.062562734, 0.08618825, 0.05518928, 0.06099359, 0.06343788, 0.051036198, 0.07479602, 0.068910114, 0.06743536, 0.05459864, 0.073672, 0.24429467, 0.16269068, 0.18529625, 0.2858092, 0.1890038, 0.11261757, 0.11963019, 0.13083294, 0.15909524, 0.09041313, 0.12069874, 0.10075178, 0.072757706, 0.08276668, 0.08892038, 0.118512906, 0.085512005, 0.095625736, 0.07142153, 0.0741145, 0.061973464, 0.07409956, 0.072502054, 0.076423615, 0.05513502, 0.07967609, 0.07627469, 0.05908631, 0.08915931, 0.06554065, 0.060412314, 0.05500552, 0.06506983, 0.06550907, 0.056336056, 0.04897256, 0.09604892, 0.04108474, 0.20077822, 0.18380745, 0.3571116, 0.3423225, 0.15768781, 0.18037012, 0.20083386, 0.13662839, 0.16415828, 0.09517999, 0.08786875, 0.072556816, 0.14899823, 0.07546686, 0.089208335, 0.10017244, 0.060648873, 0.07935924, 0.08696737, 0.07564788, 0.073797375, 0.052488647, 0.07652147, 0.05878575, 0.065517485, 0.046388995, 0.06699786, 0.054510307, 0.067785285, 0.064416595, 0.063713916, 0.058202, 0.07796492, 0.056881547, 0.054167014, 0.062271047, 0.045010548, 0.05994443, 0.07115333, 0.048619363, 0.06065409, 0.0556007, 0.050433345, 0.050734006, 0.20555823, 0.25997797, 0.15352751, 0.15165131, 0.15734072, 0.18874101, 0.28774014, 0.22165339, 0.117800474, 0.103043556, 0.0776871, 0.09808602, 0.13640882, 0.14722657, 0.11845945, 0.08097793, 0.07114005, 0.06907228, 0.0656825, 0.066363774, 0.07828481, 0.059127916, 0.061207443, 0.089118995, 0.062327467, 0.055622965, 0.040616333, 0.043450683, 0.061077632, 0.042126194, 0.045877207, 0.05280826, 0.054869037, 0.03503231, 0.058719266, 0.041786976, 0.06257601, 0.06612517, 0.079288006, 0.165962, 0.2441049, 0.277437, 0.11254653, 0.10998012, 0.15169819, 0.22685598, 0.115936264, 0.13940203, 0.16847014, 0.07042754, 0.16778257, 0.072658, 0.063971184, 0.06007817, 0.057152692, 0.06562162, 0.06078144, 0.09191194, 0.054758474, 0.062208522, 0.06096769, 0.06941041, 0.05365194, 0.084202945, 0.05086304, 0.062377382, 0.052000772, 0.06435963, 0.044627793, 0.054678753, 0.055277422, 0.05743131, 0.041704983, 0.055146907, 0.04461367, 0.029773355, 0.20900138, 0.40356255, 0.40712523, 0.16127199, 0.22143899, 0.21877961, 0.15518801, 0.44901994, 0.24843869, 0.20427234, 0.093823746, 0.11982575, 0.14880283, 0.09038022, 0.06231848, 0.080250226, 0.06362456, 0.06821805, 0.060339957, 0.071438685, 0.078725, 0.06603672, 0.055996746, 0.061252147, 0.06787635, 0.051539153, 0.056444418, 0.053600557, 0.041212406, 0.112960115, 0.08157573, 0.06964455, 0.042481653, 0.0630732, 0.048465863, 0.06287503, 0.057270795, 0.061811045, 0.15359516, 0.284824, 0.3142972, 0.19437356, 0.10183883, 0.124242894, 0.09467783, 0.12327598, 0.2201486, 0.11371506, 0.073599465, 0.08765766, 0.103827104, 0.10199246, 0.10448382, 0.06712374, 0.09781158, 0.08081625, 0.07656095, 0.08290553, 0.07176178, 0.07712567, 0.07217902, 0.046763502, 0.05735548, 0.052234944, 0.054707456, 0.0579094, 0.04444446, 0.06544118, 0.060994633, 0.04958115, 0.060619842, 0.042592622, 0.065752275, 0.045900848, 0.07113224, 0.044521526, 0.21978883, 0.23432335, 0.19008182, 0.3346865, 0.18226588, 0.16724502, 0.10453045, 0.09579079, 0.092375435, 0.07976141, 0.1501715, 0.07365548, 0.05124435, 0.05526525, 0.090245545, 0.11600269, 0.08758742, 0.057697784, 0.083865546, 0.05410126, 0.06615888, 0.06701875, 0.08496154, 0.04359038, 0.06761209, 0.070993446, 0.05440264, 0.050275188, 0.042520594, 0.073068604, 0.078940995, 0.07639555, 0.061837766, 0.045109816, 0.045998286, 0.056979466, 0.0710484, 0.039593674, 0.07913066, 0.041288156, 0.053343, 0.058147047, 0.0396611, 0.14438805, 0.23774001, 0.24036367, 0.15668665, 0.1590339, 0.15688615, 0.17412864, 0.09867248, 0.09802497, 0.11436576, 0.09689981, 0.058242403, 0.08642865, 0.079756014, 0.053966228, 0.103054926, 0.06792214, 0.048778594, 0.04967507, 0.059153486, 0.059934936, 0.08329426, 0.055844232, 0.06847621, 0.050976526, 0.059437487, 0.07154248, 0.063014105, 0.05259652, 0.06337521, 0.051768214, 0.043786403, 0.05498255, 0.043883625, 0.042128477, 0.08751224, 0.05586692, 0.043098055, 0.18447758, 0.18948454, 0.14432059, 0.27798784, 0.18948412, 0.19308604, 0.19484603, 0.080068156, 0.10862743, 0.13575839, 0.084174044, 0.059941456, 0.07512468, 0.055559825, 0.048041184, 0.055705484, 0.08239702, 0.06062657, 0.084450476, 0.07040419, 0.06438651, 0.07095811, 0.05777076, 0.061672643, 0.056041114, 0.087497056, 0.050865058, 0.07883946, 0.050029382, 0.068901934, 0.051053096, 0.04817457, 0.036321495, 0.05380397, 0.03719963, 0.03082562, 0.050173797, 0.0513682, 0.24633712, 0.1090809, 0.21783586, 0.1591198, 0.20296434, 0.13611087, 0.15214376, 0.13127495, 0.19842853, 0.19207378, 0.054667428, 0.078423426, 0.08849587, 0.085758194, 0.09480408, 0.09980959, 0.080540635, 0.080782406, 0.069749415, 0.07035624, 0.064009205, 0.045364443, 0.047461748, 0.06829146, 0.05274248, 0.08168139, 0.06579148, 0.052022412, 0.043661803, 0.058283493, 0.06899446, 0.047063682, 0.052250817, 0.04515106, 0.057088368, 0.050337855, 0.035860907, 0.043803934, 0.0867754, 0.18758316, 0.1692624, 0.23117301, 0.18506107, 0.12928386, 0.06792134, 0.08348225, 0.1842047, 0.088235214, 0.10391513, 0.16436233, 0.18043251, 0.12837085, 0.07564233, 0.062958, 0.054684088, 0.08789102, 0.13682924, 0.07692257, 0.0703271, 0.05617549, 0.04668113, 0.04557675, 0.06577359, 0.05914954, 0.05336237, 0.057259273, 0.054208424, 0.0418316, 0.04116055, 0.06540425, 0.05207317, 0.06390534, 0.053366955, 0.035433743, 0.047078036, 0.040783897, 0.07832452, 0.0924014, 0.16268076, 0.16322789, 0.2373096, 0.13825178, 0.09138987, 0.196609, 0.0756885, 0.06993404, 0.0740397, 0.0850315, 0.083234444, 0.11355892, 0.1277037, 0.1041947, 0.07390917, 0.07138083, 0.089650735, 0.07399047, 0.066000454, 0.056004662, 0.06029637, 0.05368719, 0.054280546, 0.059138644, 0.07062175, 0.05062033, 0.04388112, 0.08655213, 0.054340217, 0.051927757, 0.05018555, 0.051094968, 0.044910047, 0.03765242, 0.034581676, 0.04249439, 0.034311462, 0.04412175, 0.029634094, 0.058749825, 0.049751803, 0.046070866, 0.12495063, 0.12253057, 0.14999409, 0.17328201, 0.11263452, 0.10687413, 0.10816238, 0.07990835, 0.09911578, 0.0725953, 0.087145664, 0.08913999, 0.074614, 0.11039945, 0.104046084, 0.06822564, 0.06449924, 0.04812732, 0.07758722, 0.08612022, 0.06714617, 0.043710686, 0.049183615, 0.06294249, 0.062423278, 0.054986045, 0.05172475, 0.043199588, 0.03548169, 0.054477166, 0.044366132, 0.039926868, 0.035203565, 0.045380574, 0.048637044, 0.041257426, 0.049239468, 0.041464806, 0.069774225, 0.11797432, 0.15012974, 0.13416176, 0.13594447, 0.108266905, 0.08944843, 0.1388489, 0.27315038, 0.1050893, 0.047672477, 0.109620094, 0.12704255, 0.070437446, 0.06896627, 0.05536835, 0.07436129, 0.071188495, 0.06698981, 0.04689957, 0.06411427, 0.05515423, 0.041274842, 0.057105254, 0.04791637, 0.05466374, 0.043084003, 0.026355615, 0.03323335, 0.040382553, 0.055654574, 0.04565945, 0.04449976, 0.03983107, 0.056458123, 0.03900427, 0.051178772, 0.051482666, 0.210412, 0.31585485, 0.20292182, 0.1506449, 0.1712588, 0.13797049, 0.11646966, 0.2167921, 0.14256835, 0.08902878, 0.10293495, 0.08818491, 0.06572512, 0.0719557, 0.1009203, 0.048253223, 0.10368252, 0.05909739, 0.08644445, 0.097101875, 0.048630368, 0.043386254, 0.03881696, 0.058215536, 0.041811615, 0.05228749, 0.050176393, 0.06360815, 0.033350732, 0.038041037, 0.06874856, 0.06221748, 0.053162318, 0.04512201, 0.050731096, 0.054731682, 0.04594347, 0.036841627, 0.1445036, 0.3392939, 0.2135607, 0.25008386, 0.1832336, 0.100823574, 0.11551664, 0.098399855, 0.07267509, 0.07330502, 0.10618248, 0.10524537, 0.07497878, 0.08567477, 0.07632438, 0.05471203, 0.052580122, 0.037291616, 0.04818768, 0.054203372, 0.06096897, 0.04466891, 0.04191655, 0.043834493, 0.053475626, 0.04205431, 0.044281945, 0.05040281, 0.03788829, 0.041310523, 0.051423427, 0.04436319, 0.04705092, 0.044456337, 0.05028336, 0.03827897, 0.0554955, 0.024777217, 0.35753554, 0.27708843, 0.1481268, 0.1575179, 0.116528824, 0.09016944, 0.08598071, 0.07424264, 0.09562268, 0.055012427, 0.09112368, 0.058636475, 0.0948225, 0.056072544, 0.060304705, 0.043914627, 0.04787782, 0.051662832, 0.052879523, 0.04761051, 0.04836373, 0.05401169, 0.042570245, 0.055524964, 0.028408471, 0.044849396, 0.04991489, 0.05574619, 0.043811727, 0.046157554, 0.046382762, 0.045191802, 0.05810542, 0.048349, 0.06146034, 0.051488034, 0.038095888, 0.043125946, 0.033740114, 0.048015945, 0.036292497, 0.04164754, 0.03861506, 0.07309946, 0.09924049, 0.15057051, 0.17293094, 0.14486998, 0.115998216, 0.20666163, 0.13450952, 0.12896423, 0.046115804, 0.054970473, 0.04525851, 0.11104131, 0.06074984, 0.0647393, 0.10103039, 0.0571743, 0.048919782, 0.05209081, 0.06110271, 0.0496407, 0.048725575, 0.052740663, 0.03423818, 0.037657835, 0.039637294, 0.03260911, 0.046472758, 0.030920042, 0.046170883, 0.049514603, 0.02767473, 0.06477652, 0.03757401, 0.031462595, 0.03240839, 0.039682556, 0.027996512, 0.14896497, 0.33218813, 0.13438706, 0.1254888, 0.15275496, 0.120158285, 0.07049134, 0.10526863, 0.10293143, 0.083563745, 0.081761084, 0.052251346, 0.06959837, 0.042796824, 0.033225905, 0.056576364, 0.044927068, 0.05028138, 0.03602568, 0.06048725, 0.053781144, 0.05096441, 0.02850397, 0.05509745, 0.05360912, 0.03552345, 0.046504166, 0.058152594, 0.029614473, 0.04703542, 0.031154018, 0.06347772, 0.03816958, 0.031810075, 0.038528614, 0.031320136, 0.03821352, 0.034387633, 0.16254076, 0.34781906, 0.14479013, 0.11418005, 0.15195708, 0.13290769, 0.13910052, 0.08563145, 0.13243367, 0.12336577, 0.07466492, 0.0799953, 0.07581687, 0.081204355, 0.05691044, 0.076341204, 0.038774896, 0.07955025, 0.043569475, 0.05359887, 0.043104075, 0.054781016, 0.04684916, 0.049321286, 0.030202517, 0.044085737, 0.048024714, 0.047300108, 0.051270943, 0.03750035, 0.05282097, 0.03710291, 0.039032266, 0.03454879, 0.041102618, 0.041709766, 0.035584427, 0.06154639, 0.16392322, 0.16264991, 0.16256478, 0.16707651, 0.18924107, 0.32446012, 0.11303463, 0.16633804, 0.20294568, 0.14798649, 0.16463389, 0.11996346, 0.08969395, 0.08916779, 0.115472205, 0.0633829, 0.04566132, 0.0602647, 0.08973105, 0.09030549, 0.05211923, 0.038047004, 0.04782063, 0.047715392, 0.04543093, 0.04403302, 0.04384799, 0.03154617, 0.03441233, 0.036264915, 0.034324378, 0.034984995, 0.031207204, 0.035082936, 0.036136277, 0.041570753, 0.048957575, 0.04420303, 0.078299835, 0.19351533, 0.16429599, 0.11096397, 0.17241482, 0.124909416, 0.11042919, 0.18876666, 0.10832455, 0.08125003, 0.066000305, 0.07324977, 0.050120186, 0.083801806, 0.050512362, 0.047444902, 0.12732834, 0.09911462, 0.04842503, 0.060669854, 0.046999484, 0.058115702, 0.046158925, 0.03494765, 0.059793785, 0.041258484, 0.045365114, 0.04100146, 0.046358675, 0.048320975, 0.040700056, 0.04108144, 0.035851132, 0.034937255, 0.035167713, 0.033400137, 0.035619963, 0.0379098, 0.038647123, 0.03958798, 0.030987216, 0.03550381, 0.031665757, 0.034621865, 0.379765, 0.23441646, 0.15443985, 0.33652312, 0.2570561, 0.10043857, 0.13008082, 0.13263762, 0.14036863, 0.10647211, 0.06933616, 0.063218765, 0.059073366, 0.06434011, 0.04972749, 0.03915389, 0.05009942, 0.055042755, 0.046236075, 0.04916833, 0.047420766, 0.04305908, 0.04562754, 0.0529096, 0.0360264, 0.03775337, 0.034709245, 0.038212385, 0.030951088, 0.043514904, 0.039582577, 0.031621426, 0.029108195, 0.038350016, 0.032158058, 0.032851636, 0.039104737, 0.04093272, 0.0897557, 0.17729488, 0.15319055, 0.3497238, 0.0888364, 0.116528764, 0.054308563, 0.086503424, 0.05868833, 0.09315052, 0.04776062, 0.06991651, 0.10145108, 0.08273249, 0.052057352, 0.08021544, 0.044327162, 0.037643645, 0.06961145, 0.035316132, 0.047475815, 0.03610061, 0.04283297, 0.038508385, 0.047712088, 0.03439757, 0.03644317, 0.039996855, 0.041041106, 0.0530668, 0.039822206, 0.05179285, 0.04252931, 0.028733905, 0.031143779, 0.03847024, 0.03837069, 0.03499517, 0.13740417, 0.14030142, 0.23848453, 0.10926548, 0.059187576, 0.08489641, 0.08127779, 0.09165528, 0.061409567, 0.052263968, 0.09696271, 0.060218308, 0.06473098, 0.04494434, 0.042972382, 0.054850094, 0.028202837, 0.041775137, 0.0501691, 0.036792986, 0.039790906, 0.061250657, 0.066100135, 0.046506498, 0.04515812, 0.045162965, 0.0426645, 0.03364055, 0.043838955, 0.030767996, 0.035740815, 0.043863524, 0.03885543, 0.044784952, 0.02853994, 0.028284403, 0.02577977, 0.023310116, 0.19642814, 0.18475704, 0.114715755, 0.1549778, 0.0839127, 0.09852638, 0.045413394, 0.052297723, 0.17084204, 0.13703175, 0.13878207, 0.057809517, 0.053786606, 0.06121308, 0.08889977, 0.12172364, 0.053880464, 0.045785032, 0.045646757, 0.038530335, 0.05155465, 0.033670545, 0.05955752, 0.029933456, 0.056821335, 0.05893301, 0.035591386, 0.028597075, 0.05314274, 0.04401312, 0.05513296, 0.04401611, 0.031227099, 0.03415014, 0.04609809, 0.029256083, 0.034007195, 0.034026347, 0.056108445, 0.06252608, 0.07377085, 0.21910499, 0.09075556, 0.07257314, 0.11625516, 0.0926532, 0.16597034, 0.08630536, 0.06366894, 0.060865182, 0.05700764, 0.05326709, 0.082450844, 0.050965834, 0.051423904, 0.04016827, 0.049634848, 0.053667363, 0.060328472, 0.038829435, 0.033392638, 0.02713054, 0.031079631, 0.045977417, 0.03440323, 0.033112507, 0.038314853, 0.029026164, 0.0312782, 0.032918595, 0.037374806, 0.032505285, 0.033280898, 0.043024585, 0.025835915, 0.028385714, 0.11942405, 0.18472016, 0.1294781, 0.09493525, 0.11550258, 0.0966926, 0.1489993, 0.13775305, 0.060289532, 0.09527308, 0.07028981, 0.055621166, 0.074218705, 0.052358925, 0.052015472, 0.06479609, 0.036851026, 0.037426013, 0.04540428, 0.043634262, 0.049250934, 0.042848203, 0.029267224, 0.031780247, 0.04150936, 0.035365168, 0.024391664, 0.036729027, 0.053796567, 0.028789489, 0.049495656, 0.039596308, 0.031205738, 0.02726073, 0.04783313, 0.025606193, 0.03596269, 0.023635522, 0.082136944, 0.17945571, 0.13898422, 0.15409493, 0.07651983, 0.06166094, 0.062527224, 0.06529715, 0.078312464, 0.12612315, 0.1567907, 0.08924686, 0.086650826, 0.08227096, 0.062699474, 0.031449337, 0.041376177, 0.038300753, 0.048569657, 0.05251412, 0.038850397, 0.025022985, 0.038919702, 0.04845143, 0.034600765, 0.022030279, 0.02973751, 0.034318242, 0.037272602, 0.03634571, 0.03020243, 0.073182926, 0.047979325, 0.025218759, 0.035997674, 0.028316442, 0.04058279, 0.043100454, 0.108595625, 0.48258722, 0.2001489, 0.10147363, 0.11114137, 0.11771078, 0.06648806, 0.10923018, 0.07454028, 0.06961553, 0.073757775, 0.033246323, 0.049591087, 0.08229985, 0.050279453, 0.037394352, 0.055211544, 0.045800496, 0.033896968, 0.04333179, 0.03544548, 0.046667054, 0.033628203, 0.032702833, 0.04205228, 0.04875474, 0.027260467, 0.02779024, 0.02854234, 0.034581106, 0.030964943, 0.033619687, 0.030012488, 0.02630351, 0.032425318, 0.050639592, 0.03791516, 0.030922532, 0.11864849, 0.111897394, 0.10828104, 0.07702905, 0.11585907, 0.09919997, 0.12151485, 0.105632395, 0.041062813, 0.058928646, 0.07606238, 0.07393952, 0.05453207, 0.053649645, 0.05850403, 0.06006141, 0.042523965, 0.04340863, 0.040965863, 0.038096488, 0.03188339, 0.042141788, 0.030711848, 0.03882588, 0.04892861, 0.04054448, 0.029367525, 0.027014215, 0.03546158, 0.023685092, 0.044913244, 0.03493714, 0.027918125, 0.03539053, 0.03604834, 0.03254351, 0.028322421, 0.03407297, 0.09965644, 0.21374215, 0.14353476, 0.12943381, 0.07133086, 0.10778891, 0.07654376, 0.043047834, 0.1811784, 0.07676934, 0.054803416, 0.04321328, 0.055431575, 0.04190181, 0.05273347, 0.044133272, 0.04743074, 0.043965075, 0.032189313, 0.036372133, 0.056489576, 0.029330758, 0.033380333, 0.03967014, 0.03444845, 0.05419228, 0.047731854, 0.045022618, 0.023330463, 0.04396021, 0.036333784, 0.022413945, 0.043159988, 0.035339285, 0.029649261, 0.037065577, 0.029937781, 0.025534755, 0.032413848, 0.034752168, 0.028045213, 0.029938294, 0.038155667, 0.05999878, 0.12874456, 0.09015986, 0.17240526, 0.13753879, 0.19400132, 0.058931362, 0.0539266, 0.09888909, 0.06896733, 0.050828606, 0.07619395, 0.03457297, 0.053750534, 0.049362667, 0.04690425, 0.037691686, 0.036156163, 0.045989376, 0.045784432, 0.042752996, 0.021831132, 0.042187385, 0.05095848, 0.028854197, 0.026268011, 0.039114956, 0.028946491, 0.028238636, 0.028583402, 0.031576306, 0.046505228, 0.034169044, 0.050692465, 0.025518468, 0.032532915, 0.030571928, 0.03718945, 0.21758823, 0.19966938, 0.16424575, 0.119185336, 0.1351876, 0.05370275, 0.12742084, 0.12275652, 0.10532801, 0.10245091, 0.05578476, 0.09182558, 0.05406913, 0.07765349, 0.043578155, 0.06828653, 0.044616293, 0.033196338, 0.05406697, 0.033283457, 0.026869975, 0.034808397, 0.029017719, 0.038705498, 0.06391789, 0.043294482, 0.049450945, 0.0453595, 0.016174976, 0.027437752, 0.041374862, 0.03746266, 0.0223723, 0.03304617, 0.030931607, 0.04192908, 0.033565007, 0.021764338, 0.103624605, 0.07580691, 0.095039755, 0.11453148, 0.16354473, 0.10914165, 0.08385877, 0.15846677, 0.15852472, 0.06183695, 0.07122284, 0.054418024, 0.069133, 0.047677062, 0.06488033, 0.049565222, 0.049201056, 0.051443722, 0.039953142, 0.03383554, 0.060677566, 0.06710575, 0.026530165, 0.03832979, 0.033288963, 0.03203385, 0.029809278, 0.027682662, 0.033524487, 0.031275976, 0.029191745, 0.026989615, 0.031300835, 0.019306311, 0.033538863, 0.023974303, 0.030821547, 0.040835958, 0.08513235, 0.12747774, 0.24677016, 0.11424023, 0.114598654, 0.11191717, 0.1219175, 0.0822158, 0.106676675, 0.06620338, 0.052198023, 0.055179633, 0.049692113, 0.04536436, 0.03825671, 0.05374633, 0.031004507, 0.031022107, 0.043563355, 0.039634265, 0.039326258, 0.03451102, 0.030726787, 0.03338389, 0.04215721, 0.035123173, 0.033253472, 0.036828496, 0.03086434, 0.037409853, 0.031002594, 0.039429653, 0.033229742, 0.024107346, 0.03294632, 0.02991542, 0.03714958, 0.025753284, 0.08455882, 0.118517086, 0.20474009, 0.18621725, 0.16681747, 0.16311964, 0.12459198, 0.096971095, 0.09452454, 0.10414249, 0.064236455, 0.041201066, 0.07028978, 0.06070966, 0.07111702, 0.06279898, 0.03164801, 0.032184128, 0.057500865, 0.03910684, 0.030070081, 0.03996016, 0.026971823, 0.04934738, 0.052988693, 0.037451208, 0.042510662, 0.04180888, 0.035314683, 0.038253292, 0.03703214, 0.050774336, 0.036557693, 0.035440136, 0.037497047, 0.03380079, 0.036177028, 0.03266264, 0.033002358, 0.031060852, 0.023145668, 0.037183624, 0.030693032, 0.033373345, 0.10490153, 0.15622205, 0.19108656, 0.26184514, 0.13784234, 0.10903221, 0.10216838, 0.11343443, 0.12687796, 0.11900011, 0.09539601, 0.04736041, 0.050185997, 0.05111245, 0.058402188, 0.046517488, 0.03038349, 0.025462342, 0.060268294, 0.043199826, 0.038971648, 0.0364442, 0.03895675, 0.041721463, 0.03155169, 0.04042138, 0.03184676, 0.029379157, 0.03594502, 0.04433184, 0.035801157, 0.034515444, 0.029640304, 0.04156114, 0.027660307, 0.032173794, 0.037187673, 0.024102837, 0.081851736, 0.15138637, 0.22508544, 0.36221954, 0.12618794, 0.124801196, 0.105814256, 0.066930234, 0.071265824, 0.04877285, 0.069354795, 0.06093418, 0.044138845, 0.042146653, 0.053874593, 0.060577713, 0.04982594, 0.029500961, 0.06313129, 0.048656616, 0.041702427, 0.04935911, 0.044726323, 0.033588357, 0.04100187, 0.027525654, 0.03450922, 0.022114182, 0.042045712, 0.032201976, 0.04038668, 0.045082975, 0.03534608, 0.03432636, 0.021917898, 0.027111314, 0.030044818, 0.030562112, 0.12683783, 0.25166774, 0.106817774, 0.13900904, 0.0791499, 0.064352505, 0.050291877, 0.08559058, 0.11157352, 0.09467714, 0.048175395, 0.03781752, 0.08488929, 0.100658454, 0.06386209, 0.04206816, 0.03224801, 0.062674515, 0.04264997, 0.033251163, 0.04401245, 0.03924043, 0.027212417, 0.025702298, 0.03321522, 0.037032753, 0.036360577, 0.03167841, 0.030937122, 0.0320562, 0.032797758, 0.03691715, 0.048813015, 0.02634142, 0.0625919, 0.027375987, 0.026625877, 0.025476074, 0.07413896, 0.10721598, 0.12030974, 0.08532386, 0.10996776, 0.09255521, 0.12783724, 0.09595208, 0.10707273, 0.08492241, 0.12213243, 0.03716293, 0.050879072, 0.069006234, 0.06814, 0.03694839, 0.04393996, 0.05796881, 0.036147777, 0.022474092, 0.040966824, 0.044067025, 0.026868142, 0.028796183, 0.0273631, 0.045017064, 0.02770925, 0.027385056, 0.027615016, 0.04341587, 0.040495142, 0.025814898, 0.026474668, 0.026013019, 0.037223525, 0.028365368, 0.03175622, 0.03800178, 0.14481787, 0.15571584, 0.1249838, 0.098713875, 0.08955155, 0.09116429, 0.077293694, 0.07731189, 0.045236267, 0.08871429, 0.059854656, 0.0746405, 0.05169555, 0.071101904, 0.049186487, 0.04917118, 0.039136417, 0.024737258, 0.054076597, 0.039991014, 0.033556033, 0.05384789, 0.05242281, 0.032968428, 0.039401244, 0.031628035, 0.03855916, 0.033872593, 0.0238372, 0.033914264, 0.03217626, 0.030620525, 0.040406074, 0.03683136, 0.03656953, 0.033607673, 0.026697896, 0.036073614, 0.022520088, 0.025608579, 0.021604048, 0.022794163, 0.034156878, 0.08217289, 0.19682892, 0.1751181, 0.14689875, 0.09658005, 0.07229053, 0.07106946, 0.079733364, 0.05017549, 0.054778915, 0.036791433, 0.062255275, 0.03134533, 0.041440353, 0.06927111, 0.04817765, 0.05243542, 0.049427744, 0.0312739, 0.03190065, 0.04945591, 0.038485106, 0.040144853, 0.03808609, 0.03972478, 0.032407463, 0.035918463, 0.035854686, 0.033842403, 0.032875407, 0.043883458, 0.042084176, 0.028781548, 0.020792704, 0.032668673, 0.021910196, 0.033966515, 0.030175548, 0.05655182, 0.12279331, 0.18388571, 0.103774235, 0.07578391, 0.1051862, 0.077797435, 0.06272199, 0.05953603, 0.07769426, 0.06599351, 0.04456654, 0.032962438, 0.06703144, 0.051075097, 0.046055645, 0.039767366, 0.040847026, 0.033893973, 0.02398512, 0.020412968, 0.03598228, 0.038413677, 0.02389834, 0.022933647, 0.045630615, 0.033100437, 0.019815853, 0.03238133, 0.048931614, 0.04021394, 0.032468446, 0.03557736, 0.023459917, 0.024762927, 0.03429259, 0.022828763, 0.034856603, 0.11656258, 0.2626128, 0.15054047, 0.074089624, 0.13431145, 0.06835116, 0.045839317, 0.062452685, 0.07949884, 0.07002596, 0.063235655, 0.12101733, 0.06763422, 0.049192227, 0.04924365, 0.03948208, 0.04331151, 0.06298809, 0.037498146, 0.044355396, 0.03295492, 0.06753151, 0.0363825, 0.045397744, 0.027521798, 0.031177012, 0.039706483, 0.03934901, 0.044413228, 0.030103734, 0.03750487, 0.03191661, 0.024885695, 0.024328124, 0.024745457, 0.025852818, 0.043880496, 0.022738677, 0.08406259, 0.07679967, 0.14572027, 0.14564764, 0.09408631, 0.06420363, 0.062250964, 0.13171874, 0.06812034, 0.10196334, 0.06657759, 0.056374926, 0.07235564, 0.05695052, 0.05451528, 0.049228016, 0.037536245, 0.038413, 0.048847165, 0.0532326, 0.03832078, 0.05659041, 0.042849008, 0.03322901, 0.04394022, 0.037482254, 0.039883405, 0.050437402, 0.021665832, 0.031132722, 0.033335973, 0.023553789, 0.030651106, 0.032008488, 0.026457952, 0.02970037, 0.03084025, 0.028883046, 0.09826612, 0.09963964, 0.08986379, 0.11260369, 0.18102352, 0.16404855, 0.05195865, 0.041073117, 0.053599898, 0.057962388, 0.08000175, 0.07439851, 0.04677969, 0.08841064, 0.044933792, 0.048112832, 0.06129252, 0.05460642, 0.04863163, 0.038247224, 0.04382574, 0.035974257, 0.038133036, 0.03488276, 0.04160861, 0.049245615, 0.0307343, 0.027836809, 0.029568788, 0.022498852, 0.03312084, 0.031072043, 0.027312778, 0.036760066, 0.048102237, 0.033644356, 0.029466867, 0.02841795, 0.034653984, 0.029493872, 0.022699866, 0.021997066, 0.027306758, 0.032154366, 0.060770907, 0.0762933, 0.14563988, 0.13445675, 0.11009425, 0.08264572, 0.12765858, 0.05021266, 0.10641617, 0.05463136, 0.059986144, 0.040911816, 0.038358126, 0.045473658, 0.063411996, 0.049380694, 0.04830117, 0.033533487, 0.031841174, 0.055233974, 0.039794553, 0.030200237, 0.027443497, 0.031824555, 0.02455477, 0.044180263, 0.02153476, 0.019287128, 0.034194566, 0.026909007, 0.027344827, 0.036969714, 0.030616308, 0.039022755, 0.032690596, 0.030367553, 0.03558555, 0.03437331, 0.045257356, 0.11303267, 0.15818192, 0.08861533, 0.113063574, 0.08775718, 0.09979047, 0.07122214, 0.026645472, 0.06542981, 0.08187976, 0.04334309, 0.044571716, 0.042610496, 0.04393807, 0.035590775, 0.044098403, 0.029004442, 0.053518735, 0.039306775, 0.047783323, 0.045577917, 0.024077205, 0.026734628, 0.034006607, 0.035894, 0.0238677, 0.03631409, 0.03632565, 0.024024237, 0.030419493, 0.023675563, 0.031815622, 0.02370523, 0.027772669, 0.026962038, 0.024831006, 0.02964511, 0.09240633, 0.076133594, 0.10401135, 0.13642032, 0.06701577, 0.13142653, 0.08593115, 0.085992955, 0.0806863, 0.05509781, 0.054271888, 0.047774795, 0.07373076, 0.14781232, 0.060331, 0.031302407, 0.03434249, 0.042128075, 0.041072737, 0.041306324, 0.034258105, 0.02978717, 0.039888054, 0.03885444, 0.03859449, 0.03207823, 0.036443476, 0.049241938, 0.041290432, 0.030738715, 0.034208905, 0.027108096, 0.032897644, 0.026483294, 0.026538912, 0.018918704, 0.027489195, 0.025286019, 0.054944854, 0.16398561, 0.2798853, 0.1862567, 0.06699925, 0.076239966, 0.06536788, 0.050530195, 0.06961935, 0.06278031, 0.056967147, 0.061495744, 0.06505842, 0.099830806, 0.093456574, 0.046804406, 0.0372463, 0.041088525, 0.031478506, 0.03716641, 0.028454835, 0.028445445, 0.039878797, 0.02812076, 0.033766966, 0.03552794, 0.036179367, 0.030868674, 0.022671647, 0.02809734, 0.02394622, 0.023484778, 0.033118427, 0.03596283, 0.021012012, 0.03252852, 0.032416943, 0.021243189, 0.074866734, 0.07451465, 0.08897543, 0.13453968, 0.08037212, 0.06778135, 0.12287303, 0.1194422, 0.082104795, 0.05853124, 0.048834994, 0.049969066, 0.06557555, 0.041377924, 0.040756892, 0.03449324, 0.035576146, 0.045363396, 0.035046544, 0.03702928, 0.03179437, 0.035266887, 0.043739688, 0.039070692, 0.04015409, 0.036256783, 0.029515332, 0.03632979, 0.031575743, 0.023048766, 0.037733983, 0.03128007, 0.035210863, 0.026958521, 0.024559181, 0.01957855, 0.025250748, 0.023717757, 0.027626546, 0.028312027, 0.034562726, 0.029252535, 0.02637206, 0.112130165, 0.12613197, 0.17530017, 0.16513069, 0.16862611, 0.062749654, 0.0529598, 0.076882385, 0.10544936, 0.07877405, 0.075891376, 0.08190462, 0.03344667, 0.055729866, 0.06413225, 0.057495225, 0.04173109, 0.034692217, 0.025865633, 0.034269083, 0.040980473, 0.05361808, 0.038761176, 0.03344206, 0.031500164, 0.03185604, 0.03789712, 0.031795308, 0.03584422, 0.027065093, 0.03728121, 0.03949781, 0.033944562, 0.03993599, 0.042609204, 0.01973952, 0.021168726, 0.03872846, 0.06197933, 0.10662125, 0.083605945, 0.12934503, 0.20148869, 0.079744905, 0.053838976, 0.08990428, 0.08971379, 0.07962925, 0.041217715, 0.06877798, 0.070070386, 0.06374261, 0.0416402, 0.050927904, 0.021005563, 0.034539178, 0.051144768, 0.051756836, 0.03748675, 0.03137139, 0.040492255, 0.03713943, 0.029978363, 0.02331289, 0.033130687, 0.022418126, 0.032150153, 0.024408605, 0.033828218, 0.04249453, 0.03405998, 0.03871305, 0.021921672, 0.021459611, 0.025099501, 0.031371992, 0.09024909, 0.104852654, 0.1090704, 0.06568402, 0.070463225, 0.07550491, 0.049455058, 0.050513852, 0.14768852, 0.1242151, 0.037583772, 0.04719011, 0.063568264, 0.042420473, 0.063271314, 0.06013342, 0.043857347, 0.036248315, 0.025256291, 0.035399728, 0.02737086, 0.03777318, 0.04113708, 0.032773796, 0.026093211, 0.025444442, 0.028282749, 0.02379521, 0.02663, 0.03230359, 0.040554732, 0.042866953, 0.028545778, 0.033649784, 0.029416677, 0.03219661, 0.02896815, 0.023798669, 0.055072915, 0.058134366, 0.18069774, 0.19052358, 0.12910996, 0.12151176, 0.09733912, 0.038757846, 0.04777428, 0.06820768, 0.04219692, 0.0466593, 0.06145764, 0.07829497, 0.055290624, 0.035280216, 0.042681027, 0.04999392, 0.031716548, 0.037498392, 0.049642414, 0.038841505, 0.03128788, 0.036305003, 0.02902881, 0.020066887, 0.02449315, 0.030364638, 0.030323947, 0.038288567, 0.021201367, 0.031691324, 0.034024328, 0.028592976, 0.027555654, 0.021784164, 0.017478252, 0.023887942, 0.057733536, 0.09053662, 0.3595507, 0.07744834, 0.07377702, 0.050813507, 0.061176043, 0.06386964, 0.16870862, 0.04683587, 0.07655242, 0.065191634, 0.041682504, 0.041682754, 0.05146887, 0.031267077, 0.027401196, 0.03426223, 0.0276702, 0.03642639, 0.04507841, 0.026055427, 0.044571888, 0.040808734, 0.055102818, 0.032855008, 0.029829219, 0.033841737, 0.024997227, 0.03863452, 0.031731963, 0.025675228, 0.034244806, 0.0332717, 0.029236376, 0.03271106, 0.041650124, 0.031449594, 0.02679571, 0.02207325, 0.028493619, 0.024244364, 0.032536685, 0.023049468, 0.08925388, 0.068276495, 0.137492, 0.14590926, 0.088380076, 0.07960274, 0.10034396, 0.07670096, 0.055934098, 0.056832816, 0.05557285, 0.06181064, 0.04068668, 0.037830822, 0.03293525, 0.04106628, 0.033943906, 0.055795982, 0.0358856, 0.0388315, 0.03533556, 0.035562843, 0.043677278, 0.035305113, 0.026921717, 0.029180223, 0.02821169, 0.02536723, 0.038981844, 0.034161896, 0.027321188, 0.028241616, 0.030553497, 0.018547254, 0.024275476, 0.02300718, 0.037317943, 0.02713457, 0.057330206, 0.11695341, 0.14616962, 0.17407882, 0.078277186, 0.10198759, 0.11942679, 0.08042912, 0.09515736, 0.116543986, 0.08429851, 0.03995962, 0.055293195, 0.052598808, 0.025048537, 0.03621135, 0.055744015, 0.044748276, 0.068202205, 0.0397761, 0.04612625, 0.047000874, 0.037932888, 0.030878205, 0.023970352, 0.027867276, 0.03199195, 0.026369447, 0.034302868, 0.04127456, 0.029623985, 0.021048788, 0.028534733, 0.037972882, 0.029274665, 0.024492564, 0.03027244, 0.024148017, 0.061381504, 0.069938205, 0.1449772, 0.13877456, 0.051820006, 0.09102231, 0.119490825, 0.06896647, 0.08041992, 0.08302615, 0.05670596, 0.03308518, 0.042704068, 0.05189853, 0.031182095, 0.041474182, 0.044582028, 0.05781014, 0.036398996, 0.028065117, 0.037467066, 0.029486697, 0.027998297, 0.03360051, 0.02822898, 0.031471357, 0.03372376, 0.03265906, 0.026934894, 0.026119703, 0.026504153, 0.03774307, 0.026076399, 0.043423235, 0.024033405, 0.02940934, 0.03292921, 0.029264992, 0.18741032, 0.14010687, 0.13084166, 0.15215431, 0.065666795, 0.06747734, 0.097493716, 0.09148993, 0.10833243, 0.10666464, 0.062327735, 0.05744653, 0.059499, 0.042057168, 0.06153829, 0.031090705, 0.045772858, 0.06598145, 0.032207347, 0.03311531, 0.034984417, 0.03525437, 0.029532868, 0.033964165, 0.039695133, 0.04250437, 0.034116972, 0.025339484, 0.017972011, 0.021690125, 0.031746905, 0.03228598, 0.040896866, 0.053931054, 0.04462593, 0.032226812, 0.027805516, 0.026234154, 0.062817976, 0.112446435, 0.14944018, 0.09080978, 0.14623915, 0.08632594, 0.060294855, 0.11932168, 0.09039841, 0.09309026, 0.060224652, 0.046609472, 0.038406357, 0.055252947, 0.04142086, 0.046553645, 0.03030803, 0.044113904, 0.037272785, 0.049368095, 0.03514048, 0.041054696, 0.03451361, 0.026039219, 0.032530934, 0.036295906, 0.0415654, 0.03731161, 0.036647823, 0.038946886, 0.04615222, 0.030319722, 0.030966535, 0.027328243, 0.03493865, 0.04467147, 0.02973766, 0.022623323, 0.044600178, 0.026980158, 0.026236916, 0.029684933, 0.022625064, 0.083186254, 0.082448065, 0.08795064, 0.09163488, 0.10925058, 0.12382841, 0.08994704, 0.06530742, 0.04781653, 0.07916053, 0.048145242, 0.043980304, 0.035576507, 0.07300097, 0.08065077, 0.045207396, 0.028381577, 0.021804167, 0.04608398, 0.05202785, 0.05030757, 0.026631502, 0.034667183, 0.0340706, 0.027645653, 0.021834183, 0.03325526, 0.021442875, 0.03767707, 0.02644985, 0.02977446, 0.032812666, 0.027149087, 0.026986724, 0.021040393, 0.027634671, 0.031045372, 0.027937137, 0.12630358, 0.107165486, 0.16980362, 0.2475386, 0.18200155, 0.063813224, 0.08292571, 0.0581642, 0.077370904, 0.081224255, 0.04710792, 0.06352752, 0.05379046, 0.056574315, 0.06336847, 0.04531496, 0.03576256, 0.03957217, 0.03695315, 0.062270623, 0.049919516, 0.031959947, 0.041113187, 0.028228994, 0.039068572, 0.04469523, 0.038252484, 0.032013655, 0.027128628, 0.038527183, 0.026159866, 0.02553331, 0.033611134, 0.031009156, 0.026198808, 0.025426863, 0.03753645, 0.041404586, 0.0876061, 0.14374867, 0.1910509, 0.1513839, 0.21562418, 0.039322168, 0.056203566, 0.051474944, 0.061143234, 0.058888637, 0.062363937, 0.050555747, 0.0437196, 0.076789916, 0.04075332, 0.03704099, 0.044881776, 0.03825385, 0.038753677, 0.04701705, 0.04994169, 0.026101187, 0.03966273, 0.03531998, 0.026107065, 0.02799531, 0.031146673, 0.04009639, 0.027587675, 0.03102132, 0.02956158, 0.03371802, 0.01996985, 0.023044523, 0.020549202, 0.04884067, 0.028534854, 0.029642774, 0.029393155, 0.06737131, 0.07769565, 0.15814458, 0.11113983, 0.09167137, 0.05950516, 0.14019819, 0.060446627, 0.06936679, 0.036579005, 0.041101616, 0.038772307, 0.049029175, 0.0507192, 0.033382565, 0.046744157, 0.040130228, 0.035995945, 0.043450236, 0.033607293, 0.041318454, 0.034267712, 0.034586698, 0.027498553, 0.028684167, 0.038030304, 0.02876949, 0.034221422, 0.03505736, 0.028185885, 0.033676915, 0.027644202, 0.032392874, 0.03302999, 0.034031242, 0.027932433, 0.03180912, 0.052025955, 0.05577059, 0.073640995, 0.06870218, 0.083397254, 0.069915235, 0.057883114, 0.045874108, 0.050442606, 0.066324234, 0.028979683, 0.044437636, 0.046521638, 0.079507634, 0.055331398, 0.05019859, 0.036097255, 0.027703056, 0.04628644, 0.028303169, 0.039966535, 0.032031294, 0.029026782, 0.03767156, 0.02504363, 0.026578078, 0.035348367, 0.023433898, 0.04542449, 0.024477104, 0.026716392, 0.027231766, 0.024656668, 0.04409685, 0.031064367, 0.032468125, 0.025724813, 0.03397581, 0.020744687, 0.028445171, 0.026316926, 0.023557555, 0.033532787, 0.022730015, 0.07596703, 0.09712672, 0.12960172, 0.12712196, 0.046282515, 0.052202255, 0.07377505, 0.06012264, 0.055128343, 0.06434558, 0.060924035, 0.08011966, 0.10858226, 0.08680175, 0.054007504, 0.04805886, 0.10262964, 0.052566182, 0.030427236, 0.050807647, 0.03381308, 0.026939891, 0.026787695, 0.028124606, 0.035431284, 0.025475677, 0.03490002, 0.042756643, 0.026181523, 0.021171134, 0.029967116, 0.029113764, 0.02012594, 0.03830914, 0.03481811, 0.022078758, 0.026163895, 0.025593525, 0.110481426, 0.12983671, 0.09973086, 0.15168782, 0.119350724, 0.09557325, 0.06085478, 0.12512392, 0.075926125, 0.068904005, 0.062011223, 0.037282716, 0.04345024, 0.049454603, 0.0672499, 0.030959958, 0.037457447, 0.035390943, 0.027673097, 0.03256239, 0.04568069, 0.030464672, 0.039668825, 0.039312072, 0.03178146, 0.03943591, 0.039407045, 0.035366826, 0.030948654, 0.031279705, 0.022824114, 0.034592282, 0.041130494, 0.039628975, 0.041096937, 0.02376278, 0.026028877, 0.026690762, 0.10112401, 0.11590389, 0.10838336, 0.08596625, 0.09065416, 0.10428132, 0.075349875, 0.08175393, 0.08220163, 0.1420621, 0.08535182, 0.05965815, 0.032668874, 0.07066245, 0.047360245, 0.026022023, 0.048534922, 0.026796103, 0.027772835, 0.034413647, 0.038802337, 0.042634644, 0.03561871, 0.036827333, 0.03247542, 0.029305046, 0.026953157, 0.034498386, 0.032037478, 0.033695012, 0.027506897, 0.024019405, 0.021963377, 0.03193154, 0.030236378, 0.021481471, 0.02135188, 0.025701795, 0.11818547, 0.22926302, 0.0913075, 0.11080028, 0.05344042, 0.043933563, 0.0754998, 0.15266661, 0.10031991, 0.04511953, 0.02853642, 0.036801863, 0.050234582, 0.0657625, 0.06279417, 0.04519481, 0.048370022, 0.04646466, 0.054092433, 0.041183535, 0.046900067, 0.055401538, 0.02890029, 0.024257483, 0.034950163, 0.033500444, 0.023703432, 0.02738617, 0.03042494, 0.03693271, 0.025896259, 0.031788055, 0.05115805, 0.029397786, 0.029709848, 0.035904627, 0.024729881, 0.030623747, 0.1718629, 0.20608933, 0.08461764, 0.15292141, 0.09360745, 0.10124111, 0.090945266, 0.053884763, 0.13764064, 0.09106336, 0.056144126, 0.06557652, 0.048299167, 0.032982133, 0.043431435, 0.035108373, 0.0376807, 0.06092321, 0.03273628, 0.024813382, 0.046201866, 0.05399584, 0.049245328, 0.029273648, 0.028634533, 0.029522406, 0.028468577, 0.032477677, 0.028122893, 0.033391554, 0.026593478, 0.02817397, 0.029010529, 0.027186159, 0.022563422, 0.026228694, 0.029699225, 0.029280704, 0.025398057, 0.024697961, 0.021198973, 0.020419516, 0.028034486, 0.04052376, 0.11143919, 0.2560185, 0.106870875, 0.20780967, 0.18697152, 0.13592912, 0.06731524, 0.055565286, 0.06923685, 0.058618993, 0.04856542, 0.049370963, 0.03906069, 0.05582714, 0.036061525, 0.044756625, 0.046887495, 0.039060984, 0.033514727, 0.07134089, 0.0407203, 0.036790878, 0.03038598, 0.03113235, 0.03200764, 0.049067933, 0.03044725, 0.030477555, 0.03822646, 0.036562823, 0.032002844, 0.037616324, 0.03784123, 0.04715473, 0.022508835, 0.032149862, 0.047095403, 0.047530636, 0.098728955, 0.0839366, 0.19856872, 0.08649771, 0.1027113, 0.10229246, 0.06060713, 0.05221684, 0.04685527, 0.034161422, 0.033571612, 0.07499061, 0.097102, 0.0971362, 0.045829717, 0.037469085, 0.028703561, 0.027505577, 0.036534525, 0.041814394, 0.03354948, 0.048902225, 0.037615124, 0.030720409, 0.028795559, 0.03469904, 0.03866934, 0.036184736, 0.030797532, 0.01984424, 0.041300934, 0.03175364, 0.024313187, 0.03161777, 0.023339108, 0.033608608, 0.025492823, 0.077749245, 0.060545575, 0.113744825, 0.09139269, 0.09229674, 0.12492114, 0.0629822, 0.049612187, 0.06963402, 0.055884115, 0.08025, 0.07159289, 0.04534164, 0.038351435, 0.03897544, 0.045047272, 0.02511903, 0.029034408, 0.05086848, 0.03919984, 0.029580276, 0.034438163, 0.032257143, 0.026423624, 0.028438672, 0.03623459, 0.033068348, 0.01878571, 0.028440274, 0.025703013, 0.03685454, 0.024124926, 0.029674727, 0.032465387, 0.02198327, 0.034248956, 0.027437443, 0.02921595, 0.07400737, 0.10321033, 0.09244484, 0.09049109, 0.09016132, 0.052801955, 0.12010936, 0.105735965, 0.06389238, 0.10052299, 0.09623001, 0.047644075, 0.047438435, 0.057789613, 0.03304605, 0.058649704, 0.04341447, 0.036008608, 0.045104075, 0.03802241, 0.03497206, 0.03170501, 0.03815849, 0.031656563, 0.020453889, 0.029158032, 0.02500485, 0.039360758, 0.032438546, 0.028781675, 0.033809524, 0.036224373, 0.030539393, 0.021132926, 0.031575117, 0.032058366, 0.0302243, 0.032364134, 0.076848574, 0.11879099, 0.12125166, 0.12525974, 0.17655694, 0.13564639, 0.04739058, 0.045051705, 0.055565897, 0.058493726, 0.037563466, 0.03742131, 0.073547475, 0.0993404, 0.037614074, 0.061586607, 0.036939573, 0.030475488, 0.042013437, 0.036090266, 0.049996395, 0.054975685, 0.05627949, 0.03161637, 0.030106926, 0.0335372, 0.03258289, 0.030221319, 0.02337871, 0.032598943, 0.02964032, 0.04485147, 0.030619258, 0.031664573, 0.032251157, 0.024759876, 0.022963779, 0.037630394, 0.03788119, 0.032298, 0.026615547, 0.03798043, 0.022938615, 0.022805484, 0.08288163, 0.2044616, 0.10478676, 0.09050648, 0.05564079, 0.072548844, 0.058036566, 0.045766402, 0.07814393, 0.10394573, 0.06418931, 0.054913733, 0.0684827, 0.063661955, 0.0612796, 0.03616924, 0.039999653, 0.033531137, 0.037447423, 0.03572607, 0.048762746, 0.018249279, 0.02678799, 0.024553426, 0.04565301, 0.03323349, 0.038160708, 0.028489433, 0.025278829, 0.022953607, 0.032797817, 0.04091478, 0.032337822, 0.027034128, 0.031194186, 0.031888913, 0.030281091, 0.024145164, 0.09279719, 0.07706406, 0.060421467, 0.13637845, 0.11686705, 0.09009025, 0.056024123, 0.06484213, 0.07687883, 0.03578217, 0.050590657, 0.03652286, 0.05612739, 0.060892288, 0.05360905, 0.047406863, 0.043398097, 0.030682953, 0.030528652, 0.03186105, 0.030425059, 0.02892136, 0.042362776, 0.038768817, 0.02194336, 0.025286587, 0.031181583, 0.03267768, 0.021021439, 0.026051419, 0.030082194, 0.030545332, 0.025875267, 0.028705785, 0.02750914, 0.03995018, 0.028380288, 0.03278402, 0.04327355, 0.074755594, 0.063462555, 0.10483434, 0.13045442, 0.06656156, 0.067040704, 0.079244345, 0.09396848, 0.12952054, 0.07962407, 0.05953533, 0.04560173, 0.034196585, 0.043995097, 0.033694196, 0.024271123, 0.025073396, 0.030004818, 0.024042562, 0.027045874, 0.037169002, 0.023633907, 0.03268766, 0.039628208, 0.025548382, 0.048700184, 0.032514963, 0.033166707, 0.023440747, 0.021831498, 0.02966281, 0.03239924, 0.03182293, 0.02580234, 0.029192297, 0.026785905, 0.031024305, 0.052079786, 0.052527364, 0.09273221, 0.09275278, 0.049621295, 0.056146238, 0.03322085, 0.064988516, 0.057862926, 0.042605568, 0.039926182, 0.035376936, 0.033795543, 0.062967755, 0.049429644, 0.049436558, 0.03580053, 0.041671563, 0.04315012, 0.074950755, 0.048658174, 0.045462262, 0.029187867, 0.025880922, 0.03501774, 0.022083923, 0.024251742, 0.02938839, 0.023272835, 0.03445727, 0.027461635, 0.024246845, 0.02094901, 0.024903413, 0.03240308, 0.027451301, 0.023214767, 0.021466386, 0.05305725, 0.11427701, 0.1278803, 0.20121351, 0.15694049, 0.074227735, 0.103484966, 0.08820217, 0.07456952, 0.0546335, 0.045391116, 0.036696, 0.08775712, 0.04101018, 0.052721005, 0.030228807, 0.029443057, 0.022521203, 0.027976962, 0.035356417, 0.04425267, 0.047420073, 0.021392208, 0.025160687, 0.01855698, 0.028749475, 0.041221887, 0.025848681, 0.027061664, 0.04654125, 0.024495665, 0.024279669, 0.04450709, 0.030483183, 0.025806515, 0.023547433, 0.03645101, 0.029097818, 0.118134715, 0.13194548, 0.076921515, 0.104236625, 0.06369833, 0.07811741, 0.08663945, 0.10283486, 0.0531626, 0.044538964, 0.048166092, 0.040114157, 0.037886452, 0.047419116, 0.07742605, 0.054917485, 0.044912763, 0.03650246, 0.031299006, 0.03168229, 0.032235254, 0.03759935, 0.03391794, 0.026813516, 0.023681365, 0.026603235, 0.03855851, 0.023369927, 0.023883559, 0.02627908, 0.026535558, 0.03286426, 0.023594165, 0.023279365, 0.026782705, 0.026431346, 0.027026402, 0.03173097, 0.062451426, 0.10948581, 0.11245447, 0.08598208, 0.065825045, 0.15398824, 0.08000686, 0.08839192, 0.095403224, 0.05716909, 0.044376988, 0.07038697, 0.04609865, 0.058943063, 0.034724485, 0.04846986, 0.03593108, 0.023487456, 0.02732221, 0.030417575, 0.029124435, 0.038845647, 0.026700662, 0.0357895, 0.03474465, 0.023080932, 0.027932465, 0.02554789, 0.03664234, 0.025937984, 0.03448684, 0.021727959, 0.028017282, 0.026971271, 0.02153736, 0.029727405, 0.023561308, 0.028055778, 0.12736419, 0.10776061, 0.12742135, 0.10678941, 0.050524417, 0.059764136, 0.06333878, 0.05773138, 0.04182762, 0.0809444, 0.063305736, 0.035659324, 0.030278275, 0.03999454, 0.042098, 0.036233258, 0.027637187, 0.08023183, 0.026561888, 0.035435338, 0.026349325, 0.027396133, 0.027398879, 0.026535524, 0.0422223, 0.032503396, 0.031197406, 0.027222304, 0.023969973, 0.020852193, 0.023706041, 0.025372947, 0.023674533, 0.026194865, 0.029109092, 0.023186695, 0.021446623, 0.024733368, 0.07512492, 0.13084811, 0.07255725, 0.1410973, 0.1094068, 0.078058936, 0.062034763, 0.06378061, 0.06253537, 0.1380442, 0.07001181, 0.055234578, 0.053811062, 0.051712256, 0.039063796, 0.06997559, 0.045119725, 0.05293277, 0.03316074, 0.028463686, 0.043984167, 0.037593387, 0.02869053, 0.02110695, 0.02449792, 0.023893904, 0.031540234, 0.029033428, 0.031260397, 0.0316007, 0.028986765, 0.023677561, 0.023134116, 0.034336973, 0.027323063, 0.03108489, 0.028948339, 0.03376101, 0.17286646, 0.1769826, 0.11739338, 0.086413026, 0.12357684, 0.09007706, 0.083030954, 0.086184144, 0.08382659, 0.053095717, 0.048590634, 0.06309712, 0.040337652, 0.030306434, 0.035116877, 0.04633515, 0.027056446, 0.026621414, 0.050544996, 0.029185763, 0.03696585, 0.04093732, 0.040335104, 0.036696475, 0.04701607, 0.031418517, 0.04313188, 0.03427097, 0.02959094, 0.026285207, 0.029936746, 0.020026213, 0.036609616, 0.041004747, 0.037030123, 0.025582863, 0.032158613, 0.03102265, 0.026909376, 0.028518617, 0.025989573, 0.04419323, 0.024116198, 0.058470853, 0.050120767, 0.05863905, 0.094135664, 0.087259024, 0.07964658, 0.056234192, 0.049311783, 0.080442995, 0.06324429, 0.050972827, 0.05840804, 0.072858475, 0.06800801, 0.03535284, 0.032346904, 0.023251707, 0.04516183, 0.029626966, 0.031140154, 0.025471907, 0.045512814, 0.034826424, 0.023444688, 0.04574312, 0.04820619, 0.032771043, 0.032389093, 0.028867686, 0.024589159, 0.024409926, 0.036618266, 0.02562477, 0.023130922, 0.028665964, 0.024362445, 0.029982649, 0.028053679, 0.08025643, 0.11275891, 0.10887603, 0.0984025, 0.09395511, 0.057170805, 0.066100895, 0.08167393, 0.04222443, 0.074453115, 0.09356874, 0.058314186, 0.04660464, 0.06191292, 0.06497997, 0.04359071, 0.03827971, 0.051367924, 0.044468425, 0.030099586, 0.025898159, 0.049921963, 0.031358656, 0.022092685, 0.02767027, 0.024499, 0.025228903, 0.023508735, 0.040510356, 0.023916649, 0.038707007, 0.031163922, 0.03448405, 0.03045171, 0.027037159, 0.02182537, 0.020702938, 0.035618227, 0.090954326, 0.047771778, 0.09272594, 0.10845059, 0.08874954, 0.046839677, 0.04905758, 0.049100906, 0.084100984, 0.063481495, 0.029400311, 0.035501882, 0.06126788, 0.05473577, 0.02865456, 0.038778402, 0.045977224, 0.025145495, 0.038376212, 0.04385552, 0.029157523, 0.03355382, 0.022786394, 0.038912147, 0.0270793, 0.029638221, 0.040408753, 0.017841361, 0.034026332, 0.028073939, 0.024301268, 0.025649145, 0.032335084, 0.02172991, 0.029870084, 0.026050068, 0.029375838, 0.024112877, 0.087448694, 0.08555809, 0.052359175, 0.067776196, 0.10086659, 0.0708412, 0.08412107, 0.08553092, 0.03768651, 0.057636864, 0.060925063, 0.053094625, 0.03961113, 0.039319288, 0.032261178, 0.03810693, 0.041143417, 0.03667592, 0.024035564, 0.05873877, 0.030872568, 0.030943371, 0.025395576, 0.03641651, 0.034471925, 0.026878875, 0.044237733, 0.025772389, 0.025439119, 0.033914417, 0.024299249, 0.038220644, 0.02964027, 0.025332086, 0.031350367, 0.022935085, 0.027871862, 0.030861475, 0.10090134, 0.073831104, 0.06827526, 0.07426917, 0.07732641, 0.11757876, 0.11012337, 0.0510061, 0.090204865, 0.05407173, 0.048387405, 0.02598774, 0.036008056, 0.03435223, 0.045421988, 0.04012384, 0.035671517, 0.018807724, 0.049390405, 0.03980804, 0.038764454, 0.02927146, 0.037345614, 0.04096224, 0.032129236, 0.026243433, 0.02718732, 0.038240522, 0.03079981, 0.025128309, 0.03294044, 0.043525744, 0.029591853, 0.03271203, 0.019262435, 0.023387901, 0.02340149, 0.027239984, 0.03295264, 0.026826754, 0.029878607, 0.03754967, 0.017975945, 0.022930434, 0.078006424, 0.14456537, 0.08611752, 0.08741917, 0.051102202, 0.09619952, 0.07103143, 0.06474516, 0.071226306, 0.11327697, 0.051028498, 0.047025807, 0.062175, 0.039500806, 0.060051855, 0.04781106, 0.02792296, 0.029905664, 0.049402893, 0.031432282, 0.033388846, 0.029361725, 0.03639862, 0.0404379, 0.041620877, 0.028791383, 0.027623521, 0.026101172, 0.030980816, 0.032064237, 0.019409759, 0.029126626, 0.023099992, 0.017945413, 0.025050726, 0.0244061, 0.023503287, 0.026353667, 0.094496764, 0.07284846, 0.107804395, 0.06579415, 0.096255444, 0.106715605, 0.052504957, 0.06656649, 0.11947371, 0.07426909, 0.054977868, 0.04678419, 0.033118464, 0.031504903, 0.028744364, 0.036559727, 0.04427856, 0.023369443, 0.03467413, 0.03484619, 0.025663251, 0.023501825, 0.040455338, 0.025692116, 0.029787999, 0.030714149, 0.025929106, 0.030215396, 0.022308614, 0.021473693, 0.030227667, 0.031900574, 0.022341527, 0.02648997, 0.023342978, 0.023335535, 0.02483937, 0.024635788, 0.055317473, 0.103902765, 0.11195617, 0.12801746, 0.057804484, 0.039009724, 0.042179894, 0.04632539, 0.08187637, 0.13450587, 0.098537326, 0.04602785, 0.034964763, 0.039511573, 0.040144484, 0.023091907, 0.028137116, 0.033036076, 0.057819493, 0.030347476, 0.022278562, 0.026950246, 0.025959361, 0.029716987, 0.021894597, 0.031792156, 0.027750712, 0.020014843, 0.018940995, 0.025807839, 0.020768963, 0.031825352, 0.018624399, 0.024639206, 0.02453215, 0.025615081, 0.021181589, 0.025062246, 0.086435914, 0.112879254, 0.07461924, 0.06270391, 0.042203236, 0.061926752, 0.064868666, 0.06128974, 0.07887844, 0.05621235, 0.036267787, 0.050094806, 0.036979176, 0.02899257, 0.052083343, 0.02342338, 0.026062561, 0.03645016, 0.038300056, 0.044682175, 0.038317934, 0.031490013, 0.020811087, 0.030716112, 0.024480185, 0.031010782, 0.036292836, 0.020950057, 0.023971135, 0.02372388, 0.0359488, 0.022222456, 0.03196361, 0.030057114, 0.034572817, 0.019683234, 0.024082433, 0.025420396, 0.04950096, 0.05876236, 0.12223832, 0.16800542, 0.11286861, 0.07146127, 0.040744215, 0.07042599, 0.09861976, 0.082885765, 0.05973835, 0.0277792, 0.043975253, 0.03490702, 0.052491825, 0.030857211, 0.041573517, 0.025779136, 0.031354632, 0.032208122, 0.033850368, 0.03299263, 0.040483914, 0.031871073, 0.023306398, 0.03104778, 0.036328737, 0.02701807, 0.023525195, 0.03251426, 0.035710018, 0.037471827, 0.03393499, 0.03258887, 0.023869555, 0.027422102, 0.03287137, 0.027229287, 0.032401253, 0.01937514, 0.020625351, 0.025177415, 0.025133349, 0.055937063, 0.045043774, 0.07487064, 0.1403547, 0.09375944, 0.17149837, 0.06946248, 0.15827803, 0.057474356, 0.045078985, 0.04184085, 0.056156937, 0.05606301, 0.04813801, 0.062130507, 0.04136543, 0.04241059, 0.028965378, 0.054747682, 0.028771814, 0.028781625, 0.019930802, 0.033943452, 0.039601408, 0.021447223, 0.026447654, 0.026773466, 0.034387957, 0.030297546, 0.026359724, 0.03520215, 0.028075362, 0.024040535, 0.029358331, 0.0283615, 0.023896879, 0.023546576, 0.0206714, 0.05258335, 0.07917249, 0.068740755, 0.13211757, 0.12146212, 0.10070477, 0.06492437, 0.0893918, 0.06137228, 0.07628447, 0.040744524, 0.044491548, 0.07327337, 0.051710915, 0.041889466, 0.040419493, 0.042052656, 0.052628435, 0.026302982, 0.031364925, 0.025575247, 0.02733739, 0.02835867, 0.03364966, 0.055589087, 0.03970596, 0.025799617, 0.03019056, 0.035234164, 0.022742508, 0.027432285, 0.019540483, 0.026006185, 0.022107843, 0.027162373, 0.02853107, 0.023152376, 0.019358171, 0.07763422, 0.082457125, 0.0616942, 0.09931589, 0.18463898, 0.1291626, 0.06753729, 0.061433904, 0.08145216, 0.044372503, 0.038233068, 0.027138408, 0.04319075, 0.03555913, 0.026842609, 0.03382169, 0.03084014, 0.027981516, 0.028691912, 0.032736886, 0.02999633, 0.028283432, 0.028245358, 0.026933506, 0.03164803, 0.027233943, 0.029589772, 0.0237177, 0.028901743, 0.031853944, 0.029982764, 0.029816361, 0.026440894, 0.026547708, 0.021384383, 0.021491634, 0.023947852, 0.023846278, 0.05367444, 0.08157804, 0.085819505, 0.10567196, 0.10631827, 0.06620092, 0.09271147, 0.06553535, 0.07846329, 0.09749569, 0.08770016, 0.06750935, 0.051976547, 0.05139339, 0.043430496, 0.046740305, 0.037569556, 0.032675385, 0.037207585, 0.029722173, 0.047787443, 0.033118807, 0.021575384, 0.025644172, 0.022039456, 0.039370034, 0.02870125, 0.016701413, 0.03519526, 0.033171415, 0.025048917, 0.024234572, 0.029588891, 0.019238375, 0.018413598, 0.020911636, 0.03125137, 0.028196784, 0.04329546, 0.066942275, 0.04887563, 0.08902684, 0.10238971, 0.04551539, 0.10038192, 0.099654436, 0.084019154, 0.04329077, 0.05187647, 0.031024419, 0.034278266, 0.024189122, 0.037222255, 0.043299783, 0.035657633, 0.025587218, 0.027360376, 0.027387157, 0.025062244, 0.024331396, 0.025470925, 0.033151396, 0.037969485, 0.026456503, 0.03619926, 0.0193008, 0.020083012, 0.023554506, 0.03801893, 0.02043578, 0.017713329, 0.020329313, 0.018250197, 0.031122204, 0.024329133, 0.017501744, 0.027951488, 0.02629298, 0.01974777, 0.026757855, 0.032582693, 0.022669861, 0.04607627, 0.08839931, 0.13940032, 0.19605023, 0.062456626, 0.10758026, 0.04665792, 0.044421304, 0.056942075, 0.07246996, 0.057698157, 0.044273466, 0.05734065, 0.053344656, 0.046382446, 0.026347097, 0.03688773, 0.04390399, 0.04079841, 0.037729464, 0.026593452, 0.022816403, 0.03338889, 0.027613306, 0.0333111, 0.02718538, 0.024959689, 0.0330249, 0.032073468, 0.039004833, 0.03246864, 0.026318537, 0.028933382, 0.027474478, 0.024740903, 0.029597584, 0.026422739, 0.030588472, 0.10231615, 0.08144966, 0.12694842, 0.15030332, 0.060075026, 0.0443139, 0.07671625, 0.047000583, 0.037380565, 0.06749029, 0.06582103, 0.042941783, 0.04536031, 0.071673036, 0.036762096, 0.02791784, 0.041983657, 0.027595932, 0.046979815, 0.024573803, 0.024832617, 0.026875243, 0.030991673, 0.032206316, 0.03415479, 0.022766247, 0.027991693, 0.026030218, 0.026206033, 0.024216909, 0.023574764, 0.03500657, 0.022227446, 0.03298093, 0.0320306, 0.019356107, 0.017215835, 0.03077305, 0.112830184, 0.09537862, 0.09960798, 0.08103946, 0.06263352, 0.08517062, 0.07711715, 0.08467446, 0.057322677, 0.08894575, 0.05178691, 0.0436266, 0.045208447, 0.04663335, 0.035166908, 0.025681898, 0.036474533, 0.03486537, 0.031073727, 0.03155739, 0.023665683, 0.026587442, 0.046068672, 0.022444779, 0.036240593, 0.02793982, 0.0248156, 0.029985588, 0.02564072, 0.02121195, 0.023693815, 0.03922755, 0.032583278, 0.029479284, 0.024182616, 0.020945448, 0.032214034, 0.029037291, 0.058835357, 0.081528984, 0.069867425, 0.052207664, 0.09608243, 0.080103286, 0.0716866, 0.057252415, 0.08098489, 0.047078025, 0.03353784, 0.038450357, 0.030695241, 0.026008114, 0.040253386, 0.02831439, 0.037436415, 0.0280002, 0.037805468, 0.03760026, 0.030951528, 0.025996005, 0.028783528, 0.033368014, 0.0227012, 0.024361249, 0.025434786, 0.026333269, 0.019109216, 0.029918859, 0.01870977, 0.02128986, 0.024298763, 0.020906417, 0.019622162, 0.028471928, 0.028088974, 0.02655474, 0.094510816, 0.07873584, 0.07723485, 0.1597556, 0.072275914, 0.07310229, 0.04468318, 0.07162691, 0.068259135, 0.0321259, 0.025436526, 0.041168805, 0.037810326, 0.023929678, 0.046483025, 0.031301945, 0.040404484, 0.030243607, 0.031744506, 0.033095922, 0.02793308, 0.040924, 0.026440538, 0.028832505, 0.023288649, 0.025696779, 0.022853669, 0.03206397, 0.025017725, 0.023642605, 0.030236896, 0.02379974, 0.027528003, 0.02673226, 0.023997646, 0.024379706, 0.020935528, 0.022788392, 0.01958852, 0.019710464, 0.02782396, 0.019935837, 0.031929906, 0.10413941, 0.09489507, 0.08108022, 0.104342245, 0.09159473, 0.079767436, 0.08862759, 0.046765137, 0.054490376, 0.09168148, 0.043357026, 0.035550527, 0.04142288, 0.027200552, 0.0309192, 0.05742826, 0.031499732, 0.038216613, 0.032197174, 0.03834322, 0.031318754, 0.043143466, 0.022355923, 0.02782607, 0.02532875, 0.02498666, 0.028844193, 0.030604504, 0.026759578, 0.029170498, 0.022711996, 0.022781124, 0.02633893, 0.029224057, 0.028504752, 0.024054728, 0.019866532, 0.02281552, 0.071115956, 0.08687579, 0.06379833, 0.08314815, 0.15665343, 0.08798521, 0.09913978, 0.08771913, 0.050834436, 0.054332145, 0.031138686, 0.031630047, 0.06554293, 0.07677465, 0.025327696, 0.0555696, 0.03958876, 0.029965108, 0.030331725, 0.026327733, 0.03653396, 0.032757293, 0.03374392, 0.023684936, 0.017972933, 0.020579154, 0.025198838, 0.020302627, 0.023942614, 0.025128387, 0.01708492, 0.01706963, 0.020458717, 0.012876432, 0.016470643, 0.022324398, 0.017402584, 0.020454515, 0.06411896, 0.09561914, 0.13931601, 0.12477074, 0.079771966, 0.07695702, 0.03748557, 0.048182882, 0.1289836, 0.10245548, 0.04440696, 0.03909575, 0.03502412, 0.0449105, 0.04624644, 0.055757973, 0.037305493, 0.033294726, 0.038714655, 0.02914413, 0.02566016, 0.030012865, 0.03522985, 0.034430474, 0.025298784, 0.029493913, 0.034961168, 0.02990284, 0.025155457, 0.017080087, 0.02790063, 0.026248166, 0.020332796, 0.024280181, 0.02361438, 0.025227828, 0.021057175, 0.027084386, 0.12754717, 0.13426623, 0.065791614, 0.09153957, 0.13021964, 0.040439814, 0.05124652, 0.039044034, 0.038740538, 0.048749484, 0.048832893, 0.035523783, 0.03667696, 0.034721263, 0.030401193, 0.029864596, 0.031537723, 0.02281841, 0.023549013, 0.030719161, 0.030375063, 0.02243874, 0.029891437, 0.022950245, 0.027669989, 0.020540789, 0.02432344, 0.021826789, 0.02405131, 0.021901242, 0.027776979, 0.025757128, 0.031639826, 0.031523556, 0.017754843, 0.013779413, 0.027017241, 0.02872882, 0.075315565, 0.076906264, 0.09616362, 0.14952162, 0.086180374, 0.039611083, 0.057525944, 0.038214494, 0.07933164, 0.05802194, 0.047025047, 0.037443228, 0.036252677, 0.03069466, 0.03203952, 0.04227733, 0.035134267, 0.03413477, 0.015307374, 0.024588158, 0.031893846, 0.028063806, 0.030887218, 0.034603123, 0.030344559, 0.025705526, 0.032478046, 0.04001733, 0.02176995, 0.024857104, 0.033857558, 0.026346467, 0.03332799, 0.02726934, 0.024356972, 0.022288462, 0.024536123, 0.025069863, 0.02014744, 0.029560689, 0.018132048, 0.022515899, 0.032879323, 0.018389186, 0.13546231, 0.10040745, 0.07862251, 0.11730161, 0.048527453, 0.058145944, 0.06805365, 0.066167876, 0.07354321, 0.09457395, 0.056004286, 0.046916913, 0.045928113, 0.032073595, 0.03976039, 0.032609526, 0.02965711, 0.042572293, 0.03351829, 0.027220583, 0.048283443, 0.02321941, 0.05072903, 0.027407503, 0.029812483, 0.031907674, 0.024320282, 0.021402271, 0.025290448, 0.03315278, 0.04041878, 0.022299428, 0.03078658, 0.027174994, 0.021806523, 0.023949271, 0.022903461, 0.025153091, 0.081706695, 0.048977725, 0.08662806, 0.09825315, 0.07790087, 0.08207689, 0.052727412, 0.059497602, 0.05499952, 0.05725979, 0.03158477, 0.032521695, 0.039186984, 0.05287078, 0.041108, 0.03229881, 0.027665094, 0.0348842, 0.033106618, 0.036277987, 0.028330853, 0.046708986, 0.028105177, 0.023754483, 0.029354114, 0.030420817, 0.03105027, 0.022674479, 0.025255933, 0.03593368, 0.02019739, 0.021783067, 0.021680674, 0.022886762, 0.023462946, 0.024901683, 0.034242123, 0.025525497, 0.056933552, 0.10368229, 0.17479575, 0.09441698, 0.054278787, 0.081828326, 0.08670405, 0.1102652, 0.103164405, 0.047975015, 0.04196534, 0.03639866, 0.047673013, 0.03198026, 0.06154649, 0.049423803, 0.038446, 0.055450074, 0.036510468, 0.052308585, 0.03300428, 0.03353727, 0.031310335, 0.033743843, 0.023313867, 0.024539985, 0.0229826, 0.023773607, 0.044315636, 0.030463759, 0.039213415, 0.017055865, 0.03134978, 0.025386587, 0.02221418, 0.023359222, 0.03937553, 0.025719475, 0.060844313, 0.18774593, 0.103428885, 0.12566912, 0.065557435, 0.061922457, 0.102470614, 0.08901761, 0.057641387, 0.056856863, 0.029798191, 0.03998716, 0.030442595, 0.03163533, 0.032414, 0.03195727, 0.03467076, 0.026942102, 0.036991622, 0.05071177, 0.036986306, 0.026227113, 0.025298152, 0.041611962, 0.026733791, 0.025941351, 0.02881156, 0.026222978, 0.031032763, 0.024454629, 0.024010232, 0.026913261, 0.03311635, 0.03248659, 0.03262886, 0.016155833, 0.021495825, 0.025232594, 0.0634775, 0.08392483, 0.1618697, 0.17626284, 0.114141576, 0.07755142, 0.050572444, 0.055402115, 0.079884045, 0.061472874, 0.08297655, 0.046584334, 0.046967637, 0.058441978, 0.033195533, 0.03146487, 0.048334226, 0.043637738, 0.046761226, 0.029379455, 0.03884197, 0.032350052, 0.050622758, 0.04762015, 0.027575318, 0.03511278, 0.03055721, 0.031845, 0.034689605, 0.024978003, 0.029855128, 0.02645388, 0.027445555, 0.030721916, 0.024896337, 0.04191011, 0.025614088, 0.024960525, 0.02788458, 0.029643288, 0.030200334, 0.023219604, 0.022140332, 0.06593481, 0.08601376, 0.09152195, 0.088118054, 0.08371003, 0.098313004, 0.08597938, 0.06125873, 0.045769975, 0.07997854, 0.07319971, 0.040763486, 0.09960109, 0.038021903, 0.043076128, 0.054178182, 0.045349628, 0.02852213, 0.04408198, 0.0242523, 0.053087153, 0.028480934, 0.028781166, 0.023313409, 0.031939074, 0.027356578, 0.04463041, 0.026060423, 0.02377732, 0.027574247, 0.036391404, 0.02323634, 0.023512013, 0.023225987, 0.027897267, 0.030332154, 0.026037982, 0.022495413, 0.07550468, 0.23408307, 0.20048809, 0.10715215, 0.0737009, 0.064772524, 0.05818436, 0.13228786, 0.13399778, 0.077062994, 0.061126307, 0.043084983, 0.031271156, 0.07762801, 0.05859053, 0.0419786, 0.030837975, 0.039552834, 0.04888025, 0.030245135, 0.035786133, 0.022119453, 0.031548776, 0.02832439, 0.024592742, 0.026665678, 0.032619394, 0.03315895, 0.023761174, 0.034780104, 0.03614315, 0.040420875, 0.026375037, 0.029457692, 0.021156074, 0.032211207, 0.028431136, 0.027854864, 0.091458745, 0.10176226, 0.058676537, 0.07719799, 0.04763628, 0.05345163, 0.034719862, 0.052166473, 0.061693072, 0.050355326, 0.05567986, 0.058087185, 0.040050365, 0.04025823, 0.054149427, 0.053258292, 0.034187835, 0.0388154, 0.02530134, 0.025952766, 0.03358726, 0.026500082, 0.021596039, 0.022417856, 0.03686085, 0.026610086, 0.027644776, 0.027259616, 0.028788121, 0.032180957, 0.023571923, 0.019951476, 0.032370932, 0.027589284, 0.038531102, 0.02942645, 0.020473849, 0.018173352, 0.05808002, 0.12127244, 0.07784703, 0.09002219, 0.12091371, 0.051290352, 0.038360223, 0.06416822, 0.049021356, 0.050552312, 0.04709545, 0.044268504, 0.03751963, 0.04397609, 0.0811494, 0.055223696, 0.023415342, 0.032169305, 0.027481029, 0.035040963, 0.03453958, 0.025433788, 0.02675508, 0.019550515, 0.028938657, 0.027477447, 0.028499937, 0.030016685, 0.030033076, 0.038219817, 0.03679031, 0.03161153, 0.026198199, 0.033211377, 0.023722278, 0.03975198, 0.027071526, 0.023629235, 0.04611995, 0.14240222, 0.15618002, 0.16350925, 0.08110853, 0.043484367, 0.038684666, 0.034849986, 0.06842178, 0.04385934, 0.0567835, 0.06802467, 0.0375314, 0.040889535, 0.029810585, 0.043210495, 0.044075485, 0.026053917, 0.034879122, 0.035888337, 0.041828603, 0.05218913, 0.025849445, 0.023116414, 0.03244019, 0.03713776, 0.022511706, 0.029730953, 0.033790443, 0.03572956, 0.031128664, 0.023256272, 0.022964843, 0.04663266, 0.017912993, 0.034705356, 0.027753023, 0.025840048, 0.02090398, 0.024995398, 0.021906871, 0.027471041, 0.02709646, 0.023544468, 0.069125034, 0.092353955, 0.08739137, 0.09633347, 0.06276999, 0.047370836, 0.043781195, 0.048485577, 0.053053115, 0.049899347, 0.042515773, 0.035074554, 0.046822954, 0.03378417, 0.042835522, 0.048190217, 0.04135521, 0.04236016, 0.03253217, 0.04414442, 0.025249014, 0.029805055, 0.036152743, 0.02913717, 0.02667429, 0.027883526, 0.0250269, 0.032844223, 0.030344404, 0.023931619, 0.03071873, 0.020996975, 0.023482818, 0.02166014, 0.020785253, 0.021104075, 0.027403034, 0.02616041, 0.08586134, 0.11545366, 0.08912403, 0.058865163, 0.047314677, 0.054106146, 0.06350729, 0.058067158, 0.05840704, 0.046669878, 0.03314246, 0.04080658, 0.04077809, 0.034686435, 0.034781907, 0.034600537, 0.032198988, 0.03811614, 0.025257995, 0.034182712, 0.03966151, 0.03413549, 0.029622173, 0.029060232, 0.028983103, 0.022701424, 0.027013741, 0.032693673, 0.016621819, 0.028026128, 0.026010426, 0.01821254, 0.024918078, 0.020224458, 0.027156616, 0.020602465, 0.029320704, 0.033221833, 0.08434306, 0.15857503, 0.06699614, 0.06071631, 0.05190912, 0.04994584, 0.070561595, 0.053682677, 0.07270474, 0.108394526, 0.056878667, 0.09248613, 0.08174619, 0.047736555, 0.038396332, 0.03862174, 0.030488435, 0.027908877, 0.032959867, 0.03749151, 0.03426859, 0.0362934, 0.027143927, 0.049186703, 0.029745849, 0.029451251, 0.02878324, 0.029324638, 0.027420329, 0.020740261, 0.028422145, 0.019212496, 0.033712596, 0.026833162, 0.026232202, 0.045779057, 0.025683599, 0.026943509, 0.06032577, 0.21797037, 0.08782003, 0.06206331, 0.07957009, 0.07488591, 0.09484679, 0.060335487, 0.03697126, 0.05729063, 0.040221453, 0.032418508, 0.029665282, 0.027720693, 0.038513348, 0.038997605, 0.029881133, 0.024539456, 0.022784406, 0.01961317, 0.02645774, 0.026444918, 0.03757701, 0.045423247, 0.038685367, 0.03582526, 0.030709615, 0.02372187, 0.031915374, 0.028960407, 0.027273824, 0.025913838, 0.023821412, 0.025420401, 0.025593743, 0.025629245, 0.03303783, 0.028828874, 0.07471328, 0.108227804, 0.07625407, 0.14020865, 0.1490523, 0.094676316, 0.048865683, 0.04242661, 0.041412164, 0.07142892, 0.029081509, 0.051700797, 0.049520686, 0.042348266, 0.05489111, 0.035946894, 0.05590538, 0.04329584, 0.0355843, 0.04755872, 0.064482674, 0.058450248, 0.031189285, 0.03337782, 0.025238335, 0.023825781, 0.03777482, 0.019199692, 0.03875068, 0.035075255, 0.027327145, 0.022060413, 0.023894653, 0.02639023, 0.030165568, 0.03084058, 0.02273714, 0.019913562, 0.024641702, 0.025229784, 0.03128064, 0.025040314, 0.028674826, 0.047836017, 0.15457244, 0.1418726, 0.09601102, 0.20786452, 0.075176045, 0.08914003, 0.14638539, 0.06443528, 0.045804556, 0.029578108, 0.036624957, 0.048327677, 0.022195224, 0.05397493, 0.0507666, 0.043137856, 0.03893651, 0.026774792, 0.03483902, 0.035672624, 0.02403993, 0.050633643, 0.033779815, 0.025549913, 0.040640246, 0.023389835, 0.0285568, 0.029451417, 0.02128995, 0.022165375, 0.021805512, 0.026763815, 0.03540358, 0.027383506, 0.027533164, 0.023139449, 0.023769416, 0.03830179, 0.09796462, 0.106397025, 0.06572607, 0.04940009, 0.08220472, 0.07319069, 0.08666347, 0.08379667, 0.03819182, 0.05678184, 0.040443826, 0.037235647, 0.04488452, 0.036010776, 0.04398162, 0.027922489, 0.030487701, 0.031233339, 0.029964827, 0.03257482, 0.039234705, 0.025824131, 0.03258531, 0.022139588, 0.023579204, 0.028654478, 0.025423288, 0.03743128, 0.03403613, 0.03680383, 0.029250428, 0.01499063, 0.026732288, 0.023332343, 0.025479767, 0.030360712, 0.027148291, 0.048146546, 0.16609672, 0.19987163, 0.09436118, 0.13393678, 0.054824997, 0.08873627, 0.10146488, 0.045443915, 0.110009946, 0.05873693, 0.038271382, 0.03248173, 0.06119163, 0.051105432, 0.03421252, 0.026888756, 0.029932069, 0.03260776, 0.034025066, 0.027766837, 0.03962468, 0.04141032, 0.02949087, 0.02860901, 0.029485367, 0.02605505, 0.03513644, 0.04240591, 0.030389024, 0.050565537, 0.031159626, 0.04088086, 0.03507976, 0.0209296, 0.030437827, 0.025354102, 0.022331677, 0.05867263, 0.09187021, 0.090317085, 0.08465897, 0.2443109, 0.08491347, 0.06805341, 0.052439865, 0.05693235, 0.06994926, 0.06722, 0.037870005, 0.049336802, 0.042740792, 0.025636693, 0.029180123, 0.030416494, 0.040810026, 0.028348744, 0.03222693, 0.03302669, 0.0267641, 0.027889296, 0.028782675, 0.043581117, 0.022444414, 0.031687845, 0.030154899, 0.026713349, 0.023577647, 0.023425028, 0.02511377, 0.035753556, 0.032547973, 0.03239674, 0.0365163, 0.02558025, 0.017711155, 0.08842164, 0.17262658, 0.06393157, 0.051230878, 0.11304973, 0.048348974, 0.08008139, 0.040987283, 0.0439223, 0.06726423, 0.053206474, 0.034838025, 0.02836607, 0.04432354, 0.03133386, 0.027286168, 0.031478137, 0.02644914, 0.020133333, 0.039320614, 0.03465844, 0.03250638, 0.029338872, 0.025973117, 0.029927533, 0.04126626, 0.030421037, 0.024608705, 0.019725516, 0.027858451, 0.03693756, 0.033437658, 0.025204793, 0.019546822, 0.0253681, 0.041272778, 0.029532379, 0.023423262, 0.022050314, 0.037650313, 0.021127371, 0.017502217, 0.022915294, 0.020366304, 0.062448215, 0.0678212, 0.0824779, 0.16254409, 0.13973445, 0.07907302, 0.05069956, 0.047201864, 0.03980443, 0.08171145, 0.032957118, 0.033250682, 0.031172032, 0.04113965, 0.034561843, 0.027126005, 0.031053597, 0.048319556, 0.04329391, 0.036289383, 0.022238454, 0.037253283, 0.030974397, 0.030861929, 0.021597672, 0.019608662, 0.024883075, 0.023302637, 0.028996715, 0.025702203, 0.027599385, 0.033736546, 0.019987864, 0.01984462, 0.029945465, 0.022112902, 0.022750346, 0.02383781, 0.09201171, 0.1184701, 0.1605297, 0.13197924, 0.07543696, 0.038386006, 0.048559394, 0.12745464, 0.05689266, 0.051446486, 0.02780397, 0.05562695, 0.058921583, 0.03679335, 0.043476183, 0.035519574, 0.03266929, 0.04912998, 0.026311273, 0.031464957, 0.030575082, 0.028753702, 0.027174557, 0.035427094, 0.035932142, 0.024175018, 0.020905877, 0.034532983, 0.027455574, 0.025161728, 0.031450942, 0.025433129, 0.022473875, 0.023418766, 0.018240165, 0.04179715, 0.026741588, 0.022988709, 0.06312565, 0.10671927, 0.17909656, 0.11073921, 0.060570654, 0.08139574, 0.06735234, 0.056086395, 0.05583307, 0.050118644, 0.03643343, 0.043167245, 0.05914778, 0.04030483, 0.034338802, 0.04737583, 0.03246289, 0.03427518, 0.037487358, 0.0401325, 0.03900806, 0.027428163, 0.031634104, 0.02427094, 0.023452934, 0.038962744, 0.025677174, 0.030505978, 0.027761305, 0.035940867, 0.025233919, 0.03680312, 0.029870674, 0.039957456, 0.017072072, 0.028004339, 0.02464032, 0.02325176, 0.10242044, 0.15088785, 0.13319458, 0.07009821, 0.045559794, 0.05268004, 0.043489166, 0.07244167, 0.035834186, 0.04747479, 0.034420963, 0.049116712, 0.030976132, 0.035769984, 0.03761438, 0.03152972, 0.02224619, 0.03110283, 0.025271116, 0.03284228, 0.030144839, 0.031245222, 0.015256036, 0.02996786, 0.02884624, 0.026325354, 0.029621385, 0.030048834, 0.022105739, 0.02782535, 0.033155944, 0.02471699, 0.028597666, 0.025179852, 0.027044917, 0.034806255, 0.023691522, 0.026068136, 0.08000711, 0.060142454, 0.085285485, 0.10434827, 0.054484863, 0.06860322, 0.058521528, 0.041489013, 0.06749632, 0.09065898, 0.057799365, 0.048806496, 0.035963617, 0.052437156, 0.06334083, 0.035622865, 0.030191816, 0.028062228, 0.032746848, 0.04852388, 0.039660078, 0.030291373, 0.025128702, 0.034994274, 0.029488917, 0.03052098, 0.04158008, 0.0277015, 0.021946149, 0.026351308, 0.035147645, 0.022791002, 0.026361937, 0.02084908, 0.022652987, 0.03293629, 0.025578104, 0.020732548, 0.07721833, 0.105775796, 0.11097191, 0.18069223, 0.10930853, 0.15368211, 0.056128904, 0.047025107, 0.0678578, 0.051077377, 0.046548992, 0.045316014, 0.03419434, 0.07070118, 0.04088124, 0.03517903, 0.036592446, 0.038498327, 0.03804998, 0.04101194, 0.03312433, 0.030909808, 0.020452183, 0.024776185, 0.03814827, 0.03511168, 0.02785954, 0.019110879, 0.039600387, 0.025844084, 0.025405899, 0.026340311, 0.030756094, 0.02549021, 0.032264702, 0.031579908, 0.024915995, 0.023991331, 0.057717875, 0.16445869, 0.1690884, 0.20625676, 0.070566475, 0.08354566, 0.04908621, 0.041526876, 0.04664532, 0.110564545, 0.05478153, 0.053983476, 0.056222074, 0.028995344, 0.052232247, 0.041159485, 0.032679383, 0.047164116, 0.039550573, 0.0252239, 0.036637116, 0.029029544, 0.03296721, 0.022386972, 0.03687064, 0.030051414, 0.029881602, 0.026002511, 0.033674374, 0.02736071, 0.025626803, 0.03238454, 0.023959888, 0.02468236, 0.023946023, 0.02989234, 0.018900353, 0.027674092, 0.036647774, 0.086883456, 0.081328616, 0.06298804, 0.06343335, 0.09930878, 0.04325381, 0.039527047, 0.088432506, 0.058559153, 0.048923805, 0.06323816, 0.041278947, 0.05247734, 0.037084986, 0.03411086, 0.0430596, 0.054170616, 0.035767537, 0.019599728, 0.019588053, 0.028015384, 0.023796743, 0.0251704, 0.025558129, 0.01447956, 0.024973279, 0.026635205, 0.019333249, 0.020844882, 0.02805563, 0.029888323, 0.04125869, 0.021674313, 0.023098474, 0.021339053, 0.028583031, 0.02825642, 0.055587467, 0.067262165, 0.05856503, 0.09996556, 0.049440093, 0.033353683, 0.062286265, 0.047050416, 0.042208906, 0.056996364, 0.07073979, 0.03289381, 0.038965426, 0.053254064, 0.03621699, 0.04126867, 0.04236928, 0.04779622, 0.03840237, 0.0417628, 0.026889669, 0.030357517, 0.02339729, 0.02925943, 0.031429715, 0.024484174, 0.029265624, 0.019891217, 0.024655143, 0.02664125, 0.021494307, 0.023249995, 0.027565235, 0.03093764, 0.029289434, 0.029678317, 0.025254786, 0.026742863, 0.103507444, 0.12107389, 0.113391235, 0.08452191, 0.09594226, 0.03973825, 0.060328823, 0.079349674, 0.07108362, 0.036979962, 0.051359996, 0.054135855, 0.037509292, 0.07535866, 0.04297064, 0.03849065, 0.03850348, 0.038354248, 0.047820974, 0.04112343, 0.028211525, 0.03126109, 0.028981026, 0.027578542, 0.024847453, 0.036857694, 0.028590309, 0.027054865, 0.0228192, 0.026667755, 0.031045798, 0.028067961, 0.03621629, 0.028081233, 0.023727536, 0.01911214, 0.027390316, 0.027978906, 0.024777755, 0.02659496, 0.030171799, 0.02349588, 0.031146085, 0.04277788, 0.17311744, 0.12907755, 0.10824042, 0.0690773, 0.055808507, 0.05400544, 0.06707022, 0.055027377, 0.05168276, 0.061578512, 0.05507153, 0.031558804, 0.03621003, 0.04076622, 0.056687783, 0.034303363, 0.03781515, 0.045313727, 0.049304586, 0.062802225, 0.03256087, 0.0218422, 0.037030328, 0.034310747, 0.023032224, 0.033249192, 0.02718197, 0.031015024, 0.02580449, 0.03754251, 0.028742934, 0.017954323, 0.025131658, 0.017944803, 0.019414796, 0.031064486, 0.022496536, 0.10447492, 0.071074024, 0.07192716, 0.0625224, 0.055763572, 0.077468894, 0.06577348, 0.06553312, 0.09568705, 0.05965061, 0.05251769, 0.042381745, 0.02789348, 0.04930033, 0.032277945, 0.04731626, 0.035073318, 0.034056325, 0.036900014, 0.033177692, 0.03439672, 0.020837076, 0.027403766, 0.04578544, 0.023599075, 0.036293123, 0.025854036, 0.026477892, 0.026281435, 0.042011965, 0.027229946, 0.03215967, 0.030846247, 0.029830452, 0.025592119, 0.039171115, 0.019491363, 0.02546618, 0.08951677, 0.08234507, 0.12065937, 0.14655507, 0.072411574, 0.06134057, 0.0779928, 0.07290938, 0.05280092, 0.041046426, 0.050523818, 0.06560579, 0.06334328, 0.044944324, 0.041211512, 0.043160263, 0.028227985, 0.027937092, 0.026907764, 0.036417436, 0.03995809, 0.024368586, 0.040728748, 0.03674581, 0.023810416, 0.028987499, 0.031160232, 0.024034822, 0.029988403, 0.03938941, 0.030924631, 0.02655151, 0.03919102, 0.01968272, 0.025031408, 0.027452143, 0.03023482, 0.0315704, 0.053241234, 0.11608401, 0.10203359, 0.13657604, 0.07434223, 0.057672903, 0.061251156, 0.05528807, 0.07654165, 0.051479958, 0.051799785, 0.04826556, 0.04205633, 0.051600784, 0.056380942, 0.024685606, 0.02516306, 0.039557643, 0.030712074, 0.032637645, 0.029964067, 0.022604508, 0.027721263, 0.030315531, 0.033482462, 0.023801383, 0.03743105, 0.029405914, 0.046949573, 0.028599614, 0.016535906, 0.031829815, 0.03350984, 0.020078873, 0.02664236, 0.028437614, 0.031397328, 0.025802765, 0.054597806, 0.08574206, 0.07276848, 0.20663743, 0.10664155, 0.05429872, 0.05681975, 0.04274365, 0.061930727, 0.06164171, 0.04495518, 0.029075209, 0.0300017, 0.03638054, 0.046473514, 0.025819687, 0.028820317, 0.039452944, 0.052052066, 0.03145399, 0.036585458, 0.024683064, 0.028864924, 0.03050965, 0.024195695, 0.024573393, 0.019972995, 0.029989325, 0.028994, 0.02817541, 0.024978366, 0.029469851, 0.017321458, 0.023091715, 0.030275658, 0.028713698, 0.020241614, 0.020399136, 0.020949455, 0.024684748, 0.030416438, 0.02782244, 0.027565714, 0.024508176, 0.06562035, 0.0796282, 0.15100752, 0.101324044, 0.07089796, 0.05905672, 0.06880668, 0.07815506, 0.07338526, 0.07385841, 0.052172393, 0.057730153, 0.044839833, 0.031567298, 0.0636031, 0.04935585, 0.04152213, 0.03546958, 0.037425566, 0.04231665, 0.044239286, 0.037177283, 0.02355996, 0.025789045, 0.035028785, 0.028051715, 0.031276893, 0.022487996, 0.018637806, 0.04248518, 0.022019146, 0.03178978, 0.026505575, 0.02604737, 0.024849039, 0.029466873, 0.01819157, 0.025683265, 0.04765556, 0.09369197, 0.08072003, 0.12680446, 0.060482685, 0.036738522, 0.036462028, 0.0389539, 0.09177361, 0.08667187, 0.057628304, 0.04415078, 0.0470021, 0.06060439, 0.044300556, 0.03101109, 0.030364687, 0.038690466, 0.024775907, 0.02596953, 0.026545107, 0.038723696, 0.02428479, 0.020308934, 0.0384533, 0.041059766, 0.02026998, 0.03591131, 0.021750616, 0.020421777, 0.024788016, 0.030592684, 0.030166429, 0.028937386, 0.034776933, 0.029249715, 0.02183254, 0.02424057, 0.057943057, 0.14276418, 0.1591227, 0.16331828, 0.098927446, 0.076868616, 0.033352695, 0.050402246, 0.14094801, 0.11031565, 0.085194126, 0.07275814, 0.06049024, 0.059711073, 0.05944867, 0.06433893, 0.029785909, 0.03520388, 0.037033487, 0.045259915, 0.03079138, 0.02683441, 0.03386254, 0.028039267, 0.024485677, 0.033937912, 0.021905627, 0.030822396, 0.028376127, 0.016539052, 0.043073703, 0.020832231, 0.03676572, 0.028051065, 0.01306899, 0.023589697, 0.041455492, 0.023506874, 0.071897924, 0.0772473, 0.09150679, 0.0899422, 0.09674749, 0.083201736, 0.10465095, 0.041355927, 0.048558135, 0.080371305, 0.05529576, 0.033455938, 0.037008833, 0.043836907, 0.06144363, 0.056278072, 0.038220976, 0.026132658, 0.036928445, 0.0281621, 0.036363203, 0.028279781, 0.04006055, 0.03388588, 0.031304374, 0.02772361, 0.030507144, 0.03428476, 0.025337191, 0.024033591, 0.023689682, 0.022445653, 0.026860109, 0.02581234, 0.027800767, 0.020891093, 0.021495687, 0.02743567, 0.060383722, 0.09468817, 0.11299201, 0.09955643, 0.046805993, 0.062189605, 0.058386445, 0.12145656, 0.119198106, 0.044591304, 0.05040444, 0.047187656, 0.07159245, 0.07106099, 0.034363292, 0.030925117, 0.03271384, 0.028255265, 0.027688362, 0.02241323, 0.029191857, 0.027653666, 0.025696103, 0.023271555, 0.025935287, 0.025959717, 0.027746374, 0.031747002, 0.029936379, 0.023559649, 0.018614858, 0.022794036, 0.028983643, 0.019215312, 0.025379822, 0.020790646, 0.031091869, 0.020001708, 0.022400633, 0.022269133, 0.023770636, 0.02388709, 0.029712593, 0.12604998, 0.1397233, 0.09843681, 0.16032586, 0.10413354, 0.08495408, 0.07618031, 0.058982123, 0.05158217, 0.048941143, 0.05384737, 0.0537118, 0.039396033, 0.04295529, 0.032773305, 0.03211267, 0.033420447, 0.04149706, 0.028850304, 0.038569905, 0.024939498, 0.034264047, 0.039810747, 0.029749637, 0.029932275, 0.024803424, 0.030078994, 0.021936743, 0.027257737, 0.025086459, 0.030366456, 0.022667957, 0.04014946, 0.028600166, 0.02694962, 0.034636058, 0.022490835, 0.019308068, 0.08304931, 0.13572079, 0.06899203, 0.103141464, 0.06993217, 0.07887723, 0.0689567, 0.0799334, 0.08794216, 0.06073902, 0.0276354, 0.04062761, 0.052225336, 0.034714594, 0.040733926, 0.040437896, 0.028274035, 0.051525243, 0.043231152, 0.04215835, 0.031258427, 0.025196558, 0.031597245, 0.045003008, 0.03019076, 0.036265105, 0.024464827, 0.028619999, 0.02215399, 0.03308873, 0.020676842, 0.03107736, 0.02424629, 0.028197492, 0.02202768, 0.024219258, 0.026160954, 0.024937421, 0.05118432, 0.10598204, 0.11134586, 0.057825916, 0.054353464, 0.047452945, 0.055431258, 0.053967256, 0.034374956, 0.057243954, 0.031051915, 0.038593195, 0.050817605, 0.057029504, 0.033617824, 0.023829138, 0.024399335, 0.032679964, 0.030583294, 0.030550884, 0.030318655, 0.035879422, 0.031423267, 0.02347117, 0.031763703, 0.024609676, 0.03519252, 0.035907336, 0.028581955, 0.02604392, 0.016074093, 0.02324083, 0.024258105, 0.019732371, 0.024358874, 0.025701057, 0.03054441, 0.028964922, 0.056455184, 0.115459755, 0.12863712, 0.11675363, 0.09075975, 0.08874812, 0.05699124, 0.09085799, 0.045854956, 0.06242521, 0.041126817, 0.06821751, 0.023972873, 0.08098999, 0.055135597, 0.025649792, 0.038156148, 0.030781599, 0.024091115, 0.025596373, 0.024494348, 0.027231669, 0.024047965, 0.024523392, 0.038987357, 0.026989754, 0.029269412, 0.025701752, 0.023461623, 0.01946648, 0.022121498, 0.03079821, 0.034498423, 0.02961262, 0.022862239, 0.020427968, 0.017358577, 0.027800405, 0.047341675, 0.083371, 0.08349308, 0.14602554, 0.11139069, 0.09713138, 0.0483423, 0.068604335, 0.05784065, 0.07115067, 0.03795269, 0.0408395, 0.033552136, 0.05680461, 0.04126631, 0.024811443, 0.034108013, 0.026384005, 0.03380666, 0.04760324, 0.04241416, 0.033105537, 0.030874893, 0.024230897, 0.027307197, 0.030055953, 0.03272769, 0.036588024, 0.034629792, 0.045631737, 0.024244292, 0.02683185, 0.024082022, 0.027565503, 0.03471567, 0.033611465, 0.033520274, 0.024556223, 0.02684877, 0.028150545, 0.026183082, 0.028804678, 0.019553158, 0.033016413, 0.11669324, 0.10761276, 0.116278976, 0.10775828, 0.054370195, 0.060971547, 0.05817604, 0.041828442, 0.085934326, 0.031407416, 0.021228094, 0.028498618, 0.034159888, 0.04747633, 0.04927966, 0.041942004, 0.03116401, 0.028187608, 0.03227371, 0.029504826, 0.020905545, 0.021164866, 0.03402013, 0.039165776, 0.03072068, 0.036461025, 0.020959325, 0.02689908, 0.023137521, 0.031527467, 0.03863091, 0.030336201, 0.028797224, 0.030533236, 0.030794946, 0.028164173, 0.026233438, 0.021732707, 0.062184583, 0.12275903, 0.11791101, 0.10662101, 0.11154709, 0.053681906, 0.072192095, 0.05766177, 0.061411787, 0.07189346, 0.03635654, 0.03401225, 0.0329787, 0.046509694, 0.06582617, 0.03610781, 0.024461914, 0.030683009, 0.028659439, 0.035880353, 0.033377063, 0.029088324, 0.022485754, 0.03622885, 0.026505003, 0.021997722, 0.020974055, 0.015856456, 0.036447786, 0.028436253, 0.029263154, 0.028691942, 0.025945405, 0.023437953, 0.0237139, 0.021750547, 0.029333437, 0.022485772, 0.10928396, 0.11197211, 0.11795277, 0.094370656, 0.08730931, 0.0858598, 0.044964463, 0.07813933, 0.06576047, 0.06517804, 0.04911444, 0.049911987, 0.04917306, 0.04490806, 0.040752143, 0.035008922, 0.032858904, 0.038948726, 0.034938995, 0.033605237, 0.027367646, 0.029416269, 0.027969021, 0.022958882, 0.026197996, 0.026995385, 0.028413964, 0.02304764, 0.025606146, 0.031699613, 0.024978513, 0.029847004, 0.02572839, 0.018073143, 0.020982733, 0.03287692, 0.027596433, 0.020922923, 0.051206507, 0.086049415, 0.121655904, 0.07150634, 0.07469657, 0.07785338, 0.060060117, 0.04998278, 0.043137442, 0.048101075, 0.057511624, 0.046931494, 0.07014918, 0.036968846, 0.051460277, 0.030731164, 0.029356357, 0.03777832, 0.03134504, 0.034583423, 0.019927371, 0.029415127, 0.02064679, 0.020877633, 0.026453005, 0.034798026, 0.031798802, 0.024966348, 0.024866907, 0.028528992, 0.030727053, 0.026607225, 0.02883114, 0.029919919, 0.025396502, 0.031674936, 0.02800362, 0.030731564, 0.188658, 0.12208647, 0.16973811, 0.15399241, 0.05355039, 0.05612685, 0.043802265, 0.042262074, 0.05745455, 0.04151464, 0.0497213, 0.03760186, 0.042540852, 0.035251837, 0.045879807, 0.028785514, 0.028544044, 0.029471306, 0.035186276, 0.03672867, 0.021168161, 0.022020513, 0.035541445, 0.020604366, 0.028565288, 0.032602675, 0.032654535, 0.02216992, 0.03533283, 0.034545988, 0.024602002, 0.022803027, 0.027587969, 0.020187866, 0.03258469, 0.023888774, 0.029368892, 0.024994226, 0.028146729, 0.025620399, 0.029035669, 0.02443091, 0.019203538, 0.063999034, 0.12241317, 0.14477281, 0.07798706, 0.08570139, 0.058892798, 0.07588222, 0.044892695, 0.033626545, 0.07241781, 0.042554308, 0.036401436, 0.056743678, 0.04810182, 0.04594663, 0.042802434, 0.043460418, 0.029261442, 0.037573975, 0.027449416, 0.031288475, 0.035959512, 0.046298396, 0.02783939, 0.023433432, 0.023456628, 0.033608984, 0.026228413, 0.018217131, 0.025567675, 0.029130753, 0.023633754, 0.033893984, 0.024648786, 0.020725386, 0.03557237, 0.02041556, 0.027161404, 0.053519607, 0.06390815, 0.039445907, 0.100426495, 0.056175325, 0.03971428, 0.041440606, 0.051502127, 0.03668541, 0.072044425, 0.04805209, 0.05215519, 0.050835717, 0.025524791, 0.044771396, 0.042561542, 0.037658904, 0.02935487, 0.03088195, 0.015133748, 0.018401328, 0.020047601, 0.020669209, 0.02474277, 0.022728264, 0.032152224, 0.021882102, 0.039656308, 0.0223391, 0.037379645, 0.020510808, 0.022844387, 0.018511357, 0.023421329, 0.020371877, 0.02246305, 0.020177409, 0.020330006, 0.082712874, 0.07297382, 0.08103428, 0.064184286, 0.15498602, 0.06275569, 0.03760014, 0.1262841, 0.07029595, 0.06154876, 0.048860956, 0.08125108, 0.0464181, 0.035328344, 0.047047745, 0.045587163, 0.031217707, 0.02131177, 0.030571718, 0.044185888, 0.032216746, 0.026958805, 0.032053046, 0.031157283, 0.032456975, 0.03361957, 0.021026267, 0.04081308, 0.028859591, 0.017831847, 0.021743381, 0.019560661, 0.019034775, 0.020462573, 0.021207985, 0.023278901, 0.01688655, 0.014646395, 0.056679882, 0.07161861, 0.09455984, 0.104441956, 0.09617828, 0.07619794, 0.05777779, 0.0366976, 0.054282438, 0.091207914, 0.063269585, 0.044191312, 0.036667045, 0.022344738, 0.032670815, 0.042668328, 0.033175677, 0.03748426, 0.05287054, 0.018977152, 0.03399205, 0.023618167, 0.021456525, 0.019530552, 0.027191635, 0.02491917, 0.026242143, 0.026400033, 0.015691582, 0.016952407, 0.028197, 0.02648176, 0.02457201, 0.019106453, 0.0284069, 0.02004239, 0.027007552, 0.015131238, 0.061691698, 0.13747296, 0.15784544, 0.15036203, 0.12653805, 0.04865877, 0.07323148, 0.08211702, 0.088351585, 0.058928207, 0.04344574, 0.035037793, 0.035262916, 0.043391686, 0.027324462, 0.026442176, 0.019495804, 0.019148445, 0.027523788, 0.03198935, 0.03714377, 0.035880238, 0.03398884, 0.03093828, 0.0476555, 0.031994835, 0.020367121, 0.020876136, 0.022554792, 0.018879443, 0.01321347, 0.020319726, 0.01667421, 0.015976466, 0.018282436, 0.022981588, 0.016126584, 0.025719116, 0.022392131, 0.036168862, 0.022141319, 0.028044268, 0.018171038, 0.023219347, 0.050360806, 0.1478007, 0.09356484, 0.18707472, 0.07546222, 0.054607566, 0.045002863, 0.07878343, 0.10632339, 0.04524981, 0.043385506, 0.041576967, 0.04518098, 0.03948829, 0.04809112, 0.039683465, 0.045388557, 0.037974093, 0.030178914, 0.04319812, 0.033312276, 0.030058105, 0.025551422, 0.02486826, 0.02669799, 0.032883722, 0.033348568, 0.03945743, 0.02760932, 0.02729494, 0.0187311, 0.03798945, 0.025185362, 0.025079733, 0.03409299, 0.019146381, 0.029739562, 0.023105595, 0.059547644, 0.0655765, 0.07765627, 0.1410073, 0.064278506, 0.07847562, 0.0701687, 0.08657921, 0.055772774, 0.06734705, 0.044945806, 0.04429353, 0.044176776, 0.033533882, 0.031899825, 0.031506393, 0.0324516, 0.034065932, 0.037144724, 0.028288703, 0.04637785, 0.036574632, 0.036377493, 0.03309084, 0.029014634, 0.03407084, 0.026830943, 0.023435252, 0.0267134, 0.029589126, 0.021719424, 0.028629096, 0.02607992, 0.0160989, 0.030253017, 0.024032516, 0.020786908, 0.019411895, 0.079091616, 0.05282843, 0.079962514, 0.06391329, 0.053229872, 0.092206724, 0.07450473, 0.06787444, 0.050705425, 0.04132047, 0.053077247, 0.05294045, 0.04247761, 0.03886464, 0.03758012, 0.03725688, 0.045240786, 0.03312589, 0.035561442, 0.04784629, 0.040240347, 0.038621414, 0.029914273, 0.045990914, 0.027952291, 0.023798699, 0.020153636, 0.028542051, 0.022242464, 0.026431793, 0.029602, 0.026645506, 0.029292213, 0.02799366, 0.033040628, 0.027356561, 0.014454798, 0.01720888, 0.04771452, 0.039952297, 0.104243614, 0.10396509, 0.057173874, 0.035867825, 0.10982015, 0.074623056, 0.044525955, 0.065337, 0.057998966, 0.03786652, 0.047345966, 0.037816912, 0.032351036, 0.03126438, 0.031643778, 0.034077294, 0.03995709, 0.03718482, 0.025077017, 0.032395743, 0.03866421, 0.029473085, 0.04075454, 0.02857355, 0.029523974, 0.027096843, 0.020607846, 0.035219915, 0.038459804, 0.020843029, 0.018157976, 0.03106468, 0.027571999, 0.0323859, 0.030887585, 0.033381373, 0.12534903, 0.065847166, 0.113881685, 0.10185962, 0.06371371, 0.044015624, 0.04734807, 0.08689728, 0.10306165, 0.033700153, 0.025884246, 0.04320358, 0.060560495, 0.043804243, 0.061459366, 0.04571147, 0.029653722, 0.027185472, 0.038949046, 0.02939899, 0.027507484, 0.03845315, 0.028131325, 0.028338414, 0.04844375, 0.029757775, 0.026322328, 0.022528632, 0.022268532, 0.023100927, 0.029682562, 0.042636823, 0.025217915, 0.017644769, 0.03371497, 0.029442709, 0.023186356, 0.023698209, 0.03473563, 0.02591615, 0.023045493, 0.018639758, 0.021936962, 0.060173888, 0.08843547, 0.08705237, 0.124771856, 0.096338175, 0.066470884, 0.052813917, 0.036194216, 0.04994279, 0.04504103, 0.038385365, 0.034853466, 0.038067967, 0.054326415, 0.043086987, 0.040763784, 0.0215104, 0.04648169, 0.026052723, 0.028701613, 0.038764678, 0.024114626, 0.027377024, 0.025892459, 0.028960714, 0.025169346, 0.047087643, 0.03155813, 0.025078198, 0.026703725, 0.033122066, 0.030573908, 0.024143696, 0.02262528, 0.023245573, 0.02973997, 0.02707713, 0.032771155, 0.061904404, 0.20820732, 0.1832442, 0.08703097, 0.14041436, 0.08621347, 0.10992382, 0.08464378, 0.053274106, 0.0840771, 0.044171028, 0.030082323, 0.054662015, 0.039705753, 0.060061686, 0.06733808, 0.044625346, 0.031050371, 0.03184205, 0.020271922, 0.044524502, 0.027151236, 0.040876597, 0.029434066, 0.025959391, 0.027932199, 0.031353865, 0.02369222, 0.027651811, 0.02861647, 0.037946958, 0.025679106, 0.021491982, 0.031850595, 0.02855122, 0.019083155, 0.024511278, 0.020338297, 0.112440445, 0.096078046, 0.0660347, 0.17225593, 0.09002175, 0.05438316, 0.07337325, 0.06631364, 0.084691495, 0.06939012, 0.061230347, 0.043518234, 0.042399056, 0.048152406, 0.03120658, 0.044732045, 0.06046206, 0.04008048, 0.025506953, 0.025171908, 0.038594395, 0.020272186, 0.026877856, 0.024579939, 0.035234038, 0.02465751, 0.036860507, 0.028563289, 0.02334776, 0.032105636, 0.02255351, 0.031907015, 0.022465762, 0.025526546, 0.02144476, 0.031034933, 0.024880137, 0.026118334, 0.06402959, 0.08748617, 0.10598713, 0.25271407, 0.08815846, 0.05731784, 0.07322041, 0.10081379, 0.06422748, 0.0357241, 0.04848958, 0.042781573, 0.042599447, 0.05113102, 0.09876146, 0.06730777, 0.040517733, 0.0278978, 0.025718087, 0.033305343, 0.028614925, 0.03581612, 0.02403981, 0.040885102, 0.02814959, 0.029823547, 0.029958358, 0.021593735, 0.025804872, 0.029558357, 0.027911516, 0.020013612, 0.028133838, 0.03772719, 0.02735788, 0.02346187, 0.025880892, 0.037711985, 0.038317177, 0.11821052, 0.07697593, 0.13488464, 0.081805006, 0.051150907, 0.07554696, 0.078646824, 0.032925535, 0.07000182, 0.042814013, 0.03696058, 0.037438992, 0.03657719, 0.050900653, 0.031723455, 0.033529315, 0.025348676, 0.03705994, 0.024331274, 0.04488471, 0.025757832, 0.029266005, 0.04111105, 0.03697782, 0.02611313, 0.018880207, 0.024809266, 0.026839439, 0.024984831, 0.030260531, 0.022940593, 0.025855841, 0.022886913, 0.023512615, 0.029333971, 0.026056126, 0.032087337, 0.028446853, 0.025143765, 0.02696913, 0.02919364, 0.019341363, 0.020012073, 0.08332899, 0.09354731, 0.03584997, 0.07920048, 0.05627492, 0.054088857, 0.06497416, 0.08940381, 0.072737865, 0.060085885, 0.06448042, 0.050725266, 0.03526071, 0.02958054, 0.028622353, 0.02524514, 0.033591103, 0.029639138, 0.031761795, 0.027633576, 0.035020083, 0.023085777, 0.02496852, 0.03226066, 0.024490878, 0.026336454, 0.024820948, 0.032408405, 0.024545595, 0.02705376, 0.036851674, 0.028906493, 0.024250036, 0.029761145, 0.02997762, 0.02143068, 0.02168928, 0.037347186, 0.05862686, 0.08263746, 0.14420126, 0.15237655, 0.06680585, 0.06713407, 0.088108525, 0.09054154, 0.055348735, 0.04959865, 0.039067086, 0.049093902, 0.04457921, 0.03959477, 0.043716967, 0.033114597, 0.025188772, 0.030643275, 0.035000898, 0.03688426, 0.03153876, 0.020862348, 0.03691995, 0.027673608, 0.037527632, 0.034970295, 0.028935397, 0.020943468, 0.028007373, 0.02993883, 0.023525408, 0.03479262, 0.022901462, 0.021656437, 0.02258248, 0.022959972, 0.04703725, 0.028006233, 0.060467143, 0.097939536, 0.071448855, 0.09827224, 0.06342659, 0.06348587, 0.060468704, 0.087004244, 0.08701124, 0.08825957, 0.04606638, 0.03824379, 0.04274729, 0.03569709, 0.05269486, 0.033712123, 0.03196231, 0.024957074, 0.044022243, 0.03031302, 0.030932533, 0.030612757, 0.024480404, 0.03184408, 0.029108716, 0.030074056, 0.032449115, 0.023731198, 0.029172137, 0.024076186, 0.025085445, 0.032773595, 0.030315515, 0.02150879, 0.02611792, 0.025469799, 0.026725939, 0.023387363, 0.07512931, 0.07504978, 0.128666, 0.08329501, 0.07476137, 0.060647644, 0.080138065, 0.07978462, 0.054777097, 0.103907265, 0.031236228, 0.04901371, 0.03378588, 0.06084521, 0.033336814, 0.040355094, 0.04885215, 0.027482739, 0.027424436, 0.027482497, 0.028147992, 0.025207633, 0.027105253, 0.038715452, 0.039372794, 0.037833706, 0.026344892, 0.04010493, 0.022054529, 0.027142392, 0.026474407, 0.021413187, 0.025241166, 0.016364051, 0.04233384, 0.029621068, 0.033054356, 0.028226877, 0.06259207, 0.056527916, 0.07687438, 0.06533899, 0.10635149, 0.09654625, 0.069177106, 0.06419874, 0.043281306, 0.052415222, 0.04579525, 0.048700787, 0.05100591, 0.056043774, 0.034884516, 0.03284476, 0.032859236, 0.03765197, 0.0361925, 0.02710398, 0.039179277, 0.04202334, 0.025917202, 0.031274974, 0.029344553, 0.039909393, 0.026773619, 0.03331694, 0.028697114, 0.031098623, 0.026285822, 0.026829995, 0.019879978, 0.031943396, 0.026747728, 0.023524627, 0.03764818, 0.028629804, 0.02988798, 0.021960765, 0.027484769, 0.017952722, 0.034833092, 0.038060125, 0.071269065, 0.08839602, 0.068990625, 0.07941703, 0.062668286, 0.058014017, 0.07055669, 0.09027775, 0.07141853, 0.08333668, 0.058576696, 0.04111171, 0.027807465, 0.02982279, 0.057154473, 0.04782033, 0.06136233, 0.03879029, 0.03693665, 0.047878873, 0.025260128, 0.034570467, 0.047493786, 0.043827888, 0.017403642, 0.035688866, 0.039402436, 0.025489928, 0.032798517, 0.026203092, 0.033966195, 0.023579884, 0.019786831, 0.014008776, 0.029544946, 0.037370633, 0.0393507, 0.0909358, 0.15302932, 0.10170921, 0.0998369, 0.07435377, 0.07045609, 0.050581183, 0.06839987, 0.09223209, 0.053255264, 0.05782621, 0.047240168, 0.031907547, 0.035085913, 0.041306525, 0.05475557, 0.028611971, 0.028509533, 0.027893983, 0.03330794, 0.03844121, 0.034768604, 0.03497261, 0.04677487, 0.041924387, 0.028481022, 0.03820652, 0.033675913, 0.016894396, 0.036078926, 0.029602826, 0.02995475, 0.018454084, 0.022941554, 0.025126604, 0.023312254, 0.018935982, 0.0309775, 0.103304274, 0.14422786, 0.07541885, 0.11648492, 0.08991033, 0.05557926, 0.033341676, 0.047908574, 0.039789967, 0.04972055, 0.058862742, 0.039800677, 0.045264047, 0.060476616, 0.032103606, 0.023433512, 0.026388163, 0.043738574, 0.031185154, 0.028095877, 0.025031174, 0.027915115, 0.031211697, 0.021660892, 0.019598918, 0.02235631, 0.02094508, 0.024209691, 0.032779824, 0.022049254, 0.023499314, 0.024315968, 0.019663429, 0.0210216, 0.033837985, 0.019336129, 0.021499924, 0.024666706, 0.056245252, 0.0806463, 0.100168206, 0.046109732, 0.0807274, 0.07896199, 0.05445869, 0.043497097, 0.07574007, 0.036042012, 0.034932468, 0.038332243, 0.041558962, 0.031968664, 0.04550935, 0.04616524, 0.030032415, 0.028827809, 0.023571495, 0.031242127, 0.039678205, 0.037969757, 0.02039441, 0.028375918, 0.02186914, 0.030365013, 0.03587119, 0.028027305, 0.029130831, 0.035228282, 0.025713593, 0.029043458, 0.020647446, 0.02252896, 0.027208855, 0.025453536, 0.022123387, 0.020533111, 0.053636223, 0.11833455, 0.12850384, 0.10742318, 0.1112113, 0.06425339, 0.07239953, 0.06287051, 0.09287202, 0.061143685, 0.047960646, 0.037756827, 0.04190508, 0.058044296, 0.06721636, 0.059002563, 0.041365094, 0.03141917, 0.033160787, 0.064962454, 0.03182378, 0.04203962, 0.024348313, 0.03368571, 0.037380725, 0.029589437, 0.03503841, 0.02928394, 0.04136537, 0.02888746, 0.04021452, 0.0336095, 0.035735294, 0.02504674, 0.023139948, 0.027344778, 0.029936524, 0.038275983, 0.024193803, 0.026016083, 0.020368634, 0.02452533, 0.026529647, 0.035763122, 0.066077724, 0.11021735, 0.097882435, 0.13327084, 0.14547877, 0.041347012, 0.05146976, 0.04684857, 0.04595499, 0.034669966, 0.035021193, 0.0346725, 0.04363882, 0.060148045, 0.034004558, 0.032386586, 0.033310708, 0.04541929, 0.025060816, 0.031208524, 0.03781857, 0.031656835, 0.032005865, 0.031753432, 0.03237744, 0.039671987, 0.026543558, 0.035841614, 0.027797258, 0.022208907, 0.025851239, 0.031981196, 0.019976648, 0.026856981, 0.02579369, 0.026858816, 0.025666961, 0.017518187, 0.03546054, 0.09457602, 0.12383268, 0.10589253, 0.063419335, 0.045345113, 0.051838618, 0.07001255, 0.06511573, 0.053275447, 0.072116144, 0.049371213, 0.052515924, 0.02486073, 0.037781, 0.023662832, 0.025670115, 0.03574289, 0.042391934, 0.0395843, 0.031191299, 0.023811812, 0.020681243, 0.023594894, 0.03407074, 0.021211172, 0.02977179, 0.017489785, 0.022442227, 0.033508666, 0.031765014, 0.03379831, 0.029353734, 0.024084672, 0.033283427, 0.0332413, 0.022003006, 0.0238632, 0.06531586, 0.1636305, 0.16461308, 0.08321538, 0.08710005, 0.08068614, 0.03838863, 0.03394819, 0.06818814, 0.088045515, 0.053845946, 0.03060379, 0.044510964, 0.045816336, 0.049502786, 0.029636474, 0.027103648, 0.019728012, 0.02749924, 0.04353168, 0.05625632, 0.034380704, 0.023048874, 0.0223303, 0.02171752, 0.030859778, 0.0263329, 0.03455334, 0.02386194, 0.03678477, 0.021340797, 0.04908638, 0.02055669, 0.022766244, 0.02173472, 0.032838445, 0.032063767, 0.026780289, 0.07493964, 0.07792144, 0.08091314, 0.08106274, 0.053230643, 0.074201964, 0.06441258, 0.09116409, 0.10429204, 0.055070005, 0.02490308, 0.048731748, 0.0387994, 0.035925478, 0.044999775, 0.04486016, 0.029792933, 0.066779196, 0.044551607, 0.03773217, 0.039200787, 0.03643931, 0.029698802, 0.03595805, 0.02208226, 0.03266593, 0.024627654, 0.02774704, 0.024531392, 0.039963394, 0.022813488, 0.02598079, 0.023820596, 0.024715817, 0.031463586, 0.036002308, 0.041895993, 0.02701957, 0.06514866, 0.0860283, 0.040218636, 0.055121716, 0.037794556, 0.042864412, 0.047329843, 0.0482299, 0.043284763, 0.04530292, 0.047423694, 0.03982161, 0.05327377, 0.029351527, 0.03836912, 0.033543233, 0.035062645, 0.034086194, 0.028137887, 0.023411157, 0.027492944, 0.024585221, 0.021692133, 0.029123435, 0.024426537, 0.032785792, 0.03362344, 0.029972887, 0.022335265, 0.022048648, 0.017285664, 0.020670693, 0.027562123, 0.03385579, 0.02321349, 0.030442398, 0.02719524, 0.02101677, 0.08377912, 0.08714491, 0.12363168, 0.14583649, 0.052963424, 0.095322415, 0.055043973, 0.03600265, 0.06515038, 0.087802045, 0.05555361, 0.05082022, 0.04208349, 0.028893324, 0.055825096, 0.045073647, 0.039040066, 0.029526586, 0.028889352, 0.028922295, 0.02899223, 0.042353116, 0.029051322, 0.024105668, 0.030105067, 0.03006693, 0.024081731, 0.023063041, 0.027692067, 0.028950071, 0.023207156, 0.019276224, 0.02708713, 0.020054994, 0.039483376, 0.028981542, 0.023244226, 0.026417892, 0.06538403, 0.15994117, 0.14483911, 0.08689407, 0.06369254, 0.047809567, 0.039700486, 0.05244874, 0.11498633, 0.052713603, 0.07179343, 0.039182156, 0.06311877, 0.04129812, 0.04380304, 0.04506242, 0.038356, 0.040542647, 0.07328083, 0.03219191, 0.026150048, 0.04022949, 0.033005785, 0.0355187, 0.028631248, 0.031209206, 0.022512631, 0.040888853, 0.022907091, 0.029188221, 0.021720298, 0.026856625, 0.024785375, 0.033304825, 0.025820827, 0.02276476, 0.029393855, 0.019964008, 0.081555784, 0.062070295, 0.18808997, 0.12024672, 0.060632925, 0.060795117, 0.07333353, 0.07864264, 0.053519562, 0.049964763, 0.040661704, 0.035666484, 0.05792506, 0.055887938, 0.0631311, 0.06650111, 0.032113038, 0.054452132, 0.04790627, 0.039437085, 0.03908384, 0.031922184, 0.028124029, 0.032228347, 0.025377696, 0.024298633, 0.032062028, 0.025444081, 0.032123737, 0.024076883, 0.021657618, 0.02752736, 0.03176778, 0.025019905, 0.025823867, 0.02103601, 0.024075808, 0.032146133, 0.09900547, 0.056463886, 0.111675285, 0.073266536, 0.08111984, 0.06074046, 0.04077103, 0.069114305, 0.054961488, 0.04494547, 0.050868616, 0.036900654, 0.03574541, 0.042379458, 0.029474065, 0.04711141, 0.048547596, 0.034855437, 0.029861247, 0.020536715, 0.024592433, 0.03264003, 0.0369412, 0.038310435, 0.029067278, 0.022097372, 0.02447908, 0.04589099, 0.029219069, 0.039071552, 0.023511192, 0.040069122, 0.027077561, 0.02550523, 0.024330627, 0.021969542, 0.033321436, 0.03863157, 0.07013233, 0.09133834, 0.055464514, 0.06185957, 0.06797769, 0.13540535, 0.045185216, 0.07493342, 0.04156667, 0.041001678, 0.03551786, 0.037556957, 0.048551302, 0.035627395, 0.024597654, 0.034663245, 0.024080064, 0.030962357, 0.024901519, 0.03425477, 0.036589943, 0.032121662, 0.025168844, 0.02601734, 0.036442608, 0.02641355, 0.040575024, 0.027828643, 0.023480672, 0.04211127, 0.029636722, 0.043769564, 0.024271747, 0.02183963, 0.020606164, 0.029275203, 0.020694159, 0.023076907, 0.026658403, 0.02552244, 0.025224209, 0.033180166, 0.023383325, 0.074329056, 0.06123463, 0.083411954, 0.06771597, 0.08568679, 0.065084696, 0.052788816, 0.06700244, 0.073508784, 0.055171352, 0.03564927, 0.037730765, 0.02920743, 0.036787722, 0.03520329, 0.02662287, 0.030333782, 0.028934607, 0.020482797, 0.027215352, 0.0389918, 0.022898233, 0.0323739, 0.026673889, 0.023872226, 0.03002788, 0.030631766, 0.049271714, 0.031863343, 0.030691808, 0.02920099, 0.025695594, 0.026187211, 0.023859281, 0.024496187, 0.027338102, 0.02513152, 0.021829132, 0.04484522, 0.08428521, 0.08864793, 0.11707479, 0.15469967, 0.08186791, 0.06156077, 0.06441939, 0.058950614, 0.07362861, 0.037645288, 0.043001346, 0.042840093, 0.037539065, 0.040143292, 0.05692839, 0.033908315, 0.022396065, 0.032333683, 0.034868322, 0.036213994, 0.033201087, 0.02463797, 0.026749784, 0.03647712, 0.029939272, 0.027943172, 0.024259046, 0.03231354, 0.024774222, 0.021289604, 0.03208119, 0.027783329, 0.030681886, 0.024553949, 0.039019868, 0.030375337, 0.02677929, 0.08262504, 0.08870103, 0.058263805, 0.10346978, 0.1391784, 0.070428684, 0.06694904, 0.10255891, 0.050810535, 0.056169897, 0.03130706, 0.047991026, 0.037969306, 0.030133147, 0.05814442, 0.06034894, 0.03081058, 0.03852265, 0.030824153, 0.04064664, 0.064088725, 0.0394341, 0.03941875, 0.03174481, 0.031115307, 0.021768581, 0.027627634, 0.0341251, 0.047941215, 0.030693334, 0.031970624, 0.02350165, 0.030752942, 0.026643574, 0.027064467, 0.035148755, 0.03387667, 0.027149025, 0.050543875, 0.051854745, 0.12981056, 0.12433662, 0.07353638, 0.036633708, 0.09618484, 0.049287736, 0.061947867, 0.08147495, 0.0321517, 0.03339385, 0.036346283, 0.03588539, 0.057420336, 0.048877988, 0.033175036, 0.027702382, 0.041034207, 0.044572756, 0.030562524, 0.023292623, 0.018921906, 0.023788018, 0.034843728, 0.027253477, 0.04522328, 0.019067118, 0.045044184, 0.02174683, 0.024028532, 0.024134906, 0.019326255, 0.02888668, 0.022567887, 0.02193975, 0.028953282, 0.033216685, 0.08851813, 0.071413346, 0.11077591, 0.12784506, 0.08312768, 0.0665469, 0.056622908, 0.043588646, 0.08029944, 0.13138786, 0.05006068, 0.04320247, 0.03639254, 0.04220179, 0.04212028, 0.026070554, 0.033210568, 0.047014546, 0.041636433, 0.024588382, 0.040716738, 0.032068845, 0.032034475, 0.032577146]

Ascent losses:

{43: -0.022551864, 83: -0.021324124, 122: -0.01831977, 161: -0.019679746, 200: -0.012165433, 223: -0.01889889, 262: -0.016187804, 301: -0.012126076, 340: -0.018089402, 379: -0.015926585, 423: -0.014001693, 462: -0.012345531, 501: -0.0111080995, 540: -0.009741154, 579: -0.0149040315, 624: -0.009010584, 663: -0.0109429555, 702: -0.011529641, 741: -0.01075081, 780: -0.0110946195, 819: -0.009891164, 858: -0.009364944, 897: -0.007978183, 936: -0.0102799265, 975: -0.00991602, 1019: -0.0098507805, 1058: -0.0076780305, 1097: -0.012988267, 1136: -0.009840818, 1175: -0.008163454, 1220: -0.00776051, 1259: -0.008004026, 1298: -0.009791054, 1337: -0.0048031122, 1376: -0.007891727, 1420: -0.009191734, 1459: -0.0065407692, 1498: -0.006537151, 1537: -0.006953588, 1576: -0.0067716227, 1621: -0.004973743, 1660: -0.008008961, 1699: -0.006302277, 1738: -0.01034981, 1777: -0.004465164, 1821: -0.0078026317, 1860: -0.0055465517, 1899: -0.005606308, 1938: -0.0049448805, 1977: -0.0073733847, 2022: -0.005828596, 2061: -0.0045543625, 2100: -0.005506778, 2139: -0.0047753653, 2178: -0.0051812734, 2222: -0.0045200596, 2261: -0.004611523, 2300: -0.007659802, 2339: -0.0039250962, 2378: -0.004119111, 2423: -0.0029813813, 2462: -0.003815832, 2501: -0.0033037432, 2540: -0.0033796697, 2579: -0.0033956354, 2623: -0.0042280545, 2662: -0.0040065125, 2701: -0.002021077, 2740: -0.0025420107, 2779: -0.003666622, 2824: -0.002466715, 2863: -0.003172175, 2902: -0.0029082287, 2941: -0.0023127962, 2980: -0.0034121377, 3019: -0.0038300627, 3058: -0.0031649848, 3097: -0.00419687, 3136: -0.0032814655, 3175: -0.0037842789, 3219: -0.0033579147, 3258: -0.0035935894, 3297: -0.002968244, 3336: -0.0028383853, 3375: -0.003171915, 3420: -0.0039921366, 3459: -0.0024466568, 3498: -0.0029298528, 3537: -0.003001829, 3576: -0.0028718046, 3620: -0.0026118073, 3659: -0.0026858035, 3698: -0.0034309747, 3737: -0.002622749, 3776: -0.0019860477, 3821: -0.0024859558, 3860: -0.002648147, 3899: -0.0023609556, 3938: -0.0037344424, 3977: -0.0036893063, 4021: -0.0023328348, 4060: -0.0024109017, 4099: -0.002475492, 4138: -0.002831681, 4177: -0.0024914255, 4222: -0.0021850066, 4261: -0.0020713734, 4300: -0.002547326, 4339: -0.0035353745, 4378: -0.0025792432, 4422: -0.0028467197, 4461: -0.0025598884, 4500: -0.003336691, 4539: -0.002202529, 4578: -0.0026597816, 4623: -0.002503841, 4662: -0.0022068415, 4701: -0.0030902813, 4740: -0.0029032864, 4779: -0.0026131028, 4823: -0.0033960124, 4862: -0.0036110906, 4901: -0.0033434178, 4940: -0.003441117, 4979: -0.0041442844, 5024: -0.002681524, 5063: -0.0026657446, 5102: -0.0037754355, 5141: -0.0031636227, 5180: -0.0028214075, 5219: -0.003142874, 5258: -0.0024407874, 5297: -0.0023867984, 5336: -0.002193184, 5375: -0.0020321154, 5419: -0.0027335186, 5458: -0.0026271206, 5497: -0.0026020787, 5536: -0.0020235544, 5575: -0.0018696389, 5620: -0.00314274, 5659: -0.0026702662, 5698: -0.0029477174, 5737: -0.0022296524, 5776: -0.0026764018, 5820: -0.0024674258, 5859: -0.0035082905, 5898: -0.003893274, 5937: -0.0025602414, 5976: -0.0023160975, 6021: -0.0027797373, 6060: -0.0031829185, 6099: -0.0026848116, 6138: -0.0034850333, 6177: -0.0021178008, 6221: -0.0020374071, 6260: -0.0019506445, 6299: -0.0014875333, 6338: -0.0024143546, 6377: -0.0023856512, 6422: -0.0019564012, 6461: -0.002497511, 6500: -0.0018846049, 6539: -0.0020034325, 6578: -0.0022716408, 6622: -0.0021850944, 6661: -0.0030343356, 6700: -0.004231169, 6739: -0.0020825425, 6778: -0.002731535, 6823: -0.0022842789, 6862: -0.0022510719, 6901: -0.0028299852, 6940: -0.0022521361, 6979: -0.0029969986, 7023: -0.0022356045, 7062: -0.002341114, 7101: -0.0031715774, 7140: -0.0028047962, 7179: -0.0029203068, 7224: -0.0014567409, 7263: -0.0026195657, 7302: -0.003064643, 7341: -0.0021665862, 7380: -0.0020187814, 7419: -0.0027673198, 7458: -0.00302678, 7497: -0.0027849465, 7536: -0.0026909462, 7575: -0.0022788576, 7619: -0.0024632805, 7658: -0.0033204355, 7697: -0.0023941365, 7736: -0.0026431868, 7775: -0.0029524986, 7820: -0.002292454, 7859: -0.0021729008, 7898: -0.0026212027, 7937: -0.0037879206, 7976: -0.0032540516, 8020: -0.0019777527, 8059: -0.0021827633, 8098: -0.0028853668, 8137: -0.0021020442, 8176: -0.0015064552, 8221: -0.0020643186, 8260: -0.0022634938, 8299: -0.0024395946, 8338: -0.0030299772, 8377: -0.0025740557, 8421: -0.0024561612, 8460: -0.0027394176, 8499: -0.0015119584, 8538: -0.0026003835, 8577: -0.0025151717, 8622: -0.0020187853, 8661: -0.0021459162, 8700: -0.002494394, 8739: -0.0028638786, 8778: -0.0024793881, 8822: -0.0020983773, 8861: -0.0024352192, 8900: -0.003693079, 8939: -0.0018841868, 8978: -0.0021488757, 9023: -0.0027311493, 9062: -0.002086037, 9101: -0.0019875176, 9140: -0.002942311, 9179: -0.00320216, 9223: -0.0031188682, 9262: -0.0020049957, 9301: -0.0021885522, 9340: -0.001688471, 9379: -0.0022866537, 9424: -0.0021507586, 9463: -0.0025934037, 9502: -0.0025681942, 9541: -0.0027196917, 9580: -0.0032087092, 9619: -0.0016429569, 9658: -0.0020852906, 9697: -0.0023419268, 9736: -0.0021609608, 9775: -0.0030456607, 9819: -0.002759373, 9858: -0.0031227823, 9897: -0.002224128, 9936: -0.0022962696, 9975: -0.0025626894}
Importing data files...
['mm1d_1.csv', 'mm1d_4.csv', 'mm1d_0.csv', 'mm1d_2.csv', 'mm1d_3.csv', 'mm1d_5.csv']
Splitting data into training and test sets with a ratio of: 0.2
Total number of training samples is 38400
Total number of test samples is 9600
Length of an output spectrum is 300
Generating torch datasets
training using gradient ascent
Starting training process
Doing Evaluation on the model now
This is Epoch 0, training loss 8.78885, validation loss 8.81940
Resetting learning rate to 0.01000
Doing Evaluation on the model now
This is Epoch 10, training loss 0.37953, validation loss 0.33055
Doing Evaluation on the model now
This is Epoch 20, training loss 0.36012, validation loss 0.41799
Epoch    44: reducing learning rate of group 0 to 5.0000e-04.
Epoch    58: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 30, training loss 0.10469, validation loss 0.09886
Epoch    78: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 40, training loss 0.08245, validation loss 0.08233
Epoch    90: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 45
Mean train loss for ascent epoch 46: -0.09293829649686813
Mean eval for ascent epoch 46: 0.09117669612169266
Doing Evaluation on the model now
This is Epoch 50, training loss 1.33778, validation loss 0.36070
Epoch   104: reducing learning rate of group 0 to 5.0000e-03.
Epoch   115: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 60, training loss 0.18838, validation loss 0.25432
Epoch   126: reducing learning rate of group 0 to 1.2500e-03.
Epoch   137: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 70, training loss 0.08025, validation loss 0.05689
Epoch   148: reducing learning rate of group 0 to 3.1250e-04.
Epoch   159: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 80, training loss 0.06344, validation loss 0.06145
Epoch   170: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 85
Mean train loss for ascent epoch 86: -0.0771838128566742
Mean eval for ascent epoch 86: 0.06422554701566696
Doing Evaluation on the model now
This is Epoch 90, training loss 1.08721, validation loss 1.67032
Epoch   181: reducing learning rate of group 0 to 5.0000e-03.
Epoch   192: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 100, training loss 0.34559, validation loss 0.46672
Epoch   203: reducing learning rate of group 0 to 1.2500e-03.
Epoch   214: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 110, training loss 0.05089, validation loss 0.04965
Epoch   225: reducing learning rate of group 0 to 3.1250e-04.
Epoch   236: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 120, training loss 0.02959, validation loss 0.02733
Epoch   247: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 124
Mean train loss for ascent epoch 125: -0.06924203783273697
Mean eval for ascent epoch 125: 0.03843723237514496
Epoch   258: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 130, training loss 0.76790, validation loss 0.15807
Epoch   269: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 140, training loss 0.20999, validation loss 0.07242
Epoch   280: reducing learning rate of group 0 to 1.2500e-03.
Epoch   291: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 150, training loss 0.04107, validation loss 0.03323
Epoch   302: reducing learning rate of group 0 to 3.1250e-04.
Epoch   313: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 160, training loss 0.02418, validation loss 0.02089
Epoch   324: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 163
Mean train loss for ascent epoch 164: -0.049481380730867386
Mean eval for ascent epoch 164: 0.026184752583503723
Epoch   335: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 170, training loss 0.32963, validation loss 0.31662
Epoch   346: reducing learning rate of group 0 to 2.5000e-03.
Epoch   357: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 180, training loss 0.28359, validation loss 0.55430
Epoch   368: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 190, training loss 0.05150, validation loss 0.04113
Epoch   379: reducing learning rate of group 0 to 3.1250e-04.
Epoch   390: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 200, training loss 0.01901, validation loss 0.01834
Resetting learning rate to 0.01000
Epoch   401: reducing learning rate of group 0 to 5.0000e-04.
Epoch   412: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 210, training loss 0.02445, validation loss 0.02236
Epoch   423: reducing learning rate of group 0 to 1.2500e-04.
Epoch   434: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 219
Mean train loss for ascent epoch 220: -0.0456010140478611
Mean eval for ascent epoch 220: 0.020339488983154297
Doing Evaluation on the model now
This is Epoch 220, training loss 0.02034, validation loss 0.02120
Epoch   445: reducing learning rate of group 0 to 5.0000e-03.
Epoch   456: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 230, training loss 0.59837, validation loss 0.28870
Epoch   467: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 240, training loss 0.05067, validation loss 0.02912
Epoch   478: reducing learning rate of group 0 to 6.2500e-04.
Epoch   489: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 250, training loss 0.05466, validation loss 0.07778
Epoch   500: reducing learning rate of group 0 to 1.5625e-04.
Epoch   511: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 258
Mean train loss for ascent epoch 259: -0.07695881277322769
Mean eval for ascent epoch 259: 0.017474420368671417
Doing Evaluation on the model now
This is Epoch 260, training loss 0.59460, validation loss 0.97662
Epoch   522: reducing learning rate of group 0 to 5.0000e-03.
Epoch   533: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 270, training loss 0.06927, validation loss 0.10965
Epoch   544: reducing learning rate of group 0 to 1.2500e-03.
Epoch   555: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 280, training loss 0.07939, validation loss 0.19593
Epoch   566: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 290, training loss 0.02025, validation loss 0.01552
Epoch   577: reducing learning rate of group 0 to 1.5625e-04.
Epoch   588: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 297
Mean train loss for ascent epoch 298: -0.04667669162154198
Mean eval for ascent epoch 298: 0.02438783273100853
Doing Evaluation on the model now
This is Epoch 300, training loss 0.83361, validation loss 1.26759
Epoch   599: reducing learning rate of group 0 to 5.0000e-03.
Epoch   610: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 310, training loss 0.39362, validation loss 0.41132
Epoch   621: reducing learning rate of group 0 to 1.2500e-03.
Epoch   632: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 320, training loss 0.12942, validation loss 0.14320
Epoch   643: reducing learning rate of group 0 to 3.1250e-04.
Epoch   654: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 330, training loss 0.06365, validation loss 0.06183
Epoch   665: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 336
Mean train loss for ascent epoch 337: -0.06945087015628815
Mean eval for ascent epoch 337: 0.04814521595835686
Doing Evaluation on the model now
This is Epoch 340, training loss 1.04285, validation loss 0.89324
Epoch   676: reducing learning rate of group 0 to 5.0000e-03.
Epoch   687: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 350, training loss 0.06574, validation loss 0.09185
Epoch   698: reducing learning rate of group 0 to 1.2500e-03.
Epoch   709: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 360, training loss 0.05800, validation loss 0.05511
Epoch   720: reducing learning rate of group 0 to 3.1250e-04.
Epoch   731: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 370, training loss 0.02046, validation loss 0.01728
Epoch   742: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 375
Mean train loss for ascent epoch 376: -0.048465125262737274
Mean eval for ascent epoch 376: 0.024152418598532677
Doing Evaluation on the model now
This is Epoch 380, training loss 0.40925, validation loss 0.21965
Epoch   753: reducing learning rate of group 0 to 5.0000e-03.
Epoch   764: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 390, training loss 0.20871, validation loss 0.33420
Epoch   775: reducing learning rate of group 0 to 1.2500e-03.
Epoch   786: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 400, training loss 0.03524, validation loss 0.02997
Resetting learning rate to 0.01000
Epoch   797: reducing learning rate of group 0 to 5.0000e-04.
Epoch   808: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 410, training loss 0.02620, validation loss 0.02375
Epoch   819: reducing learning rate of group 0 to 1.2500e-04.
Epoch   830: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 419
Mean train loss for ascent epoch 420: -0.06586100161075592
Mean eval for ascent epoch 420: 0.015034206211566925
Doing Evaluation on the model now
This is Epoch 420, training loss 0.01503, validation loss 0.01841
Epoch   841: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 430, training loss 0.33551, validation loss 0.11836
Epoch   852: reducing learning rate of group 0 to 2.5000e-03.
Epoch   863: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 440, training loss 0.09319, validation loss 0.04092
Epoch   874: reducing learning rate of group 0 to 6.2500e-04.
Epoch   885: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 450, training loss 0.01899, validation loss 0.03043
Epoch   896: reducing learning rate of group 0 to 1.5625e-04.
Epoch   907: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 458
Mean train loss for ascent epoch 459: -0.051639508455991745
Mean eval for ascent epoch 459: 0.016913598403334618
Doing Evaluation on the model now
This is Epoch 460, training loss 0.27123, validation loss 0.30396
Epoch   918: reducing learning rate of group 0 to 5.0000e-03.
Epoch   929: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 470, training loss 0.08567, validation loss 0.07710
Epoch   940: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 480, training loss 0.07787, validation loss 0.08278
Epoch   951: reducing learning rate of group 0 to 6.2500e-04.
Epoch   962: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 490, training loss 0.01766, validation loss 0.01631
Epoch   973: reducing learning rate of group 0 to 1.5625e-04.
Epoch   984: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 497
Mean train loss for ascent epoch 498: -0.06159446761012077
Mean eval for ascent epoch 498: 0.022673869505524635
Doing Evaluation on the model now
This is Epoch 500, training loss 0.49554, validation loss 0.29493
Epoch   995: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1006: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 510, training loss 0.14122, validation loss 0.08717
Epoch  1017: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1028: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 520, training loss 0.02783, validation loss 0.02522
Epoch  1039: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 530, training loss 0.01630, validation loss 0.01564
Epoch  1050: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1061: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 536
Mean train loss for ascent epoch 537: -0.05538780614733696
Mean eval for ascent epoch 537: 0.017277421429753304
Doing Evaluation on the model now
This is Epoch 540, training loss 0.33053, validation loss 0.27647
Epoch  1072: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1083: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 550, training loss 0.18469, validation loss 0.37656
Epoch  1094: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1105: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 560, training loss 0.01893, validation loss 0.01771
Epoch  1116: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1127: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 570, training loss 0.01651, validation loss 0.01282
Epoch  1138: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 575
Mean train loss for ascent epoch 576: -0.06564363092184067
Mean eval for ascent epoch 576: 0.01745065115392208
Doing Evaluation on the model now
This is Epoch 580, training loss 0.29623, validation loss 0.48857
Epoch  1149: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1160: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 590, training loss 0.14088, validation loss 0.18079
Epoch  1171: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1182: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 600, training loss 0.01856, validation loss 0.05003
Resetting learning rate to 0.01000
Epoch  1193: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1204: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 610, training loss 0.02006, validation loss 0.01550
Epoch  1215: reducing learning rate of group 0 to 1.2500e-04.
Epoch  1226: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 620
Doing Evaluation on the model now
This is Epoch 620, training loss 0.01386, validation loss 0.01239
Mean train loss for ascent epoch 621: -0.04558071121573448
Mean eval for ascent epoch 621: 0.013322967104613781
Epoch  1237: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 630, training loss 0.15249, validation loss 0.11429
Epoch  1248: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1259: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 640, training loss 0.05914, validation loss 0.07695
Epoch  1270: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1281: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 650, training loss 0.01751, validation loss 0.01464
Epoch  1292: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1303: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 659
Mean train loss for ascent epoch 660: -0.04763823747634888
Mean eval for ascent epoch 660: 0.016117200255393982
Doing Evaluation on the model now
This is Epoch 660, training loss 0.01612, validation loss 0.01338
Epoch  1314: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1325: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 670, training loss 0.22483, validation loss 0.10511
Epoch  1336: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 680, training loss 0.11218, validation loss 0.05535
Epoch  1347: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1358: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 690, training loss 0.01478, validation loss 0.01255
Epoch  1369: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1380: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 698
Mean train loss for ascent epoch 699: -0.06567558646202087
Mean eval for ascent epoch 699: 0.014575578272342682
Doing Evaluation on the model now
This is Epoch 700, training loss 0.28058, validation loss 0.28953
Epoch  1391: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1402: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 710, training loss 0.14637, validation loss 0.04535
Epoch  1413: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1424: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 720, training loss 0.03691, validation loss 0.02157
Epoch  1435: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 730, training loss 0.01569, validation loss 0.01994
Epoch  1446: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1457: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 737
Mean train loss for ascent epoch 738: -0.036595676094293594
Mean eval for ascent epoch 738: 0.0160879734903574
Doing Evaluation on the model now
This is Epoch 740, training loss 0.59490, validation loss 0.44303
Epoch  1468: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1479: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 750, training loss 0.05202, validation loss 0.03684
Epoch  1490: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1501: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 760, training loss 0.03369, validation loss 0.04043
Epoch  1512: reducing learning rate of group 0 to 3.1250e-04.
Epoch  1523: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 770, training loss 0.01639, validation loss 0.01804
Epoch  1534: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 776
Mean train loss for ascent epoch 777: -0.03974786773324013
Mean eval for ascent epoch 777: 0.017787154763936996
Doing Evaluation on the model now
This is Epoch 780, training loss 0.30453, validation loss 0.69998
Epoch  1545: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1556: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 790, training loss 0.05140, validation loss 0.08430
Epoch  1567: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1578: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 800, training loss 0.01901, validation loss 0.01691
Resetting learning rate to 0.01000
Epoch  1589: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1600: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 810, training loss 0.02606, validation loss 0.02554
Epoch  1611: reducing learning rate of group 0 to 1.2500e-04.
Epoch  1622: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 820
Doing Evaluation on the model now
This is Epoch 820, training loss 0.01304, validation loss 0.01088
Mean train loss for ascent epoch 821: -0.05528908595442772
Mean eval for ascent epoch 821: 0.012783108279109001
Epoch  1633: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 830, training loss 0.13057, validation loss 0.14633
Epoch  1644: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1655: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 840, training loss 0.06485, validation loss 0.03998
Epoch  1666: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1677: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 850, training loss 0.01712, validation loss 0.01494
Epoch  1688: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1699: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 859
Mean train loss for ascent epoch 860: -0.04979463666677475
Mean eval for ascent epoch 860: 0.018722616136074066
Doing Evaluation on the model now
This is Epoch 860, training loss 0.01872, validation loss 0.02934
Epoch  1710: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 870, training loss 0.24295, validation loss 0.84472
Epoch  1721: reducing learning rate of group 0 to 2.5000e-03.
Epoch  1732: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 880, training loss 0.03787, validation loss 0.02457
Epoch  1743: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1754: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 890, training loss 0.03071, validation loss 0.03326
Epoch  1765: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1776: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 898
Mean train loss for ascent epoch 899: -0.06710407882928848
Mean eval for ascent epoch 899: 0.01922444999217987
Doing Evaluation on the model now
This is Epoch 900, training loss 0.13774, validation loss 0.15500
Epoch  1787: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1798: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 910, training loss 0.12569, validation loss 0.03305
Epoch  1809: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 920, training loss 0.05457, validation loss 0.09015
Epoch  1820: reducing learning rate of group 0 to 6.2500e-04.
Epoch  1831: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 930, training loss 0.01375, validation loss 0.01466
Epoch  1842: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1853: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 937
Mean train loss for ascent epoch 938: -0.042534708976745605
Mean eval for ascent epoch 938: 0.015063505619764328
Doing Evaluation on the model now
This is Epoch 940, training loss 0.31459, validation loss 0.21359
Epoch  1864: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1875: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 950, training loss 0.03068, validation loss 0.05082
Epoch  1886: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1897: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 960, training loss 0.01467, validation loss 0.01432
Epoch  1908: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 970, training loss 0.01715, validation loss 0.02056
Epoch  1919: reducing learning rate of group 0 to 1.5625e-04.
Epoch  1930: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 976
Mean train loss for ascent epoch 977: -0.04085633531212807
Mean eval for ascent epoch 977: 0.013479248620569706
Doing Evaluation on the model now
This is Epoch 980, training loss 0.36236, validation loss 0.62491
Epoch  1941: reducing learning rate of group 0 to 5.0000e-03.
Epoch  1952: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 990, training loss 0.07173, validation loss 0.13383
Epoch  1963: reducing learning rate of group 0 to 1.2500e-03.
Epoch  1974: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1000, training loss 0.02032, validation loss 0.01603
Resetting learning rate to 0.01000
Epoch  1985: reducing learning rate of group 0 to 5.0000e-04.
Epoch  1996: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1010, training loss 0.01902, validation loss 0.01732
Epoch  2007: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1020, training loss 0.01186, validation loss 0.01033
Epoch  2018: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1021
Mean train loss for ascent epoch 1022: -0.04404031112790108
Mean eval for ascent epoch 1022: 0.012305483222007751
Epoch  2029: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1030, training loss 0.12814, validation loss 0.10367
Epoch  2040: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2051: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1040, training loss 0.02544, validation loss 0.02400
Epoch  2062: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2073: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1050, training loss 0.01703, validation loss 0.01946
Epoch  2084: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2095: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1060
Doing Evaluation on the model now
This is Epoch 1060, training loss 0.01269, validation loss 0.01344
Mean train loss for ascent epoch 1061: -0.02870369888842106
Mean eval for ascent epoch 1061: 0.01266856212168932
Epoch  2106: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1070, training loss 0.28913, validation loss 0.29029
Epoch  2117: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2128: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1080, training loss 0.02872, validation loss 0.06476
Epoch  2139: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2150: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1090, training loss 0.01534, validation loss 0.01309
Epoch  2161: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2172: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1099
Mean train loss for ascent epoch 1100: -0.03643697500228882
Mean eval for ascent epoch 1100: 0.013032671064138412
Doing Evaluation on the model now
This is Epoch 1100, training loss 0.01303, validation loss 0.01021
Epoch  2183: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2194: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1110, training loss 0.09810, validation loss 0.13355
Epoch  2205: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1120, training loss 0.02931, validation loss 0.03412
Epoch  2216: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2227: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1130, training loss 0.01518, validation loss 0.01394
Epoch  2238: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2249: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1138
Mean train loss for ascent epoch 1139: -0.03175679221749306
Mean eval for ascent epoch 1139: 0.013730280101299286
Doing Evaluation on the model now
This is Epoch 1140, training loss 0.20579, validation loss 0.13621
Epoch  2260: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2271: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1150, training loss 0.08934, validation loss 0.11055
Epoch  2282: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2293: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1160, training loss 0.03113, validation loss 0.01723
Epoch  2304: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1170, training loss 0.01321, validation loss 0.01261
Epoch  2315: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2326: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1177
Mean train loss for ascent epoch 1178: -0.034270647913217545
Mean eval for ascent epoch 1178: 0.011699619702994823
Doing Evaluation on the model now
This is Epoch 1180, training loss 0.27960, validation loss 0.26613
Epoch  2337: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2348: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1190, training loss 0.13442, validation loss 0.20309
Epoch  2359: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2370: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1200, training loss 0.02157, validation loss 0.02288
Resetting learning rate to 0.01000
Epoch  2381: reducing learning rate of group 0 to 5.0000e-04.
Epoch  2392: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 1210, training loss 0.01187, validation loss 0.01043
Epoch  2403: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1220, training loss 0.01026, validation loss 0.00976
Epoch  2414: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1221
Mean train loss for ascent epoch 1222: -0.057838112115859985
Mean eval for ascent epoch 1222: 0.010696233250200748
Epoch  2425: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1230, training loss 0.07363, validation loss 0.07920
Epoch  2436: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2447: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1240, training loss 0.02879, validation loss 0.02593
Epoch  2458: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2469: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1250, training loss 0.01185, validation loss 0.01093
Epoch  2480: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2491: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1260
Doing Evaluation on the model now
This is Epoch 1260, training loss 0.01011, validation loss 0.00982
Mean train loss for ascent epoch 1261: -0.03939297795295715
Mean eval for ascent epoch 1261: 0.011068759486079216
Epoch  2502: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1270, training loss 0.06924, validation loss 0.06575
Epoch  2513: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2524: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1280, training loss 0.02584, validation loss 0.02635
Epoch  2535: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2546: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1290, training loss 0.01256, validation loss 0.01101
Epoch  2557: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2568: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1299
Mean train loss for ascent epoch 1300: -0.026869971305131912
Mean eval for ascent epoch 1300: 0.015378567390143871
Doing Evaluation on the model now
This is Epoch 1300, training loss 0.01538, validation loss 0.01192
Epoch  2579: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1310, training loss 0.35660, validation loss 0.85636
Epoch  2590: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2601: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1320, training loss 0.01996, validation loss 0.01547
Epoch  2612: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2623: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1330, training loss 0.01514, validation loss 0.01423
Epoch  2634: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2645: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1338
Mean train loss for ascent epoch 1339: -0.0446169450879097
Mean eval for ascent epoch 1339: 0.014099439606070518
Doing Evaluation on the model now
This is Epoch 1340, training loss 0.36004, validation loss 0.25007
Epoch  2656: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2667: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1350, training loss 0.06259, validation loss 0.04625
Epoch  2678: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1360, training loss 0.03033, validation loss 0.01421
Epoch  2689: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2700: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1370, training loss 0.01141, validation loss 0.00930
Epoch  2711: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2722: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1377
Mean train loss for ascent epoch 1378: -0.03200744464993477
Mean eval for ascent epoch 1378: 0.00909176841378212
Doing Evaluation on the model now
This is Epoch 1380, training loss 0.29132, validation loss 0.41931
Epoch  2733: reducing learning rate of group 0 to 5.0000e-03.
Epoch  2744: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1390, training loss 0.04871, validation loss 0.09613
Epoch  2755: reducing learning rate of group 0 to 1.2500e-03.
Epoch  2766: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1400, training loss 0.01890, validation loss 0.02016
Resetting learning rate to 0.01000
Epoch  2777: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 1410, training loss 0.01187, validation loss 0.01245
Epoch  2788: reducing learning rate of group 0 to 2.5000e-04.
Epoch  2799: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1420, training loss 0.01010, validation loss 0.00888
Epoch  2810: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1422
Mean train loss for ascent epoch 1423: -0.047837305814027786
Mean eval for ascent epoch 1423: 0.010079064406454563
Epoch  2821: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1430, training loss 0.16763, validation loss 0.13547
Epoch  2832: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2843: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1440, training loss 0.01669, validation loss 0.01575
Epoch  2854: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2865: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1450, training loss 0.01093, validation loss 0.00928
Epoch  2876: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1460, training loss 0.00986, validation loss 0.01051
Epoch  2887: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1461
Mean train loss for ascent epoch 1462: -0.050524551421403885
Mean eval for ascent epoch 1462: 0.012318032793700695
Epoch  2898: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1470, training loss 0.10340, validation loss 0.08047
Epoch  2909: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2920: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1480, training loss 0.03706, validation loss 0.01891
Epoch  2931: reducing learning rate of group 0 to 6.2500e-04.
Epoch  2942: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1490, training loss 0.01180, validation loss 0.01145
Epoch  2953: reducing learning rate of group 0 to 1.5625e-04.
Epoch  2964: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1500
Doing Evaluation on the model now
This is Epoch 1500, training loss 0.00997, validation loss 0.01013
Mean train loss for ascent epoch 1501: -0.0404551737010479
Mean eval for ascent epoch 1501: 0.011470356024801731
Epoch  2975: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1510, training loss 0.10612, validation loss 0.18748
Epoch  2986: reducing learning rate of group 0 to 2.5000e-03.
Epoch  2997: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1520, training loss 0.02653, validation loss 0.01978
Epoch  3008: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3019: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1530, training loss 0.01182, validation loss 0.01433
Epoch  3030: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3041: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1539
Mean train loss for ascent epoch 1540: -0.04256025329232216
Mean eval for ascent epoch 1540: 0.011788222007453442
Doing Evaluation on the model now
This is Epoch 1540, training loss 0.01179, validation loss 0.01186
Epoch  3052: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3063: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1550, training loss 0.32194, validation loss 0.63690
Epoch  3074: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1560, training loss 0.02164, validation loss 0.01838
Epoch  3085: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3096: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1570, training loss 0.01893, validation loss 0.01939
Epoch  3107: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3118: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1578
Mean train loss for ascent epoch 1579: -0.04486522078514099
Mean eval for ascent epoch 1579: 0.011536476202309132
Doing Evaluation on the model now
This is Epoch 1580, training loss 0.34008, validation loss 0.75531
Epoch  3129: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3140: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1590, training loss 0.06289, validation loss 0.08598
Epoch  3151: reducing learning rate of group 0 to 1.2500e-03.
Epoch  3162: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1600, training loss 0.05763, validation loss 0.02545
Resetting learning rate to 0.01000
Epoch  3173: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 1610, training loss 0.01307, validation loss 0.01259
Epoch  3184: reducing learning rate of group 0 to 2.5000e-04.
Epoch  3195: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1620, training loss 0.01028, validation loss 0.01054
Epoch  3206: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1622
Mean train loss for ascent epoch 1623: -0.0421316958963871
Mean eval for ascent epoch 1623: 0.012858639471232891
Epoch  3217: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1630, training loss 0.15654, validation loss 0.18970
Epoch  3228: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3239: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1640, training loss 0.03758, validation loss 0.02435
Epoch  3250: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3261: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1650, training loss 0.01310, validation loss 0.01617
Epoch  3272: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1660, training loss 0.00893, validation loss 0.00938
Epoch  3283: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1661
Mean train loss for ascent epoch 1662: -0.030173851177096367
Mean eval for ascent epoch 1662: 0.009650620631873608
Epoch  3294: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1670, training loss 0.05836, validation loss 0.03778
Epoch  3305: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3316: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1680, training loss 0.01685, validation loss 0.03112
Epoch  3327: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3338: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1690, training loss 0.01496, validation loss 0.01433
Epoch  3349: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3360: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1700
Doing Evaluation on the model now
This is Epoch 1700, training loss 0.00919, validation loss 0.01011
Mean train loss for ascent epoch 1701: -0.03404860571026802
Mean eval for ascent epoch 1701: 0.00984395481646061
Epoch  3371: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1710, training loss 0.13654, validation loss 0.27048
Epoch  3382: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3393: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1720, training loss 0.05026, validation loss 0.09642
Epoch  3404: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3415: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1730, training loss 0.01554, validation loss 0.01115
Epoch  3426: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3437: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1739
Mean train loss for ascent epoch 1740: -0.031787049025297165
Mean eval for ascent epoch 1740: 0.011062284000217915
Doing Evaluation on the model now
This is Epoch 1740, training loss 0.01106, validation loss 0.01249
Epoch  3448: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1750, training loss 0.09254, validation loss 0.05439
Epoch  3459: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3470: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1760, training loss 0.01672, validation loss 0.01249
Epoch  3481: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3492: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1770, training loss 0.00885, validation loss 0.00835
Epoch  3503: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3514: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1778
Mean train loss for ascent epoch 1779: -0.024864329025149345
Mean eval for ascent epoch 1779: 0.008091800846159458
Doing Evaluation on the model now
This is Epoch 1780, training loss 0.19016, validation loss 0.23208
Epoch  3525: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3536: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1790, training loss 0.05787, validation loss 0.03238
Epoch  3547: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1800, training loss 0.04875, validation loss 0.08316
Epoch  3558: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  3569: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 1810, training loss 0.00883, validation loss 0.00981
Epoch  3580: reducing learning rate of group 0 to 2.5000e-04.
Epoch  3591: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 1820, training loss 0.00848, validation loss 0.00777
Epoch  3602: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 1823
Mean train loss for ascent epoch 1824: -0.020116489380598068
Mean eval for ascent epoch 1824: 0.00835912674665451
Epoch  3613: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1830, training loss 0.08925, validation loss 0.07109
Epoch  3624: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3635: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1840, training loss 0.02007, validation loss 0.03417
Epoch  3646: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 1850, training loss 0.01608, validation loss 0.01912
Epoch  3657: reducing learning rate of group 0 to 3.1250e-04.
Epoch  3668: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1860, training loss 0.00913, validation loss 0.00884
Epoch  3679: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1862
Mean train loss for ascent epoch 1863: -0.028224904090166092
Mean eval for ascent epoch 1863: 0.00922467652708292
Epoch  3690: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1870, training loss 0.15159, validation loss 0.14929
Epoch  3701: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3712: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1880, training loss 0.04526, validation loss 0.03515
Epoch  3723: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3734: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1890, training loss 0.01596, validation loss 0.01121
Epoch  3745: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 1900, training loss 0.00881, validation loss 0.01001
Epoch  3756: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1901
Mean train loss for ascent epoch 1902: -0.033417411148548126
Mean eval for ascent epoch 1902: 0.01031004823744297
Epoch  3767: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1910, training loss 0.14109, validation loss 0.02624
Epoch  3778: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3789: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1920, training loss 0.04039, validation loss 0.01938
Epoch  3800: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3811: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1930, training loss 0.01584, validation loss 0.01051
Epoch  3822: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3833: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1940
Doing Evaluation on the model now
This is Epoch 1940, training loss 0.00783, validation loss 0.00773
Mean train loss for ascent epoch 1941: -0.024575822055339813
Mean eval for ascent epoch 1941: 0.00972582958638668
Epoch  3844: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 1950, training loss 0.06948, validation loss 0.05189
Epoch  3855: reducing learning rate of group 0 to 2.5000e-03.
Epoch  3866: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 1960, training loss 0.01158, validation loss 0.00885
Epoch  3877: reducing learning rate of group 0 to 6.2500e-04.
Epoch  3888: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 1970, training loss 0.01007, validation loss 0.01251
Epoch  3899: reducing learning rate of group 0 to 1.5625e-04.
Epoch  3910: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 1979
Mean train loss for ascent epoch 1980: -0.032647013664245605
Mean eval for ascent epoch 1980: 0.008406382985413074
Doing Evaluation on the model now
This is Epoch 1980, training loss 0.00841, validation loss 0.00742
Epoch  3921: reducing learning rate of group 0 to 5.0000e-03.
Epoch  3932: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 1990, training loss 0.15674, validation loss 0.06589
Epoch  3943: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2000, training loss 0.01888, validation loss 0.01040
Resetting learning rate to 0.01000
Epoch  3954: reducing learning rate of group 0 to 5.0000e-04.
Epoch  3965: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2010, training loss 0.00796, validation loss 0.00986
Epoch  3976: reducing learning rate of group 0 to 1.2500e-04.
Epoch  3987: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2018
Mean train loss for ascent epoch 2019: -0.02313845045864582
Mean eval for ascent epoch 2019: 0.007969051599502563
Doing Evaluation on the model now
This is Epoch 2020, training loss 0.06787, validation loss 0.05283
Epoch  3998: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4009: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2030, training loss 0.08481, validation loss 0.05230
Epoch  4020: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4031: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2040, training loss 0.02130, validation loss 0.01652
Epoch  4042: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2050, training loss 0.01133, validation loss 0.01014
Epoch  4053: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4064: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2057
Mean train loss for ascent epoch 2058: -0.026037832722067833
Mean eval for ascent epoch 2058: 0.008979368954896927
Doing Evaluation on the model now
This is Epoch 2060, training loss 0.12270, validation loss 0.06886
Epoch  4075: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4086: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2070, training loss 0.03551, validation loss 0.02152
Epoch  4097: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4108: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2080, training loss 0.01910, validation loss 0.01337
Epoch  4119: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4130: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2090, training loss 0.00902, validation loss 0.00968
Epoch  4141: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2096
Mean train loss for ascent epoch 2097: -0.04069012030959129
Mean eval for ascent epoch 2097: 0.013187360018491745
Doing Evaluation on the model now
This is Epoch 2100, training loss 0.17616, validation loss 0.10549
Epoch  4152: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4163: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2110, training loss 0.02545, validation loss 0.02112
Epoch  4174: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4185: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2120, training loss 0.01761, validation loss 0.01624
Epoch  4196: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4207: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2130, training loss 0.00872, validation loss 0.00758
Epoch  4218: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2135
Mean train loss for ascent epoch 2136: -0.027703391388058662
Mean eval for ascent epoch 2136: 0.009102125652134418
Doing Evaluation on the model now
This is Epoch 2140, training loss 0.30152, validation loss 0.34598
Epoch  4229: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4240: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2150, training loss 0.02327, validation loss 0.03765
Epoch  4251: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4262: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2160, training loss 0.01150, validation loss 0.01779
Epoch  4273: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4284: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2170, training loss 0.00773, validation loss 0.00778
Epoch  4295: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2174
Mean train loss for ascent epoch 2175: -0.023488206788897514
Mean eval for ascent epoch 2175: 0.008607201278209686
Epoch  4306: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2180, training loss 0.04038, validation loss 0.02451
Epoch  4317: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2190, training loss 0.08632, validation loss 0.03037
Epoch  4328: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4339: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2200, training loss 0.02224, validation loss 0.02362
Resetting learning rate to 0.01000
Epoch  4350: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4361: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2210, training loss 0.00847, validation loss 0.00798
Epoch  4372: reducing learning rate of group 0 to 1.2500e-04.
Epoch  4383: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2218
Mean train loss for ascent epoch 2219: -0.030445735901594162
Mean eval for ascent epoch 2219: 0.008877693675458431
Doing Evaluation on the model now
This is Epoch 2220, training loss 0.08011, validation loss 0.19339
Epoch  4394: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4405: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2230, training loss 0.05109, validation loss 0.04265
Epoch  4416: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2240, training loss 0.02919, validation loss 0.01223
Epoch  4427: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4438: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2250, training loss 0.00844, validation loss 0.00744
Epoch  4449: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4460: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2257
Mean train loss for ascent epoch 2258: -0.02816348895430565
Mean eval for ascent epoch 2258: 0.00833888165652752
Doing Evaluation on the model now
This is Epoch 2260, training loss 0.23374, validation loss 0.15729
Epoch  4471: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4482: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2270, training loss 0.03987, validation loss 0.01923
Epoch  4493: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4504: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2280, training loss 0.01915, validation loss 0.02250
Epoch  4515: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2290, training loss 0.00965, validation loss 0.00813
Epoch  4526: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4537: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2296
Mean train loss for ascent epoch 2297: -0.017484840005636215
Mean eval for ascent epoch 2297: 0.007656577508896589
Doing Evaluation on the model now
This is Epoch 2300, training loss 0.56782, validation loss 0.65493
Epoch  4548: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4559: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2310, training loss 0.04549, validation loss 0.02701
Epoch  4570: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4581: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2320, training loss 0.01065, validation loss 0.01110
Epoch  4592: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4603: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2330, training loss 0.00768, validation loss 0.00762
Epoch  4614: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2335
Mean train loss for ascent epoch 2336: -0.025318549945950508
Mean eval for ascent epoch 2336: 0.007316896226257086
Doing Evaluation on the model now
This is Epoch 2340, training loss 0.16545, validation loss 0.22845
Epoch  4625: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4636: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2350, training loss 0.04994, validation loss 0.04089
Epoch  4647: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4658: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2360, training loss 0.01135, validation loss 0.00950
Epoch  4669: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4680: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2370, training loss 0.00729, validation loss 0.00731
Epoch  4691: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2374
Mean train loss for ascent epoch 2375: -0.024456853047013283
Mean eval for ascent epoch 2375: 0.007581913378089666
Epoch  4702: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2380, training loss 0.22044, validation loss 0.07069
Epoch  4713: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2390, training loss 0.04540, validation loss 0.07757
Epoch  4724: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4735: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2400, training loss 0.00832, validation loss 0.00815
Resetting learning rate to 0.01000
Epoch  4746: reducing learning rate of group 0 to 5.0000e-04.
Epoch  4757: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2410, training loss 0.01121, validation loss 0.00756
Epoch  4768: reducing learning rate of group 0 to 1.2500e-04.
Epoch  4779: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2419
Mean train loss for ascent epoch 2420: -0.017079822719097137
Mean eval for ascent epoch 2420: 0.007344048470258713
Doing Evaluation on the model now
This is Epoch 2420, training loss 0.00734, validation loss 0.00805
Epoch  4790: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4801: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2430, training loss 0.05489, validation loss 0.13855
Epoch  4812: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2440, training loss 0.01389, validation loss 0.01103
Epoch  4823: reducing learning rate of group 0 to 6.2500e-04.
Epoch  4834: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2450, training loss 0.00666, validation loss 0.00604
Epoch  4845: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4856: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2458
Mean train loss for ascent epoch 2459: -0.022784775123000145
Mean eval for ascent epoch 2459: 0.007218742277473211
Doing Evaluation on the model now
This is Epoch 2460, training loss 0.13456, validation loss 0.13815
Epoch  4867: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4878: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2470, training loss 0.05945, validation loss 0.08349
Epoch  4889: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4900: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2480, training loss 0.02436, validation loss 0.01003
Epoch  4911: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2490, training loss 0.00678, validation loss 0.00928
Epoch  4922: reducing learning rate of group 0 to 1.5625e-04.
Epoch  4933: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2497
Mean train loss for ascent epoch 2498: -0.015164836309850216
Mean eval for ascent epoch 2498: 0.0061763254925608635
Doing Evaluation on the model now
This is Epoch 2500, training loss 0.33869, validation loss 0.16577
Epoch  4944: reducing learning rate of group 0 to 5.0000e-03.
Epoch  4955: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2510, training loss 0.02776, validation loss 0.01441
Epoch  4966: reducing learning rate of group 0 to 1.2500e-03.
Epoch  4977: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2520, training loss 0.01007, validation loss 0.00877
Epoch  4988: reducing learning rate of group 0 to 3.1250e-04.
Epoch  4999: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2530, training loss 0.00704, validation loss 0.00714
Epoch  5010: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2536
Mean train loss for ascent epoch 2537: -0.021122995764017105
Mean eval for ascent epoch 2537: 0.006768036168068647
Doing Evaluation on the model now
This is Epoch 2540, training loss 0.21510, validation loss 0.40138
Epoch  5021: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5032: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2550, training loss 0.03219, validation loss 0.04345
Epoch  5043: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5054: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2560, training loss 0.01776, validation loss 0.03359
Epoch  5065: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5076: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2570, training loss 0.00812, validation loss 0.00652
Epoch  5087: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2575
Mean train loss for ascent epoch 2576: -0.018432820215821266
Mean eval for ascent epoch 2576: 0.006610270589590073
Doing Evaluation on the model now
This is Epoch 2580, training loss 0.74116, validation loss 0.13025
Epoch  5098: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5109: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2590, training loss 0.01572, validation loss 0.02759
Epoch  5120: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5131: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2600, training loss 0.00943, validation loss 0.01519
Resetting learning rate to 0.01000
Epoch  5142: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5153: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2610, training loss 0.00766, validation loss 0.00735
Epoch  5164: reducing learning rate of group 0 to 1.2500e-04.
Epoch  5175: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2619
Mean train loss for ascent epoch 2620: -0.02993941865861416
Mean eval for ascent epoch 2620: 0.007196693681180477
Doing Evaluation on the model now
This is Epoch 2620, training loss 0.00720, validation loss 0.00659
Epoch  5186: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2630, training loss 0.05930, validation loss 0.01893
Epoch  5197: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5208: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2640, training loss 0.01042, validation loss 0.01031
Epoch  5219: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5230: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2650, training loss 0.00569, validation loss 0.00602
Epoch  5241: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5252: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2658
Mean train loss for ascent epoch 2659: -0.01685188151896
Mean eval for ascent epoch 2659: 0.005395628046244383
Doing Evaluation on the model now
This is Epoch 2660, training loss 0.14842, validation loss 0.08388
Epoch  5263: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5274: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2670, training loss 0.02786, validation loss 0.02222
Epoch  5285: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2680, training loss 0.01290, validation loss 0.01404
Epoch  5296: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5307: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2690, training loss 0.00678, validation loss 0.00750
Epoch  5318: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5329: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2697
Mean train loss for ascent epoch 2698: -0.029827171936631203
Mean eval for ascent epoch 2698: 0.006458653602749109
Doing Evaluation on the model now
This is Epoch 2700, training loss 0.18750, validation loss 0.07507
Epoch  5340: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5351: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2710, training loss 0.02898, validation loss 0.01231
Epoch  5362: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5373: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2720, training loss 0.00877, validation loss 0.00810
Epoch  5384: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2730, training loss 0.00564, validation loss 0.00540
Epoch  5395: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5406: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2736
Mean train loss for ascent epoch 2737: -0.021045658737421036
Mean eval for ascent epoch 2737: 0.00610111141577363
Doing Evaluation on the model now
This is Epoch 2740, training loss 0.21424, validation loss 0.14724
Epoch  5417: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5428: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2750, training loss 0.02376, validation loss 0.03118
Epoch  5439: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5450: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2760, training loss 0.02340, validation loss 0.01765
Epoch  5461: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5472: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2770, training loss 0.00667, validation loss 0.00704
Epoch  5483: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2775
Mean train loss for ascent epoch 2776: -0.015042486600577831
Mean eval for ascent epoch 2776: 0.006632195319980383
Doing Evaluation on the model now
This is Epoch 2780, training loss 0.27606, validation loss 0.69985
Epoch  5494: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5505: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2790, training loss 0.02936, validation loss 0.01639
Epoch  5516: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5527: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2800, training loss 0.01061, validation loss 0.01500
Resetting learning rate to 0.01000
Epoch  5538: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5549: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 2810, training loss 0.00634, validation loss 0.00534
Epoch  5560: reducing learning rate of group 0 to 1.2500e-04.
Epoch  5571: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 2820
Doing Evaluation on the model now
This is Epoch 2820, training loss 0.00640, validation loss 0.00572
Mean train loss for ascent epoch 2821: -0.01724894344806671
Mean eval for ascent epoch 2821: 0.005707812961190939
Epoch  5582: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 2830, training loss 0.04509, validation loss 0.06128
Epoch  5593: reducing learning rate of group 0 to 2.5000e-03.
Epoch  5604: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2840, training loss 0.02156, validation loss 0.01884
Epoch  5615: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5626: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2850, training loss 0.00866, validation loss 0.00984
Epoch  5637: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5648: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2859
Mean train loss for ascent epoch 2860: -0.021625224500894547
Mean eval for ascent epoch 2860: 0.006757635623216629
Doing Evaluation on the model now
This is Epoch 2860, training loss 0.00676, validation loss 0.00607
Epoch  5659: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5670: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2870, training loss 0.03204, validation loss 0.03905
Epoch  5681: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 2880, training loss 0.01135, validation loss 0.00743
Epoch  5692: reducing learning rate of group 0 to 6.2500e-04.
Epoch  5703: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2890, training loss 0.00635, validation loss 0.00595
Epoch  5714: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5725: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2898
Mean train loss for ascent epoch 2899: -0.021153679117560387
Mean eval for ascent epoch 2899: 0.005981969181448221
Doing Evaluation on the model now
This is Epoch 2900, training loss 0.04808, validation loss 0.06583
Epoch  5736: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5747: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2910, training loss 0.02448, validation loss 0.02908
Epoch  5758: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5769: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2920, training loss 0.01517, validation loss 0.01474
Epoch  5780: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 2930, training loss 0.00772, validation loss 0.00629
Epoch  5791: reducing learning rate of group 0 to 1.5625e-04.
Epoch  5802: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2937
Mean train loss for ascent epoch 2938: -0.018039168789982796
Mean eval for ascent epoch 2938: 0.0063252560794353485
Doing Evaluation on the model now
This is Epoch 2940, training loss 0.27186, validation loss 0.49757
Epoch  5813: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5824: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2950, training loss 0.04067, validation loss 0.04230
Epoch  5835: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5846: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 2960, training loss 0.00786, validation loss 0.01150
Epoch  5857: reducing learning rate of group 0 to 3.1250e-04.
Epoch  5868: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 2970, training loss 0.00550, validation loss 0.00577
Epoch  5879: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 2976
Mean train loss for ascent epoch 2977: -0.024922987446188927
Mean eval for ascent epoch 2977: 0.005798713769763708
Doing Evaluation on the model now
This is Epoch 2980, training loss 0.11694, validation loss 0.14957
Epoch  5890: reducing learning rate of group 0 to 5.0000e-03.
Epoch  5901: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 2990, training loss 0.04671, validation loss 0.04430
Epoch  5912: reducing learning rate of group 0 to 1.2500e-03.
Epoch  5923: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3000, training loss 0.00681, validation loss 0.00617
Resetting learning rate to 0.01000
Epoch  5934: reducing learning rate of group 0 to 5.0000e-04.
Epoch  5945: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3010, training loss 0.00642, validation loss 0.00484
Epoch  5956: reducing learning rate of group 0 to 1.2500e-04.
Epoch  5967: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3020
Doing Evaluation on the model now
This is Epoch 3020, training loss 0.00521, validation loss 0.00460
Mean train loss for ascent epoch 3021: -0.017588673159480095
Mean eval for ascent epoch 3021: 0.006055844482034445
Epoch  5978: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3030, training loss 0.06056, validation loss 0.04453
Epoch  5989: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6000: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3040, training loss 0.02069, validation loss 0.01422
Epoch  6011: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6022: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3050, training loss 0.00805, validation loss 0.00821
Epoch  6033: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6044: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3059
Mean train loss for ascent epoch 3060: -0.022256458178162575
Mean eval for ascent epoch 3060: 0.007493291981518269
Doing Evaluation on the model now
This is Epoch 3060, training loss 0.00749, validation loss 0.00538
Epoch  6055: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3070, training loss 0.06606, validation loss 0.06696
Epoch  6066: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6077: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3080, training loss 0.00755, validation loss 0.00983
Epoch  6088: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6099: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3090, training loss 0.00501, validation loss 0.00468
Epoch  6110: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6121: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3098
Mean train loss for ascent epoch 3099: -0.021066121757030487
Mean eval for ascent epoch 3099: 0.005304110702127218
Doing Evaluation on the model now
This is Epoch 3100, training loss 0.08411, validation loss 0.09609
Epoch  6132: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6143: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3110, training loss 0.01845, validation loss 0.03334
Epoch  6154: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3120, training loss 0.00900, validation loss 0.00728
Epoch  6165: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6176: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3130, training loss 0.00658, validation loss 0.00574
Epoch  6187: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6198: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3137
Mean train loss for ascent epoch 3138: -0.026212042197585106
Mean eval for ascent epoch 3138: 0.0054423087276518345
Doing Evaluation on the model now
This is Epoch 3140, training loss 0.27510, validation loss 0.06721
Epoch  6209: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6220: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3150, training loss 0.03773, validation loss 0.03691
Epoch  6231: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6242: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3160, training loss 0.01004, validation loss 0.01081
Epoch  6253: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3170, training loss 0.00638, validation loss 0.00576
Epoch  6264: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6275: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3176
Mean train loss for ascent epoch 3177: -0.021415336057543755
Mean eval for ascent epoch 3177: 0.005581161472946405
Doing Evaluation on the model now
This is Epoch 3180, training loss 0.10274, validation loss 0.12250
Epoch  6286: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6297: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3190, training loss 0.01015, validation loss 0.01316
Epoch  6308: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6319: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3200, training loss 0.00795, validation loss 0.00620
Resetting learning rate to 0.01000
Epoch  6330: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6341: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3210, training loss 0.00962, validation loss 0.00782
Epoch  6352: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3220, training loss 0.00544, validation loss 0.00529
Epoch  6363: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3221
Mean train loss for ascent epoch 3222: -0.019794445484876633
Mean eval for ascent epoch 3222: 0.005356512498110533
Epoch  6374: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3230, training loss 0.07271, validation loss 0.07608
Epoch  6385: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6396: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3240, training loss 0.01194, validation loss 0.01846
Epoch  6407: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6418: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3250, training loss 0.00588, validation loss 0.00500
Epoch  6429: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6440: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3260
Doing Evaluation on the model now
This is Epoch 3260, training loss 0.00510, validation loss 0.00588
Mean train loss for ascent epoch 3261: -0.0157474335283041
Mean eval for ascent epoch 3261: 0.005534186493605375
Epoch  6451: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3270, training loss 0.06227, validation loss 0.06596
Epoch  6462: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6473: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3280, training loss 0.01477, validation loss 0.01254
Epoch  6484: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6495: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3290, training loss 0.00541, validation loss 0.00675
Epoch  6506: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6517: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3299
Mean train loss for ascent epoch 3300: -0.021040426567196846
Mean eval for ascent epoch 3300: 0.005468361545354128
Doing Evaluation on the model now
This is Epoch 3300, training loss 0.00547, validation loss 0.00488
Epoch  6528: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6539: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3310, training loss 0.04199, validation loss 0.02075
Epoch  6550: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3320, training loss 0.00984, validation loss 0.00491
Epoch  6561: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6572: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3330, training loss 0.00550, validation loss 0.00558
Epoch  6583: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6594: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3338
Mean train loss for ascent epoch 3339: -0.023856565356254578
Mean eval for ascent epoch 3339: 0.0051795728504657745
Doing Evaluation on the model now
This is Epoch 3340, training loss 0.04834, validation loss 0.02011
Epoch  6605: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6616: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3350, training loss 0.03425, validation loss 0.02525
Epoch  6627: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6638: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3360, training loss 0.01550, validation loss 0.01641
Epoch  6649: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3370, training loss 0.00575, validation loss 0.00785
Epoch  6660: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6671: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3377
Mean train loss for ascent epoch 3378: -0.013170301914215088
Mean eval for ascent epoch 3378: 0.004775245673954487
Doing Evaluation on the model now
This is Epoch 3380, training loss 0.21904, validation loss 0.13948
Epoch  6682: reducing learning rate of group 0 to 5.0000e-03.
Epoch  6693: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3390, training loss 0.07505, validation loss 0.09722
Epoch  6704: reducing learning rate of group 0 to 1.2500e-03.
Epoch  6715: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3400, training loss 0.00715, validation loss 0.00867
Resetting learning rate to 0.01000
Epoch  6726: reducing learning rate of group 0 to 5.0000e-04.
Epoch  6737: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 3410, training loss 0.00715, validation loss 0.00596
Epoch  6748: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3420, training loss 0.00511, validation loss 0.00526
Epoch  6759: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3421
Mean train loss for ascent epoch 3422: -0.012589924968779087
Mean eval for ascent epoch 3422: 0.005682084709405899
Epoch  6770: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3430, training loss 0.10436, validation loss 0.02660
Epoch  6781: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6792: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3440, training loss 0.02224, validation loss 0.01828
Epoch  6803: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6814: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3450, training loss 0.00738, validation loss 0.00717
Epoch  6825: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6836: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3460
Doing Evaluation on the model now
This is Epoch 3460, training loss 0.00510, validation loss 0.00475
Mean train loss for ascent epoch 3461: -0.01291909534484148
Mean eval for ascent epoch 3461: 0.005181002896279097
Epoch  6847: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3470, training loss 0.06202, validation loss 0.03585
Epoch  6858: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6869: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3480, training loss 0.01415, validation loss 0.02494
Epoch  6880: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6891: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3490, training loss 0.00573, validation loss 0.00637
Epoch  6902: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6913: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3499
Mean train loss for ascent epoch 3500: -0.016089240089058876
Mean eval for ascent epoch 3500: 0.006567575503140688
Doing Evaluation on the model now
This is Epoch 3500, training loss 0.00657, validation loss 0.00621
Epoch  6924: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3510, training loss 0.04322, validation loss 0.03667
Epoch  6935: reducing learning rate of group 0 to 2.5000e-03.
Epoch  6946: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3520, training loss 0.01665, validation loss 0.01003
Epoch  6957: reducing learning rate of group 0 to 6.2500e-04.
Epoch  6968: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3530, training loss 0.00665, validation loss 0.00549
Epoch  6979: reducing learning rate of group 0 to 1.5625e-04.
Epoch  6990: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3538
Mean train loss for ascent epoch 3539: -0.0213626679033041
Mean eval for ascent epoch 3539: 0.006135585252195597
Doing Evaluation on the model now
This is Epoch 3540, training loss 0.07390, validation loss 0.06351
Epoch  7001: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7012: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3550, training loss 0.03165, validation loss 0.01847
Epoch  7023: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3560, training loss 0.01215, validation loss 0.01509
Epoch  7034: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7045: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3570, training loss 0.00599, validation loss 0.00543
Epoch  7056: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7067: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3577
Mean train loss for ascent epoch 3578: -0.012922551482915878
Mean eval for ascent epoch 3578: 0.004501292482018471
Doing Evaluation on the model now
This is Epoch 3580, training loss 0.06001, validation loss 0.04774
Epoch  7078: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7089: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3590, training loss 0.02259, validation loss 0.05489
Epoch  7100: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7111: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3600, training loss 0.00989, validation loss 0.00693
Resetting learning rate to 0.01000
Epoch  7122: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 3610, training loss 0.00631, validation loss 0.00529
Epoch  7133: reducing learning rate of group 0 to 2.5000e-04.
Epoch  7144: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3620, training loss 0.00459, validation loss 0.00521
Epoch  7155: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3622
Mean train loss for ascent epoch 3623: -0.013256457634270191
Mean eval for ascent epoch 3623: 0.0055149313993752
Epoch  7166: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3630, training loss 0.06125, validation loss 0.04148
Epoch  7177: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7188: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3640, training loss 0.02248, validation loss 0.01240
Epoch  7199: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7210: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3650, training loss 0.00577, validation loss 0.00493
Epoch  7221: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3660, training loss 0.00568, validation loss 0.00459
Epoch  7232: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3661
Mean train loss for ascent epoch 3662: -0.022945024073123932
Mean eval for ascent epoch 3662: 0.005514098796993494
Epoch  7243: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3670, training loss 0.14007, validation loss 0.39546
Epoch  7254: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7265: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3680, training loss 0.01258, validation loss 0.00605
Epoch  7276: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7287: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3690, training loss 0.00533, validation loss 0.00476
Epoch  7298: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7309: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3700
Doing Evaluation on the model now
This is Epoch 3700, training loss 0.00503, validation loss 0.00526
Mean train loss for ascent epoch 3701: -0.012019720859825611
Mean eval for ascent epoch 3701: 0.005090712569653988
Epoch  7320: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3710, training loss 0.05273, validation loss 0.08405
Epoch  7331: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7342: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3720, training loss 0.01051, validation loss 0.00776
Epoch  7353: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7364: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3730, training loss 0.00470, validation loss 0.00455
Epoch  7375: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7386: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3739
Mean train loss for ascent epoch 3740: -0.01494046114385128
Mean eval for ascent epoch 3740: 0.004739442374557257
Doing Evaluation on the model now
This is Epoch 3740, training loss 0.00474, validation loss 0.00429
Epoch  7397: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7408: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3750, training loss 0.09152, validation loss 0.10071
Epoch  7419: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3760, training loss 0.00686, validation loss 0.00690
Epoch  7430: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7441: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3770, training loss 0.00557, validation loss 0.00471
Epoch  7452: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7463: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3778
Mean train loss for ascent epoch 3779: -0.02130594663321972
Mean eval for ascent epoch 3779: 0.00529348012059927
Doing Evaluation on the model now
This is Epoch 3780, training loss 0.06698, validation loss 0.05834
Epoch  7474: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7485: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3790, training loss 0.02021, validation loss 0.01024
Epoch  7496: reducing learning rate of group 0 to 1.2500e-03.
Epoch  7507: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 3800, training loss 0.00957, validation loss 0.00959
Resetting learning rate to 0.01000
Epoch  7518: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 3810, training loss 0.00541, validation loss 0.00685
Epoch  7529: reducing learning rate of group 0 to 2.5000e-04.
Epoch  7540: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 3820, training loss 0.00534, validation loss 0.00581
Epoch  7551: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 3822
Mean train loss for ascent epoch 3823: -0.01784954033792019
Mean eval for ascent epoch 3823: 0.005597942974418402
Epoch  7562: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3830, training loss 0.08109, validation loss 0.04049
Epoch  7573: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7584: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3840, training loss 0.02950, validation loss 0.00821
Epoch  7595: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7606: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3850, training loss 0.00621, validation loss 0.00549
Epoch  7617: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 3860, training loss 0.00484, validation loss 0.00486
Epoch  7628: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3861
Mean train loss for ascent epoch 3862: -0.014143655076622963
Mean eval for ascent epoch 3862: 0.005318261217325926
Epoch  7639: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3870, training loss 0.05772, validation loss 0.09735
Epoch  7650: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7661: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3880, training loss 0.03339, validation loss 0.01557
Epoch  7672: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7683: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3890, training loss 0.00654, validation loss 0.00584
Epoch  7694: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7705: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3900
Doing Evaluation on the model now
This is Epoch 3900, training loss 0.00500, validation loss 0.00520
Mean train loss for ascent epoch 3901: -0.0121736079454422
Mean eval for ascent epoch 3901: 0.005095182452350855
Epoch  7716: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3910, training loss 0.06535, validation loss 0.05758
Epoch  7727: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7738: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3920, training loss 0.00964, validation loss 0.00652
Epoch  7749: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7760: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3930, training loss 0.00609, validation loss 0.00598
Epoch  7771: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7782: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3939
Mean train loss for ascent epoch 3940: -0.014996464364230633
Mean eval for ascent epoch 3940: 0.004925228655338287
Doing Evaluation on the model now
This is Epoch 3940, training loss 0.00493, validation loss 0.00473
Epoch  7793: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 3950, training loss 0.05700, validation loss 0.03675
Epoch  7804: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7815: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 3960, training loss 0.01072, validation loss 0.01194
Epoch  7826: reducing learning rate of group 0 to 6.2500e-04.
Epoch  7837: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 3970, training loss 0.00498, validation loss 0.00498
Epoch  7848: reducing learning rate of group 0 to 1.5625e-04.
Epoch  7859: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 3978
Mean train loss for ascent epoch 3979: -0.019621968269348145
Mean eval for ascent epoch 3979: 0.0051026479341089725
Doing Evaluation on the model now
This is Epoch 3980, training loss 0.03786, validation loss 0.02968
Epoch  7870: reducing learning rate of group 0 to 5.0000e-03.
Epoch  7881: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 3990, training loss 0.03437, validation loss 0.01481
Epoch  7892: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4000, training loss 0.00820, validation loss 0.01253
Epoch  7903: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch  7914: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 4010, training loss 0.00629, validation loss 0.00924
Epoch  7925: reducing learning rate of group 0 to 2.5000e-04.
Epoch  7936: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 4020, training loss 0.00536, validation loss 0.00660
Epoch  7947: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4023
Mean train loss for ascent epoch 4024: -0.017842857167124748
Mean eval for ascent epoch 4024: 0.006969688925892115
Epoch  7958: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4030, training loss 0.08995, validation loss 0.04260
Epoch  7969: reducing learning rate of group 0 to 2.5000e-03.
Epoch  7980: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4040, training loss 0.01784, validation loss 0.01883
Epoch  7991: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4050, training loss 0.00805, validation loss 0.00561
Epoch  8002: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8013: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4060, training loss 0.00513, validation loss 0.00562
Epoch  8024: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4062
Mean train loss for ascent epoch 4063: -0.020787501707673073
Mean eval for ascent epoch 4063: 0.006116652861237526
Epoch  8035: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4070, training loss 0.02534, validation loss 0.03167
Epoch  8046: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8057: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4080, training loss 0.01753, validation loss 0.02230
Epoch  8068: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8079: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4090, training loss 0.00850, validation loss 0.00903
Epoch  8090: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4100, training loss 0.00469, validation loss 0.00459
Epoch  8101: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4101
Mean train loss for ascent epoch 4102: -0.019255027174949646
Mean eval for ascent epoch 4102: 0.005325370468199253
Epoch  8112: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4110, training loss 0.03921, validation loss 0.03258
Epoch  8123: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8134: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4120, training loss 0.02091, validation loss 0.01543
Epoch  8145: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8156: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4130, training loss 0.00555, validation loss 0.00580
Epoch  8167: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8178: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4140
Doing Evaluation on the model now
This is Epoch 4140, training loss 0.00504, validation loss 0.00512
Mean train loss for ascent epoch 4141: -0.01721528172492981
Mean eval for ascent epoch 4141: 0.005299785640090704
Epoch  8189: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4150, training loss 0.04348, validation loss 0.05868
Epoch  8200: reducing learning rate of group 0 to 2.5000e-03.
Epoch  8211: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4160, training loss 0.01205, validation loss 0.01867
Epoch  8222: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8233: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4170, training loss 0.00593, validation loss 0.00536
Epoch  8244: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8255: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4179
Mean train loss for ascent epoch 4180: -0.012926722876727581
Mean eval for ascent epoch 4180: 0.004915923811495304
Doing Evaluation on the model now
This is Epoch 4180, training loss 0.00492, validation loss 0.00511
Epoch  8266: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8277: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4190, training loss 0.07857, validation loss 0.03845
Epoch  8288: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4200, training loss 0.00988, validation loss 0.00667
Resetting learning rate to 0.01000
Epoch  8299: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8310: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4210, training loss 0.00629, validation loss 0.00633
Epoch  8321: reducing learning rate of group 0 to 1.2500e-04.
Epoch  8332: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4218
Mean train loss for ascent epoch 4219: -0.016403453424572945
Mean eval for ascent epoch 4219: 0.005271212663501501
Doing Evaluation on the model now
This is Epoch 4220, training loss 0.05527, validation loss 0.04770
Epoch  8343: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8354: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4230, training loss 0.03942, validation loss 0.01263
Epoch  8365: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8376: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4240, training loss 0.01610, validation loss 0.01002
Epoch  8387: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4250, training loss 0.00520, validation loss 0.00543
Epoch  8398: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8409: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4257
Mean train loss for ascent epoch 4258: -0.0181258711963892
Mean eval for ascent epoch 4258: 0.005546159576624632
Doing Evaluation on the model now
This is Epoch 4260, training loss 0.73254, validation loss 0.46497
Epoch  8420: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8431: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4270, training loss 0.02694, validation loss 0.01648
Epoch  8442: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8453: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4280, training loss 0.01090, validation loss 0.00802
Epoch  8464: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8475: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4290, training loss 0.00535, validation loss 0.00543
Epoch  8486: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4296
Mean train loss for ascent epoch 4297: -0.01201077364385128
Mean eval for ascent epoch 4297: 0.0051902965642511845
Doing Evaluation on the model now
This is Epoch 4300, training loss 0.31572, validation loss 0.15597
Epoch  8497: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8508: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4310, training loss 0.02787, validation loss 0.03081
Epoch  8519: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8530: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4320, training loss 0.00788, validation loss 0.00733
Epoch  8541: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8552: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4330, training loss 0.00511, validation loss 0.00553
Epoch  8563: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4335
Mean train loss for ascent epoch 4336: -0.021105604246258736
Mean eval for ascent epoch 4336: 0.00526644429191947
Doing Evaluation on the model now
This is Epoch 4340, training loss 0.24669, validation loss 0.06497
Epoch  8574: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8585: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4350, training loss 0.01706, validation loss 0.00890
Epoch  8596: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8607: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4360, training loss 0.01201, validation loss 0.01844
Epoch  8618: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8629: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4370, training loss 0.00522, validation loss 0.00495
Epoch  8640: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4374
Mean train loss for ascent epoch 4375: -0.016369879245758057
Mean eval for ascent epoch 4375: 0.00507129542529583
Epoch  8651: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4380, training loss 0.08091, validation loss 0.10712
Epoch  8662: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4390, training loss 0.01986, validation loss 0.02499
Epoch  8673: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8684: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4400, training loss 0.00750, validation loss 0.00870
Resetting learning rate to 0.01000
Epoch  8695: reducing learning rate of group 0 to 5.0000e-04.
Epoch  8706: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4410, training loss 0.00614, validation loss 0.00549
Epoch  8717: reducing learning rate of group 0 to 1.2500e-04.
Epoch  8728: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4418
Mean train loss for ascent epoch 4419: -0.019194502383470535
Mean eval for ascent epoch 4419: 0.0052385637536644936
Doing Evaluation on the model now
This is Epoch 4420, training loss 0.06746, validation loss 0.03386
Epoch  8739: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8750: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4430, training loss 0.06838, validation loss 0.04677
Epoch  8761: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4440, training loss 0.01564, validation loss 0.02566
Epoch  8772: reducing learning rate of group 0 to 6.2500e-04.
Epoch  8783: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4450, training loss 0.00612, validation loss 0.00550
Epoch  8794: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8805: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4457
Mean train loss for ascent epoch 4458: -0.0138015728443861
Mean eval for ascent epoch 4458: 0.005347915925085545
Doing Evaluation on the model now
This is Epoch 4460, training loss 0.17288, validation loss 0.15811
Epoch  8816: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8827: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4470, training loss 0.04966, validation loss 0.03539
Epoch  8838: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8849: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4480, training loss 0.00807, validation loss 0.01001
Epoch  8860: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4490, training loss 0.00648, validation loss 0.00636
Epoch  8871: reducing learning rate of group 0 to 1.5625e-04.
Epoch  8882: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4496
Mean train loss for ascent epoch 4497: -0.01206312794238329
Mean eval for ascent epoch 4497: 0.005674012470990419
Doing Evaluation on the model now
This is Epoch 4500, training loss 0.20782, validation loss 0.18937
Epoch  8893: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8904: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4510, training loss 0.02374, validation loss 0.01776
Epoch  8915: reducing learning rate of group 0 to 1.2500e-03.
Epoch  8926: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4520, training loss 0.00673, validation loss 0.00678
Epoch  8937: reducing learning rate of group 0 to 3.1250e-04.
Epoch  8948: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4530, training loss 0.00583, validation loss 0.00498
Epoch  8959: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4535
Mean train loss for ascent epoch 4536: -0.011779418215155602
Mean eval for ascent epoch 4536: 0.005421958398073912
Doing Evaluation on the model now
This is Epoch 4540, training loss 0.19093, validation loss 0.16921
Epoch  8970: reducing learning rate of group 0 to 5.0000e-03.
Epoch  8981: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4550, training loss 0.02792, validation loss 0.01175
Epoch  8992: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9003: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4560, training loss 0.00621, validation loss 0.00606
Epoch  9014: reducing learning rate of group 0 to 3.1250e-04.
Epoch  9025: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4570, training loss 0.00485, validation loss 0.00474
Epoch  9036: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4574
Mean train loss for ascent epoch 4575: -0.011385954916477203
Mean eval for ascent epoch 4575: 0.005487898364663124
Epoch  9047: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4580, training loss 0.22800, validation loss 0.07282
Epoch  9058: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4590, training loss 0.01865, validation loss 0.01720
Epoch  9069: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9080: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4600, training loss 0.00657, validation loss 0.00825
Resetting learning rate to 0.01000
Epoch  9091: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9102: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4610, training loss 0.00687, validation loss 0.00515
Epoch  9113: reducing learning rate of group 0 to 1.2500e-04.
Epoch  9124: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4619
Mean train loss for ascent epoch 4620: -0.011461022309958935
Mean eval for ascent epoch 4620: 0.005258168093860149
Doing Evaluation on the model now
This is Epoch 4620, training loss 0.00526, validation loss 0.00560
Epoch  9135: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9146: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4630, training loss 0.06498, validation loss 0.19858
Epoch  9157: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4640, training loss 0.00767, validation loss 0.00909
Epoch  9168: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9179: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4650, training loss 0.00616, validation loss 0.00579
Epoch  9190: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9201: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4658
Mean train loss for ascent epoch 4659: -0.015426439233124256
Mean eval for ascent epoch 4659: 0.005807516630738974
Doing Evaluation on the model now
This is Epoch 4660, training loss 0.12178, validation loss 0.06725
Epoch  9212: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9223: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4670, training loss 0.06211, validation loss 0.02072
Epoch  9234: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9245: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4680, training loss 0.01166, validation loss 0.01050
Epoch  9256: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4690, training loss 0.00667, validation loss 0.00691
Epoch  9267: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9278: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4697
Mean train loss for ascent epoch 4698: -0.025232648476958275
Mean eval for ascent epoch 4698: 0.006947367452085018
Doing Evaluation on the model now
This is Epoch 4700, training loss 0.11507, validation loss 0.09283
Epoch  9289: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9300: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4710, training loss 0.03436, validation loss 0.02762
Epoch  9311: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9322: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4720, training loss 0.01296, validation loss 0.01025
Epoch  9333: reducing learning rate of group 0 to 3.1250e-04.
Epoch  9344: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4730, training loss 0.00696, validation loss 0.01098
Epoch  9355: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4736
Mean train loss for ascent epoch 4737: -0.019000692293047905
Mean eval for ascent epoch 4737: 0.0049194046296179295
Doing Evaluation on the model now
This is Epoch 4740, training loss 0.30838, validation loss 1.03419
Epoch  9366: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9377: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4750, training loss 0.02758, validation loss 0.02937
Epoch  9388: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9399: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4760, training loss 0.00879, validation loss 0.00890
Epoch  9410: reducing learning rate of group 0 to 3.1250e-04.
Epoch  9421: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4770, training loss 0.00534, validation loss 0.00663
Epoch  9432: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4775
Mean train loss for ascent epoch 4776: -0.013331937603652477
Mean eval for ascent epoch 4776: 0.006420365534722805
Doing Evaluation on the model now
This is Epoch 4780, training loss 1.23356, validation loss 8.40435
Epoch  9443: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9454: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4790, training loss 0.03920, validation loss 0.04187
Epoch  9465: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9476: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4800, training loss 0.01497, validation loss 0.01357
Resetting learning rate to 0.01000
Epoch  9487: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9498: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 4810, training loss 0.00588, validation loss 0.00655
Epoch  9509: reducing learning rate of group 0 to 1.2500e-04.
Epoch  9520: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 4819
Mean train loss for ascent epoch 4820: -0.014544044621288776
Mean eval for ascent epoch 4820: 0.006260162219405174
Doing Evaluation on the model now
This is Epoch 4820, training loss 0.00626, validation loss 0.00507
Epoch  9531: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 4830, training loss 0.09226, validation loss 0.05695
Epoch  9542: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9553: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4840, training loss 0.00844, validation loss 0.00737
Epoch  9564: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9575: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4850, training loss 0.00607, validation loss 0.00585
Epoch  9586: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9597: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4858
Mean train loss for ascent epoch 4859: -0.019467569887638092
Mean eval for ascent epoch 4859: 0.005505009554326534
Doing Evaluation on the model now
This is Epoch 4860, training loss 0.09593, validation loss 0.05918
Epoch  9608: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9619: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4870, training loss 0.07358, validation loss 0.04204
Epoch  9630: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 4880, training loss 0.01176, validation loss 0.00689
Epoch  9641: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9652: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4890, training loss 0.00490, validation loss 0.00514
Epoch  9663: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9674: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4897
Mean train loss for ascent epoch 4898: -0.01604103483259678
Mean eval for ascent epoch 4898: 0.005993391387164593
Doing Evaluation on the model now
This is Epoch 4900, training loss 0.44396, validation loss 1.19300
Epoch  9685: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9696: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4910, training loss 0.02800, validation loss 0.01837
Epoch  9707: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9718: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4920, training loss 0.01556, validation loss 0.00786
Epoch  9729: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 4930, training loss 0.00607, validation loss 0.00556
Epoch  9740: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9751: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4936
Mean train loss for ascent epoch 4937: -0.017709415405988693
Mean eval for ascent epoch 4937: 0.0055029005743563175
Doing Evaluation on the model now
This is Epoch 4940, training loss 0.05994, validation loss 0.15455
Epoch  9762: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9773: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4950, training loss 0.03364, validation loss 0.01081
Epoch  9784: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9795: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 4960, training loss 0.00845, validation loss 0.01104
Epoch  9806: reducing learning rate of group 0 to 3.1250e-04.
Epoch  9817: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 4970, training loss 0.00494, validation loss 0.00434
Epoch  9828: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 4975
Mean train loss for ascent epoch 4976: -0.02264459617435932
Mean eval for ascent epoch 4976: 0.005203957669436932
Doing Evaluation on the model now
This is Epoch 4980, training loss 0.18468, validation loss 0.73103
Epoch  9839: reducing learning rate of group 0 to 5.0000e-03.
Epoch  9850: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 4990, training loss 0.02948, validation loss 0.03303
Epoch  9861: reducing learning rate of group 0 to 1.2500e-03.
Epoch  9872: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5000, training loss 0.00797, validation loss 0.00983
Resetting learning rate to 0.01000
Epoch  9883: reducing learning rate of group 0 to 5.0000e-04.
Epoch  9894: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5010, training loss 0.00513, validation loss 0.00522
Epoch  9905: reducing learning rate of group 0 to 1.2500e-04.
Epoch  9916: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5020
Doing Evaluation on the model now
This is Epoch 5020, training loss 0.00486, validation loss 0.00457
Mean train loss for ascent epoch 5021: -0.0159088633954525
Mean eval for ascent epoch 5021: 0.004912655334919691
Epoch  9927: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5030, training loss 0.08579, validation loss 0.04099
Epoch  9938: reducing learning rate of group 0 to 2.5000e-03.
Epoch  9949: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5040, training loss 0.01194, validation loss 0.01255
Epoch  9960: reducing learning rate of group 0 to 6.2500e-04.
Epoch  9971: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5050, training loss 0.00507, validation loss 0.00528
Epoch  9982: reducing learning rate of group 0 to 1.5625e-04.
Epoch  9993: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5059
Mean train loss for ascent epoch 5060: -0.011072134599089622
Mean eval for ascent epoch 5060: 0.00545685226097703
Doing Evaluation on the model now
This is Epoch 5060, training loss 0.00546, validation loss 0.00437
Epoch 10004: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10015: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5070, training loss 0.05963, validation loss 0.04671
Epoch 10026: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5080, training loss 0.01491, validation loss 0.02139
Epoch 10037: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10048: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5090, training loss 0.00769, validation loss 0.01322
Epoch 10059: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10070: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5098
Mean train loss for ascent epoch 5099: -0.011967492289841175
Mean eval for ascent epoch 5099: 0.0051012346521019936
Doing Evaluation on the model now
This is Epoch 5100, training loss 0.15822, validation loss 0.34515
Epoch 10081: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10092: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5110, training loss 0.04056, validation loss 0.06489
Epoch 10103: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10114: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5120, training loss 0.01697, validation loss 0.01956
Epoch 10125: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5130, training loss 0.00550, validation loss 0.00784
Epoch 10136: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10147: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5137
Mean train loss for ascent epoch 5138: -0.014027831144630909
Mean eval for ascent epoch 5138: 0.0059920912608504295
Doing Evaluation on the model now
This is Epoch 5140, training loss 0.32500, validation loss 0.29797
Epoch 10158: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10169: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5150, training loss 0.03303, validation loss 0.01048
Epoch 10180: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10191: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5160, training loss 0.00883, validation loss 0.00765
Epoch 10202: reducing learning rate of group 0 to 3.1250e-04.
Epoch 10213: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5170, training loss 0.00633, validation loss 0.00600
Epoch 10224: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5176
Mean train loss for ascent epoch 5177: -0.010205980390310287
Mean eval for ascent epoch 5177: 0.004878201987594366
Doing Evaluation on the model now
This is Epoch 5180, training loss 0.34290, validation loss 0.28617
Epoch 10235: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10246: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5190, training loss 0.02833, validation loss 0.05652
Epoch 10257: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10268: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5200, training loss 0.00581, validation loss 0.00601
Resetting learning rate to 0.01000
Epoch 10279: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10290: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5210, training loss 0.00565, validation loss 0.00480
Epoch 10301: reducing learning rate of group 0 to 1.2500e-04.
Epoch 10312: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5220
Doing Evaluation on the model now
This is Epoch 5220, training loss 0.00501, validation loss 0.00522
Mean train loss for ascent epoch 5221: -0.018808836117386818
Mean eval for ascent epoch 5221: 0.005227380897849798
Epoch 10323: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5230, training loss 0.05410, validation loss 0.02692
Epoch 10334: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10345: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5240, training loss 0.01818, validation loss 0.02399
Epoch 10356: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10367: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5250, training loss 0.00763, validation loss 0.00675
Epoch 10378: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10389: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5259
Mean train loss for ascent epoch 5260: -0.01320833433419466
Mean eval for ascent epoch 5260: 0.005742973182350397
Doing Evaluation on the model now
This is Epoch 5260, training loss 0.00574, validation loss 0.00505
Epoch 10400: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5270, training loss 0.07577, validation loss 0.06368
Epoch 10411: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10422: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5280, training loss 0.01647, validation loss 0.01465
Epoch 10433: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10444: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5290, training loss 0.00681, validation loss 0.00777
Epoch 10455: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10466: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5298
Mean train loss for ascent epoch 5299: -0.016900161281228065
Mean eval for ascent epoch 5299: 0.0058227465488016605
Doing Evaluation on the model now
This is Epoch 5300, training loss 0.19515, validation loss 0.27924
Epoch 10477: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10488: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5310, training loss 0.02415, validation loss 0.04205
Epoch 10499: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5320, training loss 0.01670, validation loss 0.02326
Epoch 10510: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10521: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5330, training loss 0.00610, validation loss 0.00598
Epoch 10532: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10543: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5337
Mean train loss for ascent epoch 5338: -0.02245253324508667
Mean eval for ascent epoch 5338: 0.005431992467492819
Doing Evaluation on the model now
This is Epoch 5340, training loss 0.73292, validation loss 1.02613
Epoch 10554: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10565: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5350, training loss 0.01565, validation loss 0.01154
Epoch 10576: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10587: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5360, training loss 0.00739, validation loss 0.00679
Epoch 10598: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5370, training loss 0.00619, validation loss 0.00510
Epoch 10609: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10620: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5376
Mean train loss for ascent epoch 5377: -0.025392459705471992
Mean eval for ascent epoch 5377: 0.0061221979558467865
Doing Evaluation on the model now
This is Epoch 5380, training loss 0.47331, validation loss 0.35006
Epoch 10631: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10642: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5390, training loss 0.02279, validation loss 0.02039
Epoch 10653: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10664: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5400, training loss 0.00909, validation loss 0.00839
Resetting learning rate to 0.01000
Epoch 10675: reducing learning rate of group 0 to 5.0000e-04.
Epoch 10686: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5410, training loss 0.00837, validation loss 0.00785
Epoch 10697: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5420, training loss 0.00612, validation loss 0.00561
Epoch 10708: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5421
Mean train loss for ascent epoch 5422: -0.010192361660301685
Mean eval for ascent epoch 5422: 0.005456885788589716
Epoch 10719: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5430, training loss 0.04294, validation loss 0.02721
Epoch 10730: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10741: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5440, training loss 0.01900, validation loss 0.01414
Epoch 10752: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10763: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5450, training loss 0.00547, validation loss 0.00585
Epoch 10774: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10785: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5460
Doing Evaluation on the model now
This is Epoch 5460, training loss 0.00624, validation loss 0.00523
Mean train loss for ascent epoch 5461: -0.011311816982924938
Mean eval for ascent epoch 5461: 0.0051357983611524105
Epoch 10796: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5470, training loss 0.05592, validation loss 0.04771
Epoch 10807: reducing learning rate of group 0 to 2.5000e-03.
Epoch 10818: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5480, training loss 0.01424, validation loss 0.01223
Epoch 10829: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10840: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5490, training loss 0.00568, validation loss 0.00589
Epoch 10851: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10862: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5499
Mean train loss for ascent epoch 5500: -0.012524137273430824
Mean eval for ascent epoch 5500: 0.006376456003636122
Doing Evaluation on the model now
This is Epoch 5500, training loss 0.00638, validation loss 0.00502
Epoch 10873: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10884: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5510, training loss 0.06327, validation loss 0.02477
Epoch 10895: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5520, training loss 0.01413, validation loss 0.01157
Epoch 10906: reducing learning rate of group 0 to 6.2500e-04.
Epoch 10917: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5530, training loss 0.00706, validation loss 0.00578
Epoch 10928: reducing learning rate of group 0 to 1.5625e-04.
Epoch 10939: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5538
Mean train loss for ascent epoch 5539: -0.011246987618505955
Mean eval for ascent epoch 5539: 0.005605819635093212
Doing Evaluation on the model now
This is Epoch 5540, training loss 0.10550, validation loss 0.04987
Epoch 10950: reducing learning rate of group 0 to 5.0000e-03.
Epoch 10961: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5550, training loss 0.04032, validation loss 0.02131
Epoch 10972: reducing learning rate of group 0 to 1.2500e-03.
Epoch 10983: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5560, training loss 0.01045, validation loss 0.00918
Epoch 10994: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5570, training loss 0.00537, validation loss 0.00513
Epoch 11005: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11016: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5577
Mean train loss for ascent epoch 5578: -0.014406117610633373
Mean eval for ascent epoch 5578: 0.005677143111824989
Doing Evaluation on the model now
This is Epoch 5580, training loss 0.15788, validation loss 0.35999
Epoch 11027: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11038: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5590, training loss 0.02013, validation loss 0.02872
Epoch 11049: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11060: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5600, training loss 0.01007, validation loss 0.00635
Resetting learning rate to 0.01000
Epoch 11071: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11082: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 5610, training loss 0.00825, validation loss 0.00655
Epoch 11093: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5620, training loss 0.00489, validation loss 0.00506
Epoch 11104: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5621
Mean train loss for ascent epoch 5622: -0.011715876869857311
Mean eval for ascent epoch 5622: 0.005244965199381113
Epoch 11115: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5630, training loss 0.05690, validation loss 0.06141
Epoch 11126: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11137: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5640, training loss 0.01090, validation loss 0.01396
Epoch 11148: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11159: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5650, training loss 0.01258, validation loss 0.00874
Epoch 11170: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11181: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5660
Doing Evaluation on the model now
This is Epoch 5660, training loss 0.00553, validation loss 0.00636
Mean train loss for ascent epoch 5661: -0.013109841383993626
Mean eval for ascent epoch 5661: 0.006175023037940264
Epoch 11192: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5670, training loss 0.06851, validation loss 0.03455
Epoch 11203: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11214: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5680, training loss 0.00806, validation loss 0.00821
Epoch 11225: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11236: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5690, training loss 0.00551, validation loss 0.00456
Epoch 11247: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11258: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5699
Mean train loss for ascent epoch 5700: -0.012260356917977333
Mean eval for ascent epoch 5700: 0.004878812935203314
Doing Evaluation on the model now
This is Epoch 5700, training loss 0.00488, validation loss 0.00484
Epoch 11269: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5710, training loss 0.05026, validation loss 0.05190
Epoch 11280: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11291: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5720, training loss 0.00995, validation loss 0.01050
Epoch 11302: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11313: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5730, training loss 0.00511, validation loss 0.00528
Epoch 11324: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11335: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5738
Mean train loss for ascent epoch 5739: -0.01726747676730156
Mean eval for ascent epoch 5739: 0.005764135625213385
Doing Evaluation on the model now
This is Epoch 5740, training loss 0.10351, validation loss 0.10957
Epoch 11346: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11357: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5750, training loss 0.04211, validation loss 0.03067
Epoch 11368: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5760, training loss 0.01023, validation loss 0.01063
Epoch 11379: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11390: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5770, training loss 0.00658, validation loss 0.00508
Epoch 11401: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11412: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5777
Mean train loss for ascent epoch 5778: -0.011624425649642944
Mean eval for ascent epoch 5778: 0.005489789415150881
Doing Evaluation on the model now
This is Epoch 5780, training loss 0.21880, validation loss 0.24337
Epoch 11423: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11434: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5790, training loss 0.05312, validation loss 0.01769
Epoch 11445: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11456: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 5800, training loss 0.00683, validation loss 0.00641
Resetting learning rate to 0.01000
Epoch 11467: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 5810, training loss 0.00654, validation loss 0.00700
Epoch 11478: reducing learning rate of group 0 to 2.5000e-04.
Epoch 11489: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 5820, training loss 0.00519, validation loss 0.00525
Epoch 11500: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 5822
Mean train loss for ascent epoch 5823: -0.01354471780359745
Mean eval for ascent epoch 5823: 0.005419227760285139
Epoch 11511: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5830, training loss 0.05527, validation loss 0.05584
Epoch 11522: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11533: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5840, training loss 0.01147, validation loss 0.01418
Epoch 11544: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11555: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5850, training loss 0.00585, validation loss 0.00686
Epoch 11566: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 5860, training loss 0.00547, validation loss 0.00505
Epoch 11577: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5861
Mean train loss for ascent epoch 5862: -0.017215048894286156
Mean eval for ascent epoch 5862: 0.005534243304282427
Epoch 11588: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5870, training loss 0.06408, validation loss 0.03690
Epoch 11599: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11610: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5880, training loss 0.00867, validation loss 0.00664
Epoch 11621: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11632: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5890, training loss 0.00520, validation loss 0.00504
Epoch 11643: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11654: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5900
Doing Evaluation on the model now
This is Epoch 5900, training loss 0.00498, validation loss 0.00550
Mean train loss for ascent epoch 5901: -0.011743203736841679
Mean eval for ascent epoch 5901: 0.0058693052269518375
Epoch 11665: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 5910, training loss 0.05891, validation loss 0.07380
Epoch 11676: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11687: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5920, training loss 0.01105, validation loss 0.00934
Epoch 11698: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11709: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5930, training loss 0.00635, validation loss 0.00474
Epoch 11720: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11731: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5939
Mean train loss for ascent epoch 5940: -0.021288374438881874
Mean eval for ascent epoch 5940: 0.0063221813179552555
Doing Evaluation on the model now
This is Epoch 5940, training loss 0.00632, validation loss 0.00541
Epoch 11742: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11753: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5950, training loss 0.07393, validation loss 0.06040
Epoch 11764: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 5960, training loss 0.01122, validation loss 0.00905
Epoch 11775: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11786: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 5970, training loss 0.00520, validation loss 0.00505
Epoch 11797: reducing learning rate of group 0 to 1.5625e-04.
Epoch 11808: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 5978
Mean train loss for ascent epoch 5979: -0.017525965347886086
Mean eval for ascent epoch 5979: 0.005080047994852066
Doing Evaluation on the model now
This is Epoch 5980, training loss 0.07702, validation loss 0.06155
Epoch 11819: reducing learning rate of group 0 to 5.0000e-03.
Epoch 11830: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 5990, training loss 0.02439, validation loss 0.03564
Epoch 11841: reducing learning rate of group 0 to 1.2500e-03.
Epoch 11852: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6000, training loss 0.01511, validation loss 0.01643
Resetting learning rate to 0.01000
Epoch 11863: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 6010, training loss 0.01040, validation loss 0.00977
Epoch 11874: reducing learning rate of group 0 to 2.5000e-04.
Epoch 11885: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6020, training loss 0.00501, validation loss 0.00482
Epoch 11896: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6022
Mean train loss for ascent epoch 6023: -0.017999663949012756
Mean eval for ascent epoch 6023: 0.005406895652413368
Epoch 11907: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6030, training loss 0.07102, validation loss 0.05846
Epoch 11918: reducing learning rate of group 0 to 2.5000e-03.
Epoch 11929: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6040, training loss 0.00934, validation loss 0.01509
Epoch 11940: reducing learning rate of group 0 to 6.2500e-04.
Epoch 11951: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6050, training loss 0.00914, validation loss 0.01454
Epoch 11962: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6060, training loss 0.00797, validation loss 0.00672
Epoch 11973: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6061
Mean train loss for ascent epoch 6062: -0.021648632362484932
Mean eval for ascent epoch 6062: 0.005939058493822813
Epoch 11984: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6070, training loss 0.05729, validation loss 0.14254
Epoch 11995: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12006: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6080, training loss 0.01534, validation loss 0.02428
Epoch 12017: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12028: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6090, training loss 0.00662, validation loss 0.00609
Epoch 12039: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12050: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6100
Doing Evaluation on the model now
This is Epoch 6100, training loss 0.00473, validation loss 0.00504
Mean train loss for ascent epoch 6101: -0.013425135053694248
Mean eval for ascent epoch 6101: 0.005283663049340248
Epoch 12061: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6110, training loss 0.15971, validation loss 0.12170
Epoch 12072: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12083: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6120, training loss 0.00984, validation loss 0.00697
Epoch 12094: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12105: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6130, training loss 0.00567, validation loss 0.00482
Epoch 12116: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12127: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6139
Mean train loss for ascent epoch 6140: -0.012108058668673038
Mean eval for ascent epoch 6140: 0.0050792996771633625
Doing Evaluation on the model now
This is Epoch 6140, training loss 0.00508, validation loss 0.00434
Epoch 12138: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6150, training loss 0.12223, validation loss 0.04808
Epoch 12149: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12160: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6160, training loss 0.02025, validation loss 0.02260
Epoch 12171: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12182: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6170, training loss 0.00533, validation loss 0.00526
Epoch 12193: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12204: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6178
Mean train loss for ascent epoch 6179: -0.013165909796953201
Mean eval for ascent epoch 6179: 0.004418888594955206
Doing Evaluation on the model now
This is Epoch 6180, training loss 0.12020, validation loss 0.03605
Epoch 12215: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12226: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6190, training loss 0.02727, validation loss 0.03516
Epoch 12237: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6200, training loss 0.00983, validation loss 0.00845
Epoch 12248: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 12259: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 6210, training loss 0.00643, validation loss 0.00573
Epoch 12270: reducing learning rate of group 0 to 2.5000e-04.
Epoch 12281: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 6220, training loss 0.00492, validation loss 0.00485
Epoch 12292: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6223
Mean train loss for ascent epoch 6224: -0.014690584503114223
Mean eval for ascent epoch 6224: 0.004860108718276024
Epoch 12303: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6230, training loss 0.05600, validation loss 0.09732
Epoch 12314: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12325: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6240, training loss 0.01092, validation loss 0.01243
Epoch 12336: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6250, training loss 0.00748, validation loss 0.01217
Epoch 12347: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12358: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6260, training loss 0.00462, validation loss 0.00453
Epoch 12369: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6262
Mean train loss for ascent epoch 6263: -0.011475536040961742
Mean eval for ascent epoch 6263: 0.004892876371741295
Epoch 12380: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6270, training loss 0.09361, validation loss 0.23394
Epoch 12391: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12402: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6280, training loss 0.01725, validation loss 0.01473
Epoch 12413: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12424: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6290, training loss 0.00571, validation loss 0.00517
Epoch 12435: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6300, training loss 0.00498, validation loss 0.00469
Epoch 12446: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6301
Mean train loss for ascent epoch 6302: -0.010797636583447456
Mean eval for ascent epoch 6302: 0.005391797516494989
Epoch 12457: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6310, training loss 0.05755, validation loss 0.09758
Epoch 12468: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12479: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6320, training loss 0.01033, validation loss 0.00753
Epoch 12490: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12501: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6330, training loss 0.00555, validation loss 0.00476
Epoch 12512: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12523: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6340
Doing Evaluation on the model now
This is Epoch 6340, training loss 0.00516, validation loss 0.00526
Mean train loss for ascent epoch 6341: -0.016032828018069267
Mean eval for ascent epoch 6341: 0.005840022582560778
Epoch 12534: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6350, training loss 0.10679, validation loss 0.04068
Epoch 12545: reducing learning rate of group 0 to 2.5000e-03.
Epoch 12556: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6360, training loss 0.01277, validation loss 0.00959
Epoch 12567: reducing learning rate of group 0 to 6.2500e-04.
Epoch 12578: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6370, training loss 0.00639, validation loss 0.00618
Epoch 12589: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12600: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6379
Mean train loss for ascent epoch 6380: -0.011301921680569649
Mean eval for ascent epoch 6380: 0.007748431991785765
Doing Evaluation on the model now
This is Epoch 6380, training loss 0.00775, validation loss 0.00619
Epoch 12611: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12622: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6390, training loss 0.07865, validation loss 0.09222
Epoch 12633: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6400, training loss 0.01064, validation loss 0.00918
Resetting learning rate to 0.01000
Epoch 12644: reducing learning rate of group 0 to 5.0000e-04.
Epoch 12655: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6410, training loss 0.00534, validation loss 0.00573
Epoch 12666: reducing learning rate of group 0 to 1.2500e-04.
Epoch 12677: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6418
Mean train loss for ascent epoch 6419: -0.01634116657078266
Mean eval for ascent epoch 6419: 0.007610962726175785
Doing Evaluation on the model now
This is Epoch 6420, training loss 0.06305, validation loss 0.03361
Epoch 12688: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12699: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6430, training loss 0.02599, validation loss 0.01276
Epoch 12710: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12721: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6440, training loss 0.01445, validation loss 0.00583
Epoch 12732: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6450, training loss 0.00814, validation loss 0.00496
Epoch 12743: reducing learning rate of group 0 to 1.5625e-04.
Epoch 12754: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6457
Mean train loss for ascent epoch 6458: -0.011559803038835526
Mean eval for ascent epoch 6458: 0.005032734479755163
Doing Evaluation on the model now
This is Epoch 6460, training loss 0.14140, validation loss 0.08358
Epoch 12765: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12776: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6470, training loss 0.03464, validation loss 0.03375
Epoch 12787: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12798: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6480, training loss 0.00735, validation loss 0.00756
Epoch 12809: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12820: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6490, training loss 0.00594, validation loss 0.00563
Epoch 12831: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6496
Mean train loss for ascent epoch 6497: -0.015773436054587364
Mean eval for ascent epoch 6497: 0.005805804394185543
Doing Evaluation on the model now
This is Epoch 6500, training loss 0.61660, validation loss 0.56147
Epoch 12842: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12853: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6510, training loss 0.03242, validation loss 0.04302
Epoch 12864: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12875: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6520, training loss 0.01231, validation loss 0.00582
Epoch 12886: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12897: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6530, training loss 0.00506, validation loss 0.00489
Epoch 12908: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6535
Mean train loss for ascent epoch 6536: -0.011587962508201599
Mean eval for ascent epoch 6536: 0.005123588722199202
Doing Evaluation on the model now
This is Epoch 6540, training loss 0.15116, validation loss 0.08064
Epoch 12919: reducing learning rate of group 0 to 5.0000e-03.
Epoch 12930: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6550, training loss 0.02183, validation loss 0.06206
Epoch 12941: reducing learning rate of group 0 to 1.2500e-03.
Epoch 12952: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6560, training loss 0.00949, validation loss 0.00832
Epoch 12963: reducing learning rate of group 0 to 3.1250e-04.
Epoch 12974: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6570, training loss 0.00550, validation loss 0.00760
Epoch 12985: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6574
Mean train loss for ascent epoch 6575: -0.014364385046064854
Mean eval for ascent epoch 6575: 0.0057796877808868885
Epoch 12996: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6580, training loss 0.15010, validation loss 0.03605
Epoch 13007: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6590, training loss 0.03114, validation loss 0.02802
Epoch 13018: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13029: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6600, training loss 0.00696, validation loss 0.00702
Resetting learning rate to 0.01000
Epoch 13040: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13051: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6610, training loss 0.00551, validation loss 0.00532
Epoch 13062: reducing learning rate of group 0 to 1.2500e-04.
Epoch 13073: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6618
Mean train loss for ascent epoch 6619: -0.01088250894099474
Mean eval for ascent epoch 6619: 0.005223751068115234
Doing Evaluation on the model now
This is Epoch 6620, training loss 0.11453, validation loss 0.12285
Epoch 13084: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13095: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6630, training loss 0.05428, validation loss 0.02480
Epoch 13106: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6640, training loss 0.00844, validation loss 0.00737
Epoch 13117: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13128: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6650, training loss 0.00620, validation loss 0.00490
Epoch 13139: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13150: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6657
Mean train loss for ascent epoch 6658: -0.0169546939432621
Mean eval for ascent epoch 6658: 0.005504650995135307
Doing Evaluation on the model now
This is Epoch 6660, training loss 0.16784, validation loss 0.35434
Epoch 13161: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13172: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6670, training loss 0.02467, validation loss 0.04587
Epoch 13183: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13194: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6680, training loss 0.00828, validation loss 0.00505
Epoch 13205: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6690, training loss 0.00514, validation loss 0.00634
Epoch 13216: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13227: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6696
Mean train loss for ascent epoch 6697: -0.01674521714448929
Mean eval for ascent epoch 6697: 0.005296966526657343
Doing Evaluation on the model now
This is Epoch 6700, training loss 0.37279, validation loss 0.28857
Epoch 13238: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13249: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6710, training loss 0.02266, validation loss 0.01093
Epoch 13260: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13271: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6720, training loss 0.00893, validation loss 0.01042
Epoch 13282: reducing learning rate of group 0 to 3.1250e-04.
Epoch 13293: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6730, training loss 0.00599, validation loss 0.00520
Epoch 13304: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6735
Mean train loss for ascent epoch 6736: -0.010326812043786049
Mean eval for ascent epoch 6736: 0.00515860877931118
Doing Evaluation on the model now
This is Epoch 6740, training loss 0.17466, validation loss 0.16036
Epoch 13315: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13326: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6750, training loss 0.03256, validation loss 0.01365
Epoch 13337: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13348: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6760, training loss 0.00915, validation loss 0.00690
Epoch 13359: reducing learning rate of group 0 to 3.1250e-04.
Epoch 13370: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6770, training loss 0.00544, validation loss 0.00526
Epoch 13381: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6774
Mean train loss for ascent epoch 6775: -0.012129581533372402
Mean eval for ascent epoch 6775: 0.005340820178389549
Epoch 13392: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 6780, training loss 0.16792, validation loss 0.31668
Epoch 13403: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6790, training loss 0.04430, validation loss 0.02869
Epoch 13414: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13425: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6800, training loss 0.00884, validation loss 0.00749
Resetting learning rate to 0.01000
Epoch 13436: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13447: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 6810, training loss 0.00480, validation loss 0.00452
Epoch 13458: reducing learning rate of group 0 to 1.2500e-04.
Epoch 13469: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 6819
Mean train loss for ascent epoch 6820: -0.011426410637795925
Mean eval for ascent epoch 6820: 0.00464362557977438
Doing Evaluation on the model now
This is Epoch 6820, training loss 0.00464, validation loss 0.00462
Epoch 13480: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13491: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6830, training loss 0.07217, validation loss 0.08537
Epoch 13502: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 6840, training loss 0.02112, validation loss 0.01708
Epoch 13513: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13524: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6850, training loss 0.00618, validation loss 0.00493
Epoch 13535: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13546: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6858
Mean train loss for ascent epoch 6859: -0.016051040962338448
Mean eval for ascent epoch 6859: 0.005965471267700195
Doing Evaluation on the model now
This is Epoch 6860, training loss 0.13328, validation loss 0.19615
Epoch 13557: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13568: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6870, training loss 0.03071, validation loss 0.02856
Epoch 13579: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13590: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6880, training loss 0.01035, validation loss 0.01125
Epoch 13601: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 6890, training loss 0.00527, validation loss 0.00550
Epoch 13612: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13623: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6897
Mean train loss for ascent epoch 6898: -0.015441381372511387
Mean eval for ascent epoch 6898: 0.005378631874918938
Doing Evaluation on the model now
This is Epoch 6900, training loss 0.15691, validation loss 0.17838
Epoch 13634: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13645: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6910, training loss 0.02761, validation loss 0.01117
Epoch 13656: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13667: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6920, training loss 0.02003, validation loss 0.01400
Epoch 13678: reducing learning rate of group 0 to 3.1250e-04.
Epoch 13689: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6930, training loss 0.00523, validation loss 0.00501
Epoch 13700: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6936
Mean train loss for ascent epoch 6937: -0.017154639586806297
Mean eval for ascent epoch 6937: 0.005688140168786049
Doing Evaluation on the model now
This is Epoch 6940, training loss 0.87687, validation loss 1.08073
Epoch 13711: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13722: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6950, training loss 0.02255, validation loss 0.00997
Epoch 13733: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13744: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 6960, training loss 0.00628, validation loss 0.00626
Epoch 13755: reducing learning rate of group 0 to 3.1250e-04.
Epoch 13766: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 6970, training loss 0.00553, validation loss 0.00539
Epoch 13777: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 6975
Mean train loss for ascent epoch 6976: -0.016726471483707428
Mean eval for ascent epoch 6976: 0.005869351793080568
Doing Evaluation on the model now
This is Epoch 6980, training loss 0.43581, validation loss 1.15775
Epoch 13788: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13799: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 6990, training loss 0.03447, validation loss 0.04556
Epoch 13810: reducing learning rate of group 0 to 1.2500e-03.
Epoch 13821: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7000, training loss 0.00686, validation loss 0.00855
Resetting learning rate to 0.01000
Epoch 13832: reducing learning rate of group 0 to 5.0000e-04.
Epoch 13843: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7010, training loss 0.00484, validation loss 0.00489
Epoch 13854: reducing learning rate of group 0 to 1.2500e-04.
Epoch 13865: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7019
Mean train loss for ascent epoch 7020: -0.013182632625102997
Mean eval for ascent epoch 7020: 0.005790847819298506
Doing Evaluation on the model now
This is Epoch 7020, training loss 0.00579, validation loss 0.00441
Epoch 13876: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7030, training loss 0.05032, validation loss 0.06233
Epoch 13887: reducing learning rate of group 0 to 2.5000e-03.
Epoch 13898: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7040, training loss 0.01176, validation loss 0.01214
Epoch 13909: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13920: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7050, training loss 0.00531, validation loss 0.00562
Epoch 13931: reducing learning rate of group 0 to 1.5625e-04.
Epoch 13942: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7058
Mean train loss for ascent epoch 7059: -0.022110529243946075
Mean eval for ascent epoch 7059: 0.005375792738050222
Doing Evaluation on the model now
This is Epoch 7060, training loss 0.06299, validation loss 0.11610
Epoch 13953: reducing learning rate of group 0 to 5.0000e-03.
Epoch 13964: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7070, training loss 0.04055, validation loss 0.03495
Epoch 13975: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7080, training loss 0.01746, validation loss 0.01505
Epoch 13986: reducing learning rate of group 0 to 6.2500e-04.
Epoch 13997: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7090, training loss 0.00630, validation loss 0.00759
Epoch 14008: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14019: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7097
Mean train loss for ascent epoch 7098: -0.015113080851733685
Mean eval for ascent epoch 7098: 0.005803153850138187
Doing Evaluation on the model now
This is Epoch 7100, training loss 0.20627, validation loss 0.22177
Epoch 14030: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14041: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7110, training loss 0.04910, validation loss 0.07265
Epoch 14052: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14063: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7120, training loss 0.01151, validation loss 0.00750
Epoch 14074: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7130, training loss 0.00653, validation loss 0.00871
Epoch 14085: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14096: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7136
Mean train loss for ascent epoch 7137: -0.012162933126091957
Mean eval for ascent epoch 7137: 0.005220350343734026
Doing Evaluation on the model now
This is Epoch 7140, training loss 0.07441, validation loss 0.16306
Epoch 14107: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14118: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7150, training loss 0.01646, validation loss 0.02690
Epoch 14129: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14140: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7160, training loss 0.00665, validation loss 0.00875
Epoch 14151: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14162: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7170, training loss 0.00610, validation loss 0.00532
Epoch 14173: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7175
Mean train loss for ascent epoch 7176: -0.015881597995758057
Mean eval for ascent epoch 7176: 0.005019724369049072
Doing Evaluation on the model now
This is Epoch 7180, training loss 0.76496, validation loss 1.54034
Epoch 14184: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14195: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7190, training loss 0.03172, validation loss 0.03483
Epoch 14206: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14217: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7200, training loss 0.00623, validation loss 0.00575
Resetting learning rate to 0.01000
Epoch 14228: reducing learning rate of group 0 to 5.0000e-04.
Epoch 14239: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7210, training loss 0.00585, validation loss 0.00464
Epoch 14250: reducing learning rate of group 0 to 1.2500e-04.
Epoch 14261: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7220
Doing Evaluation on the model now
This is Epoch 7220, training loss 0.00483, validation loss 0.00435
Mean train loss for ascent epoch 7221: -0.013314562849700451
Mean eval for ascent epoch 7221: 0.004888390190899372
Epoch 14272: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7230, training loss 0.04700, validation loss 0.06688
Epoch 14283: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14294: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7240, training loss 0.00966, validation loss 0.00817
Epoch 14305: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14316: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7250, training loss 0.00536, validation loss 0.00472
Epoch 14327: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14338: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7259
Mean train loss for ascent epoch 7260: -0.013224232941865921
Mean eval for ascent epoch 7260: 0.005401575937867165
Doing Evaluation on the model now
This is Epoch 7260, training loss 0.00540, validation loss 0.00469
Epoch 14349: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14360: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7270, training loss 0.03304, validation loss 0.03129
Epoch 14371: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7280, training loss 0.01942, validation loss 0.01444
Epoch 14382: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14393: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7290, training loss 0.00624, validation loss 0.00728
Epoch 14404: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14415: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7298
Mean train loss for ascent epoch 7299: -0.019504645839333534
Mean eval for ascent epoch 7299: 0.0057281372137367725
Doing Evaluation on the model now
This is Epoch 7300, training loss 0.08899, validation loss 0.17624
Epoch 14426: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14437: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7310, training loss 0.06029, validation loss 0.02814
Epoch 14448: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14459: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7320, training loss 0.01191, validation loss 0.01669
Epoch 14470: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7330, training loss 0.00550, validation loss 0.00503
Epoch 14481: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14492: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7337
Mean train loss for ascent epoch 7338: -0.008844035677611828
Mean eval for ascent epoch 7338: 0.004714364185929298
Doing Evaluation on the model now
This is Epoch 7340, training loss 0.39408, validation loss 0.16588
Epoch 14503: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14514: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7350, training loss 0.01939, validation loss 0.00986
Epoch 14525: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14536: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7360, training loss 0.00968, validation loss 0.00648
Epoch 14547: reducing learning rate of group 0 to 3.1250e-04.
Epoch 14558: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 7370, training loss 0.00535, validation loss 0.00555
Epoch 14569: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7376
Mean train loss for ascent epoch 7377: -0.010372680611908436
Mean eval for ascent epoch 7377: 0.0064790821634233
Doing Evaluation on the model now
This is Epoch 7380, training loss 0.39781, validation loss 0.22334
Epoch 14580: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14591: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7390, training loss 0.02002, validation loss 0.01896
Epoch 14602: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14613: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7400, training loss 0.00828, validation loss 0.00853
Resetting learning rate to 0.01000
Epoch 14624: reducing learning rate of group 0 to 5.0000e-04.
Epoch 14635: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7410, training loss 0.00669, validation loss 0.00458
Epoch 14646: reducing learning rate of group 0 to 1.2500e-04.
Epoch 14657: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7420
Doing Evaluation on the model now
This is Epoch 7420, training loss 0.00506, validation loss 0.00441
Mean train loss for ascent epoch 7421: -0.013962316326797009
Mean eval for ascent epoch 7421: 0.005888109095394611
Epoch 14668: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7430, training loss 0.05010, validation loss 0.06233
Epoch 14679: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14690: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7440, training loss 0.01243, validation loss 0.00746
Epoch 14701: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14712: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7450, training loss 0.00579, validation loss 0.00714
Epoch 14723: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14734: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7459
Mean train loss for ascent epoch 7460: -0.01232637744396925
Mean eval for ascent epoch 7460: 0.0053952643647789955
Doing Evaluation on the model now
This is Epoch 7460, training loss 0.00540, validation loss 0.00523
Epoch 14745: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7470, training loss 0.03969, validation loss 0.05386
Epoch 14756: reducing learning rate of group 0 to 2.5000e-03.
Epoch 14767: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7480, training loss 0.00969, validation loss 0.00959
Epoch 14778: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14789: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7490, training loss 0.00491, validation loss 0.00482
Epoch 14800: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14811: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7498
Mean train loss for ascent epoch 7499: -0.012532126158475876
Mean eval for ascent epoch 7499: 0.007825280539691448
Doing Evaluation on the model now
This is Epoch 7500, training loss 0.08179, validation loss 0.26212
Epoch 14822: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14833: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7510, training loss 0.02857, validation loss 0.02434
Epoch 14844: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7520, training loss 0.01192, validation loss 0.00599
Epoch 14855: reducing learning rate of group 0 to 6.2500e-04.
Epoch 14866: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7530, training loss 0.00803, validation loss 0.00864
Epoch 14877: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14888: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7537
Mean train loss for ascent epoch 7538: -0.011695755645632744
Mean eval for ascent epoch 7538: 0.005437146872282028
Doing Evaluation on the model now
This is Epoch 7540, training loss 0.21301, validation loss 0.14975
Epoch 14899: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14910: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7550, training loss 0.02308, validation loss 0.01447
Epoch 14921: reducing learning rate of group 0 to 1.2500e-03.
Epoch 14932: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7560, training loss 0.01020, validation loss 0.00625
Epoch 14943: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7570, training loss 0.00603, validation loss 0.00529
Epoch 14954: reducing learning rate of group 0 to 1.5625e-04.
Epoch 14965: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7576
Mean train loss for ascent epoch 7577: -0.017302898690104485
Mean eval for ascent epoch 7577: 0.005814317613840103
Doing Evaluation on the model now
This is Epoch 7580, training loss 0.60464, validation loss 1.26387
Epoch 14976: reducing learning rate of group 0 to 5.0000e-03.
Epoch 14987: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7590, training loss 0.03073, validation loss 0.02671
Epoch 14998: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15009: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7600, training loss 0.00652, validation loss 0.00661
Resetting learning rate to 0.01000
Epoch 15020: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15031: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7610, training loss 0.00658, validation loss 0.00592
Epoch 15042: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7620, training loss 0.00555, validation loss 0.00523
Epoch 15053: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7621
Mean train loss for ascent epoch 7622: -0.0146102886646986
Mean eval for ascent epoch 7622: 0.0060821338556706905
Epoch 15064: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7630, training loss 0.09394, validation loss 0.04974
Epoch 15075: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15086: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7640, training loss 0.01645, validation loss 0.01904
Epoch 15097: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15108: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7650, training loss 0.00517, validation loss 0.00489
Epoch 15119: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15130: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7660
Doing Evaluation on the model now
This is Epoch 7660, training loss 0.00534, validation loss 0.00567
Mean train loss for ascent epoch 7661: -0.018401607871055603
Mean eval for ascent epoch 7661: 0.0050760614685714245
Epoch 15141: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7670, training loss 0.10104, validation loss 0.06349
Epoch 15152: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15163: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7680, training loss 0.01178, validation loss 0.01829
Epoch 15174: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15185: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7690, training loss 0.00633, validation loss 0.00601
Epoch 15196: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15207: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7699
Mean train loss for ascent epoch 7700: -0.014547251164913177
Mean eval for ascent epoch 7700: 0.005278018303215504
Doing Evaluation on the model now
This is Epoch 7700, training loss 0.00528, validation loss 0.00502
Epoch 15218: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15229: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7710, training loss 0.05066, validation loss 0.02889
Epoch 15240: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7720, training loss 0.01020, validation loss 0.01140
Epoch 15251: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15262: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7730, training loss 0.00590, validation loss 0.00501
Epoch 15273: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15284: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7738
Mean train loss for ascent epoch 7739: -0.011161498725414276
Mean eval for ascent epoch 7739: 0.004633708856999874
Doing Evaluation on the model now
This is Epoch 7740, training loss 0.13776, validation loss 0.08313
Epoch 15295: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15306: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7750, training loss 0.03175, validation loss 0.01720
Epoch 15317: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15328: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7760, training loss 0.01024, validation loss 0.01190
Epoch 15339: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7770, training loss 0.00525, validation loss 0.00545
Epoch 15350: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15361: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7777
Mean train loss for ascent epoch 7778: -0.015034008771181107
Mean eval for ascent epoch 7778: 0.004958045668900013
Doing Evaluation on the model now
This is Epoch 7780, training loss 0.18362, validation loss 0.32044
Epoch 15372: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15383: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7790, training loss 0.03183, validation loss 0.03698
Epoch 15394: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15405: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 7800, training loss 0.00750, validation loss 0.00746
Resetting learning rate to 0.01000
Epoch 15416: reducing learning rate of group 0 to 5.0000e-04.
Epoch 15427: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 7810, training loss 0.00574, validation loss 0.00556
Epoch 15438: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 7820, training loss 0.00457, validation loss 0.00414
Epoch 15449: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 7821
Mean train loss for ascent epoch 7822: -0.01436874270439148
Mean eval for ascent epoch 7822: 0.005179211031645536
Epoch 15460: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7830, training loss 0.05836, validation loss 0.02523
Epoch 15471: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15482: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7840, training loss 0.02528, validation loss 0.03653
Epoch 15493: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15504: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7850, training loss 0.00744, validation loss 0.00723
Epoch 15515: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15526: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7860
Doing Evaluation on the model now
This is Epoch 7860, training loss 0.00480, validation loss 0.00484
Mean train loss for ascent epoch 7861: -0.014509146101772785
Mean eval for ascent epoch 7861: 0.005140111781656742
Epoch 15537: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7870, training loss 0.07255, validation loss 0.02042
Epoch 15548: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15559: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7880, training loss 0.01326, validation loss 0.02063
Epoch 15570: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15581: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7890, training loss 0.00527, validation loss 0.00528
Epoch 15592: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15603: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7899
Mean train loss for ascent epoch 7900: -0.011154351755976677
Mean eval for ascent epoch 7900: 0.0054239812307059765
Doing Evaluation on the model now
This is Epoch 7900, training loss 0.00542, validation loss 0.00519
Epoch 15614: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 7910, training loss 0.09578, validation loss 0.05982
Epoch 15625: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15636: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7920, training loss 0.01302, validation loss 0.01008
Epoch 15647: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15658: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7930, training loss 0.00698, validation loss 0.00703
Epoch 15669: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15680: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7938
Mean train loss for ascent epoch 7939: -0.016646943986415863
Mean eval for ascent epoch 7939: 0.005692531354725361
Doing Evaluation on the model now
This is Epoch 7940, training loss 0.22077, validation loss 0.45098
Epoch 15691: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15702: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7950, training loss 0.04852, validation loss 0.05143
Epoch 15713: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 7960, training loss 0.01939, validation loss 0.01626
Epoch 15724: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15735: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 7970, training loss 0.00617, validation loss 0.00599
Epoch 15746: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15757: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 7977
Mean train loss for ascent epoch 7978: -0.013634667731821537
Mean eval for ascent epoch 7978: 0.00608723983168602
Doing Evaluation on the model now
This is Epoch 7980, training loss 0.11707, validation loss 0.21020
Epoch 15768: reducing learning rate of group 0 to 5.0000e-03.
Epoch 15779: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 7990, training loss 0.02921, validation loss 0.00986
Epoch 15790: reducing learning rate of group 0 to 1.2500e-03.
Epoch 15801: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8000, training loss 0.02095, validation loss 0.01879
Resetting learning rate to 0.01000
Epoch 15812: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 8010, training loss 0.00556, validation loss 0.00550
Epoch 15823: reducing learning rate of group 0 to 2.5000e-04.
Epoch 15834: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8020, training loss 0.00559, validation loss 0.00511
Epoch 15845: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8022
Mean train loss for ascent epoch 8023: -0.01653015799820423
Mean eval for ascent epoch 8023: 0.005259339697659016
Epoch 15856: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8030, training loss 0.07209, validation loss 0.16366
Epoch 15867: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15878: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8040, training loss 0.01042, validation loss 0.00536
Epoch 15889: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15900: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8050, training loss 0.00613, validation loss 0.00681
Epoch 15911: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8060, training loss 0.00507, validation loss 0.00469
Epoch 15922: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8061
Mean train loss for ascent epoch 8062: -0.013538362458348274
Mean eval for ascent epoch 8062: 0.005080752540379763
Epoch 15933: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8070, training loss 0.08828, validation loss 0.08484
Epoch 15944: reducing learning rate of group 0 to 2.5000e-03.
Epoch 15955: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8080, training loss 0.01298, validation loss 0.01766
Epoch 15966: reducing learning rate of group 0 to 6.2500e-04.
Epoch 15977: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8090, training loss 0.00655, validation loss 0.00482
Epoch 15988: reducing learning rate of group 0 to 1.5625e-04.
Epoch 15999: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8100
Doing Evaluation on the model now
This is Epoch 8100, training loss 0.00467, validation loss 0.00521
Mean train loss for ascent epoch 8101: -0.012792346999049187
Mean eval for ascent epoch 8101: 0.004713136702775955
Epoch 16010: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8110, training loss 0.12987, validation loss 0.13965
Epoch 16021: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16032: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8120, training loss 0.01608, validation loss 0.02557
Epoch 16043: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16054: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8130, training loss 0.00609, validation loss 0.00552
Epoch 16065: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16076: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8139
Mean train loss for ascent epoch 8140: -0.014067473821341991
Mean eval for ascent epoch 8140: 0.005141083616763353
Doing Evaluation on the model now
This is Epoch 8140, training loss 0.00514, validation loss 0.00624
Epoch 16087: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16098: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8150, training loss 0.08275, validation loss 0.08657
Epoch 16109: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8160, training loss 0.02609, validation loss 0.02081
Epoch 16120: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16131: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8170, training loss 0.00490, validation loss 0.00485
Epoch 16142: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16153: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8178
Mean train loss for ascent epoch 8179: -0.014867340214550495
Mean eval for ascent epoch 8179: 0.005109416786581278
Doing Evaluation on the model now
This is Epoch 8180, training loss 0.09043, validation loss 0.05637
Epoch 16164: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16175: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8190, training loss 0.05180, validation loss 0.03756
Epoch 16186: reducing learning rate of group 0 to 1.2500e-03.
Epoch 16197: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8200, training loss 0.02228, validation loss 0.01932
Resetting learning rate to 0.01000
Epoch 16208: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 8210, training loss 0.00823, validation loss 0.00761
Epoch 16219: reducing learning rate of group 0 to 2.5000e-04.
Epoch 16230: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8220, training loss 0.00494, validation loss 0.00546
Epoch 16241: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8222
Mean train loss for ascent epoch 8223: -0.015624014660716057
Mean eval for ascent epoch 8223: 0.005440380424261093
Epoch 16252: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8230, training loss 0.11888, validation loss 0.11007
Epoch 16263: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16274: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8240, training loss 0.02792, validation loss 0.01796
Epoch 16285: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16296: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8250, training loss 0.00782, validation loss 0.00573
Epoch 16307: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8260, training loss 0.00575, validation loss 0.00597
Epoch 16318: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8261
Mean train loss for ascent epoch 8262: -0.013435592874884605
Mean eval for ascent epoch 8262: 0.005290651693940163
Epoch 16329: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8270, training loss 0.06317, validation loss 0.14819
Epoch 16340: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16351: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8280, training loss 0.02208, validation loss 0.03371
Epoch 16362: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16373: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8290, training loss 0.00932, validation loss 0.00641
Epoch 16384: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16395: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8300
Doing Evaluation on the model now
This is Epoch 8300, training loss 0.00459, validation loss 0.00483
Mean train loss for ascent epoch 8301: -0.012509460560977459
Mean eval for ascent epoch 8301: 0.005010357592254877
Epoch 16406: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8310, training loss 0.06112, validation loss 0.03150
Epoch 16417: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16428: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8320, training loss 0.02853, validation loss 0.05482
Epoch 16439: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16450: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8330, training loss 0.00591, validation loss 0.00739
Epoch 16461: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16472: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8339
Mean train loss for ascent epoch 8340: -0.02043496072292328
Mean eval for ascent epoch 8340: 0.005984107963740826
Doing Evaluation on the model now
This is Epoch 8340, training loss 0.00598, validation loss 0.00523
Epoch 16483: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8350, training loss 0.22968, validation loss 0.50675
Epoch 16494: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16505: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8360, training loss 0.02032, validation loss 0.01541
Epoch 16516: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16527: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8370, training loss 0.00703, validation loss 0.00944
Epoch 16538: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16549: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8378
Mean train loss for ascent epoch 8379: -0.011977589689195156
Mean eval for ascent epoch 8379: 0.0060915471985936165
Doing Evaluation on the model now
This is Epoch 8380, training loss 0.22682, validation loss 0.30640
Epoch 16560: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16571: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8390, training loss 0.06535, validation loss 0.02610
Epoch 16582: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8400, training loss 0.01758, validation loss 0.01812
Epoch 16593: reducing learning rate of group 0 to 6.2500e-04.
Resetting learning rate to 0.01000
Epoch 16604: reducing learning rate of group 0 to 5.0000e-04.
Doing Evaluation on the model now
This is Epoch 8410, training loss 0.01027, validation loss 0.00972
Epoch 16615: reducing learning rate of group 0 to 2.5000e-04.
Epoch 16626: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 8420, training loss 0.00530, validation loss 0.00501
Epoch 16637: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8423
Mean train loss for ascent epoch 8424: -0.018021171912550926
Mean eval for ascent epoch 8424: 0.006227749865502119
Epoch 16648: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8430, training loss 0.08012, validation loss 0.20200
Epoch 16659: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16670: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8440, training loss 0.02190, validation loss 0.01991
Epoch 16681: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8450, training loss 0.01232, validation loss 0.00707
Epoch 16692: reducing learning rate of group 0 to 3.1250e-04.
Epoch 16703: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8460, training loss 0.00469, validation loss 0.00448
Epoch 16714: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8462
Mean train loss for ascent epoch 8463: -0.016187207773327827
Mean eval for ascent epoch 8463: 0.0051873535849153996
Epoch 16725: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8470, training loss 0.06351, validation loss 0.05584
Epoch 16736: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16747: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8480, training loss 0.02071, validation loss 0.01355
Epoch 16758: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16769: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8490, training loss 0.00596, validation loss 0.00663
Epoch 16780: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8500, training loss 0.00549, validation loss 0.00600
Epoch 16791: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8501
Mean train loss for ascent epoch 8502: -0.020088914781808853
Mean eval for ascent epoch 8502: 0.005839977413415909
Epoch 16802: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8510, training loss 0.08615, validation loss 0.04731
Epoch 16813: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16824: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8520, training loss 0.01204, validation loss 0.01420
Epoch 16835: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16846: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8530, training loss 0.00530, validation loss 0.00496
Epoch 16857: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16868: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8540
Doing Evaluation on the model now
This is Epoch 8540, training loss 0.00760, validation loss 0.01430
Mean train loss for ascent epoch 8541: -0.016195155680179596
Mean eval for ascent epoch 8541: 0.007186811417341232
Epoch 16879: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8550, training loss 0.18137, validation loss 0.13634
Epoch 16890: reducing learning rate of group 0 to 2.5000e-03.
Epoch 16901: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8560, training loss 0.01758, validation loss 0.01052
Epoch 16912: reducing learning rate of group 0 to 6.2500e-04.
Epoch 16923: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8570, training loss 0.00673, validation loss 0.00917
Epoch 16934: reducing learning rate of group 0 to 1.5625e-04.
Epoch 16945: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8579
Mean train loss for ascent epoch 8580: -0.017247293144464493
Mean eval for ascent epoch 8580: 0.00590323843061924
Doing Evaluation on the model now
This is Epoch 8580, training loss 0.00590, validation loss 0.00500
Epoch 16956: reducing learning rate of group 0 to 5.0000e-03.
Epoch 16967: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8590, training loss 0.08140, validation loss 0.06862
Epoch 16978: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8600, training loss 0.01299, validation loss 0.01749
Resetting learning rate to 0.01000
Epoch 16989: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17000: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8610, training loss 0.00535, validation loss 0.00503
Epoch 17011: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17022: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8618
Mean train loss for ascent epoch 8619: -0.015797117725014687
Mean eval for ascent epoch 8619: 0.005755805876106024
Doing Evaluation on the model now
This is Epoch 8620, training loss 0.17876, validation loss 0.27919
Epoch 17033: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17044: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8630, training loss 0.02431, validation loss 0.06712
Epoch 17055: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17066: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8640, training loss 0.01033, validation loss 0.00861
Epoch 17077: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8650, training loss 0.00685, validation loss 0.00554
Epoch 17088: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17099: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8657
Mean train loss for ascent epoch 8658: -0.0181971937417984
Mean eval for ascent epoch 8658: 0.005562174133956432
Doing Evaluation on the model now
This is Epoch 8660, training loss 0.92332, validation loss 1.37023
Epoch 17110: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17121: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8670, training loss 0.04353, validation loss 0.04600
Epoch 17132: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17143: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8680, training loss 0.00945, validation loss 0.01108
Epoch 17154: reducing learning rate of group 0 to 3.1250e-04.
Epoch 17165: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8690, training loss 0.00568, validation loss 0.00601
Epoch 17176: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8696
Mean train loss for ascent epoch 8697: -0.02766183763742447
Mean eval for ascent epoch 8697: 0.007241441402584314
Doing Evaluation on the model now
This is Epoch 8700, training loss 0.30186, validation loss 0.19962
Epoch 17187: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17198: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8710, training loss 0.02767, validation loss 0.02137
Epoch 17209: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17220: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8720, training loss 0.00701, validation loss 0.00641
Epoch 17231: reducing learning rate of group 0 to 3.1250e-04.
Epoch 17242: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8730, training loss 0.00572, validation loss 0.00668
Epoch 17253: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8735
Mean train loss for ascent epoch 8736: -0.014085198752582073
Mean eval for ascent epoch 8736: 0.005330000538378954
Doing Evaluation on the model now
This is Epoch 8740, training loss 0.24816, validation loss 0.08929
Epoch 17264: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17275: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8750, training loss 0.03803, validation loss 0.05356
Epoch 17286: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17297: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8760, training loss 0.00958, validation loss 0.01148
Epoch 17308: reducing learning rate of group 0 to 3.1250e-04.
Epoch 17319: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8770, training loss 0.00559, validation loss 0.00704
Epoch 17330: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8774
Mean train loss for ascent epoch 8775: -0.019385749474167824
Mean eval for ascent epoch 8775: 0.005899528041481972
Epoch 17341: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8780, training loss 0.11933, validation loss 0.03726
Epoch 17352: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8790, training loss 0.03114, validation loss 0.02004
Epoch 17363: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17374: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8800, training loss 0.00808, validation loss 0.00943
Resetting learning rate to 0.01000
Epoch 17385: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17396: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 8810, training loss 0.00524, validation loss 0.00499
Epoch 17407: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17418: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 8818
Mean train loss for ascent epoch 8819: -0.014143143780529499
Mean eval for ascent epoch 8819: 0.0055085704661905766
Doing Evaluation on the model now
This is Epoch 8820, training loss 0.14038, validation loss 0.06497
Epoch 17429: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17440: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8830, training loss 0.05019, validation loss 0.03739
Epoch 17451: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 8840, training loss 0.01425, validation loss 0.02195
Epoch 17462: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17473: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8850, training loss 0.00851, validation loss 0.00789
Epoch 17484: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17495: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8857
Mean train loss for ascent epoch 8858: -0.01851150207221508
Mean eval for ascent epoch 8858: 0.005792975891381502
Doing Evaluation on the model now
This is Epoch 8860, training loss 0.19427, validation loss 0.32059
Epoch 17506: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17517: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8870, training loss 0.04449, validation loss 0.13988
Epoch 17528: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17539: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8880, training loss 0.01097, validation loss 0.00618
Epoch 17550: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 8890, training loss 0.00554, validation loss 0.00656
Epoch 17561: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17572: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8896
Mean train loss for ascent epoch 8897: -0.01732901856303215
Mean eval for ascent epoch 8897: 0.005413133651018143
Doing Evaluation on the model now
This is Epoch 8900, training loss 0.36584, validation loss 0.38574
Epoch 17583: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17594: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8910, training loss 0.02425, validation loss 0.01161
Epoch 17605: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17616: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8920, training loss 0.00650, validation loss 0.00498
Epoch 17627: reducing learning rate of group 0 to 3.1250e-04.
Epoch 17638: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8930, training loss 0.00678, validation loss 0.00631
Epoch 17649: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8935
Mean train loss for ascent epoch 8936: -0.025877630338072777
Mean eval for ascent epoch 8936: 0.0074950275011360645
Doing Evaluation on the model now
This is Epoch 8940, training loss 0.78806, validation loss 0.59356
Epoch 17660: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17671: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8950, training loss 0.03398, validation loss 0.05885
Epoch 17682: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17693: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 8960, training loss 0.00687, validation loss 0.00719
Epoch 17704: reducing learning rate of group 0 to 3.1250e-04.
Epoch 17715: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 8970, training loss 0.00530, validation loss 0.00520
Epoch 17726: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 8974
Mean train loss for ascent epoch 8975: -0.01237820740789175
Mean eval for ascent epoch 8975: 0.005365084391087294
Epoch 17737: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 8980, training loss 0.22127, validation loss 0.22198
Epoch 17748: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 8990, training loss 0.03422, validation loss 0.09823
Epoch 17759: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17770: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9000, training loss 0.00715, validation loss 0.00703
Resetting learning rate to 0.01000
Epoch 17781: reducing learning rate of group 0 to 5.0000e-04.
Epoch 17792: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9010, training loss 0.00592, validation loss 0.00888
Epoch 17803: reducing learning rate of group 0 to 1.2500e-04.
Epoch 17814: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9019
Mean train loss for ascent epoch 9020: -0.01483615580946207
Mean eval for ascent epoch 9020: 0.00562032125890255
Doing Evaluation on the model now
This is Epoch 9020, training loss 0.00562, validation loss 0.00547
Epoch 17825: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17836: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9030, training loss 0.05815, validation loss 0.02962
Epoch 17847: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9040, training loss 0.01890, validation loss 0.01583
Epoch 17858: reducing learning rate of group 0 to 6.2500e-04.
Epoch 17869: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9050, training loss 0.00600, validation loss 0.00520
Epoch 17880: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17891: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9058
Mean train loss for ascent epoch 9059: -0.013672116212546825
Mean eval for ascent epoch 9059: 0.00500713475048542
Doing Evaluation on the model now
This is Epoch 9060, training loss 0.08996, validation loss 0.03642
Epoch 17902: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17913: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9070, training loss 0.03011, validation loss 0.05310
Epoch 17924: reducing learning rate of group 0 to 1.2500e-03.
Epoch 17935: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9080, training loss 0.01594, validation loss 0.02487
Epoch 17946: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9090, training loss 0.00517, validation loss 0.00607
Epoch 17957: reducing learning rate of group 0 to 1.5625e-04.
Epoch 17968: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9097
Mean train loss for ascent epoch 9098: -0.016362696886062622
Mean eval for ascent epoch 9098: 0.0058996607549488544
Doing Evaluation on the model now
This is Epoch 9100, training loss 0.24925, validation loss 0.42001
Epoch 17979: reducing learning rate of group 0 to 5.0000e-03.
Epoch 17990: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9110, training loss 0.02414, validation loss 0.04493
Epoch 18001: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18012: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9120, training loss 0.01524, validation loss 0.01509
Epoch 18023: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18034: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9130, training loss 0.00556, validation loss 0.00467
Epoch 18045: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9136
Mean train loss for ascent epoch 9137: -0.013507219031453133
Mean eval for ascent epoch 9137: 0.005748332943767309
Doing Evaluation on the model now
This is Epoch 9140, training loss 0.39755, validation loss 0.58521
Epoch 18056: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18067: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9150, training loss 0.03680, validation loss 0.02944
Epoch 18078: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18089: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9160, training loss 0.00819, validation loss 0.00712
Epoch 18100: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18111: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9170, training loss 0.00492, validation loss 0.00650
Epoch 18122: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9175
Mean train loss for ascent epoch 9176: -0.012844777666032314
Mean eval for ascent epoch 9176: 0.004745515529066324
Doing Evaluation on the model now
This is Epoch 9180, training loss 0.45811, validation loss 0.13603
Epoch 18133: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18144: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9190, training loss 0.02958, validation loss 0.01278
Epoch 18155: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18166: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9200, training loss 0.00927, validation loss 0.00704
Resetting learning rate to 0.01000
Epoch 18177: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18188: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9210, training loss 0.00542, validation loss 0.00635
Epoch 18199: reducing learning rate of group 0 to 1.2500e-04.
Epoch 18210: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9219
Mean train loss for ascent epoch 9220: -0.017235005274415016
Mean eval for ascent epoch 9220: 0.0061319018714129925
Doing Evaluation on the model now
This is Epoch 9220, training loss 0.00613, validation loss 0.00546
Epoch 18221: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9230, training loss 0.05274, validation loss 0.05532
Epoch 18232: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18243: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9240, training loss 0.00896, validation loss 0.01320
Epoch 18254: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18265: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9250, training loss 0.00571, validation loss 0.00740
Epoch 18276: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18287: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9258
Mean train loss for ascent epoch 9259: -0.014325235970318317
Mean eval for ascent epoch 9259: 0.005230110138654709
Doing Evaluation on the model now
This is Epoch 9260, training loss 0.15804, validation loss 0.08842
Epoch 18298: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18309: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9270, training loss 0.03018, validation loss 0.04045
Epoch 18320: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9280, training loss 0.01121, validation loss 0.00838
Epoch 18331: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18342: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9290, training loss 0.00577, validation loss 0.00562
Epoch 18353: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18364: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9297
Mean train loss for ascent epoch 9298: -0.02497025765478611
Mean eval for ascent epoch 9298: 0.0053803143091499805
Doing Evaluation on the model now
This is Epoch 9300, training loss 0.16202, validation loss 0.25916
Epoch 18375: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18386: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9310, training loss 0.02985, validation loss 0.01380
Epoch 18397: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18408: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9320, training loss 0.00695, validation loss 0.00634
Epoch 18419: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9330, training loss 0.00568, validation loss 0.00567
Epoch 18430: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18441: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9336
Mean train loss for ascent epoch 9337: -0.014161384664475918
Mean eval for ascent epoch 9337: 0.004687442909926176
Doing Evaluation on the model now
This is Epoch 9340, training loss 0.12217, validation loss 0.09996
Epoch 18452: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18463: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9350, training loss 0.02148, validation loss 0.03473
Epoch 18474: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18485: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9360, training loss 0.00635, validation loss 0.00718
Epoch 18496: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18507: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9370, training loss 0.00574, validation loss 0.00782
Epoch 18518: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9375
Mean train loss for ascent epoch 9376: -0.016115132719278336
Mean eval for ascent epoch 9376: 0.005404943134635687
Doing Evaluation on the model now
This is Epoch 9380, training loss 0.10320, validation loss 0.15734
Epoch 18529: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18540: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9390, training loss 0.02273, validation loss 0.01547
Epoch 18551: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18562: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9400, training loss 0.00529, validation loss 0.00550
Resetting learning rate to 0.01000
Epoch 18573: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18584: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9410, training loss 0.00548, validation loss 0.00573
Epoch 18595: reducing learning rate of group 0 to 1.2500e-04.
Epoch 18606: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9420
Doing Evaluation on the model now
This is Epoch 9420, training loss 0.00535, validation loss 0.00533
Mean train loss for ascent epoch 9421: -0.01624542474746704
Mean eval for ascent epoch 9421: 0.0058639515191316605
Epoch 18617: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9430, training loss 0.07496, validation loss 0.09210
Epoch 18628: reducing learning rate of group 0 to 2.5000e-03.
Epoch 18639: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9440, training loss 0.01111, validation loss 0.00596
Epoch 18650: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18661: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9450, training loss 0.00530, validation loss 0.00529
Epoch 18672: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18683: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9459
Mean train loss for ascent epoch 9460: -0.015623335726559162
Mean eval for ascent epoch 9460: 0.005026794970035553
Doing Evaluation on the model now
This is Epoch 9460, training loss 0.00503, validation loss 0.00513
Epoch 18694: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18705: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9470, training loss 0.05335, validation loss 0.02642
Epoch 18716: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9480, training loss 0.00898, validation loss 0.01125
Epoch 18727: reducing learning rate of group 0 to 6.2500e-04.
Epoch 18738: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9490, training loss 0.00553, validation loss 0.00489
Epoch 18749: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18760: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9498
Mean train loss for ascent epoch 9499: -0.014262281358242035
Mean eval for ascent epoch 9499: 0.005258785095065832
Doing Evaluation on the model now
This is Epoch 9500, training loss 0.25809, validation loss 1.50323
Epoch 18771: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18782: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9510, training loss 0.03310, validation loss 0.02396
Epoch 18793: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18804: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9520, training loss 0.01311, validation loss 0.01817
Epoch 18815: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9530, training loss 0.00709, validation loss 0.00617
Epoch 18826: reducing learning rate of group 0 to 1.5625e-04.
Epoch 18837: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9537
Mean train loss for ascent epoch 9538: -0.012718448415398598
Mean eval for ascent epoch 9538: 0.005926213227212429
Doing Evaluation on the model now
This is Epoch 9540, training loss 0.91068, validation loss 0.62612
Epoch 18848: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18859: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9550, training loss 0.03997, validation loss 0.02664
Epoch 18870: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18881: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9560, training loss 0.00783, validation loss 0.00750
Epoch 18892: reducing learning rate of group 0 to 3.1250e-04.
Epoch 18903: reducing learning rate of group 0 to 1.5625e-04.
Doing Evaluation on the model now
This is Epoch 9570, training loss 0.00614, validation loss 0.00545
Epoch 18914: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9576
Mean train loss for ascent epoch 9577: -0.022177232429385185
Mean eval for ascent epoch 9577: 0.005601305048912764
Doing Evaluation on the model now
This is Epoch 9580, training loss 0.48393, validation loss 0.57484
Epoch 18925: reducing learning rate of group 0 to 5.0000e-03.
Epoch 18936: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9590, training loss 0.01968, validation loss 0.01907
Epoch 18947: reducing learning rate of group 0 to 1.2500e-03.
Epoch 18958: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9600, training loss 0.00681, validation loss 0.00561
Resetting learning rate to 0.01000
Epoch 18969: reducing learning rate of group 0 to 5.0000e-04.
Epoch 18980: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9610, training loss 0.00531, validation loss 0.00605
Epoch 18991: reducing learning rate of group 0 to 1.2500e-04.
Epoch 19002: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9620
Doing Evaluation on the model now
This is Epoch 9620, training loss 0.00546, validation loss 0.00511
Mean train loss for ascent epoch 9621: -0.016796380281448364
Mean eval for ascent epoch 9621: 0.005285780876874924
Epoch 19013: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9630, training loss 0.06367, validation loss 0.08085
Epoch 19024: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19035: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9640, training loss 0.01550, validation loss 0.02381
Epoch 19046: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19057: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9650, training loss 0.00807, validation loss 0.00656
Epoch 19068: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19079: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9659
Mean train loss for ascent epoch 9660: -0.023675402626395226
Mean eval for ascent epoch 9660: 0.007685183081775904
Doing Evaluation on the model now
This is Epoch 9660, training loss 0.00769, validation loss 0.00814
Epoch 19090: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9670, training loss 0.06047, validation loss 0.04692
Epoch 19101: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19112: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9680, training loss 0.01883, validation loss 0.01714
Epoch 19123: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19134: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9690, training loss 0.00836, validation loss 0.00859
Epoch 19145: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19156: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9698
Mean train loss for ascent epoch 9699: -0.015597696416079998
Mean eval for ascent epoch 9699: 0.007789140567183495
Doing Evaluation on the model now
This is Epoch 9700, training loss 0.30380, validation loss 0.78266
Epoch 19167: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19178: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9710, training loss 0.05071, validation loss 0.01349
Epoch 19189: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9720, training loss 0.01125, validation loss 0.01787
Epoch 19200: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19211: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9730, training loss 0.00651, validation loss 0.00688
Epoch 19222: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19233: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9737
Mean train loss for ascent epoch 9738: -0.021019402891397476
Mean eval for ascent epoch 9738: 0.005541997030377388
Doing Evaluation on the model now
This is Epoch 9740, training loss 0.51236, validation loss 0.66229
Epoch 19244: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19255: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9750, training loss 0.03133, validation loss 0.01981
Epoch 19266: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19277: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9760, training loss 0.00870, validation loss 0.00598
Epoch 19288: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9770, training loss 0.00761, validation loss 0.00739
Epoch 19299: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19310: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9776
Mean train loss for ascent epoch 9777: -0.012881633825600147
Mean eval for ascent epoch 9777: 0.005866778548806906
Doing Evaluation on the model now
This is Epoch 9780, training loss 0.27328, validation loss 0.09797
Epoch 19321: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19332: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9790, training loss 0.03863, validation loss 0.02178
Epoch 19343: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19354: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9800, training loss 0.00603, validation loss 0.00551
Resetting learning rate to 0.01000
Epoch 19365: reducing learning rate of group 0 to 5.0000e-04.
Epoch 19376: reducing learning rate of group 0 to 2.5000e-04.
Doing Evaluation on the model now
This is Epoch 9810, training loss 0.00558, validation loss 0.00589
Epoch 19387: reducing learning rate of group 0 to 1.2500e-04.
Doing Evaluation on the model now
This is Epoch 9820, training loss 0.00583, validation loss 0.00544
Epoch 19398: reducing learning rate of group 0 to 6.2500e-05.
ascent epoch 9821
Mean train loss for ascent epoch 9822: -0.01035708375275135
Mean eval for ascent epoch 9822: 0.005005805287510157
Epoch 19409: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9830, training loss 0.15312, validation loss 0.26465
Epoch 19420: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19431: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9840, training loss 0.01815, validation loss 0.03829
Epoch 19442: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19453: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9850, training loss 0.00508, validation loss 0.00471
Epoch 19464: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19475: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9860
Doing Evaluation on the model now
This is Epoch 9860, training loss 0.00518, validation loss 0.00465
Mean train loss for ascent epoch 9861: -0.011815876699984074
Mean eval for ascent epoch 9861: 0.004906461574137211
Epoch 19486: reducing learning rate of group 0 to 5.0000e-03.
Doing Evaluation on the model now
This is Epoch 9870, training loss 0.12781, validation loss 0.03219
Epoch 19497: reducing learning rate of group 0 to 2.5000e-03.
Epoch 19508: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9880, training loss 0.01154, validation loss 0.00990
Epoch 19519: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19530: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9890, training loss 0.00551, validation loss 0.00579
Epoch 19541: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19552: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9899
Mean train loss for ascent epoch 9900: -0.013542614877223969
Mean eval for ascent epoch 9900: 0.005584945436567068
Doing Evaluation on the model now
This is Epoch 9900, training loss 0.00558, validation loss 0.00526
Epoch 19563: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19574: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9910, training loss 0.06494, validation loss 0.36302
Epoch 19585: reducing learning rate of group 0 to 1.2500e-03.
Doing Evaluation on the model now
This is Epoch 9920, training loss 0.01094, validation loss 0.01437
Epoch 19596: reducing learning rate of group 0 to 6.2500e-04.
Epoch 19607: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9930, training loss 0.00606, validation loss 0.00581
Epoch 19618: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19629: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9938
Mean train loss for ascent epoch 9939: -0.01531982421875
Mean eval for ascent epoch 9939: 0.0043585700914263725
Doing Evaluation on the model now
This is Epoch 9940, training loss 0.07256, validation loss 0.08399
Epoch 19640: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19651: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9950, training loss 0.03694, validation loss 0.02845
Epoch 19662: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19673: reducing learning rate of group 0 to 6.2500e-04.
Doing Evaluation on the model now
This is Epoch 9960, training loss 0.01165, validation loss 0.02028
Epoch 19684: reducing learning rate of group 0 to 3.1250e-04.
Doing Evaluation on the model now
This is Epoch 9970, training loss 0.00446, validation loss 0.00359
Epoch 19695: reducing learning rate of group 0 to 1.5625e-04.
Epoch 19706: reducing learning rate of group 0 to 7.8125e-05.
ascent epoch 9977
Mean train loss for ascent epoch 9978: -0.008559434674680233
Mean eval for ascent epoch 9978: 0.003782491898164153
Doing Evaluation on the model now
This is Epoch 9980, training loss 1.26479, validation loss 0.50598
Epoch 19717: reducing learning rate of group 0 to 5.0000e-03.
Epoch 19728: reducing learning rate of group 0 to 2.5000e-03.
Doing Evaluation on the model now
This is Epoch 9990, training loss 0.03090, validation loss 0.04129
Epoch 19739: reducing learning rate of group 0 to 1.2500e-03.
Epoch 19750: reducing learning rate of group 0 to 6.2500e-04.
POS: 
[8.850781, 8.64193, 8.147692, 5.5745554, 1.4668461, 1.1448846, 0.97247136, 0.82505, 0.69055223, 0.5279426, 0.5292764, 0.4325038, 0.6031284, 0.40989316, 0.50237715, 0.47358403, 0.32672423, 0.5374493, 0.43651384, 0.66997814, 0.4485506, 0.44090286, 0.29795974, 0.2283897, 0.2573658, 0.24642882, 0.25813302, 0.26795083, 0.3051605, 0.22073638, 0.22543235, 0.21462525, 0.26644588, 0.19788125, 0.22340499, 0.24231265, 0.20929542, 0.2117472, 0.26358438, 0.17935105, 0.2045457, 0.19465603, 0.22746553, 0.21012452, 0.18277983, 0.1825262, 4.1944847, 1.8039231, 1.2365175, 1.2714607, 1.0552844, 0.7709528, 0.78130037, 0.6277228, 0.643581, 1.002299, 1.1931044, 0.48055673, 0.406662, 0.32255796, 0.28095153, 0.60525626, 0.39410105, 0.24959469, 0.23530273, 0.23495813, 0.19516551, 0.2710993, 0.30108607, 0.17407313, 0.23416336, 0.18512721, 0.21041761, 0.3160458, 0.2141686, 0.25053918, 0.24489924, 0.28180197, 0.18355061, 0.26731938, 0.13668337, 0.13859881, 0.14336812, 0.18451908, 0.14153151, 0.7582871, 0.4590834, 0.53893644, 1.2042983, 1.1377846, 0.58614093, 0.65243953, 0.69160974, 0.51693964, 0.32685018, 0.42561442, 0.33362082, 0.23696752, 0.44804183, 0.5614709, 0.2197373, 0.19835183, 0.16558589, 0.21280149, 0.29755622, 0.26528302, 0.19694877, 0.23079215, 0.13987894, 0.22461337, 0.21217339, 0.18227275, 0.19274409, 0.14205459, 0.26976386, 0.15898278, 0.161422, 0.13843705, 0.15992998, 0.19486243, 0.11314418, 0.1506909, 0.17866883, 0.84790903, 0.5191099, 0.9067686, 1.2093846, 0.8810506, 0.45656636, 0.3909565, 0.3200457, 0.25192544, 0.35998535, 0.20402026, 0.30440798, 0.26905218, 0.30708683, 0.23400174, 0.48376504, 0.24056688, 0.28569356, 0.2854123, 0.27349496, 0.2513838, 0.21844578, 0.31234458, 0.19923382, 0.17815477, 0.13923433, 0.17224513, 0.175011, 0.1685247, 0.19261256, 0.18977444, 0.18595698, 0.1644715, 0.119680226, 0.19080444, 0.19240291, 0.16973166, 0.21313187, 0.5628998, 0.94043803, 1.093053, 0.5841985, 0.58520204, 0.5492304, 0.5819112, 0.26394978, 0.80513173, 0.65377975, 0.2277571, 0.18490693, 0.40938565, 0.3408069, 0.25979003, 0.4701645, 0.25916418, 0.28024712, 0.21012796, 0.26332214, 0.18672751, 0.225461, 0.16877553, 0.14531569, 0.20113362, 0.15981293, 0.1547807, 0.23163615, 0.18757729, 0.1723774, 0.18121621, 0.13309968, 0.13636528, 0.17498948, 0.102702476, 0.14328818, 0.20893183, 0.15082437, 0.14147048, 0.1977544, 0.19224118, 0.24164228, 0.22363208, 0.1784942, 0.13950402, 0.14508982, 0.13249528, 0.187196, 0.16410345, 0.1527522, 0.1307754, 0.15975803, 0.19811454, 0.13959652, 0.1328949, 0.7425718, 0.9069762, 0.785645, 0.9831349, 0.3266492, 0.46971074, 0.2913542, 0.48431197, 0.6050692, 0.8752167, 0.3261467, 0.2929519, 0.28418383, 0.21927151, 0.3334383, 0.319374, 0.29470456, 0.21212856, 0.14687811, 0.18544757, 0.17870323, 0.13815004, 0.15969375, 0.13947895, 0.13422185, 0.15519822, 0.18566102, 0.10626492, 0.21850194, 0.22364676, 0.15553756, 0.15728536, 0.15715887, 0.15951523, 0.17392495, 0.14266485, 0.15360892, 0.16021381, 0.73849815, 0.52150446, 0.5182504, 0.5446522, 0.42648897, 0.31229386, 0.33743826, 0.45671692, 0.26017752, 0.22731224, 0.2278551, 0.21088251, 0.24441466, 0.3671935, 0.2075027, 0.28244498, 0.23312382, 0.19640738, 0.18605678, 0.2599315, 0.23065145, 0.26423413, 0.2615788, 0.1272514, 0.15425272, 0.127622, 0.18923634, 0.19437546, 0.09682605, 0.22570886, 0.10235333, 0.16903779, 0.121457174, 0.11722264, 0.1371203, 0.09657212, 0.09613573, 0.17503878, 0.6849911, 0.6487354, 1.1732514, 0.6038824, 0.5156792, 0.49074945, 0.47236016, 0.31817156, 0.29500508, 0.42375588, 0.7286388, 0.6045443, 0.3346039, 0.20316608, 0.26985875, 0.25598863, 0.20974553, 0.25411066, 0.3209274, 0.22305118, 0.25156608, 0.28049332, 0.27441317, 0.20889826, 0.24231416, 0.2672503, 0.2221785, 0.17452638, 0.21194391, 0.21254925, 0.16187955, 0.2018019, 0.20464094, 0.18372667, 0.18180254, 0.15216692, 0.1648692, 0.21062525, 0.29650792, 0.5401228, 0.98513645, 0.65951425, 0.73693484, 0.5240379, 0.45880342, 0.32832137, 0.21671438, 0.26955497, 0.30565318, 0.29392684, 0.12606637, 0.22899242, 0.20763184, 0.22234274, 0.16599661, 0.3086852, 0.37485787, 0.38001648, 0.27356985, 0.16819938, 0.1260928, 0.2629853, 0.14263253, 0.14340696, 0.1806767, 0.08303119, 0.20302686, 0.14161351, 0.14437497, 0.100027874, 0.19975634, 0.098100536, 0.10501205, 0.11038945, 0.16642164, 0.12786587, 0.307055, 0.718831, 0.79225427, 0.44969043, 0.3695088, 0.58055496, 0.23513353, 0.32706475, 0.35662696, 0.46974322, 0.30570775, 0.16995086, 0.17849052, 0.3024443, 0.19967781, 0.1982013, 0.16457392, 0.12806149, 0.1914144, 0.19667758, 0.17582944, 0.16641246, 0.12530813, 0.19915152, 0.13134761, 0.13048613, 0.10180328, 0.17562783, 0.1896529, 0.19256619, 0.14127827, 0.14614618, 0.19780138, 0.13398248, 0.14267029, 0.14277217, 0.13264525, 0.08964159, 0.15228023, 0.114394, 0.11623674, 0.10952244, 0.11742926, 0.6109941, 1.0870879, 0.7391224, 0.80398285, 0.44074827, 0.77119243, 0.42597, 0.7440448, 0.63957775, 0.46571726, 0.22879016, 0.18230642, 0.20344836, 0.24533367, 0.2016155, 0.18021448, 0.14137289, 0.15823159, 0.2977077, 0.25087166, 0.17148417, 0.12779899, 0.14881827, 0.15590133, 0.19833505, 0.17405795, 0.21481328, 0.18885593, 0.15868106, 0.13048576, 0.12893975, 0.17029162, 0.13207017, 0.1095648, 0.15032496, 0.11371448, 0.14261127, 0.15048179, 0.3121998, 0.44553316, 0.6443268, 0.26635876, 0.42636767, 0.5104869, 0.6546899, 0.23966111, 0.5348959, 0.38839522, 0.1855111, 0.17128888, 0.22790296, 0.16744725, 0.2522644, 0.14664921, 0.16698475, 0.23874517, 0.15738276, 0.15946113, 0.22910103, 0.17773108, 0.17511497, 0.17375895, 0.15667556, 0.23991841, 0.124599695, 0.15504146, 0.22456606, 0.14659493, 0.11607205, 0.17088354, 0.11606149, 0.19400604, 0.12826541, 0.10992188, 0.14884691, 0.0907177, 0.4336315, 0.55016464, 0.81233376, 0.38126597, 0.54953146, 0.3946837, 0.30586475, 0.23704378, 0.18751816, 0.24378711, 0.2000284, 0.23052748, 0.28148508, 0.39642817, 0.45533678, 0.3800304, 0.26496783, 0.19730762, 0.29509273, 0.34715858, 0.1696317, 0.15581736, 0.23856097, 0.11461828, 0.13406901, 0.14678054, 0.1378315, 0.15044454, 0.1544414, 0.13328436, 0.098246105, 0.095776014, 0.11756079, 0.118725464, 0.1147913, 0.18554176, 0.102737114, 0.11963646, 0.38957304, 0.5766332, 0.42169166, 0.2826217, 1.0666597, 0.51269174, 0.21207453, 0.4675942, 0.2839426, 0.36849287, 0.3063974, 0.30815586, 0.3122303, 0.26108846, 0.26441872, 0.15886764, 0.15000685, 0.15740858, 0.2595777, 0.17168264, 0.12307167, 0.14486124, 0.09918241, 0.13654442, 0.16032197, 0.13012296, 0.12793064, 0.15132241, 0.10329491, 0.09132396, 0.10545831, 0.103027396, 0.08851595, 0.117629506, 0.12791592, 0.13198434, 0.1003643, 0.12555423, 0.73564494, 0.6995753, 0.5076499, 0.38014203, 0.34370318, 0.2351995, 0.27032918, 0.2586276, 0.29332906, 0.29810253, 0.33139214, 0.14730327, 0.14630847, 0.2632544, 0.23482892, 0.21244797, 0.12913294, 0.17350866, 0.21798109, 0.11185843, 0.09919936, 0.17114055, 0.099815406, 0.16802308, 0.14181343, 0.24916334, 0.1495184, 0.12038664, 0.16209972, 0.095661014, 0.09586192, 0.13200632, 0.09476159, 0.08872891, 0.16845429, 0.098081656, 0.121570915, 0.09392652, 0.120187975, 0.08875153, 0.11180082, 0.09880724, 0.11249162, 0.14276232, 0.29940945, 0.5592431, 0.24033418, 0.35322642, 0.253757, 0.21992563, 0.31832424, 0.42667675, 0.27204302, 0.33046198, 0.1788101, 0.21844493, 0.32706857, 0.17267793, 0.23833561, 0.19751625, 0.1457269, 0.13477646, 0.20105997, 0.20144123, 0.12033715, 0.11690033, 0.113572925, 0.13951592, 0.100010484, 0.13264644, 0.082746446, 0.13042262, 0.14063315, 0.08896554, 0.1214357, 0.12746385, 0.114166245, 0.115689695, 0.101350814, 0.11054538, 0.101202205, 0.10618004, 0.38116598, 0.8159822, 0.60668516, 0.5220729, 0.4278847, 0.2570261, 0.42407227, 0.39374042, 0.19855891, 0.2751146, 0.290657, 0.13648339, 0.1220507, 0.11595238, 0.16304103, 0.11131551, 0.270111, 0.14341255, 0.140451, 0.33176875, 0.20391195, 0.12928347, 0.118534304, 0.09283996, 0.16319045, 0.1376722, 0.10104938, 0.10478143, 0.112099074, 0.07974834, 0.11737554, 0.11663186, 0.12465527, 0.11883294, 0.09114959, 0.11856487, 0.1381804, 0.1459583, 0.2865092, 0.38838547, 0.4303789, 0.5700031, 0.41903266, 0.30585966, 0.29749706, 0.55524325, 0.40968853, 0.37886724, 0.1829499, 0.1245714, 0.314257, 0.26998973, 0.17091519, 0.1454175, 0.11016104, 0.14333823, 0.09819328, 0.20011921, 0.13631694, 0.12642409, 0.13666058, 0.12820718, 0.10380868, 0.18063822, 0.13894702, 0.08052255, 0.17077488, 0.09774892, 0.12455625, 0.14281476, 0.1314854, 0.14362809, 0.08316872, 0.17298402, 0.10241098, 0.15108669, 0.44333437, 0.6075899, 0.8691296, 0.3663137, 0.30798855, 0.22968902, 0.24535628, 0.19288099, 0.3591903, 0.3051659, 0.18564035, 0.15969153, 0.3114141, 0.19045874, 0.19226125, 0.14135483, 0.14675562, 0.14123054, 0.14497072, 0.15500179, 0.11026685, 0.087607525, 0.16917638, 0.1670788, 0.13780491, 0.13393354, 0.08111865, 0.117993735, 0.10414503, 0.18009774, 0.18026705, 0.103384756, 0.08895451, 0.10528036, 0.13335568, 0.08493861, 0.12710738, 0.10918896, 0.43665344, 0.31183347, 0.3112205, 0.24640255, 0.8532752, 0.4594527, 0.14829634, 0.20733815, 0.27787548, 0.4433016, 0.14908068, 0.15406106, 0.18039937, 0.12157503, 0.17528419, 0.17035155, 0.11343877, 0.13643706, 0.09373847, 0.08226424, 0.11348216, 0.15943076, 0.116439156, 0.09546844, 0.1302132, 0.11157228, 0.13374126, 0.17579266, 0.16457687, 0.12347785, 0.15433453, 0.11954739, 0.11993212, 0.08204103, 0.1056538, 0.13140096, 0.08286442, 0.09642833, 0.1119234, 0.09193456, 0.10988428, 0.08409697, 0.086569846, 0.19408269, 0.34340242, 0.6924182, 0.7242777, 0.55492324, 0.5588037, 0.31843752, 0.21739604, 0.19968121, 0.35279244, 0.24593867, 0.11907044, 0.16854404, 0.14449942, 0.18937056, 0.13718358, 0.16863883, 0.14331977, 0.12035256, 0.17959177, 0.10688335, 0.1353106, 0.11728918, 0.08578648, 0.1551894, 0.118607484, 0.09118023, 0.13328035, 0.07934401, 0.09526286, 0.078932054, 0.071368776, 0.1269259, 0.11518832, 0.09153808, 0.083076075, 0.09862616, 0.1059251, 0.5257786, 0.35027924, 0.64495814, 0.49477488, 0.32401273, 0.29477084, 0.24124332, 0.28150034, 0.25241747, 0.28031513, 0.2540683, 0.135977, 0.1982303, 0.1300416, 0.16014081, 0.123102024, 0.19924176, 0.17892468, 0.15991904, 0.10440729, 0.1163897, 0.1364366, 0.12062957, 0.119454645, 0.1331565, 0.065454975, 0.08315609, 0.09644468, 0.09182858, 0.10561094, 0.12950414, 0.12921393, 0.12795, 0.10245825, 0.093007185, 0.09795861, 0.095101565, 0.1007118, 0.22701855, 0.33311623, 0.4267694, 0.2796267, 0.35520816, 0.1703281, 0.1198954, 0.16134848, 0.32783744, 0.34121892, 0.17983331, 0.18325865, 0.14493406, 0.18289459, 0.29484868, 0.13681848, 0.12643988, 0.13576928, 0.121441506, 0.23863876, 0.11765372, 0.17477718, 0.1056092, 0.08006623, 0.08703572, 0.11162153, 0.08787955, 0.09619724, 0.067804955, 0.10153888, 0.10116291, 0.09333702, 0.11116141, 0.10293416, 0.1423712, 0.10863792, 0.08486632, 0.083161585, 0.1414406, 0.33684716, 0.4370303, 0.3534244, 0.3003676, 0.4329672, 0.28948408, 0.37783858, 0.26191735, 0.18881962, 0.1407314, 0.175367, 0.08010956, 0.15195623, 0.1417829, 0.19866717, 0.121123366, 0.17991455, 0.13137822, 0.1324492, 0.09642003, 0.08342141, 0.063149475, 0.112378344, 0.090139315, 0.13857116, 0.074923635, 0.07315546, 0.08953847, 0.1301383, 0.08692209, 0.11845277, 0.08935115, 0.090119936, 0.087100126, 0.1289844, 0.13376187, 0.08237466, 0.31319422, 0.5790749, 0.3657507, 0.29971203, 0.33235177, 0.44017872, 0.37612534, 0.26615006, 0.4316608, 0.3455581, 0.13010211, 0.09840387, 0.1479428, 0.2321816, 0.1719557, 0.22680514, 0.11218081, 0.14109431, 0.14515367, 0.13317764, 0.107064046, 0.13134974, 0.11325121, 0.12188205, 0.11661916, 0.12122246, 0.120308414, 0.1582578, 0.085520945, 0.120963246, 0.09603276, 0.08243165, 0.090825275, 0.09743646, 0.073725976, 0.083799444, 0.0970074, 0.11725833, 0.14533558, 0.09790754, 0.083035536, 0.06367397, 0.09094155, 0.09887059, 0.18859693, 0.30255583, 0.3412443, 0.6169792, 0.31020838, 0.25340024, 0.34192398, 0.23595461, 0.23025909, 0.28860965, 0.13409527, 0.13756035, 0.17816037, 0.20341782, 0.163025, 0.12690413, 0.16399045, 0.11199315, 0.12861805, 0.119829, 0.07055429, 0.14498469, 0.08111459, 0.112373814, 0.10162985, 0.082766786, 0.104094684, 0.10471249, 0.1606193, 0.068926275, 0.112319306, 0.09223352, 0.12579854, 0.08899035, 0.14687125, 0.08556876, 0.09077424, 0.08786532, 0.14020574, 0.3087679, 0.19263557, 0.18618976, 0.12141323, 0.13473229, 0.38781682, 0.5401759, 0.3316782, 0.37970936, 0.13157612, 0.15245783, 0.28167006, 0.16790934, 0.16206585, 0.14831284, 0.15116696, 0.11347136, 0.13528433, 0.13824873, 0.099722646, 0.08723439, 0.07012276, 0.09152411, 0.09797619, 0.09622815, 0.10579339, 0.08524601, 0.07369214, 0.08343521, 0.120685026, 0.09697047, 0.10132151, 0.08325335, 0.072456576, 0.113754824, 0.064912885, 0.11186114, 0.46902427, 0.41835633, 0.40895846, 0.4617037, 0.2211628, 0.15682526, 0.29307526, 0.26622304, 0.101840034, 0.13515113, 0.17369306, 0.14836815, 0.12517649, 0.12374742, 0.2063994, 0.1426475, 0.19700721, 0.09446287, 0.13822052, 0.11180201, 0.09598294, 0.08590812, 0.13577141, 0.12990431, 0.07230499, 0.12964289, 0.10943376, 0.1117794, 0.102628164, 0.0906646, 0.08738122, 0.076490454, 0.107136965, 0.0834757, 0.08614328, 0.0770121, 0.1322614, 0.06132385, 0.24462983, 0.27163243, 0.47244367, 0.39854607, 0.17187934, 0.15366186, 0.15786485, 0.2701943, 0.18211319, 0.15293352, 0.12996975, 0.15725179, 0.17997593, 0.17694677, 0.12212465, 0.09191559, 0.10510632, 0.101089604, 0.1114062, 0.121379614, 0.10980689, 0.12412189, 0.1320175, 0.100307, 0.075463176, 0.15264072, 0.08734071, 0.08334745, 0.090198494, 0.080322206, 0.08083036, 0.06950102, 0.09643702, 0.0602194, 0.061482523, 0.08226784, 0.064710595, 0.08333444, 0.4731344, 0.34582898, 0.3509945, 0.29041705, 0.15805282, 0.22461061, 0.43273354, 0.25827298, 0.16077828, 0.16030993, 0.10961738, 0.25595325, 0.17783074, 0.1416779, 0.11892623, 0.12729125, 0.08013363, 0.06791783, 0.12846473, 0.1029229, 0.12969582, 0.10654884, 0.11984772, 0.16768472, 0.06144048, 0.12793368, 0.13729918, 0.07423207, 0.07642785, 0.098568514, 0.08305621, 0.043222386, 0.090358675, 0.0904358, 0.07236407, 0.110805094, 0.09249334, 0.07182232, 0.08801718, 0.07811542, 0.08805067, 0.077577546, 0.066562936, 0.31178495, 0.69800264, 0.6731818, 0.5859529, 0.30432698, 0.13709073, 0.14897834, 0.14052124, 0.2049945, 0.23165108, 0.12573616, 0.13905087, 0.07824701, 0.09932928, 0.10994143, 0.08383966, 0.08525088, 0.10842406, 0.121231064, 0.10396312, 0.11410827, 0.13349521, 0.102262795, 0.06668229, 0.06827968, 0.13399126, 0.08567188, 0.06086581, 0.10205372, 0.10125464, 0.0938556, 0.09337794, 0.07770533, 0.06063618, 0.07847609, 0.10336385, 0.09868352, 0.06663416, 0.13991234, 0.22084482, 0.365751, 0.3485905, 0.30946416, 0.22032025, 0.27586257, 0.1914589, 0.111899465, 0.11754357, 0.075844035, 0.12854159, 0.096442, 0.22463731, 0.28377175, 0.13057896, 0.12967852, 0.09105949, 0.115023196, 0.1000517, 0.100361474, 0.12416607, 0.060692202, 0.092190966, 0.09382218, 0.11566797, 0.10873015, 0.092430174, 0.061115164, 0.10328212, 0.10503543, 0.10246788, 0.06339753, 0.08137299, 0.100813225, 0.06638137, 0.074638516, 0.08393098, 0.29687297, 0.46340695, 0.28151685, 0.21256442, 0.19300471, 0.22559129, 0.21589309, 0.20209765, 0.34916362, 0.32732245, 0.21490188, 0.17874071, 0.1761403, 0.1871615, 0.12020887, 0.13171554, 0.16131277, 0.123152986, 0.09093105, 0.081242904, 0.091661975, 0.12434752, 0.068939306, 0.07269984, 0.07661937, 0.09074944, 0.08838654, 0.13224314, 0.105082266, 0.074522555, 0.07801988, 0.08219526, 0.09767131, 0.08320565, 0.06673837, 0.08032292, 0.09219764, 0.09827785, 0.4089944, 0.41539392, 0.21038419, 0.247782, 0.24633475, 0.3381784, 0.20978098, 0.14040416, 0.15407448, 0.18937084, 0.10864992, 0.17433155, 0.11778984, 0.08021558, 0.1352557, 0.103361204, 0.1527033, 0.09288574, 0.09793228, 0.13314147, 0.06776479, 0.05570395, 0.110075735, 0.102609254, 0.080001205, 0.06447846, 0.10922303, 0.09940428, 0.045716822, 0.09200512, 0.07648431, 0.07478115, 0.09838285, 0.07691956, 0.08564565, 0.055084832, 0.07925848, 0.07516659, 0.39918366, 0.32533824, 0.2968145, 0.27981266, 0.20194903, 0.20979762, 0.16345258, 0.18879546, 0.14844629, 0.11999919, 0.12642157, 0.16003895, 0.16851623, 0.118272565, 0.1447658, 0.106663525, 0.06481013, 0.07625816, 0.11371722, 0.07120308, 0.11830948, 0.072130226, 0.06962673, 0.10833216, 0.058964014, 0.08085885, 0.10301645, 0.06930703, 0.077737905, 0.09026939, 0.08964863, 0.06799102, 0.086748086, 0.055144228, 0.06656577, 0.076051794, 0.07275109, 0.070377946, 0.073073335, 0.10096782, 0.055761565, 0.06826812, 0.07406658, 0.05410849, 0.18962292, 0.1972317, 0.28978297, 0.44783872, 0.2799997, 0.21924026, 0.23531131, 0.16056095, 0.22569712, 0.17854956, 0.11378549, 0.1269074, 0.13428073, 0.20992105, 0.23611097, 0.13707653, 0.049498197, 0.08108721, 0.12137244, 0.10116447, 0.0859463, 0.095312096, 0.09144466, 0.057686586, 0.07906399, 0.079230845, 0.04474879, 0.08759482, 0.045968533, 0.06360904, 0.13506016, 0.06500867, 0.061117373, 0.07494816, 0.06918207, 0.04642214, 0.08064629, 0.058111843, 0.26314962, 0.4365929, 0.40466323, 0.49387872, 0.21856502, 0.23542397, 0.17333847, 0.17385793, 0.20002492, 0.16651998, 0.1473962, 0.1017555, 0.09292574, 0.07146241, 0.13277106, 0.16634971, 0.11276909, 0.11766511, 0.0793915, 0.09489486, 0.06845251, 0.101543844, 0.07175157, 0.09608643, 0.08703613, 0.07516976, 0.09399197, 0.08142611, 0.093849234, 0.12710764, 0.08779818, 0.052360617, 0.061223086, 0.062175494, 0.09763797, 0.07493982, 0.05709694, 0.062110435, 0.14925921, 0.2555591, 0.3016271, 0.34811693, 0.1725246, 0.08053228, 0.16807938, 0.15897477, 0.13162391, 0.14960851, 0.11542097, 0.11498263, 0.09655518, 0.103934504, 0.09758654, 0.15391757, 0.09319242, 0.08831117, 0.09726633, 0.07753379, 0.1618826, 0.09996255, 0.0942292, 0.11044379, 0.06415304, 0.114870965, 0.08576016, 0.06585193, 0.08741942, 0.05363713, 0.073865294, 0.059751473, 0.076811746, 0.071257375, 0.09043322, 0.09620162, 0.09339275, 0.06409743, 0.15657188, 0.45001245, 0.2972809, 0.67721426, 0.20119673, 0.094888695, 0.0982165, 0.107362054, 0.14428754, 0.45681447, 0.22832, 0.16772905, 0.09591094, 0.15153992, 0.086007416, 0.13723457, 0.14694649, 0.08478501, 0.09373834, 0.095784366, 0.08230841, 0.07794818, 0.09548062, 0.080866076, 0.095197074, 0.05661218, 0.08593017, 0.071339265, 0.096744806, 0.11534753, 0.061141115, 0.07879956, 0.048658222, 0.07148296, 0.08003202, 0.058420446, 0.070412055, 0.053888183, 0.33196187, 0.4889311, 0.38041416, 0.20453586, 0.26365337, 0.30763564, 0.3430646, 0.17930217, 0.14399166, 0.19546287, 0.12013566, 0.15810482, 0.19024472, 0.1048637, 0.10045302, 0.088941835, 0.12502038, 0.085745685, 0.1197227, 0.100520656, 0.17784157, 0.07616637, 0.09991992, 0.087690294, 0.055841953, 0.069064654, 0.07188299, 0.08516125, 0.08642342, 0.07272357, 0.06039138, 0.055473767, 0.077616826, 0.092397675, 0.07525147, 0.07780312, 0.10691915, 0.07714847, 0.094758965, 0.09537718, 0.06839349, 0.065235786, 0.09049052, 0.1870782, 0.27658603, 0.21271111, 0.38104963, 0.36266732, 0.21173976, 0.2234613, 0.101178266, 0.12934677, 0.16537626, 0.0855674, 0.109238535, 0.065290354, 0.10352069, 0.15715681, 0.13225657, 0.12036683, 0.085328825, 0.08668051, 0.073650844, 0.10005312, 0.08674822, 0.099763036, 0.06660121, 0.06753292, 0.13316183, 0.082811214, 0.06913165, 0.08213421, 0.10329387, 0.09933147, 0.08659028, 0.06682228, 0.05432959, 0.0730439, 0.049827766, 0.08494995, 0.06878055, 0.14451809, 0.18389465, 0.22887366, 0.26698932, 0.59448355, 0.32417682, 0.31296682, 0.16420057, 0.14863154, 0.14928189, 0.22343719, 0.22714467, 0.11517762, 0.09652565, 0.10402375, 0.08409186, 0.119966984, 0.08591056, 0.15735221, 0.11493729, 0.19333085, 0.08634793, 0.086384945, 0.070856, 0.08862404, 0.09098496, 0.12300198, 0.060830463, 0.054908663, 0.06715283, 0.046822, 0.07184438, 0.0773353, 0.06998408, 0.059678305, 0.06581121, 0.060588747, 0.04803905, 0.20238362, 0.16702625, 0.35804638, 0.23885374, 0.30097035, 0.3188504, 0.18741387, 0.19764964, 0.14997849, 0.2656437, 0.1259163, 0.1028431, 0.09123524, 0.11671572, 0.11118724, 0.093396686, 0.07566098, 0.07462416, 0.1367341, 0.11697771, 0.08899037, 0.103849076, 0.077543855, 0.09465303, 0.08450773, 0.08555466, 0.07566107, 0.11031987, 0.09924478, 0.072762765, 0.078353435, 0.089852035, 0.06480799, 0.08137474, 0.07576878, 0.07863842, 0.0658702, 0.046585273, 0.13594717, 0.23584044, 0.26717803, 0.15972215, 0.39114293, 0.31151995, 0.10495629, 0.18709281, 0.109558396, 0.13443892, 0.11719904, 0.06574768, 0.0961298, 0.12683022, 0.14064969, 0.16889179, 0.10786203, 0.077978924, 0.13829657, 0.062326185, 0.08652102, 0.072832, 0.04721426, 0.07508858, 0.08577511, 0.063893765, 0.07814465, 0.097576134, 0.08922062, 0.055742472, 0.06190556, 0.059081785, 0.08346577, 0.05696526, 0.07977746, 0.08927996, 0.07844152, 0.062388685, 0.32134536, 0.21254782, 0.2180287, 0.14952092, 0.19924267, 0.16119152, 0.14832753, 0.15531209, 0.087713994, 0.20134684, 0.15142468, 0.12050241, 0.091072276, 0.12800841, 0.12323203, 0.13004886, 0.12922613, 0.08131737, 0.08852347, 0.11775407, 0.121890046, 0.108899705, 0.06710855, 0.096685976, 0.10749738, 0.07194466, 0.07524937, 0.054243527, 0.06826309, 0.06539056, 0.06942265, 0.08564756, 0.08602194, 0.06276592, 0.07147626, 0.050694183, 0.092512466, 0.070617676, 0.05753546, 0.0675473, 0.07285205, 0.065335326, 0.0747475, 0.059931975, 0.25524464, 0.28937355, 0.19584605, 0.39394212, 0.30917004, 0.1483994, 0.27905056, 0.2518092, 0.15141416, 0.1397941, 0.13493003, 0.13754171, 0.08735548, 0.102660276, 0.08966763, 0.09567966, 0.06427803, 0.10685879, 0.10307334, 0.08117317, 0.08044433, 0.073713444, 0.075876, 0.065411985, 0.088359706, 0.086224124, 0.06428103, 0.07893252, 0.07667881, 0.06278711, 0.08840031, 0.05326207, 0.07839923, 0.06013975, 0.06353725, 0.060870815, 0.12436843, 0.07061941, 0.1804154, 0.22073962, 0.16821097, 0.13256189, 0.16704549, 0.0890369, 0.20225462, 0.11567037, 0.14488345, 0.08519892, 0.05898027, 0.07618149, 0.10692006, 0.11317345, 0.09882325, 0.06479922, 0.1463412, 0.12462244, 0.13553135, 0.13397951, 0.08677764, 0.06174162, 0.103919044, 0.060322303, 0.0856012, 0.09204112, 0.08766436, 0.0703268, 0.08550009, 0.070810236, 0.058092162, 0.07141347, 0.081307374, 0.07386774, 0.05207166, 0.060148735, 0.082093984, 0.058391426, 0.24510753, 0.21315174, 0.27869806, 0.3215155, 0.14467356, 0.1478345, 0.21874662, 0.15046155, 0.14434375, 0.17718756, 0.12287047, 0.12309802, 0.083054684, 0.088802904, 0.13396838, 0.101418465, 0.064806126, 0.12689349, 0.08158603, 0.073678106, 0.10779984, 0.11871681, 0.05588965, 0.09883099, 0.10074937, 0.08504927, 0.063964896, 0.08039308, 0.061406456, 0.051274475, 0.046627935, 0.07473987, 0.0568307, 0.07658168, 0.07243292, 0.04906251, 0.061362736, 0.035656486, 0.18946397, 0.21850061, 0.24307269, 0.10521668, 0.08414084, 0.11461563, 0.17018604, 0.16819243, 0.102757506, 0.23209178, 0.08162554, 0.11792082, 0.12214571, 0.06620141, 0.08205305, 0.054847553, 0.06559536, 0.09327864, 0.056435578, 0.081408605, 0.07604267, 0.077289775, 0.09147181, 0.07726858, 0.060839884, 0.101598464, 0.06807065, 0.07932353, 0.08366782, 0.08937265, 0.06289438, 0.06450996, 0.058984086, 0.07296664, 0.07114062, 0.064508736, 0.058401197, 0.078951836, 0.10050639, 0.247065, 0.20123523, 0.13610367, 0.1547153, 0.15375537, 0.12992942, 0.2181856, 0.18854101, 0.15693048, 0.10522398, 0.08142652, 0.09876194, 0.09192899, 0.12023598, 0.108556844, 0.13563614, 0.09329984, 0.10099705, 0.09091103, 0.07342893, 0.0802659, 0.0754439, 0.05711849, 0.0987816, 0.048413374, 0.07892499, 0.050917715, 0.07023813, 0.06487751, 0.06708054, 0.05379629, 0.042362023, 0.05472746, 0.039378613, 0.05803743, 0.040811796, 0.06764573, 0.1055593, 0.1602859, 0.25306946, 0.32446402, 0.48427004, 0.16890897, 0.12710999, 0.11239928, 0.14748633, 0.16009969, 0.12640978, 0.07932195, 0.07452335, 0.123508565, 0.118787326, 0.09543297, 0.08484002, 0.087784916, 0.058183625, 0.07399279, 0.07842881, 0.0570501, 0.080597304, 0.078724705, 0.112257026, 0.09420241, 0.059003573, 0.07523047, 0.08385437, 0.09489595, 0.061470114, 0.11180506, 0.06637641, 0.057708237, 0.079133086, 0.069476105, 0.058493137, 0.053381726, 0.104896404, 0.12808348, 0.15449303, 0.4396926, 0.25734556, 0.14543003, 0.21956208, 0.15619393, 0.1999696, 0.2224863, 0.11141243, 0.084424, 0.12281115, 0.08484583, 0.101810955, 0.13537316, 0.08240577, 0.062224615, 0.09501373, 0.1012347, 0.09203837, 0.1265942, 0.11616832, 0.056125145, 0.09272481, 0.0718766, 0.05361369, 0.059113823, 0.07839866, 0.065740034, 0.056081723, 0.058349058, 0.042662613, 0.077776134, 0.053498957, 0.06208254, 0.06361415, 0.0737397, 0.1646334, 0.16060765, 0.1389569, 0.21485625, 0.12904012, 0.087004654, 0.08340938, 0.13075475, 0.11191669, 0.13310221, 0.12102308, 0.10669702, 0.10447452, 0.07100689, 0.088998884, 0.056563992, 0.0781079, 0.06590332, 0.0425799, 0.064245194, 0.08187385, 0.07547618, 0.093493275, 0.07260442, 0.059873167, 0.049956214, 0.051465273, 0.04737467, 0.049798466, 0.05583466, 0.06600062, 0.06331511, 0.04919977, 0.055682074, 0.064946465, 0.07122852, 0.061378103, 0.09278687, 0.21925622, 0.14933231, 0.4426177, 0.28308025, 0.2391141, 0.15327816, 0.13370064, 0.12472411, 0.10478332, 0.11223872, 0.1493217, 0.10689442, 0.08565837, 0.08364338, 0.088317305, 0.055108108, 0.08768015, 0.058085285, 0.046311136, 0.059756856, 0.065514386, 0.06856655, 0.0531643, 0.06327422, 0.07685917, 0.0838617, 0.099148095, 0.06413709, 0.05771359, 0.04852163, 0.047837242, 0.05907003, 0.057708345, 0.059420962, 0.081301324, 0.06186255, 0.075071014, 0.042791106, 0.25434327, 0.16843681, 0.123683274, 0.26763484, 0.094709374, 0.1850758, 0.37067023, 0.19705462, 0.11754131, 0.11778782, 0.08980129, 0.12191806, 0.10463196, 0.104124874, 0.17614782, 0.1090533, 0.073130794, 0.06405654, 0.058615018, 0.08120578, 0.06115737, 0.05222516, 0.053807165, 0.060475558, 0.05941972, 0.10274881, 0.09649134, 0.04184397, 0.056055352, 0.085338354, 0.04699304, 0.05197256, 0.057420723, 0.08990883, 0.046368457, 0.07558944, 0.06285352, 0.07363172, 0.05207396, 0.046228454, 0.067501806, 0.047256384, 0.044427082, 0.13566865, 0.21058778, 0.22366862, 0.14852805, 0.09083757, 0.06258831, 0.2472167, 0.15244627, 0.25871685, 0.13477987, 0.086032666, 0.07932535, 0.05607949, 0.07400463, 0.05637088, 0.09768242, 0.07480353, 0.101827644, 0.09401502, 0.0560955, 0.0717771, 0.08169819, 0.081643805, 0.06434053, 0.055191197, 0.044496927, 0.049358465, 0.044379987, 0.056534458, 0.060947243, 0.06127714, 0.04799409, 0.059077036, 0.04573208, 0.05250396, 0.061177563, 0.06250298, 0.05528398, 0.21225147, 0.33559528, 0.31711724, 0.21524121, 0.17918326, 0.20662394, 0.1140781, 0.17250681, 0.11612803, 0.121326596, 0.052410595, 0.07752297, 0.08111901, 0.10044118, 0.053804956, 0.10949391, 0.07710854, 0.056586504, 0.09095949, 0.06591967, 0.08681595, 0.073403046, 0.05110942, 0.06847242, 0.07507102, 0.087600276, 0.07527635, 0.050174586, 0.07344177, 0.06386294, 0.052469872, 0.05660997, 0.063667, 0.057082545, 0.050848734, 0.06396516, 0.055415668, 0.061017625, 0.21217881, 0.623375, 0.46950647, 0.28025147, 0.17751873, 0.23060179, 0.17465095, 0.15321483, 0.21280758, 0.12077086, 0.09353582, 0.13052966, 0.13153994, 0.13361287, 0.076613456, 0.074968934, 0.05325975, 0.06385237, 0.067128174, 0.10603235, 0.05112381, 0.07633519, 0.06797781, 0.043424934, 0.04589932, 0.0381782, 0.06376339, 0.05170935, 0.06362126, 0.059004158, 0.059056807, 0.047676407, 0.04018099, 0.05095495, 0.07720678, 0.043045014, 0.03800688, 0.044232655, 0.19590722, 0.2834257, 0.32707638, 0.14536709, 0.3715004, 0.11782354, 0.06713674, 0.08481941, 0.20683283, 0.14363933, 0.14687742, 0.0427163, 0.069360964, 0.12435809, 0.07828909, 0.05666613, 0.10211847, 0.074252084, 0.084564924, 0.059641134, 0.06903894, 0.071522065, 0.061392803, 0.07020813, 0.060169376, 0.055177443, 0.08331031, 0.049749985, 0.05237473, 0.06970868, 0.04805071, 0.047029324, 0.061576642, 0.038232375, 0.04678947, 0.04150554, 0.05701243, 0.06709048, 0.15775423, 0.16047075, 0.12768777, 0.20489646, 0.1727422, 0.10173426, 0.101156965, 0.0871047, 0.124834135, 0.2511284, 0.08698036, 0.09357316, 0.10110982, 0.049988292, 0.096068405, 0.08290117, 0.05026592, 0.10403164, 0.08652968, 0.09324356, 0.06167693, 0.055709194, 0.074193776, 0.06394432, 0.054495305, 0.058373, 0.0883279, 0.074602865, 0.062428888, 0.04862081, 0.06441355, 0.05290822, 0.054169342, 0.049931478, 0.06616403, 0.040652048, 0.076439925, 0.04843533, 0.032006558, 0.058726978, 0.048553064, 0.07179123, 0.08652527, 0.053823482, 0.15968564, 0.18395188, 0.17384756, 0.16397251, 0.099042594, 0.08022907, 0.22331285, 0.17943023, 0.10956179, 0.11436486, 0.19276781, 0.102256685, 0.08167512, 0.05381731, 0.12331176, 0.07213763, 0.06538813, 0.052601926, 0.03841927, 0.07953668, 0.06697943, 0.06026901, 0.048224702, 0.048065975, 0.04490268, 0.06706475, 0.058736324, 0.049531937, 0.043197062, 0.054460894, 0.03472439, 0.053070474, 0.04418416, 0.04238936, 0.05479942, 0.06038787, 0.04939997, 0.05967143, 0.21982524, 0.14088921, 0.18671954, 0.15641639, 0.10113297, 0.07478087, 0.16716607, 0.2718267, 0.12925719, 0.10054573, 0.13703133, 0.12797803, 0.09802904, 0.08796114, 0.06833682, 0.043483406, 0.068760775, 0.07156564, 0.06336067, 0.079826124, 0.085311584, 0.04853082, 0.081155375, 0.07091896, 0.09170841, 0.075122476, 0.032219753, 0.042032987, 0.039638564, 0.04385686, 0.04452489, 0.047047134, 0.057556704, 0.039693624, 0.055070393, 0.044545904, 0.039015736, 0.051736824, 0.17412159, 0.21970242, 0.1128598, 0.32079676, 0.142292, 0.117356926, 0.068318024, 0.100934796, 0.08270487, 0.14256889, 0.0986152, 0.06514225, 0.09041029, 0.07752644, 0.08221523, 0.062036306, 0.05987494, 0.07533872, 0.08830597, 0.06283526, 0.067810796, 0.04092817, 0.07495302, 0.05905884, 0.037310876, 0.04515575, 0.052293044, 0.043114286, 0.062298123, 0.062148288, 0.05257312, 0.04787332, 0.036651377, 0.059111916, 0.04329644, 0.05118526, 0.047234744, 0.041097954, 0.16949861, 0.15339881, 0.1589312, 0.2470812, 0.25056976, 0.18318735, 0.08096232, 0.08445287, 0.06573784, 0.14181967, 0.05131548, 0.072094075, 0.07337675, 0.18198106, 0.06804384, 0.05302653, 0.04520281, 0.07731025, 0.058138665, 0.0549372, 0.044753265, 0.040913705, 0.12757511, 0.06072646, 0.052492708, 0.062825434, 0.054212444, 0.046258815, 0.046992466, 0.05370903, 0.055237744, 0.05744295, 0.036531884, 0.03105799, 0.037790783, 0.054016013, 0.03507847, 0.046196215, 0.14267766, 0.15920013, 0.17622907, 0.13480142, 0.14676924, 0.092850596, 0.114808775, 0.121431075, 0.09949581, 0.13519439, 0.109221496, 0.062019136, 0.06994144, 0.06400118, 0.07346194, 0.057414815, 0.037502035, 0.06303408, 0.07167291, 0.08222753, 0.07461876, 0.05691886, 0.051900838, 0.0734442, 0.06375709, 0.09360022, 0.04508507, 0.055686824, 0.058947314, 0.03905131, 0.04891964, 0.053149443, 0.054421175, 0.060528792, 0.037039485, 0.046035416, 0.035545964, 0.040520035, 0.059358843, 0.063320145, 0.054611105, 0.054367643, 0.04356441, 0.1304409, 0.32164308, 0.1722383, 0.18569337, 0.26590392, 0.2315683, 0.13302268, 0.09204046, 0.13457651, 0.09194478, 0.08129353, 0.08144394, 0.094463125, 0.08056929, 0.07591093, 0.07629448, 0.035165835, 0.05689863, 0.047358867, 0.06468496, 0.048458338, 0.040973593, 0.04104275, 0.0404634, 0.04025212, 0.039633762, 0.051433086, 0.04369469, 0.045229178, 0.041470975, 0.049563643, 0.049460623, 0.050300594, 0.047310222, 0.040222745, 0.05165334, 0.034014, 0.029942498, 0.13270476, 0.14367592, 0.079007916, 0.11658932, 0.14207324, 0.08344338, 0.07395827, 0.12408898, 0.11813008, 0.12765884, 0.066212304, 0.05290292, 0.054207858, 0.06691618, 0.1079753, 0.06289718, 0.05897533, 0.05631706, 0.06606128, 0.060157206, 0.060761213, 0.085537635, 0.06506413, 0.051573966, 0.06101851, 0.0974311, 0.049242552, 0.052085314, 0.03797157, 0.045528676, 0.057749797, 0.052428763, 0.03554691, 0.03922586, 0.03404428, 0.0552137, 0.036667157, 0.05606233, 0.4145366, 0.16486616, 0.3062071, 0.36553437, 0.3435937, 0.1648509, 0.07710655, 0.106809236, 0.11864664, 0.10382841, 0.11038867, 0.068584904, 0.06551193, 0.07595246, 0.05620368, 0.058906224, 0.07201926, 0.06261816, 0.087187074, 0.06111992, 0.04116547, 0.06984935, 0.049030583, 0.043796144, 0.08001461, 0.036413062, 0.04262991, 0.044002566, 0.051095553, 0.0602544, 0.044179026, 0.03299888, 0.06524262, 0.04378515, 0.039766897, 0.051700648, 0.047090244, 0.035039812, 0.16056357, 0.16367924, 0.12777665, 0.14793488, 0.1279415, 0.06465671, 0.097232364, 0.09808326, 0.11199571, 0.09228424, 0.16764137, 0.05873073, 0.08696949, 0.072991475, 0.054490577, 0.06504026, 0.05669891, 0.05276902, 0.051211957, 0.06049279, 0.06365589, 0.11574613, 0.07365309, 0.048404202, 0.033919964, 0.04755687, 0.0470863, 0.06000766, 0.06722041, 0.048901506, 0.031010848, 0.0705251, 0.046805155, 0.03310445, 0.040296853, 0.04488783, 0.045717474, 0.052628033, 0.1262894, 0.22812356, 0.15915252, 0.19947793, 0.18991917, 0.10864392, 0.10066552, 0.11520209, 0.07470509, 0.06947969, 0.07960762, 0.07621824, 0.058925252, 0.081044346, 0.06558929, 0.057421684, 0.088440895, 0.08415678, 0.14316626, 0.074201085, 0.051326618, 0.064088665, 0.03855687, 0.076358706, 0.055476673, 0.04302625, 0.04524472, 0.05527291, 0.04512691, 0.05171052, 0.052341837, 0.04593881, 0.027366335, 0.03449054, 0.03842685, 0.048497606, 0.05392767, 0.035896663, 0.04980186, 0.045059003, 0.026839526, 0.042383216, 0.03803892, 0.040669415, 0.110352814, 0.16095485, 0.14119412, 0.1309601, 0.121859, 0.117773116, 0.10215713, 0.08463052, 0.07157392, 0.06925597, 0.058620382, 0.057357173, 0.11031551, 0.12442127, 0.1035886, 0.10390578, 0.07128155, 0.082443304, 0.04978004, 0.08552247, 0.053254504, 0.068013035, 0.06727433, 0.05370868, 0.038974997, 0.041287117, 0.06017288, 0.047618207, 0.054306734, 0.03927621, 0.07469132, 0.0316869, 0.048393752, 0.04439797, 0.04374793, 0.03033419, 0.037458293, 0.048291635, 0.19796659, 0.12104496, 0.122170135, 0.12716326, 0.06577742, 0.10372513, 0.085714094, 0.06943144, 0.10433215, 0.078781776, 0.073171504, 0.059210595, 0.08353571, 0.09260926, 0.08752213, 0.06420308, 0.04399237, 0.059282973, 0.06219108, 0.057019122, 0.09016281, 0.06432895, 0.039185334, 0.054649197, 0.04903548, 0.062557556, 0.030257586, 0.043358255, 0.036783416, 0.03655161, 0.040103234, 0.04310317, 0.03738944, 0.05722784, 0.04542501, 0.062780365, 0.044265084, 0.054698203, 0.04987817, 0.09177754, 0.17203718, 0.13096678, 0.09842175, 0.13429214, 0.078090586, 0.14953494, 0.1636094, 0.06903404, 0.06386067, 0.06769514, 0.049723893, 0.0618029, 0.05700075, 0.084528826, 0.07067029, 0.041021615, 0.0722426, 0.0694626, 0.057368685, 0.04874739, 0.046777193, 0.08374286, 0.072669186, 0.041623905, 0.04800852, 0.034860726, 0.054630555, 0.055377573, 0.04798152, 0.054604016, 0.037995428, 0.04524796, 0.05821792, 0.03810533, 0.034876116, 0.042132065, 0.10788765, 0.2251405, 0.2114956, 0.20401895, 0.119836144, 0.2358942, 0.110404916, 0.080226436, 0.09414051, 0.1387551, 0.13395019, 0.06971791, 0.09268726, 0.06910763, 0.06171079, 0.051057488, 0.05621402, 0.059789363, 0.05269178, 0.052894227, 0.056254316, 0.051053204, 0.060328316, 0.041191537, 0.043431986, 0.045102965, 0.036013268, 0.04642028, 0.051218446, 0.038785007, 0.044834238, 0.030333146, 0.03779959, 0.04624267, 0.028464757, 0.04651165, 0.047799762, 0.030615235, 0.15262513, 0.19965416, 0.14297958, 0.14014171, 0.09164014, 0.06849502, 0.08229077, 0.0684392, 0.14512816, 0.16035512, 0.094768845, 0.0506548, 0.08526287, 0.06599963, 0.041043095, 0.046669886, 0.059641574, 0.05852593, 0.055280197, 0.04158595, 0.056727905, 0.037638396, 0.038297527, 0.060718015, 0.037463844, 0.06533436, 0.049717396, 0.047541607, 0.04899531, 0.056586295, 0.07152113, 0.028420461, 0.047223676, 0.03782941, 0.035004552, 0.047496285, 0.034650397, 0.037937663, 0.04114903, 0.031534858, 0.043384552, 0.036230344, 0.033218574, 0.095610514, 0.08712176, 0.20436671, 0.15806611, 0.17180151, 0.22420634, 0.13190486, 0.08790846, 0.115756325, 0.1498669, 0.09340045, 0.07321088, 0.06677301, 0.084049374, 0.06258728, 0.06606166, 0.06218764, 0.03631839, 0.05733945, 0.052478377, 0.062311504, 0.046993345, 0.049039084, 0.053424157, 0.037844703, 0.09218393, 0.062259838, 0.033302635, 0.04451751, 0.052030798, 0.03199203, 0.03848486, 0.040263776, 0.04189638, 0.049662646, 0.03439342, 0.066889115, 0.038746346, 0.106721364, 0.10976599, 0.09007691, 0.08668301, 0.095556974, 0.22698405, 0.14711034, 0.13910055, 0.07260978, 0.10134543, 0.058461618, 0.04967464, 0.06792845, 0.0629374, 0.07484475, 0.06567309, 0.06481847, 0.04842502, 0.030504936, 0.04354109, 0.05963963, 0.048974205, 0.051376164, 0.03310574, 0.038345546, 0.033854354, 0.040385615, 0.029632872, 0.03354171, 0.0418828, 0.07118357, 0.04971689, 0.025597462, 0.03819005, 0.040026646, 0.048883315, 0.04545385, 0.045107264, 0.08880589, 0.158188, 0.3156516, 0.18949215, 0.11982522, 0.08133998, 0.14364946, 0.12973265, 0.10460646, 0.078285016, 0.060250245, 0.048922792, 0.049386915, 0.048643094, 0.06304889, 0.052295197, 0.06646734, 0.05368234, 0.056246392, 0.052198064, 0.040724948, 0.06367386, 0.03999185, 0.06129428, 0.044974614, 0.065993875, 0.04654895, 0.039680075, 0.059107292, 0.04004428, 0.055790056, 0.04729016, 0.06998724, 0.062137064, 0.030113852, 0.029640423, 0.031932052, 0.032495838, 0.19116895, 0.28814122, 0.25301018, 0.1664751, 0.11459954, 0.09480358, 0.090731904, 0.10129283, 0.07144421, 0.11608478, 0.06142775, 0.08091743, 0.06511921, 0.049230777, 0.046871196, 0.05015296, 0.057886537, 0.07313777, 0.075792275, 0.078785256, 0.060907576, 0.04617364, 0.055899408, 0.041329663, 0.05102167, 0.055214852, 0.03503451, 0.04168485, 0.042500228, 0.066054635, 0.04822751, 0.036750466, 0.047655884, 0.049342137, 0.027934367, 0.04269116, 0.043506566, 0.031313512, 0.10081572, 0.17307402, 0.12929998, 0.16542168, 0.123577945, 0.1107829, 0.14323604, 0.087102525, 0.14900897, 0.16263694, 0.06228049, 0.051466696, 0.04362923, 0.07349997, 0.036829047, 0.07653047, 0.066519044, 0.057998292, 0.044607922, 0.064764485, 0.037196483, 0.03807672, 0.05160333, 0.08224984, 0.056467533, 0.04715221, 0.046025164, 0.051546395, 0.04933899, 0.03667886, 0.036830492, 0.05340799, 0.05597031, 0.038837504, 0.05230258, 0.043611024, 0.03556996, 0.04143723, 0.045966733, 0.045353778, 0.02853544, 0.035866346, 0.036728386, 0.029131526, 0.17342947, 0.22191495, 0.19494657, 0.15663177, 0.10040268, 0.13258362, 0.08526962, 0.13286392, 0.06332366, 0.088484265, 0.07063514, 0.03527225, 0.061861843, 0.05380735, 0.084608756, 0.075567946, 0.043212686, 0.05214262, 0.05500536, 0.041219767, 0.05663615, 0.039753698, 0.031937312, 0.042459782, 0.033634268, 0.03569497, 0.041085478, 0.03173649, 0.035315134, 0.029832853, 0.03442421, 0.04343381, 0.047275916, 0.038086973, 0.038418483, 0.027683415, 0.027467659, 0.032028887, 0.15366788, 0.117864594, 0.12665331, 0.17906773, 0.094594285, 0.08153494, 0.05055933, 0.09278173, 0.08084215, 0.109498754, 0.08297479, 0.077840135, 0.058070533, 0.06648395, 0.04896908, 0.049263753, 0.04424193, 0.06125902, 0.03734912, 0.055932652, 0.03963622, 0.03652884, 0.038908657, 0.033595234, 0.032580484, 0.04003499, 0.031567376, 0.049668066, 0.03608366, 0.032550737, 0.039797183, 0.042672943, 0.034447785, 0.032700185, 0.04308205, 0.03633112, 0.031105757, 0.036630753, 0.110088274, 0.19098987, 0.100387104, 0.09989107, 0.17158274, 0.12366977, 0.073552944, 0.06778544, 0.10926756, 0.097735755, 0.069444835, 0.08914296, 0.08134146, 0.055476245, 0.076540925, 0.04626404, 0.038239315, 0.049017794, 0.037375502, 0.040517658, 0.059789803, 0.048360273, 0.05127286, 0.04821082, 0.034752466, 0.022738803, 0.02516725, 0.029824875, 0.034322362, 0.039244264, 0.03415369, 0.052435163, 0.036618084, 0.03893881, 0.043450385, 0.043317325, 0.03557039, 0.02059756, 0.06162424, 0.06213231, 0.08104666, 0.16200116, 0.14330235, 0.10408136, 0.049177215, 0.10066763, 0.124652684, 0.13330181, 0.07150011, 0.049193256, 0.051876873, 0.059940375, 0.05989637, 0.041531526, 0.03825305, 0.057435364, 0.030445283, 0.043170065, 0.046304908, 0.045042466, 0.037623126, 0.048723068, 0.03367601, 0.03468949, 0.029836467, 0.03799199, 0.043434758, 0.039214373, 0.034326974, 0.03872894, 0.052255467, 0.029186882, 0.028974727, 0.035053283, 0.037134174, 0.031539746, 0.16588376, 0.18616733, 0.16303577, 0.1392679, 0.08050156, 0.10042321, 0.06516232, 0.11840917, 0.18185702, 0.09713651, 0.056999017, 0.12442389, 0.13472797, 0.082935244, 0.11350729, 0.051742565, 0.045140337, 0.048405897, 0.037884742, 0.047734536, 0.04878783, 0.037104834, 0.04235757, 0.028579542, 0.034449726, 0.0574836, 0.0421572, 0.033623826, 0.040264264, 0.02770738, 0.03452261, 0.04278254, 0.04583384, 0.028965786, 0.03921466, 0.032470893, 0.039309923, 0.023685787, 0.030340195, 0.026428843, 0.04959361, 0.030638667, 0.04475114, 0.15377648, 0.12950759, 0.13405107, 0.15620032, 0.16085787, 0.1006385, 0.09397342, 0.11852815, 0.14961071, 0.15752213, 0.08668578, 0.06690471, 0.08141631, 0.0449533, 0.06525294, 0.051569548, 0.048892416, 0.049268834, 0.036581054, 0.027706265, 0.0299919, 0.055085614, 0.0438683, 0.029329205, 0.040050637, 0.041398533, 0.044600397, 0.030528495, 0.044429097, 0.043313466, 0.029738486, 0.029037856, 0.02591146, 0.028439669, 0.031978846, 0.03554051, 0.030978465, 0.028048182, 0.2623684, 0.13381884, 0.11257593, 0.12593135, 0.14272787, 0.07034925, 0.1013344, 0.1726996, 0.08570441, 0.08882212, 0.070697784, 0.05394577, 0.11235831, 0.038136404, 0.038858756, 0.033210963, 0.030190863, 0.02240698, 0.044091336, 0.05069344, 0.04469772, 0.03402705, 0.037760317, 0.034838434, 0.047494162, 0.037816048, 0.036032792, 0.030500256, 0.036910366, 0.039185263, 0.034413256, 0.04425811, 0.025602609, 0.02684033, 0.02666571, 0.03221515, 0.032358468, 0.028941596, 0.11084373, 0.13187246, 0.24987316, 0.25807056, 0.19225273, 0.10842934, 0.078715846, 0.080433905, 0.07092021, 0.05206488, 0.062742375, 0.06263621, 0.06656888, 0.10039625, 0.04587757, 0.061375286, 0.027296474, 0.044343084, 0.06784826, 0.049978953, 0.063416466, 0.05919447, 0.034724385, 0.044173356, 0.050032325, 0.056356, 0.045442704, 0.062897444, 0.047522675, 0.025869848, 0.028181145, 0.033853516, 0.052011024, 0.027943589, 0.031580266, 0.027278, 0.029549452, 0.021374661, 0.096398756, 0.12866111, 0.08658333, 0.21310073, 0.09995358, 0.062087547, 0.074691795, 0.047499407, 0.05155909, 0.09738446, 0.04740643, 0.056416504, 0.043818194, 0.051837213, 0.10129437, 0.06728953, 0.046207894, 0.027230337, 0.058963325, 0.02622906, 0.032171912, 0.05342284, 0.026055884, 0.045406986, 0.036413424, 0.03425506, 0.04923941, 0.049983747, 0.039944552, 0.043309595, 0.029090144, 0.031755622, 0.026992155, 0.028394938, 0.03693691, 0.022721767, 0.030979834, 0.024926914, 0.0945861, 0.07051302, 0.16011213, 0.1725938, 0.09590707, 0.0895107, 0.123592354, 0.05368452, 0.084212415, 0.093036786, 0.047687445, 0.07703481, 0.07943346, 0.100224815, 0.07201757, 0.03960372, 0.040848363, 0.030030169, 0.055080302, 0.05892761, 0.04490391, 0.035202798, 0.036551233, 0.059018537, 0.021891491, 0.02615754, 0.05409803, 0.024426866, 0.03171049, 0.03605389, 0.03244867, 0.031393778, 0.028671766, 0.04314076, 0.028121132, 0.041456476, 0.03443426, 0.046471857, 0.031459425, 0.040796757, 0.026294606, 0.023021057, 0.03966525, 0.026799908, 0.20287699, 0.19724634, 0.14990532, 0.15800445, 0.08005205, 0.079113096, 0.09466672, 0.077140704, 0.08590697, 0.071093604, 0.09583062, 0.055893917, 0.06420019, 0.071043335, 0.034454703, 0.052059688, 0.050676752, 0.034734074, 0.0582417, 0.056007724, 0.03666898, 0.049588956, 0.035796605, 0.038151167, 0.038493223, 0.032449, 0.030394431, 0.031518135, 0.025871163, 0.040178247, 0.037605155, 0.053571444, 0.037970636, 0.03866164, 0.051605564, 0.05476604, 0.028948018, 0.03497193, 0.10256378, 0.06555908, 0.18775275, 0.20029011, 0.08653408, 0.15988281, 0.08390582, 0.17404252, 0.07587838, 0.080602564, 0.060024902, 0.039346267, 0.054537773, 0.051111184, 0.03793492, 0.034501716, 0.06135022, 0.037211668, 0.024502624, 0.032054495, 0.03856436, 0.04091085, 0.039363813, 0.037977036, 0.05004139, 0.049244367, 0.037422206, 0.031198319, 0.031116981, 0.04285484, 0.046456378, 0.029293738, 0.030182082, 0.030234002, 0.03240815, 0.03357262, 0.025422938, 0.03615882, 0.068846755, 0.11579337, 0.07909558, 0.056521226, 0.063719876, 0.11076582, 0.0816504, 0.07341811, 0.06841514, 0.18203358, 0.10752108, 0.06559468, 0.041597903, 0.0636192, 0.06246686, 0.038999062, 0.05359338, 0.029135203, 0.039987225, 0.027032021, 0.06353973, 0.078417905, 0.039876815, 0.04323534, 0.061342966, 0.03247885, 0.036643248, 0.026495539, 0.023907607, 0.028724043, 0.043932326, 0.026978904, 0.038588617, 0.024772294, 0.031792503, 0.025095487, 0.024982734, 0.029237298, 0.10439263, 0.126207, 0.16222122, 0.08361957, 0.058263052, 0.068464994, 0.04536114, 0.14985655, 0.1507657, 0.09779113, 0.077273086, 0.111888185, 0.09840224, 0.050738208, 0.05729381, 0.055588357, 0.044999227, 0.026913527, 0.037887577, 0.029606754, 0.04030609, 0.04176512, 0.05380957, 0.032360286, 0.037145097, 0.050032806, 0.033713356, 0.025011353, 0.037956983, 0.033394825, 0.038521368, 0.041571405, 0.03860761, 0.047888797, 0.033208594, 0.043192238, 0.045387764, 0.022062128, 0.10164819, 0.1206067, 0.14741983, 0.23127127, 0.119262554, 0.09307899, 0.12081636, 0.06858636, 0.057994872, 0.071157366, 0.058800984, 0.08481339, 0.052196473, 0.054604243, 0.05347757, 0.04339974, 0.046070207, 0.03462305, 0.04519706, 0.038182806, 0.032409754, 0.032390434, 0.034081943, 0.043144558, 0.030803973, 0.030922977, 0.045277797, 0.034590695, 0.060854014, 0.021571591, 0.021150647, 0.043810617, 0.029816333, 0.026746185, 0.0405173, 0.02468502, 0.026052171, 0.028793857, 0.03249031, 0.026514594, 0.03897306, 0.025784941, 0.03506208, 0.07659932, 0.20101616, 0.17122617, 0.16187304, 0.12347469, 0.1618585, 0.0889555, 0.13416553, 0.067217454, 0.12811802, 0.06889581, 0.088092774, 0.05929193, 0.038487535, 0.05825777, 0.07322647, 0.060420714, 0.051827043, 0.06265421, 0.04712571, 0.05351867, 0.037074566, 0.027529987, 0.04774483, 0.03624015, 0.04576215, 0.039084893, 0.031349525, 0.03367912, 0.04463327, 0.040165875, 0.026546225, 0.02357276, 0.03437546, 0.031634245, 0.030726194, 0.020275284, 0.031706974, 0.078701705, 0.11591556, 0.13612473, 0.08183351, 0.14224967, 0.12849009, 0.085634805, 0.061334226, 0.07898221, 0.085031435, 0.057943124, 0.05331652, 0.05115898, 0.055862416, 0.07121329, 0.061960615, 0.066080645, 0.0741909, 0.043226864, 0.04870272, 0.035581827, 0.050798275, 0.027408715, 0.038466148, 0.038092677, 0.045067977, 0.04171829, 0.034712736, 0.02179413, 0.03652406, 0.03782321, 0.04115818, 0.033068575, 0.034282837, 0.032686703, 0.047895104, 0.027108394, 0.031182734, 0.09351052, 0.103900574, 0.09982482, 0.21637942, 0.32051784, 0.09381743, 0.077934146, 0.1024708, 0.08011855, 0.0759638, 0.047406334, 0.063070364, 0.05515936, 0.071395375, 0.04122737, 0.06722707, 0.044527378, 0.043033846, 0.043821953, 0.039117206, 0.047083527, 0.05250929, 0.040936362, 0.04880993, 0.02961527, 0.034354374, 0.028359408, 0.042163502, 0.027391003, 0.051742405, 0.033461746, 0.041252747, 0.04235032, 0.025984863, 0.04460111, 0.032723423, 0.022615064, 0.020266939, 0.14155136, 0.15213685, 0.07259253, 0.14191115, 0.18535735, 0.08589722, 0.058889132, 0.060054135, 0.053637084, 0.060787484, 0.061042637, 0.06478805, 0.037923485, 0.03240155, 0.037254006, 0.0651222, 0.051773533, 0.034415673, 0.047093667, 0.03856983, 0.038215682, 0.038430993, 0.025484541, 0.03505747, 0.027723394, 0.0351451, 0.04145383, 0.038098365, 0.025735492, 0.027533993, 0.026502013, 0.04398834, 0.04017384, 0.025389964, 0.03735888, 0.03244235, 0.020209357, 0.018829778, 0.053558916, 0.10678953, 0.13618122, 0.19158152, 0.10730495, 0.0933097, 0.08255205, 0.079292476, 0.056270793, 0.07688769, 0.062370878, 0.040636934, 0.048822038, 0.043534577, 0.069161125, 0.053854365, 0.057202768, 0.082751386, 0.0606441, 0.030783331, 0.037562072, 0.029545179, 0.04330107, 0.03303006, 0.049526814, 0.04825464, 0.03148272, 0.030745722, 0.026477186, 0.025253423, 0.0333733, 0.031906147, 0.030821795, 0.037749194, 0.028541254, 0.0333968, 0.02540594, 0.0473591, 0.029678991, 0.031728223, 0.032959856, 0.03444338, 0.040331043, 0.040061105, 0.08345039, 0.08487611, 0.12345674, 0.17860949, 0.060781058, 0.07981113, 0.058843393, 0.081776164, 0.099980064, 0.05974244, 0.05811109, 0.055875264, 0.073397085, 0.05315885, 0.05794041, 0.04279424, 0.036299743, 0.02204254, 0.025032533, 0.040768195, 0.03789801, 0.027884716, 0.02824471, 0.037012115, 0.041728396, 0.041235264, 0.04056835, 0.033698726, 0.030088123, 0.03192173, 0.031094309, 0.034222692, 0.03157246, 0.031623393, 0.027690148, 0.026409842, 0.03766005, 0.032534163, 0.07222826, 0.09527086, 0.10867361, 0.14182243, 0.13933787, 0.051784564, 0.06176876, 0.07215669, 0.13868421, 0.15659793, 0.06811164, 0.05914677, 0.06944228, 0.050813306, 0.039772898, 0.061657593, 0.04111557, 0.039103467, 0.03668263, 0.04082911, 0.055185072, 0.043939874, 0.027812755, 0.023067, 0.04470741, 0.04007562, 0.045002922, 0.026772603, 0.034003474, 0.03811019, 0.030997735, 0.029062225, 0.029895937, 0.031285092, 0.030534657, 0.025904175, 0.0293373, 0.032172572, 0.12094175, 0.1535269, 0.20990077, 0.07762645, 0.08905731, 0.06725159, 0.09097048, 0.052012596, 0.044244863, 0.060645297, 0.05888486, 0.051385716, 0.072868824, 0.059907347, 0.0499806, 0.047814973, 0.05121552, 0.054024383, 0.03343064, 0.04152173, 0.034681346, 0.04310146, 0.03067202, 0.04130479, 0.036526415, 0.03072167, 0.028584274, 0.030648446, 0.024811963, 0.029608881, 0.0396065, 0.0338601, 0.029464895, 0.024978176, 0.027562669, 0.02550873, 0.03270818, 0.023373011, 0.06997039, 0.11529502, 0.120919265, 0.098030195, 0.084897205, 0.087257616, 0.1320273, 0.08881245, 0.06827785, 0.09979026, 0.046545193, 0.040734492, 0.030419981, 0.0458071, 0.053492296, 0.049432445, 0.027587038, 0.042476874, 0.02534357, 0.040896926, 0.036564708, 0.033118956, 0.03079651, 0.028874993, 0.050768964, 0.031253662, 0.022507852, 0.027777167, 0.03319347, 0.026083112, 0.029944286, 0.026869457, 0.03203512, 0.03467084, 0.021729007, 0.032861244, 0.026953403, 0.036052138, 0.08870703, 0.24399471, 0.13197024, 0.15226553, 0.09929421, 0.10119892, 0.053222902, 0.20580088, 0.13509227, 0.106974825, 0.045222387, 0.034965266, 0.041512668, 0.045502, 0.06388289, 0.05529536, 0.04926591, 0.03726374, 0.058034427, 0.037526473, 0.033447344, 0.031642783, 0.0345782, 0.03669126, 0.030148057, 0.02488399, 0.030888291, 0.024925852, 0.034190074, 0.03579305, 0.033115376, 0.024611425, 0.043842483, 0.02893653, 0.02461332, 0.031337697, 0.029105393, 0.03750168, 0.06498271, 0.11527616, 0.10346058, 0.09032708, 0.10016381, 0.07250132, 0.11641438, 0.094881, 0.0740274, 0.068241335, 0.05751158, 0.050679643, 0.042316023, 0.045641094, 0.077530675, 0.048178304, 0.028991438, 0.054282337, 0.03643171, 0.04534253, 0.05542494, 0.042237498, 0.029947588, 0.032279626, 0.03129055, 0.039468072, 0.035012845, 0.0474523, 0.03698867, 0.036059324, 0.024000118, 0.042458903, 0.03179519, 0.048975386, 0.034200925, 0.02561438, 0.027901784, 0.02836817, 0.18164428, 0.16550794, 0.14640388, 0.1735253, 0.08620657, 0.068705216, 0.077490084, 0.12277775, 0.08148068, 0.13757205, 0.039814346, 0.04706872, 0.05487174, 0.045499705, 0.044247087, 0.04344386, 0.025608392, 0.035709716, 0.05653641, 0.059279624, 0.03150293, 0.02920784, 0.022974195, 0.034864906, 0.055632666, 0.0306123, 0.03932452, 0.033996895, 0.030186033, 0.03729014, 0.03481156, 0.030567542, 0.04108774, 0.032437693, 0.071520425, 0.03937236, 0.036895737, 0.022394527, 0.109077305, 0.116102956, 0.13202107, 0.111553244, 0.04664202, 0.102806054, 0.067094825, 0.08075824, 0.05039064, 0.054334477, 0.058759563, 0.0589119, 0.062244873, 0.044075258, 0.051771354, 0.06277702, 0.06563109, 0.06922531, 0.056191847, 0.05959952, 0.037922513, 0.04160368, 0.034043178, 0.035023086, 0.030231481, 0.033810977, 0.043200523, 0.037670113, 0.027034627, 0.031845387, 0.036810998, 0.03234278, 0.026120547, 0.031682972, 0.039182376, 0.028985867, 0.024729615, 0.03841612, 0.13714136, 0.3542364, 0.23268725, 0.10022452, 0.19166021, 0.090637684, 0.06289655, 0.075285494, 0.09410525, 0.06564411, 0.0509741, 0.04990941, 0.049943004, 0.041456915, 0.034894384, 0.035566583, 0.046045914, 0.03383981, 0.04401622, 0.046895016, 0.034312606, 0.03794999, 0.033806305, 0.04661207, 0.030038279, 0.035478584, 0.045845907, 0.033365645, 0.043838926, 0.037529044, 0.028170228, 0.026336083, 0.031921346, 0.022015069, 0.026947714, 0.03616148, 0.02858016, 0.027012574, 0.06536142, 0.09776629, 0.17131476, 0.09566524, 0.09910176, 0.06889481, 0.09345735, 0.077366054, 0.060505435, 0.07536257, 0.036415584, 0.06185342, 0.03286646, 0.051950738, 0.035880234, 0.051978625, 0.045435578, 0.042170253, 0.054724585, 0.036536675, 0.049250804, 0.04162434, 0.03329011, 0.037199758, 0.04864246, 0.03241554, 0.04488791, 0.050746303, 0.028561404, 0.032350495, 0.039204806, 0.047095794, 0.02795045, 0.04842642, 0.031668507, 0.03264063, 0.026273072, 0.03747227, 0.049460106, 0.030190555, 0.028047182, 0.0311001, 0.030865857, 0.112007506, 0.20049663, 0.12477559, 0.2662989, 0.15211554, 0.06129749, 0.052705515, 0.06253737, 0.0735872, 0.07561649, 0.06160947, 0.03669617, 0.056766078, 0.05055643, 0.052417893, 0.07450492, 0.086482115, 0.04766905, 0.050971523, 0.045793932, 0.04671466, 0.061070662, 0.04153666, 0.04600258, 0.034174263, 0.03235657, 0.036573924, 0.043904323, 0.031103881, 0.03122089, 0.037353605, 0.039178465, 0.03524994, 0.021787744, 0.036290552, 0.035768487, 0.027160192, 0.035010535, 0.067114845, 0.11194229, 0.102208555, 0.0813844, 0.1321037, 0.110663794, 0.087577425, 0.05995114, 0.09185161, 0.08993618, 0.12556398, 0.07287953, 0.064332165, 0.03961158, 0.045317616, 0.04919774, 0.07473114, 0.06352521, 0.061630774, 0.03507197, 0.05137072, 0.041253723, 0.034014698, 0.037059516, 0.028991103, 0.04212803, 0.043411236, 0.03526267, 0.036419883, 0.03186141, 0.031184338, 0.04476781, 0.04164188, 0.032236204, 0.03450842, 0.019490823, 0.024346033, 0.024434222, 0.06131066, 0.16685586, 0.0983933, 0.064825036, 0.18517691, 0.090664595, 0.054759685, 0.057113998, 0.053021044, 0.063204505, 0.08225817, 0.07393959, 0.030383894, 0.02800886, 0.04842642, 0.057173535, 0.034854654, 0.042360593, 0.04672327, 0.05309864, 0.0730475, 0.039340068, 0.03242879, 0.03261067, 0.026794305, 0.031904574, 0.03268459, 0.031031543, 0.027909225, 0.024035549, 0.032260757, 0.03618192, 0.031541634, 0.027752683, 0.04243911, 0.036157098, 0.024276858, 0.025243416, 0.07540923, 0.087083854, 0.11328263, 0.088285364, 0.1843601, 0.057807375, 0.06388792, 0.08281101, 0.13225564, 0.080341086, 0.05209486, 0.045851555, 0.052864462, 0.052531097, 0.03297566, 0.04605829, 0.051827736, 0.039852083, 0.031142429, 0.027946815, 0.03413996, 0.042541992, 0.026876023, 0.026066994, 0.045078877, 0.02151265, 0.03548687, 0.032511383, 0.026832549, 0.02374011, 0.03065583, 0.03438629, 0.034521397, 0.027357852, 0.02597271, 0.034220822, 0.028040051, 0.031052612, 0.1114119, 0.28630343, 0.23856208, 0.14827979, 0.14922205, 0.05454024, 0.09783011, 0.08347204, 0.10516037, 0.057323024, 0.08706002, 0.047505382, 0.04263542, 0.0442594, 0.04790756, 0.048851885, 0.03315591, 0.03713762, 0.032389775, 0.04576164, 0.038504418, 0.03616023, 0.028962176, 0.04022167, 0.03662625, 0.04526132, 0.028723469, 0.026773697, 0.026903387, 0.033538852, 0.026981046, 0.05401851, 0.025460523, 0.04598074, 0.036316648, 0.038026057, 0.030096192, 0.037309468, 0.027799208, 0.029159762, 0.0342521, 0.031988043, 0.024134768, 0.039246008, 0.068645746, 0.112196185, 0.1636399, 0.09948268, 0.09385047, 0.043450244, 0.07951247, 0.03902845, 0.04650558, 0.085204385, 0.077686526, 0.05598601, 0.049649686, 0.045341134, 0.065758854, 0.05345837, 0.04429409, 0.039436236, 0.035973668, 0.03432402, 0.052302875, 0.032844793, 0.0340617, 0.034036998, 0.040815137, 0.033563104, 0.03653921, 0.04281441, 0.026302898, 0.03289784, 0.038021944, 0.026866918, 0.022067225, 0.032604586, 0.031893417, 0.03037323, 0.028475955, 0.030333275, 0.09577024, 0.09913721, 0.23424582, 0.1310605, 0.06078391, 0.049247347, 0.06004272, 0.05978357, 0.10225348, 0.08449335, 0.07689212, 0.047368333, 0.07537319, 0.046496615, 0.06394273, 0.036597934, 0.030804954, 0.043897055, 0.046026424, 0.033411227, 0.041367553, 0.033901256, 0.06075122, 0.06346664, 0.030557705, 0.04180981, 0.029333895, 0.03339485, 0.022650577, 0.04164022, 0.03230489, 0.02428858, 0.030163163, 0.0378758, 0.040224668, 0.030754447, 0.03024814, 0.031083195, 0.1007838, 0.09513562, 0.112293355, 0.112634, 0.06420618, 0.12865093, 0.08957591, 0.04356055, 0.044547018, 0.06692632, 0.05655884, 0.073382914, 0.0844139, 0.046301197, 0.050875407, 0.035156816, 0.038720384, 0.03219192, 0.035737235, 0.041955326, 0.04135717, 0.040853053, 0.05244331, 0.041672852, 0.030075366, 0.032436352, 0.028267434, 0.033749152, 0.03671272, 0.024228282, 0.025827361, 0.035066795, 0.032099392, 0.035361547, 0.030012649, 0.026260123, 0.026347583, 0.044387475, 0.076389775, 0.10711882, 0.12118332, 0.15862712, 0.07911386, 0.055851348, 0.101308934, 0.1116636, 0.041563176, 0.05489122, 0.06430985, 0.041592095, 0.048063908, 0.08242053, 0.081119664, 0.050965834, 0.04171167, 0.03956728, 0.033619832, 0.034076363, 0.0417665, 0.041264657, 0.0402793, 0.018957106, 0.030970877, 0.031503905, 0.029241815, 0.02357302, 0.04405247, 0.03210035, 0.04129685, 0.027809607, 0.038014993, 0.023736931, 0.024718896, 0.02829875, 0.03880239, 0.023956029, 0.08444012, 0.07786101, 0.17873845, 0.10022104, 0.10325177, 0.08015172, 0.09345204, 0.06287451, 0.10288223, 0.09717228, 0.07963333, 0.037414663, 0.03133239, 0.050154336, 0.029247299, 0.04637247, 0.026900664, 0.03837197, 0.04124118, 0.036699, 0.042320695, 0.02850633, 0.048858054, 0.041308574, 0.03691009, 0.034793448, 0.03120406, 0.020946698, 0.026449606, 0.029427037, 0.035281826, 0.029056476, 0.032736886, 0.036559783, 0.02535867, 0.034840688, 0.030410115, 0.027686192, 0.033868134, 0.0319958, 0.03283239, 0.033700097, 0.04058926, 0.07008218, 0.11168186, 0.25406048, 0.1514361, 0.100480855, 0.054355033, 0.069867566, 0.03975591, 0.12326072, 0.07756791, 0.0572492, 0.06598505, 0.070436254, 0.053846624, 0.043400493, 0.04240823, 0.044059828, 0.04666177, 0.0443142, 0.040153284, 0.058461558, 0.040264003, 0.036798794, 0.024485152, 0.028347937, 0.033847366, 0.028330877, 0.030596783, 0.040499296, 0.045405176, 0.02997973, 0.022006722, 0.033379316, 0.022704992, 0.022023713, 0.03988445, 0.029644793, 0.03167747, 0.1010847, 0.16648011, 0.11639788, 0.11633541, 0.11517141, 0.082606755, 0.06636974, 0.0693033, 0.08289092, 0.067389175, 0.11937937, 0.065209635, 0.048786636, 0.04293199, 0.053427942, 0.04045903, 0.040734142, 0.033254832, 0.026357435, 0.03494482, 0.033728115, 0.030783787, 0.03300085, 0.032324255, 0.037694253, 0.0313561, 0.019089313, 0.028736064, 0.030558009, 0.028875617, 0.02697252, 0.042847298, 0.036884673, 0.028453516, 0.030597128, 0.031214664, 0.03177135, 0.029702328, 0.07902399, 0.17396487, 0.23147151, 0.118131496, 0.09864622, 0.047922533, 0.06599843, 0.07058415, 0.12461998, 0.062839024, 0.03345471, 0.04413098, 0.059923366, 0.045761038, 0.053082384, 0.043546822, 0.05161178, 0.039972987, 0.042678375, 0.04560115, 0.034666866, 0.04311262, 0.036181208, 0.030143017, 0.02380327, 0.034441233, 0.040046927, 0.039560925, 0.030858159, 0.03128051, 0.033113167, 0.027484013, 0.023897283, 0.043033995, 0.039177194, 0.03565811, 0.028419068, 0.027820839, 0.06926405, 0.10014889, 0.05962589, 0.1263084, 0.12530327, 0.09920279, 0.08830684, 0.06881088, 0.07259385, 0.08348443, 0.093629666, 0.078152366, 0.06415937, 0.079142496, 0.066919364, 0.04717448, 0.02115572, 0.029809669, 0.031015232, 0.029006595, 0.035663236, 0.05024866, 0.0486602, 0.026978677, 0.03634488, 0.02784414, 0.025136426, 0.03786264, 0.024078012, 0.03267752, 0.02595134, 0.027502948, 0.03129427, 0.025125498, 0.034804415, 0.028320381, 0.022143787, 0.03761219, 0.1161595, 0.11985385, 0.14167269, 0.11095613, 0.11116195, 0.078417376, 0.053420234, 0.10305792, 0.062187225, 0.07145823, 0.030601704, 0.054967176, 0.046719488, 0.03675446, 0.03863221, 0.063838996, 0.03939519, 0.032277383, 0.03827391, 0.03926722, 0.042844582, 0.035523728, 0.034748647, 0.031488575, 0.03131933, 0.027120508, 0.048940945, 0.030873587, 0.037887864, 0.038580403, 0.035582814, 0.028122274, 0.024154013, 0.031126237, 0.025780648, 0.026841557, 0.028689036, 0.024159668, 0.029455226, 0.028671265, 0.046142925, 0.036015127, 0.024780994, 0.026449786, 0.07214718, 0.13379784, 0.08061118, 0.08795763, 0.0749848, 0.07546358, 0.08071346, 0.035787713, 0.11368128, 0.07963976, 0.048859738, 0.08628402, 0.059176683, 0.042825278, 0.080435954, 0.056690168, 0.035005398, 0.027703542, 0.03123807, 0.034398012, 0.037188005, 0.03480456, 0.033199564, 0.03238828, 0.043771174, 0.03279521, 0.02673839, 0.03577932, 0.026371956, 0.026367852, 0.03465655, 0.029375205, 0.031312317, 0.030283118, 0.037237387, 0.020290432, 0.03765415, 0.048185892, 0.052332148, 0.23158076, 0.16937761, 0.13642147, 0.07553782, 0.09915651, 0.05561838, 0.035002083, 0.06479768, 0.06689095, 0.058308713, 0.03551572, 0.047816645, 0.05542464, 0.064390995, 0.053310405, 0.05375721, 0.033055775, 0.039013434, 0.05597754, 0.03579923, 0.046118982, 0.038743936, 0.031506214, 0.03272614, 0.04068583, 0.03090444, 0.032343306, 0.021666298, 0.05520086, 0.0345773, 0.029227812, 0.030360602, 0.03052708, 0.0347385, 0.024790328, 0.031033356, 0.015666103, 0.09385945, 0.16243482, 0.19781649, 0.15408504, 0.17986198, 0.14064609, 0.06551344, 0.066759735, 0.08739653, 0.06029493, 0.064767316, 0.091424964, 0.06991112, 0.07474707, 0.044856485, 0.039237913, 0.044447824, 0.043003626, 0.03911602, 0.061822403, 0.046860855, 0.037179157, 0.04072032, 0.04290198, 0.028074158, 0.03755343, 0.030916179, 0.03573367, 0.026924992, 0.035452094, 0.040454835, 0.032832135, 0.023548394, 0.029255835, 0.022669632, 0.035328988, 0.032816887, 0.029211795, 0.18114227, 0.1562018, 0.08877621, 0.06324291, 0.07018427, 0.050241362, 0.06253312, 0.055460785, 0.0323827, 0.07384305, 0.089892186, 0.052331775, 0.054927167, 0.064739026, 0.06425271, 0.03820495, 0.035013884, 0.031644195, 0.028979177, 0.0436443, 0.041033123, 0.045502882, 0.04035782, 0.039252657, 0.04231512, 0.04864543, 0.027623013, 0.025882373, 0.02559561, 0.036601257, 0.040122762, 0.023596948, 0.02677598, 0.025609683, 0.030508555, 0.022750514, 0.024889594, 0.029730344, 0.088617064, 0.15250416, 0.13162525, 0.10185519, 0.07830921, 0.062121466, 0.07955192, 0.048853625, 0.06881084, 0.113118224, 0.086647965, 0.04588178, 0.06323856, 0.04708232, 0.041555554, 0.03599, 0.044570986, 0.058372416, 0.029628662, 0.050382197, 0.038106706, 0.026227359, 0.039326936, 0.03760623, 0.0384485, 0.042142496, 0.039223954, 0.034808736, 0.03879694, 0.03838184, 0.02538832, 0.024124002, 0.021654598, 0.035770267, 0.030041635, 0.026154276, 0.025816726, 0.030946478, 0.03148211, 0.030308736, 0.024599686, 0.023527855, 0.02841252, 0.040324904, 0.14906988, 0.32224324, 0.15564862, 0.08564767, 0.075864054, 0.068492174, 0.07982036, 0.07846805, 0.067438506, 0.055581283, 0.053461846, 0.04944894, 0.039078265, 0.05683735, 0.032641865, 0.032852557, 0.02906233, 0.037647184, 0.04304576, 0.0383197, 0.035026204, 0.042412907, 0.030060621, 0.038956035, 0.04121708, 0.043329883, 0.03695986, 0.0381175, 0.024919646, 0.026905261, 0.029767146, 0.03698135, 0.02420202, 0.029318882, 0.030697616, 0.027997708, 0.022732008, 0.08776798, 0.09286196, 0.06369899, 0.07736622, 0.123374686, 0.12065387, 0.09359075, 0.09484871, 0.059148386, 0.11556365, 0.053735487, 0.03223011, 0.037492204, 0.056024075, 0.037755143, 0.02942404, 0.037742995, 0.027108276, 0.053033106, 0.040244464, 0.030932728, 0.038072433, 0.0483966, 0.029693631, 0.02656937, 0.03054221, 0.03426629, 0.040179092, 0.027145648, 0.03913921, 0.03885459, 0.041386873, 0.024615692, 0.025489911, 0.029015923, 0.024176199, 0.037534907, 0.026187748, 0.16307613, 0.18366571, 0.16463783, 0.10191108, 0.16225798, 0.13205938, 0.09489995, 0.04504455, 0.05617216, 0.080326654, 0.064510696, 0.03858899, 0.03954351, 0.044558544, 0.05334942, 0.045767985, 0.04362294, 0.024115942, 0.025529608, 0.046184383, 0.0461092, 0.040397633, 0.04536796, 0.034204394, 0.025453916, 0.02854, 0.03372658, 0.0327614, 0.028373515, 0.036743652, 0.03072574, 0.026192753, 0.033732112, 0.03134319, 0.027784687, 0.02156246, 0.040418923, 0.024004668, 0.096303284, 0.2208715, 0.21186571, 0.114886776, 0.11195519, 0.07767175, 0.07153077, 0.07056521, 0.052480593, 0.050810255, 0.055315506, 0.04094862, 0.074961044, 0.047287423, 0.0412971, 0.04403368, 0.037451338, 0.05941251, 0.043047473, 0.03290682, 0.031932484, 0.029070942, 0.038552906, 0.040539853, 0.06581135, 0.026080914, 0.036783885, 0.031404972, 0.026976407, 0.03579348, 0.04036124, 0.03456216, 0.02969071, 0.029888555, 0.027741589, 0.025798215, 0.031834457, 0.03295409, 0.06907289, 0.103050165, 0.18354207, 0.16780989, 0.21686947, 0.16272402, 0.069815926, 0.09308975, 0.07169614, 0.06360509, 0.07981718, 0.056523368, 0.06781351, 0.059103966, 0.060920108, 0.05184685, 0.048568632, 0.04355107, 0.037153054, 0.03386149, 0.036431517, 0.047700506, 0.041275788, 0.043653876, 0.035964638, 0.03590513, 0.044211064, 0.03507885, 0.036186643, 0.026231574, 0.03563556, 0.035701532, 0.035904557, 0.03456866, 0.036864772, 0.04849363, 0.0328956, 0.024254993, 0.03979611, 0.04459553, 0.03026787, 0.03467859, 0.029224938, 0.03746469, 0.18479012, 0.1873898, 0.27682328, 0.09759279, 0.07726524, 0.054863144, 0.15528636, 0.07227355, 0.05058934, 0.06648298, 0.10474065, 0.048994206, 0.06966159, 0.07854086, 0.047554255, 0.038543034, 0.058593128, 0.043741185, 0.05673362, 0.041636664, 0.037715394, 0.041580863, 0.033664543, 0.02990138, 0.030446557, 0.028147656, 0.036418807, 0.027300054, 0.04085805, 0.034343846, 0.03529349, 0.033636317, 0.033379126, 0.03102264, 0.02846483, 0.024141964, 0.045513123, 0.039882604, 0.12998493, 0.09728364, 0.1169758, 0.07224452, 0.05793023, 0.035811633, 0.11073479, 0.079339184, 0.08910753, 0.06857504, 0.065371506, 0.054205894, 0.04026861, 0.049052753, 0.033792045, 0.031260315, 0.06889229, 0.049545597, 0.046810266, 0.041556247, 0.037733484, 0.03267734, 0.034060974, 0.038882483, 0.029906088, 0.037500355, 0.035509642, 0.025970727, 0.026905518, 0.029003736, 0.031001072, 0.044428933, 0.028905397, 0.024610035, 0.03346197, 0.025138048, 0.023792872, 0.032229867, 0.09016101, 0.10203136, 0.219516, 0.10249726, 0.070578, 0.07531279, 0.046783138, 0.051509775, 0.07167549, 0.042762656, 0.058302466, 0.039326746, 0.03940067, 0.07051453, 0.09383335, 0.031986266, 0.054046184, 0.055730015, 0.042186517, 0.025797632, 0.03132982, 0.04615915, 0.04308483, 0.037963934, 0.032195285, 0.026727576, 0.04325895, 0.026796674, 0.033954795, 0.04755322, 0.026394065, 0.027564978, 0.024261236, 0.041776508, 0.024869373, 0.036070798, 0.02950035, 0.027842814, 0.07815466, 0.13651785, 0.08039471, 0.1003065, 0.097163424, 0.12007418, 0.10054534, 0.112639844, 0.07416672, 0.08286125, 0.062356416, 0.05439075, 0.054633666, 0.04620844, 0.03582883, 0.038010288, 0.050822586, 0.03580965, 0.05787004, 0.036481332, 0.04040576, 0.03840084, 0.036197074, 0.031258494, 0.031840995, 0.033043075, 0.037708677, 0.033287495, 0.027759505, 0.026490336, 0.025863249, 0.02802538, 0.030710403, 0.03574268, 0.044212304, 0.02786408, 0.024047887, 0.022678467, 0.060433697, 0.15612367, 0.0949622, 0.05917711, 0.0662141, 0.058045994, 0.12885518, 0.09579011, 0.092172295, 0.040685326, 0.049631577, 0.037431527, 0.05073223, 0.036425874, 0.03883003, 0.05701025, 0.040962327, 0.045515306, 0.06444513, 0.037653804, 0.044357833, 0.046262432, 0.0525056, 0.043785192, 0.039573986, 0.055694133, 0.04405591, 0.041730497, 0.027590202, 0.031239247, 0.029306926, 0.038509354, 0.025148565, 0.04362162, 0.034936428, 0.03277406, 0.032979295, 0.02581129, 0.028411087, 0.02431473, 0.027177958, 0.029437918, 0.034150552, 0.09450531, 0.2098947, 0.16432284, 0.14145975, 0.11825731, 0.08134818, 0.12771554, 0.057120472, 0.038963534, 0.052765645, 0.03437397, 0.0633474, 0.035964467, 0.03563592, 0.061779223, 0.047613103, 0.04917852, 0.038966756, 0.040855028, 0.037622076, 0.029850915, 0.03057786, 0.034287523, 0.032884587, 0.03841399, 0.049083244, 0.059046242, 0.039218236, 0.032134663, 0.029886302, 0.033388402, 0.024378177, 0.026655288, 0.026540779, 0.03175923, 0.03722547, 0.044504404, 0.029749628, 0.08028587, 0.15151156, 0.2330514, 0.13274191, 0.08259299, 0.056833718, 0.051311698, 0.05415061, 0.06869248, 0.04310232, 0.052914124, 0.045043595, 0.039585274, 0.04668631, 0.045539487, 0.044165462, 0.04643848, 0.035721164, 0.033562064, 0.03973251, 0.041836698, 0.049093846, 0.05348101, 0.03636162, 0.023217592, 0.03620148, 0.023120983, 0.03820991, 0.0325762, 0.03515342, 0.02694908, 0.04092146, 0.026220584, 0.0232833, 0.039697822, 0.044721108, 0.02878802, 0.02930903, 0.08532021, 0.06673993, 0.21355702, 0.13686755, 0.10725172, 0.17281951, 0.05541265, 0.05515105, 0.08273807, 0.044716224, 0.038801484, 0.038633265, 0.04769413, 0.037482493, 0.07799311, 0.06665895, 0.07653798, 0.044948027, 0.039118674, 0.033639334, 0.035206567, 0.031429954, 0.023829972, 0.02944778, 0.024774553, 0.039877165, 0.038966067, 0.023012182, 0.032317728, 0.020609783, 0.025519965, 0.028707162, 0.03385194, 0.02681352, 0.02600363, 0.021642197, 0.03631734, 0.029766, 0.088011354, 0.227938, 0.08483785, 0.08007914, 0.10450505, 0.07830546, 0.09550615, 0.077366255, 0.0692741, 0.04406743, 0.071159504, 0.06511763, 0.054609925, 0.067529626, 0.050253063, 0.054782014, 0.058091182, 0.051177897, 0.035979424, 0.026441392, 0.030355481, 0.05151584, 0.039752785, 0.028585032, 0.040114325, 0.023654211, 0.038991846, 0.02483708, 0.02929665, 0.028277127, 0.03239777, 0.02650426, 0.038628083, 0.038763136, 0.023981255, 0.024718119, 0.031139249, 0.028727701, 0.21055575, 0.09599891, 0.09474853, 0.11089448, 0.08439158, 0.07170115, 0.06666294, 0.07317825, 0.06632009, 0.0664741, 0.06257473, 0.084432974, 0.039437372, 0.046562247, 0.04430815, 0.04462245, 0.035135776, 0.02797333, 0.04488483, 0.0434319, 0.04422774, 0.024155345, 0.033197653, 0.0365393, 0.03568131, 0.037086975, 0.04109018, 0.033430975, 0.03938465, 0.027333071, 0.02962862, 0.03396159, 0.026118219, 0.03368652, 0.027413977, 0.024249101, 0.02967452, 0.043711234, 0.034333594, 0.02849935, 0.026894823, 0.033598732, 0.029371701, 0.021895347, 0.09831239, 0.24872124, 0.17412272, 0.17219062, 0.059233736, 0.051761623, 0.08253829, 0.07339971, 0.0798555, 0.09678222, 0.040773835, 0.04379453, 0.04508997, 0.03886762, 0.055898026, 0.04181277, 0.044813477, 0.040619712, 0.029940998, 0.03244246, 0.030479858, 0.031196333, 0.035295464, 0.032220438, 0.033976294, 0.0258437, 0.02811586, 0.04409372, 0.0329695, 0.025774322, 0.025863832, 0.044132125, 0.025752734, 0.027478337, 0.032496758, 0.03980344, 0.023716753, 0.019776627, 0.09421752, 0.124334596, 0.15229233, 0.2529576, 0.1755085, 0.086903475, 0.061131258, 0.057030868, 0.077825926, 0.07393349, 0.051142123, 0.038080543, 0.059853405, 0.06274464, 0.044087548, 0.04895111, 0.046119224, 0.04563309, 0.03047878, 0.026955545, 0.037972778, 0.031089507, 0.036408897, 0.026382212, 0.032564756, 0.0367492, 0.042780418, 0.028487591, 0.029350813, 0.025010329, 0.02626302, 0.035428423, 0.036349997, 0.024068698, 0.01984089, 0.02981835, 0.035438452, 0.02027112, 0.15228882, 0.3134834, 0.18279052, 0.10557167, 0.09288726, 0.094689675, 0.07627925, 0.050964125, 0.11067827, 0.051511813, 0.079271734, 0.048831504, 0.04423693, 0.028359657, 0.046194743, 0.031762753, 0.047778636, 0.034452368, 0.024062252, 0.034430396, 0.032858323, 0.027025506, 0.031569112, 0.029327646, 0.03636383, 0.03560786, 0.03270183, 0.043286473, 0.031707697, 0.02659075, 0.02865668, 0.040524427, 0.03281035, 0.03273911, 0.03734123, 0.029910699, 0.025807414, 0.04373204, 0.0767534, 0.13210519, 0.07793645, 0.108252786, 0.04458781, 0.041883778, 0.0457461, 0.055091307, 0.050837755, 0.060738865, 0.05014076, 0.07575159, 0.046352796, 0.05297972, 0.09841151, 0.049356036, 0.04949521, 0.029657913, 0.04764797, 0.04319354, 0.03696894, 0.03387317, 0.04029601, 0.04504459, 0.043933596, 0.04173276, 0.03458215, 0.02971496, 0.031358052, 0.02463385, 0.027804673, 0.03892077, 0.04139189, 0.035942595, 0.024577219, 0.022389984, 0.026838165, 0.020296244, 0.104682334, 0.062298015, 0.10232007, 0.13278322, 0.05725515, 0.057343557, 0.05036305, 0.04824671, 0.058267336, 0.041051865, 0.0574793, 0.07273972, 0.054419525, 0.04678302, 0.02845756, 0.02669515, 0.041373257, 0.037800714, 0.056715395, 0.057493687, 0.025487656, 0.04415942, 0.047258686, 0.021801181, 0.032722104, 0.038285073, 0.034505486, 0.022275053, 0.033836424, 0.030780224, 0.038674805, 0.034016415, 0.02292132, 0.029183704, 0.024134204, 0.030931706, 0.02599515, 0.024831548, 0.025127724, 0.029606113, 0.034945007, 0.02413526, 0.029261267, 0.11775142, 0.17348205, 0.153514, 0.17165846, 0.067001335, 0.09870969, 0.06395104, 0.07358034, 0.14839979, 0.13598126, 0.11295784, 0.053193577, 0.045333937, 0.04026856, 0.03272432, 0.051144034, 0.038337, 0.031850293, 0.04835346, 0.043791324, 0.03193686, 0.037728757, 0.036038727, 0.029158153, 0.035204586, 0.03253757, 0.04316886, 0.029534148, 0.034071703, 0.029612398, 0.03982975, 0.033643514, 0.025416048, 0.023046585, 0.02755016, 0.028781427, 0.044116527, 0.034317326, 0.09158091, 0.1085274, 0.08346628, 0.15749653, 0.1725756, 0.07989211, 0.06507178, 0.0592325, 0.07911474, 0.1085767, 0.049597837, 0.03508815, 0.03301652, 0.048907895, 0.03666268, 0.048983227, 0.03359861, 0.041079145, 0.059990846, 0.032648176, 0.042121805, 0.036361832, 0.046372794, 0.03863272, 0.04293862, 0.047126587, 0.039237052, 0.04059658, 0.020202983, 0.03384602, 0.03186633, 0.024571175, 0.02366367, 0.044378642, 0.03926656, 0.032688987, 0.02397889, 0.021028068, 0.063274816, 0.08887287, 0.07444765, 0.11029722, 0.077208675, 0.079956435, 0.05603108, 0.2279647, 0.12621185, 0.11896895, 0.06116029, 0.050039023, 0.045198668, 0.06472262, 0.037489213, 0.052059967, 0.042775586, 0.034284435, 0.03683727, 0.040644914, 0.046273574, 0.035458215, 0.045897342, 0.039513625, 0.03130346, 0.03702705, 0.024888687, 0.032645922, 0.04369709, 0.030695265, 0.038838275, 0.025092954, 0.047947876, 0.0376076, 0.03759679, 0.024851367, 0.031202137, 0.030787898, 0.08169987, 0.11521136, 0.15646596, 0.071421854, 0.10535614, 0.2116982, 0.10483242, 0.088531174, 0.09508222, 0.14794245, 0.0528236, 0.041095566, 0.05396535, 0.029280575, 0.034382693, 0.05203052, 0.04846065, 0.06282073, 0.06234437, 0.059671905, 0.05256488, 0.04716685, 0.03708889, 0.026081884, 0.02893494, 0.017722478, 0.027764484, 0.030642996, 0.027494248, 0.034474775, 0.026426664, 0.028030753, 0.031375118, 0.0296119, 0.02551445, 0.033322334, 0.02771284, 0.030454008, 0.09834204, 0.07661938, 0.07639581, 0.082331836, 0.07132778, 0.11359891, 0.09824521, 0.06128976, 0.06564584, 0.059750605, 0.053496443, 0.0533595, 0.038757734, 0.040743805, 0.07289493, 0.039265376, 0.038217567, 0.060028903, 0.04260439, 0.03004335, 0.03184449, 0.04309788, 0.040771782, 0.04217908, 0.03604609, 0.02683994, 0.028294692, 0.03967087, 0.036411185, 0.036174674, 0.03220917, 0.034411874, 0.023008894, 0.044241004, 0.028325241, 0.03055118, 0.037814885, 0.025950832, 0.023894245, 0.025276931, 0.042612918, 0.022480534, 0.025354235, 0.023629928, 0.07571788, 0.083603546, 0.17044067, 0.09453247, 0.07612454, 0.056029804, 0.046596907, 0.115197204, 0.076418504, 0.070924304, 0.03619225, 0.05558464, 0.03934259, 0.05341372, 0.042984795, 0.04130733, 0.03079367, 0.03631631, 0.03607468, 0.031507462, 0.03422862, 0.02154658, 0.03358274, 0.031317603, 0.033238392, 0.034477238, 0.03432406, 0.029633915, 0.028060235, 0.029948238, 0.025805736, 0.034323864, 0.035801195, 0.023476958, 0.033503424, 0.02215516, 0.021804139, 0.026554948, 0.06178549, 0.20305058, 0.25893494, 0.14691134, 0.09841847, 0.075431526, 0.11688834, 0.12694213, 0.10597033, 0.070361905, 0.053031057, 0.044672035, 0.100397155, 0.085808784, 0.06552432, 0.051469672, 0.039422125, 0.03277466, 0.034310807, 0.042979687, 0.03569454, 0.02995282, 0.03976209, 0.035004545, 0.032718588, 0.027837615, 0.043181386, 0.031087875, 0.030550957, 0.02444998, 0.034119643, 0.02348696, 0.030999152, 0.019950002, 0.025501126, 0.03338799, 0.026232962, 0.03604604, 0.11707955, 0.10787718, 0.06768217, 0.15504569, 0.08296307, 0.049306624, 0.094617285, 0.06226251, 0.070869684, 0.10374761, 0.046335157, 0.085020125, 0.064416334, 0.060989268, 0.04986409, 0.029211769, 0.043478608, 0.035628196, 0.037790786, 0.03347205, 0.033191696, 0.04236896, 0.040738493, 0.025556838, 0.03383406, 0.030736258, 0.03709546, 0.025340177, 0.027092865, 0.03270415, 0.025835266, 0.032625686, 0.030038513, 0.030188693, 0.03663364, 0.039553553, 0.027991837, 0.024031572, 0.09488207, 0.1722178, 0.16771768, 0.0822315, 0.058218874, 0.109684885, 0.09377688, 0.10646189, 0.09922528, 0.060759984, 0.057127, 0.05050688, 0.050157916, 0.04208249, 0.0577651, 0.040842306, 0.04946377, 0.059665103, 0.043039687, 0.04168352, 0.06526427, 0.038268197, 0.043269433, 0.031582315, 0.023782713, 0.028997874, 0.031613946, 0.036872253, 0.033353895, 0.03787199, 0.028913883, 0.032752875, 0.02369588, 0.046841845, 0.022002883, 0.038248926, 0.030293029, 0.042336382, 0.105087206, 0.18895678, 0.09809345, 0.11981821, 0.15628888, 0.052503552, 0.09004495, 0.041993327, 0.08247821, 0.11477089, 0.091033086, 0.055093616, 0.054656554, 0.042503785, 0.040220454, 0.03468222, 0.05977254, 0.036988966, 0.034142956, 0.046412934, 0.028728127, 0.028045278, 0.037279442, 0.03417823, 0.030294776, 0.046100836, 0.03424136, 0.04074452, 0.026556583, 0.020951746, 0.036567003, 0.040424045, 0.026561351, 0.028211465, 0.025943944, 0.036673725, 0.032974564, 0.039081167, 0.07180376, 0.12737668, 0.10251567, 0.102533266, 0.077029064, 0.072215736, 0.06871089, 0.116391204, 0.08590438, 0.092680596, 0.052047387, 0.04884986, 0.062291514, 0.033847433, 0.053185645, 0.03447773, 0.03412746, 0.039953884, 0.043410648, 0.044778265, 0.03605023, 0.02638239, 0.03187012, 0.033414867, 0.032311942, 0.030979427, 0.02589457, 0.025748666, 0.031001018, 0.03358299, 0.025088584, 0.021616375, 0.025265362, 0.031951576, 0.033320796, 0.026569555, 0.033589844, 0.0204795, 0.065072306, 0.0795381, 0.108133025, 0.12270572, 0.05650159, 0.057185713, 0.05895705, 0.05454402, 0.07999055, 0.09203026, 0.06737467, 0.057740524, 0.04121672, 0.054935858, 0.04093894, 0.038591802, 0.04307164, 0.045750633, 0.03267303, 0.043899566, 0.031868692, 0.028685346, 0.026877802, 0.03455125, 0.039929383, 0.04518298, 0.027470592, 0.041850947, 0.03131561, 0.027338738, 0.033450246, 0.02863741, 0.027832847, 0.02397466, 0.023916299, 0.033810508, 0.041723512, 0.02368254, 0.0956552, 0.15794453, 0.15015893, 0.13365884, 0.06517932, 0.175117, 0.05416119, 0.07223115, 0.07687093, 0.112801716, 0.04391961, 0.037241645, 0.05558851, 0.028856713, 0.03721121, 0.04962939, 0.041960828, 0.052684497, 0.038600195, 0.031710193, 0.04147822, 0.039440542, 0.04661937, 0.03740241, 0.025409287, 0.038093828, 0.028857237, 0.029287621, 0.034206748, 0.032608956, 0.040972855, 0.031569, 0.026664712, 0.03437809, 0.03619576, 0.02260566, 0.024031952, 0.027375083, 0.1356308, 0.15643196, 0.10071413, 0.05613937, 0.05892416, 0.0483309, 0.055973604, 0.061913002, 0.058958393, 0.10121695, 0.04067522, 0.047849353, 0.046460923, 0.04042439, 0.06301881, 0.06388201, 0.031425405, 0.033672366, 0.03549363, 0.05585565, 0.040613383, 0.04028938, 0.045793775, 0.05069346, 0.033949934, 0.030548315, 0.03465469, 0.027774816, 0.024699463, 0.038868994, 0.030460022, 0.029498884, 0.025783742, 0.030229384, 0.03296209, 0.021970024, 0.025231427, 0.025449757, 0.09127322, 0.13350904, 0.18287306, 0.20925185, 0.097280964, 0.054335047, 0.057959512, 0.044551305, 0.08629471, 0.06516352, 0.029504633, 0.038029607, 0.041676, 0.033870623, 0.048787083, 0.046196513, 0.04038563, 0.040893923, 0.028224166, 0.02864699, 0.038568724, 0.034546904, 0.023257265, 0.031207677, 0.029198373, 0.044776343, 0.05709697, 0.034275178, 0.03251494, 0.03399616, 0.061282177, 0.036150627, 0.033050448, 0.032030735, 0.026380902, 0.030900672, 0.038213097, 0.02800763, 0.018557766, 0.032972354, 0.022668567, 0.037452374, 0.027368207, 0.11724102, 0.20677993, 0.1648671, 0.1114354, 0.12408882, 0.089736745, 0.074607424, 0.057077933, 0.042984057, 0.04672898, 0.04425029, 0.055052638, 0.061258882, 0.05866315, 0.030298833, 0.036787134, 0.06330876, 0.035695862, 0.039844688, 0.024538519, 0.03607256, 0.031560253, 0.0357892, 0.033313297, 0.03732902, 0.026653575, 0.033972356, 0.041268617, 0.032899726, 0.044966448, 0.031077357, 0.03429611, 0.037104357, 0.031979855, 0.036848105, 0.03538737, 0.027463486, 0.03465822, 0.05684948, 0.08152999, 0.11137997, 0.098962255, 0.09526522, 0.050710313, 0.078775235, 0.072356485, 0.10700197, 0.114304006, 0.07181463, 0.053029224, 0.043027878, 0.043556366, 0.06779346, 0.0641567, 0.077652976, 0.041485097, 0.048224803, 0.046681967, 0.03386995, 0.038749564, 0.038649045, 0.04611692, 0.039996523, 0.041301984, 0.026999015, 0.031556763, 0.024354331, 0.026864422, 0.025890877, 0.04964843, 0.027690962, 0.035437163, 0.027582709, 0.024356144, 0.02828901, 0.028311547, 0.10008396, 0.108380355, 0.18062045, 0.08463635, 0.121794075, 0.10052234, 0.10119056, 0.08436183, 0.061054017, 0.08638197, 0.093578614, 0.05403118, 0.045032233, 0.03686943, 0.025312912, 0.063075304, 0.03966703, 0.042102236, 0.028235558, 0.027926087, 0.024332156, 0.056317613, 0.03402463, 0.04826191, 0.035074636, 0.045740902, 0.024899598, 0.027820146, 0.0314621, 0.03382397, 0.030592496, 0.029094439, 0.027957546, 0.030420477, 0.019516742, 0.038092516, 0.031497598, 0.030034913, 0.041810464, 0.06325222, 0.07669055, 0.10400206, 0.10585909, 0.12136096, 0.09057527, 0.09021816, 0.06506464, 0.09519793, 0.053118788, 0.030023593, 0.08588688, 0.06720285, 0.04008288, 0.05072566, 0.029996404, 0.036511417, 0.027248818, 0.04665061, 0.027389187, 0.045942295, 0.035205003, 0.033355672, 0.03873953, 0.041859847, 0.047213107, 0.028765816, 0.036919855, 0.038378626, 0.034512978, 0.027880564, 0.023547161, 0.025411459, 0.02579316, 0.030343903, 0.041836772, 0.032852042, 0.08709536, 0.16310616, 0.08363801, 0.11143813, 0.12997174, 0.1700347, 0.12445677, 0.11209469, 0.0887368, 0.060063172, 0.06921708, 0.030397808, 0.03723166, 0.046353817, 0.07589827, 0.042755276, 0.035068035, 0.026556969, 0.03639957, 0.04321637, 0.036092907, 0.03905024, 0.04324592, 0.029207679, 0.029432738, 0.041389488, 0.028421706, 0.032580864, 0.025461104, 0.043039195, 0.041379075, 0.027454775, 0.018572185, 0.03057091, 0.021519909, 0.025436647, 0.031260595, 0.03482828, 0.028882995, 0.031272553, 0.016206881, 0.02345678, 0.0232979, 0.02091659, 0.079933874, 0.10980767, 0.13869776, 0.15141207, 0.10067348, 0.062153816, 0.107691705, 0.0496109, 0.04516216, 0.07200053, 0.043959018, 0.03518948, 0.045075864, 0.042242836, 0.029941298, 0.026906408, 0.036330312, 0.032928832, 0.037113756, 0.05030823, 0.045692433, 0.041056205, 0.023925766, 0.029669417, 0.03863551, 0.037501905, 0.03251755, 0.040875692, 0.02964069, 0.02398845, 0.035411805, 0.033801626, 0.029581532, 0.035127666, 0.027796768, 0.029168839, 0.032298665, 0.040947154, 0.12821016, 0.20521185, 0.09547396, 0.12306626, 0.08285532, 0.06529977, 0.05820403, 0.17019086, 0.040114656, 0.07333158, 0.05829365, 0.04465185, 0.052172586, 0.03909841, 0.07528574, 0.032914806, 0.038551103, 0.03140438, 0.042260814, 0.030489262, 0.032386202, 0.04582877, 0.029792383, 0.033458017, 0.024870181, 0.032702092, 0.023791429, 0.03531684, 0.03635201, 0.023651088, 0.0378495, 0.035523146, 0.027585434, 0.029836247, 0.024324011, 0.022692865, 0.0367448, 0.028857464, 0.097752675, 0.14563668, 0.110409446, 0.10900448, 0.055466168, 0.059485782, 0.04838444, 0.06806629, 0.054208342, 0.0443977, 0.033109605, 0.05399726, 0.040661894, 0.049009968, 0.12420343, 0.0606866, 0.036937702, 0.03831728, 0.027323723, 0.04315258, 0.044454698, 0.054981597, 0.030105151, 0.03109132, 0.04664475, 0.0366056, 0.020463837, 0.030409496, 0.03214393, 0.03137796, 0.02710398, 0.032988977, 0.028738316, 0.024627531, 0.030640354, 0.030486992, 0.03244962, 0.039980393, 0.1048965, 0.22047094, 0.3082215, 0.23333007, 0.11426469, 0.074369945, 0.055806216, 0.10734714, 0.08067178, 0.07129901, 0.056627244, 0.058048863, 0.039354935, 0.043303546, 0.08441709, 0.04469196, 0.04418843, 0.04447133, 0.033424586, 0.028877461, 0.049798995, 0.037408724, 0.025593773, 0.026784131, 0.06677384, 0.031033074, 0.028967055, 0.024751559, 0.03632216, 0.025761003, 0.03944385, 0.0415171, 0.026798273, 0.034542542, 0.027089113, 0.029527921, 0.03610183, 0.032557994, 0.047742605, 0.15599276, 0.1696021, 0.14278819, 0.19366586, 0.06514904, 0.094174616, 0.06024729, 0.05579552, 0.04908046, 0.071909115, 0.050332475, 0.034826826, 0.052531645, 0.04677856, 0.03047043, 0.056404993, 0.03362943, 0.05844273, 0.03583226, 0.05098133, 0.03839842, 0.026740698, 0.02547971, 0.03270092, 0.03427808, 0.038345665, 0.048847783, 0.035996046, 0.04086445, 0.041417878, 0.034464043, 0.03266079, 0.019878384, 0.03079766, 0.02553777, 0.034077097, 0.03854272, 0.039997306, 0.024523044, 0.045202173, 0.029810203, 0.02846352, 0.055429745, 0.1607614, 0.084987536, 0.10156772, 0.0689955, 0.05598635, 0.047971953, 0.112090014, 0.0969408, 0.06291873, 0.035303444, 0.031142194, 0.04676392, 0.03554869, 0.053459506, 0.04313484, 0.041005485, 0.02861327, 0.04580371, 0.033639576, 0.028685369, 0.03491196, 0.03451394, 0.027194802, 0.021955485, 0.038177714, 0.031158928, 0.02701104, 0.039953336, 0.025595233, 0.03166878, 0.030677116, 0.045746025, 0.028231585, 0.025160464, 0.029281264, 0.021299649, 0.028170718, 0.06607308, 0.05178962, 0.15856016, 0.16786398, 0.07551998, 0.045043968, 0.07879096, 0.090780854, 0.051773682, 0.05357912, 0.058294225, 0.05756691, 0.04258697, 0.07568817, 0.06471914, 0.05704086, 0.053331066, 0.036007293, 0.043167006, 0.043966103, 0.023909332, 0.037558865, 0.029586053, 0.031866256, 0.038993318, 0.03712981, 0.02702033, 0.039882485, 0.043651916, 0.031779837, 0.037357707, 0.026001178, 0.024911668, 0.039849073, 0.032969695, 0.027607918, 0.028084282, 0.020025522, 0.15862596, 0.083332114, 0.06685177, 0.080957085, 0.09428629, 0.096328326, 0.055543512, 0.1046731, 0.13668218, 0.08404471, 0.07001742, 0.06429476, 0.061063044, 0.042057604, 0.0470152, 0.032170027, 0.043999974, 0.03543889, 0.044761885, 0.029194035, 0.056589477, 0.03912245, 0.03643158, 0.042793263, 0.025473485, 0.036956154, 0.031244805, 0.0507515, 0.03681784, 0.028495716, 0.041914333, 0.043363858, 0.02352854, 0.027685074, 0.022225425, 0.029075598, 0.03319169, 0.023398712, 0.07162815, 0.07328488, 0.04652637, 0.08326962, 0.14887516, 0.085458696, 0.059688557, 0.07548857, 0.07682936, 0.15884437, 0.10871544, 0.060516465, 0.040394753, 0.03354579, 0.026673065, 0.08412395, 0.06378918, 0.03516132, 0.037174493, 0.044509586, 0.033366237, 0.037769463, 0.032189175, 0.030361589, 0.031719852, 0.038880963, 0.03711969, 0.031249857, 0.037420694, 0.024494251, 0.021474522, 0.032771494, 0.032956928, 0.031913623, 0.03977792, 0.029007388, 0.033077646, 0.022958472, 0.084188476, 0.21656182, 0.26042426, 0.19487546, 0.11887013, 0.14261128, 0.16033395, 0.12793863, 0.053461194, 0.08567979, 0.121955015, 0.06603665, 0.04905792, 0.08557503, 0.069339655, 0.047999833, 0.05062374, 0.028027471, 0.037644077, 0.038817503, 0.034713954, 0.025569936, 0.032191526, 0.025437549, 0.04929153, 0.042080577, 0.037201095, 0.051799517, 0.028310644, 0.028350614, 0.032317903, 0.027214276, 0.034136776, 0.027753973, 0.029757308, 0.040343083, 0.026017139, 0.035123546, 0.026428295, 0.031811554, 0.032963507, 0.025427796, 0.020846516, 0.02794123, 0.056076538, 0.0988275, 0.08549327, 0.11740941, 0.06412355, 0.071630776, 0.07580763, 0.076860845, 0.07094344, 0.081678465, 0.06525829, 0.05132603, 0.05721988, 0.056674812, 0.047736607, 0.04863549, 0.030181363, 0.028628202, 0.03797662, 0.02880775, 0.036052406, 0.028077051, 0.05153421, 0.03853124, 0.030846747, 0.02809527, 0.034430947, 0.02984598, 0.03220881, 0.028603371, 0.03108937, 0.025553687, 0.021573026, 0.023918614, 0.037474796, 0.030975722, 0.023248892, 0.029081574, 0.05334004, 0.065936916, 0.055097405, 0.08711961, 0.041018207, 0.041789707, 0.051104505, 0.057899382, 0.043956485, 0.052531756, 0.04730974, 0.03440541, 0.042215105, 0.03256445, 0.052819572, 0.021274524, 0.040049713, 0.034456234, 0.033787202, 0.0412629, 0.04237152, 0.025811337, 0.035076473, 0.022571044, 0.025965123, 0.038291883, 0.02423325, 0.028841408, 0.050341137, 0.030431785, 0.02242824, 0.03305115, 0.04294943, 0.033756945, 0.037291013, 0.02931821, 0.025994448, 0.029574601, 0.07874603, 0.10607687, 0.16000281, 0.148526, 0.083407104, 0.093412295, 0.050794613, 0.09767166, 0.13055076, 0.056987368, 0.0726824, 0.052182388, 0.06501041, 0.041916355, 0.04231537, 0.073495805, 0.08251017, 0.043994475, 0.030031407, 0.033849735, 0.02962726, 0.03492136, 0.029320644, 0.040675826, 0.0388494, 0.04454564, 0.033598393, 0.028292164, 0.0329669, 0.028319083, 0.02792263, 0.029293697, 0.031180821, 0.032168206, 0.02409793, 0.025996326, 0.022579603, 0.0259664, 0.06918253, 0.17917657, 0.15166824, 0.14274319, 0.08346572, 0.048600543, 0.09802206, 0.08353738, 0.072642, 0.041352436, 0.06274472, 0.052397348, 0.04727272, 0.030817358, 0.07883991, 0.07373257, 0.04591759, 0.04449439, 0.03351453, 0.04044174, 0.05201528, 0.029687308, 0.03203081, 0.036348153, 0.032112088, 0.04145045, 0.027663387, 0.022671325, 0.037233304, 0.023785308, 0.02400852, 0.025283827, 0.024904147, 0.040330645, 0.030352684, 0.029440215, 0.027154882, 0.025342213, 0.095550016, 0.12943803, 0.12007731, 0.1372322, 0.067912735, 0.07477645, 0.040206354, 0.121386915, 0.13213064, 0.08689256, 0.043128457, 0.038801923, 0.03411087, 0.043579355, 0.04273186, 0.038688768, 0.03690055, 0.037829567, 0.04634896, 0.04469785, 0.05799522, 0.045698997, 0.0283294, 0.035917763, 0.025228037, 0.053121798, 0.041035026, 0.037792902, 0.030762898, 0.031906214, 0.04069297, 0.030185424, 0.038095485, 0.029592052, 0.032024454, 0.030876493, 0.025564766, 0.04094848, 0.04305496, 0.03209393, 0.029748484, 0.03244065, 0.02456287, 0.10955787, 0.120856546, 0.09574978, 0.07458592, 0.08550312, 0.07277209, 0.13174637, 0.06334235, 0.07369859, 0.061184518, 0.06621426, 0.066154905, 0.09061871, 0.079490766, 0.13957794, 0.08464291, 0.033106517, 0.040233623, 0.027578944, 0.0426478, 0.034123432, 0.030926792, 0.035340913, 0.031362683, 0.043576468, 0.030480849, 0.043009274, 0.03455683, 0.032917, 0.025515433, 0.03829608, 0.025742665, 0.024534632, 0.03396135, 0.028975753, 0.02495259, 0.022011725, 0.024617797, 0.07370846, 0.083673574, 0.10369409, 0.1177746, 0.09642928, 0.08541555, 0.056312073, 0.051010363, 0.057115085, 0.04539928, 0.037379287, 0.06703448, 0.064296685, 0.05588712, 0.07233019, 0.072823755, 0.0813271, 0.033691544, 0.035345882, 0.037924338, 0.026519703, 0.036602877, 0.05062971, 0.039850224, 0.03841344, 0.028730562, 0.033071175, 0.025224257, 0.02492625, 0.025394283, 0.024757132, 0.04588046, 0.024896216, 0.03287984, 0.02753142, 0.033156764, 0.02064155, 0.0303773, 0.056877088, 0.087412715, 0.0773502, 0.064768866, 0.074807316, 0.06852815, 0.1438133, 0.105847046, 0.056167915, 0.106783174, 0.062812075, 0.06103057, 0.05841622, 0.079782546, 0.053693164, 0.044665277, 0.06762846, 0.049883045, 0.03792785, 0.023941355, 0.04670053, 0.027218778, 0.03782763, 0.03287696, 0.026050381, 0.03238349, 0.036982656, 0.024481947, 0.031344146, 0.03273009, 0.030036468, 0.036484674, 0.029842345, 0.02439541, 0.033209134, 0.03144277, 0.026743418, 0.026461093, 0.062105305, 0.06131516, 0.082734644, 0.09934241, 0.19043529, 0.07659418, 0.114680655, 0.07599956, 0.045636382, 0.057374615, 0.11414799, 0.05199547, 0.038731743, 0.035380106, 0.0351753, 0.05642618, 0.06807256, 0.04752991, 0.05516463, 0.045617476, 0.059526388, 0.0326478, 0.026701018, 0.03018189, 0.031545974, 0.032894958, 0.044479422, 0.028257398, 0.03954414, 0.033190638, 0.020266002, 0.033447232, 0.037078805, 0.022395497, 0.030672817, 0.030915301, 0.022853808, 0.026074424, 0.09885867, 0.12504262, 0.28118724, 0.11225436, 0.081621885, 0.08032857, 0.068521194, 0.058523912, 0.099428944, 0.09884043, 0.063644536, 0.065269224, 0.04944903, 0.058577448, 0.040482733, 0.046180714, 0.04259236, 0.03165369, 0.046560436, 0.029380156, 0.03237901, 0.031248903, 0.031195516, 0.03328356, 0.04132389, 0.04276069, 0.03850053, 0.038556244, 0.027920937, 0.027275283, 0.024248233, 0.03858084, 0.02397671, 0.02877199, 0.018723397, 0.042635452, 0.030396974, 0.02362412, 0.03349004, 0.030590123, 0.02840816, 0.026480025, 0.029327393, 0.030649424, 0.11840454, 0.06318489, 0.08606682, 0.07627374, 0.11003795, 0.15550928, 0.077366024, 0.10470805, 0.08466818, 0.053944066, 0.052564904, 0.031710237, 0.05794914, 0.10240356, 0.049161628, 0.040722977, 0.03224824, 0.047168616, 0.05378029, 0.04841386, 0.03255713, 0.029749548, 0.026609793, 0.032999944, 0.045565307, 0.039732862, 0.039427016, 0.016510272, 0.033721976, 0.029341578, 0.033897344, 0.039840594, 0.032438517, 0.03296122, 0.029451925, 0.023680411, 0.024641627, 0.025001677, 0.0624286, 0.06701966, 0.08152651, 0.06587622, 0.050842237, 0.051280905, 0.03558567, 0.031799205, 0.07398052, 0.11555738, 0.041142944, 0.06758638, 0.060195822, 0.02933973, 0.046734463, 0.0573045, 0.045340583, 0.041863482, 0.03842079, 0.043476254, 0.035020094, 0.04402014, 0.031606287, 0.029517679, 0.04052936, 0.025617635, 0.027380839, 0.027619885, 0.04440047, 0.030241681, 0.02678394, 0.027687518, 0.03216874, 0.028283719, 0.027744606, 0.029413054, 0.023130842, 0.03405437, 0.12794079, 0.11103374, 0.12733401, 0.08298553, 0.050927088, 0.10260372, 0.048949916, 0.046162296, 0.09723676, 0.08637388, 0.059740055, 0.04433856, 0.043989357, 0.04200189, 0.041610345, 0.0329989, 0.021727994, 0.03868055, 0.031847972, 0.036287032, 0.038462054, 0.039414838, 0.034027126, 0.052176535, 0.022527862, 0.025178475, 0.031971626, 0.033441674, 0.025357606, 0.035256106, 0.027055934, 0.022720471, 0.031814285, 0.023135783, 0.032979596, 0.022663498, 0.017403819, 0.031050287, 0.08193509, 0.10093574, 0.095708035, 0.06850002, 0.08302151, 0.07366238, 0.08315668, 0.066542685, 0.05946576, 0.061233584, 0.072994806, 0.0723441, 0.064662606, 0.06473815, 0.049960196, 0.050281953, 0.049734116, 0.029020652, 0.044333335, 0.035378505, 0.036554992, 0.035349, 0.030590754, 0.032590438, 0.031020315, 0.04227647, 0.03669164, 0.030817239, 0.026069874, 0.038185153, 0.02781011, 0.044772793, 0.02262239, 0.030644711, 0.017958626, 0.02055801, 0.032260947, 0.030457923, 0.05839921, 0.16772924, 0.08601584, 0.14246707, 0.05505594, 0.09845336, 0.110686466, 0.036375597, 0.042180482, 0.0872502, 0.08436892, 0.0732766, 0.039698992, 0.046723075, 0.07943277, 0.04251899, 0.036603533, 0.02984755, 0.037578862, 0.042666823, 0.024421128, 0.031184413, 0.0392952, 0.029420787, 0.045004442, 0.04525579, 0.028576657, 0.03673543, 0.027324608, 0.027308382, 0.03209121, 0.02352643, 0.026672658, 0.019143926, 0.021343783, 0.027117494, 0.022605296, 0.03353229, 0.030094078, 0.0347177, 0.027904065, 0.020811444, 0.0459942, 0.07376947, 0.09453016, 0.108201265, 0.14355874, 0.119573854, 0.069818676, 0.07225547, 0.06286123, 0.0863866, 0.06565863, 0.034227658, 0.06696949, 0.075898536, 0.03154402, 0.04815607, 0.057223603, 0.039446622, 0.0406373, 0.04676036, 0.037742525, 0.03200684, 0.033180248, 0.039034232, 0.028132772, 0.025854275, 0.027086442, 0.028344851, 0.036121003, 0.026223313, 0.030983668, 0.020305874, 0.032997727, 0.038634103, 0.036420323, 0.030667933, 0.022899851, 0.03453234, 0.021849573, 0.049557798, 0.13130748, 0.18699865, 0.08609195, 0.09049125, 0.11671428, 0.14101255, 0.14941883, 0.081020914, 0.03991807, 0.04235735, 0.035082623, 0.0621941, 0.04702954, 0.04562432, 0.04217525, 0.057361, 0.035557617, 0.0382249, 0.07387681, 0.038999345, 0.04526347, 0.024560334, 0.04547444, 0.038618103, 0.030606136, 0.045051087, 0.028163219, 0.03128816, 0.031131282, 0.033578746, 0.035690818, 0.03747797, 0.026562242, 0.027648637, 0.031770974, 0.03856757, 0.033412613, 0.0595222, 0.2556731, 0.18425596, 0.22884127, 0.26450315, 0.12686737, 0.07485243, 0.07660012, 0.0713449, 0.06514541, 0.06287076, 0.043642677, 0.053752936, 0.059285577, 0.075750895, 0.049274202, 0.04030099, 0.04207494, 0.03147244, 0.031849865, 0.06477161, 0.06258931, 0.061303403, 0.06279669, 0.055810753, 0.04677649, 0.04079445, 0.049564462, 0.060248394, 0.036572427, 0.039026897, 0.032894902, 0.029369712, 0.027355319, 0.0258058, 0.050382167, 0.04348535, 0.028286003, 0.20746219, 0.19282007, 0.1748019, 0.07928263, 0.14305747, 0.13816765, 0.0698908, 0.14690977, 0.10467864, 0.11420102, 0.07851523, 0.08581716, 0.04996294, 0.051053178, 0.055826314, 0.058482464, 0.060150035, 0.050384715, 0.030366132, 0.06263996, 0.032926023, 0.030851984, 0.057304595, 0.039290283, 0.05235243, 0.0628394, 0.041434392, 0.057585653, 0.037666287, 0.045805175, 0.034586187, 0.04278072, 0.038435727, 0.03369381, 0.036874034, 0.0356748, 0.028294155, 0.03953731, 0.062035304, 0.075590394, 0.16802324, 0.14745633, 0.17404802, 0.0963843, 0.0826778, 0.06766913, 0.10204479, 0.15138766, 0.06988254, 0.073177546, 0.06371081, 0.07955689, 0.0745445, 0.045314096, 0.049162604, 0.05350665, 0.037366044, 0.037959903, 0.05544074, 0.07925431, 0.04197368, 0.064882904, 0.066744156, 0.045646098, 0.034494713, 0.04309987, 0.02878412, 0.038996205, 0.033379354, 0.033825148, 0.036626194, 0.030242732, 0.02420441, 0.039922234, 0.03492616, 0.041429915, 0.04496088, 0.023619527, 0.037710667, 0.040474545, 0.042181205, 0.032125667, 0.07634115, 0.18913059, 0.13250507, 0.08739629, 0.07841879, 0.084780596, 0.08421723, 0.107438914, 0.036391962, 0.07296641, 0.03823478, 0.049321912, 0.04768837, 0.037911285, 0.040474683, 0.044168543, 0.032122344, 0.043554008, 0.05298661, 0.040722035, 0.03830816, 0.045481745, 0.03907875, 0.03982696, 0.038634602, 0.041050307, 0.031336118, 0.036735937, 0.034009274, 0.032729123, 0.03432888, 0.038896386, 0.029807866, 0.02985692, 0.035898145, 0.02583986, 0.02635377, 0.03340552, 0.059346247, 0.09267157, 0.10604279, 0.21025757, 0.13455334, 0.091547005, 0.12089111, 0.10295375, 0.11021196, 0.08304287, 0.045353606, 0.04262308, 0.05435428, 0.042396363, 0.03607031, 0.034350373, 0.058367975, 0.035664726, 0.040097296, 0.03566143, 0.028851105, 0.034606528, 0.023331696, 0.058990963, 0.0518108, 0.024861906, 0.034394782, 0.028995344, 0.027630467, 0.03419335, 0.028390788, 0.0297111, 0.028081251, 0.036149353, 0.02462405, 0.029270323, 0.031886842, 0.032647103, 0.096680254, 0.104142144, 0.107288785, 0.19230892, 0.08663187, 0.0716428, 0.083710656, 0.096324444, 0.18492952, 0.096311904, 0.047859102, 0.049310196, 0.035937455, 0.048857737, 0.05332979, 0.028730804, 0.047795765, 0.034321614, 0.05567877, 0.05286345, 0.052115895, 0.03418754, 0.0597397, 0.053015735, 0.037113912, 0.043176986, 0.042671792, 0.03910682, 0.02563759, 0.034722153, 0.027486786, 0.049866196, 0.049045287, 0.022434281, 0.025295962, 0.056877863, 0.03829645, 0.03454123, 0.0703005, 0.0870973, 0.11979451, 0.06524398, 0.11272906, 0.074668735, 0.041027464, 0.060325723, 0.051680904, 0.06468656, 0.07201321, 0.041643452, 0.044266015, 0.078857854, 0.0361727, 0.06309476, 0.05455866, 0.046906937, 0.043211605, 0.062122628, 0.04556848, 0.044632975, 0.04146555, 0.040350206, 0.04730912, 0.072384074, 0.02521129, 0.028132668, 0.026729483, 0.026407333, 0.04040475, 0.036644742, 0.04759621, 0.026638985, 0.030611465, 0.039657414, 0.03437783, 0.027574586, 0.08248216, 0.10163479, 0.14602207, 0.44551498, 0.09739966, 0.08189008, 0.08202185, 0.15715253, 0.16091992, 0.062537216, 0.058193713, 0.06592815, 0.052198213, 0.05578671, 0.057197187, 0.06150453, 0.037116222, 0.04125616, 0.04890688, 0.05125013, 0.04969066, 0.04270324, 0.036782358, 0.047128536, 0.04525708, 0.050309584, 0.035132032, 0.051209923, 0.039440863, 0.03640464, 0.03970912, 0.030884294, 0.041483823, 0.036228973, 0.034025997, 0.032550573, 0.033778198, 0.025450414, 0.028116401, 0.025835592, 0.029585311, 0.027911259, 0.039850403, 0.05307647, 0.09619455, 0.064320244, 0.2143372, 0.0824245, 0.06596404, 0.13143966, 0.07576071, 0.1174022, 0.116625145, 0.08708377, 0.045493312, 0.050223626, 0.047323383, 0.044110045, 0.05007906, 0.053756025, 0.055023458, 0.039015476, 0.03650797, 0.05265899, 0.01996996, 0.034526337, 0.03418768, 0.050882287, 0.036500067, 0.03753785, 0.030581301, 0.031983495, 0.038919125, 0.031139644, 0.040121485, 0.041624703, 0.03395553, 0.026509006, 0.026822412, 0.03197854, 0.036287107, 0.056114957, 0.17463376, 0.15949671, 0.14689822, 0.08116208, 0.07336658, 0.06754195, 0.07427912, 0.09884745, 0.11363151, 0.055184055, 0.056918476, 0.03742046, 0.07824193, 0.058351845, 0.04562738, 0.04887094, 0.0532777, 0.053323653, 0.048991505, 0.04675136, 0.04595185, 0.035258424, 0.033564962, 0.035640713, 0.03589894, 0.039808676, 0.041886043, 0.028106216, 0.035914272, 0.03958831, 0.031733144, 0.0435153, 0.021471621, 0.023274716, 0.027211236, 0.027561689, 0.025898503, 0.08143837, 0.082027964, 0.095500305, 0.13934618, 0.06746858, 0.07932494, 0.057689, 0.04634966, 0.05693234, 0.04776742, 0.046914194, 0.07728776, 0.060379494, 0.07167586, 0.047769208, 0.10997275, 0.040542368, 0.04798153, 0.035964314, 0.031804558, 0.044156283, 0.04470388, 0.03826296, 0.03601904, 0.05852449, 0.037024762, 0.036758784, 0.03006943, 0.04487645, 0.044623327, 0.03163352, 0.045890193, 0.0467668, 0.029857878, 0.032043148, 0.0352061, 0.030661698, 0.033741623, 0.08832712, 0.17428839, 0.102841996, 0.07330838, 0.22115898, 0.18474294, 0.103009984, 0.08870894, 0.072635114, 0.12338243, 0.12315354, 0.06664177, 0.05497783, 0.05814194, 0.04004367, 0.06231471, 0.06808545, 0.05634308, 0.028893925, 0.042329166, 0.04481788, 0.042709637, 0.03868715, 0.031705137, 0.043784186, 0.039154265, 0.05488975, 0.046674028, 0.03424763, 0.026628532, 0.034533445, 0.038515393, 0.02599473, 0.03125567, 0.03871253, 0.041828383, 0.025397632, 0.044011384, 0.08059126, 0.21961729, 0.3062705, 0.19310841, 0.10581154, 0.06855826, 0.08681105, 0.056393936, 0.10998445, 0.0815737, 0.0843517, 0.1140193, 0.06411646, 0.03092538, 0.04202012, 0.10045657, 0.06287962, 0.05725845, 0.057922438, 0.054307867, 0.037190326, 0.03952215, 0.054965585, 0.034659866, 0.079177596, 0.076671354, 0.04877264, 0.037561744, 0.053544294, 0.047589164, 0.035743155, 0.037230592, 0.031923223, 0.03346793, 0.025091942, 0.037237477, 0.028611967, 0.047979806, 0.044302873, 0.028107574, 0.035052493, 0.026272427, 0.052547388, 0.034267392, 0.058156665, 0.086817436, 0.11179315, 0.084509805, 0.04528097, 0.10408074, 0.142416, 0.060779195, 0.07059285, 0.07877471, 0.04022955, 0.043700986, 0.032710303, 0.04575063, 0.046354357, 0.042267393, 0.051776674, 0.04896167, 0.057017263, 0.033138923, 0.030945256, 0.028553441, 0.031653833, 0.041148566, 0.058967106, 0.03640446, 0.03259643, 0.036476016, 0.03475733, 0.042107467, 0.05503835, 0.03462076, 0.057220403, 0.02986057, 0.02637052, 0.020920353, 0.037393156, 0.03942129, 0.12476294, 0.13334388, 0.106353424, 0.113827355, 0.07078981, 0.060367767, 0.036238316, 0.081602864, 0.08731888, 0.13225025, 0.04775704, 0.062189385, 0.06844594, 0.052329075, 0.06474421, 0.042652983, 0.07046042, 0.039229028, 0.045554455, 0.035778273, 0.0419734, 0.04782163, 0.04598228, 0.037024643, 0.030642224, 0.05386621, 0.049822688, 0.03235635, 0.041724406, 0.03439544, 0.032878555, 0.03893136, 0.033040848, 0.03219528, 0.03244993, 0.028639546, 0.046023007, 0.04686184, 0.10818399, 0.118849404, 0.11137537, 0.10797714, 0.06453448, 0.07001211, 0.10396821, 0.049074758, 0.09141167, 0.07064069, 0.12091382, 0.043548696, 0.051538054, 0.07783567, 0.068930894, 0.058875334, 0.045148205, 0.038335312, 0.041481182, 0.049850654, 0.036683723, 0.04135608, 0.041229405, 0.04371676, 0.02856259, 0.044474572, 0.03079246, 0.025298737, 0.048098553, 0.030248407, 0.04709063, 0.040121455, 0.036338996, 0.03635746, 0.031075377, 0.032786112, 0.03196317, 0.0430667, 0.10974393, 0.14961393, 0.07625137, 0.07767772, 0.04403703, 0.09051858, 0.04893282, 0.042947244, 0.15499897, 0.13172938, 0.0900569, 0.07152207, 0.11662394, 0.07753316, 0.06572387, 0.04698823, 0.06598107, 0.08000048, 0.03845518, 0.04174226, 0.029214922, 0.046955217, 0.0399462, 0.033303462, 0.032464687, 0.039035484, 0.036111444, 0.04003498, 0.04071066, 0.03860873, 0.04683629, 0.032048594, 0.039538294, 0.041860674, 0.030189859, 0.030210298, 0.053304985, 0.028780226, 0.07941915, 0.12391025, 0.11471022, 0.11756917, 0.1574264, 0.06610416, 0.085553505, 0.055134207, 0.04145087, 0.038707577, 0.068954304, 0.045405574, 0.06303297, 0.035166297, 0.04616376, 0.04834504, 0.03339097, 0.028834531, 0.051579088, 0.036791038, 0.056303307, 0.06471379, 0.05205703, 0.036956757, 0.031248942, 0.030588077, 0.031720094, 0.033281066, 0.041024048, 0.030862814, 0.04650532, 0.045698654, 0.0318954, 0.024694664, 0.03153804, 0.037262224, 0.044750012, 0.020525903, 0.108188756, 0.1281991, 0.17769173, 0.076801755, 0.06541526, 0.081716135, 0.0892091, 0.083038665, 0.08307895, 0.063602515, 0.04475973, 0.037819713, 0.04251644, 0.04382792, 0.05377984, 0.050520927, 0.031052846, 0.0420703, 0.041364532, 0.049321793, 0.040774554, 0.04920585, 0.035161305, 0.03995084, 0.032322504, 0.03770032, 0.028741024, 0.04086039, 0.02902406, 0.041590497, 0.038984526, 0.027712574, 0.031673994, 0.029167539, 0.025791511, 0.043216392, 0.025174769, 0.0361081, 0.06796322, 0.34490436, 0.11231811, 0.1443284, 0.06880165, 0.07792241, 0.07111891, 0.06419049, 0.06792898, 0.06572051, 0.036127556, 0.073476054, 0.07419442, 0.07869231, 0.08398212, 0.038608547, 0.042442717, 0.036010694, 0.04331566, 0.06436974, 0.03425877, 0.04898983, 0.03366929, 0.059195932, 0.048608456, 0.029943306, 0.04077246, 0.045007173, 0.05512252, 0.027905121, 0.039012164, 0.035996914, 0.028883828, 0.03542417, 0.0316306, 0.040398173, 0.03309982, 0.025963122, 0.15210609, 0.18781246, 0.1666211, 0.15788773, 0.12007278, 0.13050003, 0.05556245, 0.099023074, 0.11349447, 0.07017489, 0.079559654, 0.038890913, 0.04425479, 0.052927006, 0.04815804, 0.06403941, 0.0441806, 0.047093164, 0.03824943, 0.037480284, 0.058692694, 0.03425929, 0.029305354, 0.0456781, 0.034964897, 0.046155196, 0.036668837, 0.029027058, 0.027356979, 0.026983349, 0.06348727, 0.03901859, 0.06639154, 0.034779336, 0.034035016, 0.030262953, 0.038142994, 0.031155728, 0.06242762, 0.08459649, 0.066737324, 0.077222824, 0.06519684, 0.09110826, 0.03638936, 0.08460131, 0.09177366, 0.06172763, 0.045852907, 0.050524797, 0.07778004, 0.07347438, 0.045408826, 0.0383142, 0.034753103, 0.038823523, 0.043141473, 0.03693749, 0.03294317, 0.03175071, 0.03849711, 0.032530982, 0.03637418, 0.02934532, 0.03525035, 0.038883578, 0.033249192, 0.04651046, 0.046602324, 0.026722625, 0.027912952, 0.03436648, 0.036539223, 0.03469512, 0.043062996, 0.03553994, 0.06808152, 0.088455856, 0.12981711, 0.12957133, 0.064442225, 0.047242034, 0.079089634, 0.13885546, 0.11747688, 0.09692986, 0.075682536, 0.03893049, 0.05211857, 0.040917072, 0.04220418, 0.05292677, 0.050451472, 0.06438673, 0.06226559, 0.04434664, 0.03164051, 0.04217329, 0.03839349, 0.025559045, 0.037114054, 0.037508022, 0.03589104, 0.04340776, 0.04268845, 0.03862192, 0.04675084, 0.048880763, 0.0475897, 0.03461267, 0.032176863, 0.024023518, 0.07515694, 0.055984642, 0.029055329, 0.040345002, 0.04377445, 0.03322153, 0.0340438, 0.09305873, 0.31571627, 0.15992968, 0.13790826, 0.08925548, 0.21139087, 0.08378373, 0.10508122, 0.12488868, 0.09491512, 0.08253292, 0.09693203, 0.061916627, 0.07009458, 0.05250986, 0.07604415, 0.06582427, 0.049797744, 0.052529067, 0.035839666, 0.043123785, 0.045383126, 0.036888827, 0.044060897, 0.041859042, 0.0663359, 0.04449778, 0.043482233, 0.034596406, 0.053341012, 0.038156826, 0.031983968, 0.022673823, 0.027902406, 0.043860834, 0.05087879, 0.029925868, 0.026253147, 0.087060966, 0.10678132, 0.09510044, 0.0834821, 0.15000217, 0.07015739, 0.051627368, 0.121915266, 0.10888865, 0.06861975, 0.049758103, 0.09571187, 0.09482688, 0.06314497, 0.05468459, 0.061325863, 0.040539797, 0.041922472, 0.03202348, 0.031766143, 0.03481733, 0.0460687, 0.04523923, 0.037049085, 0.047963, 0.046897188, 0.030430935, 0.032406177, 0.027382648, 0.022908496, 0.029021746, 0.035700504, 0.025967056, 0.024323445, 0.024533162, 0.038082432, 0.04118057, 0.035915617, 0.05989654, 0.07036668, 0.09486606, 0.11178888, 0.080008134, 0.10910032, 0.171519, 0.06926932, 0.084388085, 0.10410778, 0.08455774, 0.036110517, 0.04131035, 0.063846, 0.05700151, 0.057195332, 0.03396488, 0.064100325, 0.03972765, 0.036104213, 0.03535536, 0.027495448, 0.02839215, 0.05792847, 0.037548102, 0.039367042, 0.035203695, 0.021530258, 0.030191073, 0.038984757, 0.03842895, 0.037085205, 0.04376481, 0.030262148, 0.04457449, 0.045177475, 0.033666655, 0.029378954, 0.057451285, 0.10582012, 0.16943379, 0.10947072, 0.09463703, 0.0729844, 0.08368029, 0.10959314, 0.059527207, 0.05106474, 0.05976227, 0.08055386, 0.049043197, 0.054824885, 0.07005506, 0.03440965, 0.05906592, 0.045710176, 0.060282305, 0.047809165, 0.0611111, 0.031678837, 0.029566182, 0.04078427, 0.043920588, 0.03406625, 0.026090404, 0.03001748, 0.030925842, 0.049516115, 0.02835542, 0.038960416, 0.042675357, 0.03490239, 0.032674678, 0.038432185, 0.039577734, 0.024994858, 0.0441382, 0.0895981, 0.17406358, 0.1604396, 0.0842481, 0.083673835, 0.08726812, 0.12719476, 0.069513895, 0.068800725, 0.072805054, 0.06491818, 0.035131168, 0.041436836, 0.04143949, 0.08213564, 0.05094572, 0.045708347, 0.04746937, 0.026784467, 0.048141275, 0.03948946, 0.030762462, 0.047383334, 0.031235598, 0.03361294, 0.04143838, 0.040099673, 0.041389875, 0.044462036, 0.04238866, 0.032007523, 0.02728769, 0.028815692, 0.04248863, 0.03647132, 0.029264258, 0.033637367, 0.03752076, 0.041083917, 0.029195542, 0.027767852, 0.055594195, 0.041582424, 0.09327303, 0.11749358, 0.10144108, 0.13134868, 0.04739427, 0.10859394, 0.06475652, 0.04448868, 0.026720496, 0.082571894, 0.048935782, 0.048497073, 0.048637353, 0.051140375, 0.07634109, 0.080243364, 0.04935159, 0.06175781, 0.054713916, 0.06504832, 0.04393759, 0.058416374, 0.031559397, 0.04044506, 0.036602486, 0.04721605, 0.04367815, 0.03436876, 0.03604588, 0.037622966, 0.038951572, 0.036396857, 0.03315752, 0.030428836, 0.031146074, 0.05386027, 0.03685934, 0.03161778, 0.065295555, 0.094472416, 0.10897209, 0.23568472, 0.16375537, 0.096727565, 0.06493806, 0.08398021, 0.051110912, 0.08548676, 0.054069012, 0.045343123, 0.10994841, 0.052969348, 0.055972923, 0.08893197, 0.04958253, 0.040225994, 0.045737132, 0.03764785, 0.035740312, 0.04334165, 0.036796045, 0.028822485, 0.030099295, 0.036575835, 0.04487178, 0.036591932, 0.032594405, 0.040386103, 0.03197784, 0.033671744, 0.036264498, 0.03419562, 0.035816777, 0.031569626, 0.026005924, 0.038505975, 0.146239, 0.15499547, 0.12449897, 0.092459366, 0.05881908, 0.06200948, 0.042679146, 0.041087497, 0.112474255, 0.13242073, 0.0482057, 0.051167775, 0.07083953, 0.05910195, 0.074487545, 0.06664502, 0.0523701, 0.057053596, 0.060704086, 0.06081854, 0.04555431, 0.05806038, 0.062045116, 0.05030144, 0.03476366, 0.036333248, 0.04774731, 0.04539176, 0.038806427, 0.03326362, 0.028364612, 0.024883147, 0.04992655, 0.023651406, 0.039636098, 0.0258062, 0.041465085, 0.036142968, 0.10122027, 0.14190726, 0.16505092, 0.17559664, 0.09007955, 0.0767737, 0.067074396, 0.096498236, 0.05288588, 0.048511434, 0.0617048, 0.045040544, 0.070529364, 0.042012975, 0.058701325, 0.037587952, 0.05944112, 0.07898179, 0.040206004, 0.050780088, 0.045976993, 0.03722825, 0.04807096, 0.04010137, 0.037214782, 0.029870698, 0.035206866, 0.033375263, 0.035152152, 0.032944918, 0.060364548, 0.029775318, 0.032490123, 0.030008834, 0.029668143, 0.039103817, 0.037415378, 0.022887543, 0.13309732, 0.15672259, 0.11495368, 0.13739249, 0.12038601, 0.10126921, 0.116143286, 0.08932858, 0.07788783, 0.06742105, 0.05839245, 0.036290027, 0.050277147, 0.048567146, 0.059148688, 0.04648444, 0.046626497, 0.052461512, 0.03585797, 0.03632213, 0.03496184, 0.043700587, 0.061045934, 0.051527273, 0.050348986, 0.03879795, 0.0309839, 0.040040065, 0.035773236, 0.04119786, 0.03259773, 0.035062175, 0.042303793, 0.042299856, 0.029154593, 0.034622822, 0.03020625, 0.03057163, 0.031526137, 0.02870006, 0.033586692, 0.028357452, 0.024874642, 0.07474842, 0.1553636, 0.22911835, 0.19399333, 0.17994916, 0.08270283, 0.1459138, 0.13345225, 0.076264575, 0.048659895, 0.031674538, 0.04718083, 0.06828142, 0.071091205, 0.05391837, 0.056763723, 0.089134715, 0.04910482, 0.03271318, 0.040820625, 0.06625328, 0.044949543, 0.041865148, 0.039511375, 0.026789926, 0.049372926, 0.040682428, 0.033078063, 0.037040953, 0.032871448, 0.034550186, 0.032173045, 0.035297196, 0.034652896, 0.050282076, 0.030135361, 0.027378604, 0.02627667, 0.109825626, 0.11508999, 0.15860906, 0.08853885, 0.15937477, 0.077157475, 0.060176365, 0.08655602, 0.052210275, 0.09569111, 0.05344259, 0.042790126, 0.04211954, 0.039886307, 0.073736966, 0.05938996, 0.03229635, 0.040967356, 0.035713207, 0.045373574, 0.032313015, 0.037241388, 0.031344198, 0.03334561, 0.046776224, 0.029592756, 0.03323093, 0.049097687, 0.049278807, 0.028406639, 0.04154373, 0.031795133, 0.028310621, 0.046171635, 0.025992196, 0.026891652, 0.031284053, 0.034246, 0.090204366, 0.12903576, 0.12766972, 0.12643898, 0.057276413, 0.072358996, 0.09387078, 0.074341975, 0.09030404, 0.09401102, 0.062764384, 0.04384439, 0.07012862, 0.057155646, 0.064987406, 0.06771786, 0.040639207, 0.024520578, 0.03555495, 0.044586126, 0.033092484, 0.044995338, 0.042020924, 0.036060404, 0.051028304, 0.037267473, 0.02884957, 0.035065517, 0.027692832, 0.02592967, 0.035951912, 0.0482028, 0.036933172, 0.035065185, 0.04134571, 0.035836175, 0.033308264, 0.041492146, 0.080037005, 0.0627756, 0.07840029, 0.0709669, 0.11150601, 0.10299391, 0.06412896, 0.059514634, 0.107146524, 0.08059838, 0.09639696, 0.07785403, 0.035060097, 0.043815523, 0.05754215, 0.052169412, 0.03754629, 0.039612982, 0.04205965, 0.040573243, 0.05331604, 0.033495866, 0.041653775, 0.043240238, 0.04775496, 0.031379033, 0.03214688, 0.03531374, 0.0239313, 0.02554013, 0.029228982, 0.02628127, 0.05463624, 0.0305964, 0.029286658, 0.039856445, 0.026800686, 0.04555983, 0.04767806, 0.088562615, 0.07930495, 0.10971075, 0.12656192, 0.041475266, 0.06594766, 0.10064918, 0.14434402, 0.059024196, 0.090951085, 0.053987917, 0.054616977, 0.042144496, 0.04070204, 0.038699303, 0.038483568, 0.06841674, 0.045059208, 0.039298583, 0.041593026, 0.03809543, 0.031471223, 0.02668818, 0.033183075, 0.040199813, 0.034884177, 0.042230833, 0.040542096, 0.03374516, 0.031153763, 0.039578084, 0.033131056, 0.029640304, 0.03186239, 0.031232303, 0.035444826, 0.028612953, 0.046042386, 0.03856905, 0.023590555, 0.025539722, 0.038087744, 0.04371511, 0.10422539, 0.12719232, 0.11026912, 0.09619532, 0.058434054, 0.14140062, 0.084978774, 0.100786045, 0.076055646, 0.07286599, 0.069574215, 0.10971495, 0.07478227, 0.06469821, 0.060908355, 0.044427186, 0.030624047, 0.039816845, 0.044763915, 0.06013607, 0.0383977, 0.039168168, 0.027203852, 0.035497915, 0.033137057, 0.028073227, 0.034796387, 0.03514338, 0.053352028, 0.03964852, 0.03622429, 0.025943046, 0.029239925, 0.038813204, 0.029653536, 0.03695118, 0.03507367, 0.04037589, 0.078883514, 0.2256188, 0.16456802, 0.17528084, 0.096436165, 0.09219602, 0.051217552, 0.096153095, 0.10833564, 0.073994294, 0.060919404, 0.05677249, 0.055690456, 0.05090524, 0.05642327, 0.060941193, 0.046272866, 0.04418497, 0.048096415, 0.024109097, 0.026982808, 0.03579432, 0.02744424, 0.04857526, 0.026757043, 0.032633595, 0.041150678, 0.041789494, 0.02748922, 0.054993454, 0.040973384, 0.024975987, 0.033624995, 0.037889022, 0.029382622, 0.035510484, 0.029382769, 0.03571863, 0.11662366, 0.16252394, 0.08821406, 0.11067614, 0.07111548, 0.079097874, 0.061042704, 0.039066948, 0.03520616, 0.077402666, 0.049635064, 0.0483235, 0.04109485, 0.054762118, 0.0622754, 0.037518393, 0.039895877, 0.0640285, 0.028123714, 0.023323994, 0.039985627, 0.04615904, 0.025437864, 0.033836935, 0.043664765, 0.038265336, 0.038810883, 0.023193413, 0.052717365, 0.050913353, 0.041467458, 0.048404153, 0.02690009, 0.030793171, 0.024238572, 0.03475554, 0.04320188, 0.041839223, 0.14457583, 0.23920825, 0.07720437, 0.08238687, 0.07023004, 0.08395619, 0.05081139, 0.08612376, 0.05737397, 0.07253332, 0.06289985, 0.045773037, 0.057466645, 0.05067141, 0.052344456, 0.042727105, 0.037729453, 0.045738876, 0.057359036, 0.04276443, 0.041938037, 0.03421293, 0.03434819, 0.056854613, 0.04673512, 0.055376023, 0.039458845, 0.028893154, 0.030010745, 0.0378967, 0.041878186, 0.0426063, 0.03928765, 0.034371685, 0.04750692, 0.024992596, 0.040713657, 0.033486474, 0.059778515, 0.16989045, 0.12471311, 0.2118588, 0.20891754, 0.07979941, 0.08323779, 0.047598194, 0.07313654, 0.06496451, 0.06090346, 0.040072054, 0.03460308, 0.04195294, 0.036909852, 0.06442905, 0.037678894, 0.04581397, 0.043696932, 0.05414552, 0.046249535, 0.03907113, 0.028421415, 0.043356124, 0.028441364, 0.03329709, 0.050048586, 0.03034496, 0.030898035, 0.027239727, 0.041162364, 0.037944127, 0.03293704, 0.028270647, 0.042180613, 0.03643607, 0.023063526, 0.03933857, 0.03842695, 0.04001838, 0.0259023, 0.04503084, 0.046007037, 0.12351062, 0.11428001, 0.13429077, 0.1615352, 0.18783414, 0.101366095, 0.098858126, 0.076838315, 0.10297935, 0.080078125, 0.06986299, 0.10196308, 0.0717327, 0.11271391, 0.09060912, 0.10283874, 0.06336085, 0.07047312, 0.05461364, 0.060643554, 0.050943315, 0.06164879, 0.049092636, 0.049894102, 0.05947024, 0.053911876, 0.058945548, 0.04750376, 0.039889604, 0.05624573, 0.06387749, 0.07415857, 0.053749908, 0.069596864, 0.03640148, 0.034333337, 0.04308948, 0.037415374, 0.077866375, 0.18141711, 0.15084223, 0.24706973, 0.109366, 0.07033744, 0.1327476, 0.07989254, 0.079638235, 0.09193359, 0.06388465, 0.054251954, 0.07758119, 0.13410076, 0.07012009, 0.06773565, 0.054005492, 0.076016195, 0.060847234, 0.080690324, 0.05008806, 0.05343262, 0.07006596, 0.0786646, 0.05340446, 0.052515496, 0.03621344, 0.04847254, 0.04753212, 0.040383946, 0.04437597, 0.036176175, 0.03275355, 0.033027977, 0.04993296, 0.032350875, 0.026750881, 0.05200871, 0.279464, 0.14347535, 0.11502118, 0.122237824, 0.10368461, 0.077279136, 0.10416757, 0.09720332, 0.092712216, 0.08962142, 0.084687404, 0.05687474, 0.07903044, 0.039745104, 0.046028946, 0.06215507, 0.093131945, 0.03412192, 0.03832455, 0.032594275, 0.07027832, 0.048302714, 0.039398, 0.040488865, 0.037087925, 0.040246423, 0.05565944, 0.033179812, 0.041111145, 0.030870644, 0.037103698, 0.043127336, 0.032690465, 0.033391938, 0.035688173, 0.04343296, 0.032510128, 0.041506916, 0.19009212, 0.14278239, 0.15177353, 0.19670773, 0.09887292, 0.06936018, 0.08351531, 0.08627102, 0.08072788, 0.105944745, 0.0611504, 0.088628955, 0.048127234, 0.029349867, 0.047695704, 0.045065146, 0.027822642, 0.042107373, 0.038060226, 0.057734314, 0.053748697, 0.04777993, 0.03434091, 0.05341759, 0.047190674, 0.046038497, 0.05153573, 0.04024001, 0.04148491, 0.03239132, 0.04057324, 0.055971716, 0.05480016, 0.03084986, 0.030472076, 0.034933865, 0.057621837, 0.0318035, 0.1216709, 0.14718981, 0.104220375, 0.12470082, 0.13710895, 0.096664645, 0.06744114, 0.1005352, 0.09485404, 0.102488704, 0.071026295, 0.03954308, 0.075277574, 0.05384957, 0.06456032, 0.08813825, 0.07453364, 0.042870183, 0.057215013, 0.049371794, 0.056512758, 0.040096056, 0.028555077, 0.08751858, 0.04181729, 0.052485522, 0.04413026, 0.03640985, 0.041879788, 0.036773797, 0.045976907, 0.030128172, 0.039026406, 0.034164295, 0.028903052, 0.02925604, 0.024332166, 0.03277565, 0.018998856, 0.030490948, 0.023932861, 0.036541678, 0.044979807, 0.04096507, 0.06114019, 0.09204254, 0.23327832, 0.16476965, 0.055619296, 0.056842867, 0.07894667, 0.16401272, 0.08643502, 0.08231069, 0.052070156, 0.056583203, 0.061771166, 0.05506707, 0.070469595, 0.05043193, 0.03456248, 0.045548372, 0.05256551, 0.045892, 0.05217775, 0.032422327, 0.03143561, 0.02470628, 0.03614059, 0.032289393, 0.022889307, 0.031350207, 0.040880993, 0.034214027, 0.041718267, 0.026811892, 0.025351658, 0.04596382, 0.028484508, 0.024037879, 0.049264822, 0.030246016, 0.07592708, 0.0636085, 0.07089307, 0.2707873, 0.14996791, 0.106303215, 0.11460319, 0.17554373, 0.06846562, 0.062392462, 0.0524149, 0.079991974, 0.042608775, 0.061460488, 0.043463234, 0.048560906, 0.043074567, 0.042399693, 0.037017412, 0.039918847, 0.03704092, 0.03814953, 0.04600139, 0.044470407, 0.03739906, 0.025487548, 0.025498157, 0.038194574, 0.039200872, 0.05113808, 0.046275508, 0.039086144, 0.030003583, 0.031897534, 0.03238825, 0.04203649, 0.033039555, 0.047526244, 0.0941118, 0.1896308, 0.10052846, 0.15591608, 0.071475424, 0.09909074, 0.09640418, 0.09019251, 0.12838018, 0.0888007, 0.10242416, 0.069935896, 0.047603566, 0.038711485, 0.056964975, 0.044855487, 0.029714258, 0.031069078, 0.0536895, 0.034066558, 0.038704135, 0.03758402, 0.024798846, 0.039522327, 0.027215952, 0.040675282, 0.025985068, 0.03128429, 0.030741008, 0.04998502, 0.032956094, 0.022262385, 0.0463675, 0.042332504, 0.033554737, 0.015159282, 0.027917655, 0.040169027, 0.077477075, 0.07185942, 0.08954008, 0.14642203, 0.14313896, 0.1272923, 0.05277015, 0.04191652, 0.06343875, 0.12775221, 0.06380561, 0.08404442, 0.06813704, 0.03675287, 0.062129404, 0.051926486, 0.0345111, 0.030958153, 0.034418464, 0.05310242, 0.028693745, 0.030486131, 0.027604645, 0.02546481, 0.032950535, 0.037577145, 0.02583478, 0.025639659, 0.040639542, 0.027861021, 0.023141744, 0.022392834, 0.04200558, 0.026271509, 0.030932963, 0.019995304, 0.025719369, 0.036173202, 0.16357313, 0.07340841, 0.10462593, 0.1988859, 0.096888155, 0.055509027, 0.061215494, 0.042510387, 0.056429017, 0.0763365, 0.026841003, 0.046167746, 0.06572775, 0.043827955, 0.0392367, 0.039712716, 0.042180747, 0.032947484, 0.041477285, 0.029772544, 0.061763003]

Ascent losses:

{46: -0.0929383, 86: -0.07718381, 125: -0.06924204, 164: -0.04948138, 220: -0.045601014, 259: -0.07695881, 298: -0.04667669, 337: -0.06945087, 376: -0.048465125, 420: -0.065861, 459: -0.05163951, 498: -0.061594468, 537: -0.055387806, 576: -0.06564363, 621: -0.04558071, 660: -0.047638237, 699: -0.06567559, 738: -0.036595676, 777: -0.039747868, 821: -0.055289086, 860: -0.049794637, 899: -0.06710408, 938: -0.04253471, 977: -0.040856335, 1022: -0.04404031, 1061: -0.028703699, 1100: -0.036436975, 1139: -0.031756792, 1178: -0.034270648, 1222: -0.057838112, 1261: -0.039392978, 1300: -0.026869971, 1339: -0.044616945, 1378: -0.032007445, 1423: -0.047837306, 1462: -0.05052455, 1501: -0.040455174, 1540: -0.042560253, 1579: -0.04486522, 1623: -0.042131696, 1662: -0.030173851, 1701: -0.034048606, 1740: -0.03178705, 1779: -0.024864329, 1824: -0.02011649, 1863: -0.028224904, 1902: -0.03341741, 1941: -0.024575822, 1980: -0.032647014, 2019: -0.02313845, 2058: -0.026037833, 2097: -0.04069012, 2136: -0.027703391, 2175: -0.023488207, 2219: -0.030445736, 2258: -0.028163489, 2297: -0.01748484, 2336: -0.02531855, 2375: -0.024456853, 2420: -0.017079823, 2459: -0.022784775, 2498: -0.015164836, 2537: -0.021122996, 2576: -0.01843282, 2620: -0.029939419, 2659: -0.016851882, 2698: -0.029827172, 2737: -0.021045659, 2776: -0.015042487, 2821: -0.017248943, 2860: -0.021625225, 2899: -0.02115368, 2938: -0.018039169, 2977: -0.024922987, 3021: -0.017588673, 3060: -0.022256458, 3099: -0.021066122, 3138: -0.026212042, 3177: -0.021415336, 3222: -0.019794445, 3261: -0.015747434, 3300: -0.021040427, 3339: -0.023856565, 3378: -0.013170302, 3422: -0.012589925, 3461: -0.012919095, 3500: -0.01608924, 3539: -0.021362668, 3578: -0.0129225515, 3623: -0.013256458, 3662: -0.022945024, 3701: -0.012019721, 3740: -0.014940461, 3779: -0.021305947, 3823: -0.01784954, 3862: -0.014143655, 3901: -0.012173608, 3940: -0.014996464, 3979: -0.019621968, 4024: -0.017842857, 4063: -0.020787502, 4102: -0.019255027, 4141: -0.017215282, 4180: -0.012926723, 4219: -0.016403453, 4258: -0.018125871, 4297: -0.012010774, 4336: -0.021105604, 4375: -0.01636988, 4419: -0.019194502, 4458: -0.013801573, 4497: -0.012063128, 4536: -0.011779418, 4575: -0.011385955, 4620: -0.011461022, 4659: -0.015426439, 4698: -0.025232648, 4737: -0.019000692, 4776: -0.013331938, 4820: -0.014544045, 4859: -0.01946757, 4898: -0.016041035, 4937: -0.017709415, 4976: -0.022644596, 5021: -0.015908863, 5060: -0.011072135, 5099: -0.011967492, 5138: -0.014027831, 5177: -0.01020598, 5221: -0.018808836, 5260: -0.013208334, 5299: -0.016900161, 5338: -0.022452533, 5377: -0.02539246, 5422: -0.010192362, 5461: -0.011311817, 5500: -0.012524137, 5539: -0.011246988, 5578: -0.014406118, 5622: -0.011715877, 5661: -0.013109841, 5700: -0.012260357, 5739: -0.017267477, 5778: -0.011624426, 5823: -0.013544718, 5862: -0.017215049, 5901: -0.011743204, 5940: -0.021288374, 5979: -0.017525965, 6023: -0.017999664, 6062: -0.021648632, 6101: -0.013425135, 6140: -0.012108059, 6179: -0.01316591, 6224: -0.0146905845, 6263: -0.011475536, 6302: -0.010797637, 6341: -0.016032828, 6380: -0.011301922, 6419: -0.016341167, 6458: -0.011559803, 6497: -0.015773436, 6536: -0.0115879625, 6575: -0.014364385, 6619: -0.010882509, 6658: -0.016954694, 6697: -0.016745217, 6736: -0.010326812, 6775: -0.0121295815, 6820: -0.011426411, 6859: -0.016051041, 6898: -0.015441381, 6937: -0.01715464, 6976: -0.016726471, 7020: -0.013182633, 7059: -0.02211053, 7098: -0.015113081, 7137: -0.012162933, 7176: -0.015881598, 7221: -0.013314563, 7260: -0.013224233, 7299: -0.019504646, 7338: -0.008844036, 7377: -0.010372681, 7421: -0.013962316, 7460: -0.012326377, 7499: -0.012532126, 7538: -0.011695756, 7577: -0.017302899, 7622: -0.014610289, 7661: -0.018401608, 7700: -0.014547251, 7739: -0.011161499, 7778: -0.015034009, 7822: -0.014368743, 7861: -0.014509146, 7900: -0.011154352, 7939: -0.016646944, 7978: -0.013634668, 8023: -0.016530158, 8062: -0.013538362, 8101: -0.012792347, 8140: -0.014067474, 8179: -0.01486734, 8223: -0.015624015, 8262: -0.013435593, 8301: -0.012509461, 8340: -0.02043496, 8379: -0.01197759, 8424: -0.018021172, 8463: -0.016187208, 8502: -0.020088915, 8541: -0.016195156, 8580: -0.017247293, 8619: -0.015797118, 8658: -0.018197194, 8697: -0.027661838, 8736: -0.014085199, 8775: -0.01938575, 8819: -0.014143144, 8858: -0.018511502, 8897: -0.017329019, 8936: -0.02587763, 8975: -0.012378207, 9020: -0.014836156, 9059: -0.013672116, 9098: -0.016362697, 9137: -0.013507219, 9176: -0.012844778, 9220: -0.017235005, 9259: -0.014325236, 9298: -0.024970258, 9337: -0.014161385, 9376: -0.016115133, 9421: -0.016245425, 9460: -0.015623336, 9499: -0.014262281, 9538: -0.012718448, 9577: -0.022177232, 9621: -0.01679638, 9660: -0.023675403, 9699: -0.015597696, 9738: -0.021019403, 9777: -0.012881634, 9822: -0.010357084, 9861: -0.011815877, 9900: -0.013542615, 9939: -0.015319824, 9978: -0.008559435}
strength0.001_run: [0.009257758, 0.0023792703]
strength0.005_run: [0.0126868095, 0.0027862925]
strength0.01_run: [0.017998055, 0.00407589]
strength0.05_run: [0.009199308, 0.002472107]
strength0.1_run: [0.012876432, 0.0038891148]
strength0.5_run: [0.015159282, 0.0036479516]
